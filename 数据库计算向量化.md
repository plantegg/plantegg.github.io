
# 数据库计算向量化

## volcano火山模型

对于如下一条SQL, 数据库会将它解析成一颗数，这棵树每个节点就是一个operator(简单理解就是一个函数，进行一次计算处理)

```sql
SELECT pv.siteId, user.nickame
FROM pv JOIN user
ON pv.siteId = user.siteId AND pv.userId = user.id
WHERE pv.siteId = 123;
```

![Relation Algebra](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/数据库计算向量化/relation-algebra.png)

可以看到火山模型实现简单，根据不同的SQL只需要将operator进行组装（类似搭积木一样），就能得到一个递归调用结构，每行数据按照这个调用逻辑经过每个operator进行嵌套处理就得到最终结果。框架结构性好容易扩展

但是火山模型效率不高:

1.  每个operator拆分必须到最小粒度，导致嵌套调用过多过深；
1.  嵌套都是虚函数无法内联；
1.  这个处理逻辑整体对CPU流水线不友好，CPU希望你不停地给我数据我按一个固定的逻辑(流程)来处理是最好的。

## 向量化加速的CPU原理

向量化加速的CPU原理:

-   [内存访问比CPU计算慢两个数量级](https://topic.atatech.org/articles/210128)
-   [cpu按cache_line从内存取数据，取一个数据和取多个数据代价一样](https://ata.alibaba-inc.com/articles/214221)
-   以及数据局部性原理

如下图，表示的是for循环每次跳K个int，在K小于16的时候虽然循环次数逐渐减少到原来的1/16, 但是总时间没变，因为一直是访问的同一个cache里面的数据。 到16个之后就会产生突变（跨了cache_line），再后面32、64、128的时间减少来源于循环次数的减少，因为如论如何每次循环都需要访问内存加载数据到cache_line中.

Cache_line大小是64，正好16个int，也就是存取1个或者16个int的代价基本是一样的。

```
for (int i = 0; i < arr.Length; i += K) arr[i] *= 3;
```

![running times of this loop for different step values (/Users/ren/src/blog/951413iMgBlog/image6.png)](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/数据库计算向量化/image6.png)

另外 一个大家耳熟能详的案例是对一个二维数组**逐行遍历**和**逐列遍历**的时间差异，循环次数一样，但是因为二维数组按行保存，所以逐行遍历对cache line 更友好，最终按行访问效率更高:

```
const int row = 1024;
const int col = 512
int matrix[row][col];
//逐行遍历耗时0.081ms
int sum_row=0;
for(int _r=0; _r<row; _r++) {
    for(int _c=0; _c<col; _c++){
        sum_row += matrix[_r][_c];
    }
}
//逐列遍历耗时1.069ms
int sum_col=0;
for(int _c=0; _c<col; _c++) {
    for(int _r=0; _r<row; _r++){
        sum_col += matrix[_r][_c];
    }
}
```

了解了以上CPU运算的原理我们再来看向量化就很简单了

## 向量化

向量化执行的思想就是不再一次处理一行数据，而是一次处理一批数据来均摊开销：假设每次通过 operator tree 生成一行结果的开销是 C 的话，经典模型的计算框架总开销就是 C * N，其中 N 为参与计算的总行数，如果把计算引擎每次生成一行数据的模型改为每次生成一批数据的话，因为每次调用的开销是相对恒定的(参照上面的cache_line数据)，所以计算框架的总开销就可以减小到C * N / M，其中 M 是每批数据的行数，这样每一行的开销就减小为原来的 1 / M，当 M 比较大时，计算框架的开销就不会成为系统瓶颈了。

举例来说，对于一个实现两个 int 相加的 expression，在向量化之前，其实现可能是这样的：

```cpp
class ExpressionIntAdd extends Expression {
        Datum eval(Row input) {
                int left = input.getInt(leftIndex);
                int right = input.getInt(rightIndex);
                return new Datum(left+right);
        }
}
```

在向量化之后，其实现可能会变为这样：

```cpp
class VectorExpressionIntAdd extends VectorExpression {
        int[] eval(int[] left, int[] right) {
                int[] ret = new int[input.length];
                for(int i = 0; i < input.length; i++) {
                  //利用cache局部性原理一次取多个数据和取一个代价一样
                  ret[i] = new Datum(left[i] + right[i]);
                }
                return ret;
        }
}
```

很明显对比向量化之前的版本，向量化之后的版本不再是每次只处理一条数据，而是每次能处理一批数据，而且这种向量化的计算模式在计算过程中也具有更好的数据局部性。

向量化--Vector、批量化（一次处理一批数据）。向量化核心是利用数据局部性原理，一次取一个和取一批的时延基本是同样的。volcanno模型每次都是取一个处理一个，跳转到别的算子；而向量化是取一批处理一批后再跳转。整个过程中最耗时是取数据（访问内存比CPU计算慢两个数量级）

**如果把向量化计算改成批量化处理应该就好理解多了，但是low，向量化多玄乎啊**

为了支持这种批量处理数据的需求，CPU设计厂家又搞出了SIMD这种大杀器

### [SIMD (Single Instruction Multiple Data，单指令多数据)](https://www.atatech.org/articles/211563)

简单理解SIMD就是相对于之前一个指令(一般是一个时钟周期)操作一个数据，但现在有了SIMD就可以在一个时钟周期操作一批数据，这个批如果是64，那么性能就提升了64倍。

英特尔在1996年率先引入了MMX（Multi Media eXtensions）多媒体扩展指令集，也开创了**SIMD**（Single Instruction Multiple Data，单指令多数据）指令集之先河，即在一个周期内一个指令可以完成多个数据操作，MMX指令集的出现让当时的MMX Pentium处理器大出风头。

**SSE**（Streaming SIMD Extensions，流式单指令多数据扩展）指令集是1999年英特尔在Pentium III处理器中率先推出的，并将矢量处理能力从64位扩展到了128位。

AVX 所代表的单指令多数据（Single Instruction Multi Data，SIMD）指令集，是近年来 CPU 提升 IPC（每时钟周期指令数）上为数不多的重要革新。随着每次数据宽度的提升，CPU 的性能都会大幅提升，但同时晶体管数量和能耗也会有相应的提升。因此在对功耗有较高要求的场景，如笔记本电脑或服务器中，CPU 运行 AVX 应用时需要降低频率从而降低功耗。

> 2013 年， 英特尔 发布了**AVX**-**512 指令**集，其**指令**宽度扩展为512bit，每个时钟周期内可打包32 次双精度或64 次单精度浮点运算，因此在图像/ 音视频处理、数据分析、科学计算、数据加密和压缩和 深度学习 等应用场景中，会带来更强大的性能表现，理论上浮点性能翻倍，整数计算则增加约33% 的性能。


Linus Torvalds (不认可AVX512，他认为性价比不高，希望把这些晶体管用作他用)：

> AVX512 有很明显的缺点。我宁愿看到那些晶体管被用于其他更相关的事情。即使同样是用于进行浮点数学运算（通过 GPU 来做，而不是通过 AVX512 在 CPU 上），或者直接给我更多的核心（有着更多单线程性能，而且没有 AVX512 这样的垃圾），就像 AMD 所做的一样。
> 
> 我希望通过常规的整数代码来达到自己能力的极限，而不是通过 AVX512 这样的功率病毒来达到最高频率（因为人们最终还是会拿它来做 memory-to-memory copy），还占据了核心的很大面积。


向量化当然也非常希望利用SIMD(跟GPU为什么挖矿比CPU快是一样的道理)

这里可以参考为什么这20年CPU主频基本都在2G-3G附近不再提升但是性能仍然遵循摩尔定律在提升。

## 参考资料

[CPU的生产和概念](https://www.atatech.org/articles/211563)
[CPU性能和CACHE](https://topic.atatech.org/articles/210128)
[十年后数据库还是不敢拥抱NUMA](https://www.atatech.org/articles/205974)
[Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的](https://www.atatech.org/articles/157681)
[Perf IPC以及CPU性能](https://ata.alibaba-inc.com/articles/213694)
[AMD/海光/鲲鹏/Intel CPU性能大比拼](https://www.atatech.org/articles/212194)
[一次海光X86物理机资源竞争压测的调优--AMD Zen1架构CPU](https://www.atatech.org/articles/205002)
[CPU 性能和Cache Line ---- CPU系列终结篇](https://ata.alibaba-inc.com/articles/214221)



Reference:

