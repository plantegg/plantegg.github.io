
# 网络硬件相关知识

程序员很难有机会接触到底层的一些东西,尤其是偏硬件部分,所以记录下

## 光纤和普通网线的性能差异

以下都是在4.19内核的UOS，光纤交换机为锐捷，服务器是华为鲲鹏920

![image.png](https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/553e1c5fff2dd04a668434f0da4f9d90.png)

光纤稳定性好很多，平均rt是网线的三分之一，最大值则是网线的十分之一. 上述场景下光纤的带宽大约是网线的1.5倍. 实际光纤理论带宽一般都是万M, 网线是千M.

光纤接口：

<img src="https://ata2-img.oss-cn-zhangjiakou.aliyuncs.com/b67715de1b8e143f6fc17ba574bcf0c4.png" alt="image.png" style="zoom:60%;" />

### 单模光纤和多模光纤

下图绿色是多模光纤(Multi Mode Fiber),黄色是单模光纤(Single Mode Fiber), 因为光纤最好能和光模块匹配, 我们测试用的光模块都是多模的, 单模光纤线便宜,但是对应的光模块贵多了。

多模光模块工作波长为850nm，单模光模块工作波长为1310nm或1550nm, 从成本上来看，单模光模块所使用的设备多出多模光模块两倍，总体成本远高于多模光模块，但单模光模块的传输距离也要长于多模光模块，单模光模块最远传输距离为100km，多模光模块最远传输距离为2km。因单模光纤的传输原理为使光纤直射到中心，所以主要用作远距离数据传输，而多模光纤则为多通路传播模式，所以主要用于短距离数据传输。单模光模块适用于对距离和传输速率要求较高的大型网络中，多模光模块主要用于短途网路。

![image-20210831211315077](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/网络相关知识/image-20210831211315077.png)

ping结果比较:

```shell
[aliyun@uos15 11:00 /home/aliyun]  以下88都是光口、89都是电口。
$ping -c 10 10.88.88.16 //光纤
PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.
64 bytes from 10.88.88.16: icmp_seq=1 ttl=64 time=0.058 ms
64 bytes from 10.88.88.16: icmp_seq=2 ttl=64 time=0.049 ms
64 bytes from 10.88.88.16: icmp_seq=3 ttl=64 time=0.053 ms
64 bytes from 10.88.88.16: icmp_seq=4 ttl=64 time=0.040 ms
64 bytes from 10.88.88.16: icmp_seq=5 ttl=64 time=0.053 ms
64 bytes from 10.88.88.16: icmp_seq=6 ttl=64 time=0.043 ms
64 bytes from 10.88.88.16: icmp_seq=7 ttl=64 time=0.038 ms
64 bytes from 10.88.88.16: icmp_seq=8 ttl=64 time=0.050 ms
64 bytes from 10.88.88.16: icmp_seq=9 ttl=64 time=0.043 ms
64 bytes from 10.88.88.16: icmp_seq=10 ttl=64 time=0.064 ms

--- 10.88.88.16 ping statistics ---
10 packets transmitted, 10 received, 0% packet loss, time 159ms
rtt min/avg/max/mdev = 0.038/0.049/0.064/0.008 ms

[aliyun@uos15 11:01 /home/aliyun]
$ping -c 10 10.88.89.16 //电口
PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.
64 bytes from 10.88.89.16: icmp_seq=1 ttl=64 time=0.087 ms
64 bytes from 10.88.89.16: icmp_seq=2 ttl=64 time=0.053 ms
64 bytes from 10.88.89.16: icmp_seq=3 ttl=64 time=0.095 ms
64 bytes from 10.88.89.16: icmp_seq=4 ttl=64 time=0.391 ms
64 bytes from 10.88.89.16: icmp_seq=5 ttl=64 time=0.051 ms
64 bytes from 10.88.89.16: icmp_seq=6 ttl=64 time=0.343 ms
64 bytes from 10.88.89.16: icmp_seq=7 ttl=64 time=0.045 ms
64 bytes from 10.88.89.16: icmp_seq=8 ttl=64 time=0.341 ms
64 bytes from 10.88.89.16: icmp_seq=9 ttl=64 time=0.054 ms
64 bytes from 10.88.89.16: icmp_seq=10 ttl=64 time=0.066 ms

--- 10.88.89.16 ping statistics ---
10 packets transmitted, 10 received, 0% packet loss, time 149ms
rtt min/avg/max/mdev = 0.045/0.152/0.391/0.136 ms

[aliyun@uos15 11:02 /u01]
$scp uos.tar aliyun@10.88.89.16:/tmp/
uos.tar                                  100% 3743MB 111.8MB/s   00:33    

[aliyun@uos15 11:03 /u01]
$scp uos.tar aliyun@10.88.88.16:/tmp/
uos.tar                                   100% 3743MB 178.7MB/s   00:20    

[aliyun@uos15 11:07 /u01]
$sudo ping -f 10.88.89.16
PING 10.88.89.16 (10.88.89.16) 56(84) bytes of data.
--- 10.88.89.16 ping statistics ---
284504 packets transmitted, 284504 received, 0% packet loss, time 702ms
rtt min/avg/max/mdev = 0.019/0.040/1.014/0.013 ms, ipg/ewma 0.048/0.042 ms

[aliyun@uos15 11:07 /u01]
$sudo ping -f 10.88.88.16
PING 10.88.88.16 (10.88.88.16) 56(84) bytes of data.
--- 10.88.88.16 ping statistics ---
299748 packets transmitted, 299748 received, 0% packet loss, time 242ms
rtt min/avg/max/mdev = 0.012/0.016/0.406/0.006 ms, pipe 2, ipg/ewma 0.034/0.014 ms
```

## 多网卡bonding

```shell
#cat ifcfg-bond0
DEVICE=bond0
TYPE=Bond
ONBOOT=yes
BOOTPROTO=static
IPADDR=10.176.7.11
NETMASK=255.255.255.0

#cat /etc/sysconfig/network-scripts/ifcfg-enp33s0f0
DEVICE=enp33s0f0
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=none
MASTER=bond0
SLAVE=yes

#cat /etc/sysconfig/network-scripts/ifcfg-enp33s0f1
DEVICE=enp33s0f1
TYPE=Ethernet
ONBOOT=yes
BOOTPROTO=none
MASTER=bond0
SLAVE=yes

#cat /proc/net/bonding/bond0

----加载内核bonding模块, mode=0 是RR负载均衡模式
#cat /etc/modprobe.d/bonding.conf
# modprobe bonding
alias bond0 bonding
options bond0 mode=0 miimon=100  //这一行也可以放到bond0配置文件中,比如:BONDING_OPTS="miimon=100 mode=4 xmit_hash_policy=layer3+4"
```

网卡绑定mode共有七种(0~6) bond0、bond1、bond2、bond3、bond4、bond5、bond6

常用的有三种

mode=0：平衡负载模式，有自动备援，但需要”Switch”支援及设定。

mode=1：自动备援模式，其中一条线若断线，其他线路将会自动备援。

mode=6：平衡负载模式，有自动备援，不必”Switch”支援及设定。

需要说明的是如果想做成mode 0的负载均衡,仅仅设置这里options bond0 miimon=100 mode=0是不够的,与网卡相连的交换机必须做特殊配置（这两个端口应该采取聚合方式），因为做bonding的这两块网卡是使用同一个MAC地址.从原理分析一下（bond运行在mode 0下）：

mode 0下bond所绑定的网卡的IP都被修改成相同的mac地址，如果这些网卡都被接在同一个交换机，那么交换机的arp表里这个mac地址对应的端口就有多 个，那么交换机接受到发往这个mac地址的包应该往哪个端口转发呢？正常情况下mac地址是全球唯一的，一个mac地址对应多个端口肯定使交换机迷惑了。所以 mode0下的bond如果连接到交换机，交换机这几个端口应该采取聚合方式（cisco称为 ethernetchannel，foundry称为portgroup），因为交换机做了聚合后，聚合下的几个端口也被捆绑成一个mac地址.我们的解 决办法是，两个网卡接入不同的交换机即可。

mode6模式下无需配置交换机，因为做bonding的这两块网卡是使用不同的MAC地址。

mod=5，即：(balance-tlb) Adaptive transmit load balancing（适配器传输负载均衡）

特点：不需要任何特别的switch(交换机)支持的通道bonding。在每个slave上根据当前的负载（根据速度计算）分配外出流量。如果正在接受数据的slave出故障了，另一个slave接管失败的slave的MAC地址。

该模式的必要条件：ethtool支持获取每个slave的速率.

## 网络中断和绑核

### ethtool

```shell
#ethtool -i p1p1   //查询网卡bus-info
driver: mlx5_core
version: 5.0-0
firmware-version: 14.27.1016 (MT_2420110004)
expansion-rom-version:
bus-info: 0000:21:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: no
supports-register-dump: no
supports-priv-flags: yes

//根据bus-info找到中断id
#cat /proc/interrupts | grep 0000:21:00.0 | awk -F: '{print $1}' | wc -l

//修改网卡队列数
sudo ethtool -L eth0  combined 2 （不能超过网卡最大队列数）

然后检查是否生效了(不需要重启应用和机器，实时生效)：
sudo ethtool -l eth0
```

根据网卡bus-info可以找到对应的irq id

手工绑核脚本:

```shell
#!/bin/bash
#irq_list=(`cat /proc/interrupts | grep enp131s0 | awk -F: '{print $1}'`)
intf=$1
irq_list=(cat /proc/interrupts | grep `ethtool -i $intf |grep bus-info | awk  '{ print $2 }'` | awk -F: '{print $1}')
cpunum=48  # 修改为所在node的第一个Core
for irq in ${irq_list[@]}
do
echo $cpunum > /proc/irq/$irq/smp_affinity_list
echo `cat /proc/irq/$irq/smp_affinity_list`
(( cpunum+=1 ))
done
```

检查绑定结果: sh irqCheck.sh enp131s0

```c
# 网卡名
intf=$1
irqID=`ethtool -i $intf |grep bus-info | awk  '{ print $2 }'`
log=irqSet-`date "+%Y%m%d-%H%M%S"`.log
# 可用的CPU数
cpuNum=$(cat /proc/cpuinfo |grep processor -c)
# RX TX中断列表
irqListRx=$(cat /proc/interrupts | grep ${irqID} | awk -F':' '{print $1}')
irqListTx=$(cat /proc/interrupts | grep ${irqID} | awk -F':' '{print $1}')
# 绑定接收中断rx irq
for irqRX in ${irqListRx[@]}
do
cat /proc/irq/${irqRX}/smp_affinity_list
done
# 绑定发送中断tx irq
for irqTX in ${irqListTx[@]}
do
cat /proc/irq/${irqTX}/smp_affinity_list
done
```

### [irqbalance](https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/7/html/performance_tuning_guide/appe-red_hat_enterprise_linux-performance_tuning_guide-tool_reference)

**irqbalance** 是一个命令行工具，在处理器中分配硬件中断以提高系统性能。默认设置下在后台程序运行，但只可通过 `--oneshot` 选项运行一次。

以下参数可用于提高性能。

-   --powerthresh

    CPU 进入节能模式之前，设定可空闲的 CPU 数量。如果有大于阀值数量的 CPU 是大于一个标准的偏差，该差值低于平均软中断工作负载，以及没有 CPU 是大于一个标准偏差，且该偏差高出平均，并有多于一个的 irq 分配给它们，一个 CPU 将处于节能模式。在节能模式中，CPU 不是 irqbalance 的一部分，所以它在有必要时才会被唤醒。

-   --hintpolicy

    决定如何解决 irq 内核关联提示。有效值为 `exact`（总是应用 irq 关联提示）、`subset` （irq 是平衡的，但分配的对象是关联提示的子集）、或者 `ignore`（irq 完全被忽略）。

-   --policyscript

    通过设备路径、当作参数的irq号码以及 **irqbalance** 预期的零退出代码，定义脚本位置以执行每个中断请求。定义的脚本能指定零或多键值对来指导管理传递的 irq 中 **irqbalance**。下列是为效键值对：ban有效值为 `true`（从平衡中排除传递的 irq）或 `false`（该 irq 表现平衡）。balance_level允许用户重写传递的 irq 平衡度。默认设置下，平衡度基于拥有 irq 设备的 PCI 设备种类。有效值为 `none`、`package`、`cache`、或 `core`。numa_node允许用户重写视作为本地传送 irq 的 NUMA 节点。如果本地节点的信息没有限定于 ACPI ，则设备被视作与所有节点距离相等。有效值为识别特定 NUMA 节点的整数（从0开始）和 `-1`，规定 irq 应被视作与所有节点距离相等。

-   --banirq

    将带有指定中断请求号码的中断添加至禁止中断的列表。

也可以使用 *`IRQBALANCE_BANNED_CPUS`* 环境变量来指定被 **irqbalance** 忽略的 CPU 掩码。

```shell
//默认irqbalance绑定一个numa, -1指定多个numa
echo -1 >/sys/bus/pci/devices/`ethtool -i p1p1 |grep bus-info | awk  '{ print $2 }'`/numa_node ; 
// 目录 /sys/class/net/p1p1/ link到了 /sys/bus/pci/devices/`ethtool -i p1p1 |grep bus-info | awk  '{ print $2 }'` 

执行 irqbalance --debug 进行调试
```

#### [irqbalance的流程](https://blog.csdn.net/whrszzc/article/details/50533866)

初始化的过程只是建立链表的过程，暂不描述，只考虑正常运行状态时的流程
-处理间隔是10s
-清除所有中断的负载值
-/proc/interrupts读取中断，并记录中断数
-/proc/stat读取每个cpu的负载，并依次计算每个层次每个节点的负载以及每个中断的负载
-通过平衡算法找出需要重新分配的中断
-把需要重新分配的中断加入到新的节点中
-配置smp_affinity使处理生效

### 网卡软中断以及内存远近的测试结论

一般网卡中断会占用一些CPU，如果把网卡中断挪到其它node的core上，在鲲鹏920上测试，业务跑在node3，网卡中断分别在node0和node3，QPS分别是：179000 VS 175000

如果将业务跑在node0上，网卡中断分别在node0和node1上得到的QPS分别是：204000 VS 212000

以上测试的时候业务进程分配的内存全限制在node0上

```
#/root/numa-maps-summary.pl </proc/123853/numa_maps
N0        :      5085548 ( 19.40 GB)
N1        :         4479 (  0.02 GB)
N2        :            1 (  0.00 GB)
active    :            0 (  0.00 GB)
anon      :      5085455 ( 19.40 GB)
dirty     :      5085455 ( 19.40 GB)
kernelpagesize_kB:         2176 (  0.01 GB)
mapmax    :          348 (  0.00 GB)
mapped    :         4626 (  0.02 GB)
```

从以上测试数据可以看到在这个内存分布场景下，如果就近访问内存性能有20%以上的提升

### 阿里云绑核脚本

通常情况下，Linux的网卡中断是由一个CPU核心来处理的，当承担高流量的场景下，会出现一些诡异的情况（网卡尚未达到瓶颈，但是却出现丢包的情况）

这种时候，我们最好看下网卡中断是不是缺少调优。

优化3要点：网卡多队列+irq affinity亲缘性设置+关闭irqbalance (systemctl stop irqbalance)

目前阿里云官方提供的centos和ubuntu镜像里面，已经自带了优化脚本，内容如下:

**centos7的脚本路径在 /usr/sbin/ecs_mq_rps_rfs 具体内容如下：**

```shell
#!/bin/bash
# This is the default setting of networking multiqueue and irq affinity
# 1. enable multiqueue if available
# 2. irq affinity optimization
# 3. stop irqbalance service
# set and check multiqueue

function set_check_multiqueue()
{
    eth=$1
    log_file=$2
    queue_num=$(ethtool -l $eth | grep -ia5 'pre-set' | grep -i combined | awk {'print $2'})
    if [ $queue_num -gt 1 ]; then
        # set multiqueue
        ethtool -L $eth combined $queue_num
        # check multiqueue setting
        cur_q_num=$(ethtool -l $eth | grep -iA5 current | grep -i combined | awk {'print $2'})
        if [ "X$queue_num" != "X$cur_q_num" ]; then
            echo "Failed to set $eth queue size to $queue_num" >> $log_file
            echo "after setting, pre-set queue num: $queue_num , current: $cur_q_num" >> $log_file
            return 1
        else
            echo "OK. set $eth queue size to $queue_num" >> $log_file
        fi
    else
        echo "only support $queue_num queue; no need to enable multiqueue on $eth" >> $log_file
    fi
}

#set irq affinity
function set_irq_smpaffinity()
{
    log_file=$1
    node_dir=/sys/devices/system/node
    for i in $(ls -d $node_dir/node*); do
        i=${i/*node/}
    done

    echo "max node :$i" >> $log_file
    node_cpumax=$(cat /sys/devices/system/node/node${i}/cpulist |awk -F- '{print $NF}')
    irqs=($(cat /proc/interrupts |grep virtio |grep put | awk -F: '{print $1}'))
    core=0
    for irq in ${irqs[@]};do
        VEC=$core
        if [ $VEC -ge 32 ];then
            let "IDX = $VEC / 32"
            MASK_FILL=""
            MASK_ZERO="00000000"
            for ((i=1; i<=$IDX;i++))
                do
                    MASK_FILL="${MASK_FILL},${MASK_ZERO}"
                done
            let "VEC -= 32 * $IDX"
            MASK_TMP=$((1<<$VEC))
            MASK=$(printf "%X%s" $MASK_TMP $MASK_FILL)
        else
            MASK_TMP=$((1<<$VEC))
            MASK=$(printf "%X" $MASK_TMP)
        fi
        echo $MASK > /proc/irq/$irq/smp_affinity
        echo "mask:$MASK, irq:$irq" >> $log_file
        core=$(((core+1)%(node_cpumax+1)))
    done
}

# stop irqbalance service
function stop_irqblance()
{
    log_file=$1
    ret=0
    if [ "X" != "X$(ps -ef | grep irqbalance | grep -v grep)" ]; then
        if which systemctl;then
            systemctl stop irqbalance
        else
            service irqbalance stop
        fi
        if [ $? -ne 0 ]; then
            echo "Failed to stop irqbalance" >> $log_file
            ret=1
        fi
    else
       echo "OK. irqbalance stoped." >> $log_file
    fi
    return $ret
}
# main logic
function main()
{
    ecs_network_log=/var/log/ecs_network_optimization.log
    ret_value=0
    echo "running $0" > $ecs_network_log
    echo "========  ECS network setting starts $(date +'%Y-%m-%d %H:%M:%S') ========" >> $ecs_network_log
    # we assume your NIC interface(s) is/are like eth*
    eth_dirs=$(ls -d /sys/class/net/eth*)
    if [ "X$eth_dirs" = "X" ]; then
        echo "ERROR! can not find any ethX in /sys/class/net/ dir." >> $ecs_network_log
        ret_value=1
    fi
    for i in $eth_dirs
    do
        cur_eth=$(basename $i)
        echo "optimize network performance: current device $cur_eth" >> $ecs_network_log
        # only optimize virtio_net device
        driver=$(basename $(readlink $i/device/driver))
        if ! echo $driver | grep -q virtio; then
            echo "ignore device $cur_eth with driver $driver" >> $ecs_network_log
            continue
        fi
        echo "set and check multiqueue on $cur_eth" >> $ecs_network_log
        set_check_multiqueue $cur_eth $ecs_network_log
        if [ $? -ne 0 ]; then
            echo "Failed to set multiqueue on $cur_eth" >> $ecs_network_log
            ret_value=1
        fi
    done
    stop_irqblance  $ecs_network_log
    set_irq_smpaffinity $ecs_network_log
    echo "========  ECS network setting END $(date +'%Y-%m-%d %H:%M:%S')  ========" >> $ecs_network_log
    return $ret_value
}


# program starts here
main
exit $?
```

查询的rps绑定情况的脚本 get_rps.sh

```shell
#!/bin/bash
# 获取当前rps情况
for i in $(ls /sys/class/net/eth0/queues/rx-*/rps_cpus); do 
  echo $i
  cat $i
done
```

## 查看网卡和numa的关系

```
#yum install lshw -y
#lshw -C network -short
H/W path               Device          Class      Description
=============================================================
/0/100/0/9/0           eth0            network    MT27710 Family [ConnectX-4 Lx]
/0/100/0/9/0.1         eth1            network    MT27710 Family [ConnectX-4 Lx]
/1                     e41358fae4ee_h  network    Ethernet interface
/2                     86b0637ef1e1_h  network    Ethernet interface
/3                     a6706e785f53_h  network    Ethernet interface
/4                     d351290e50a0_h  network    Ethernet interface
/5                     1a9e5df98dd1_h  network    Ethernet interface
/6                     766ec0dab599_h  network    Ethernet interface
/7                     bond0.11        network    Ethernet interface
/8                     ea004888c217_h  network    Ethernet interface
```

以及：

```
lscpu | grep -i numa
numactl --hardware
cat /proc/interrupts | egrep -i "CPU|rx"
```

[Check if the network interfaces are tied to Numa](https://ixnfo.com/en/how-to-find-out-on-which-numa-node-network-interfaces.html) (if -1 means not tied, if 0, then to numa0):

```
cat /sys/class/net/eth0/device/numa_node
```

You can see which NAMA the network card belongs to, for example, using lstopo:

```
yum install hwloc -y
lstopo
lstopo --logical
lstopo --logical --output-format png > lstopo.png

--
[root@hygon3 10:58 /root]  //hygon 7280 CPU
#lstopo --logical
Machine (503GB total)               //总内存大小
  NUMANode L#0 (P#0 252GB)          //socket0、numa0 的内存大小
    Package L#0
      L3 L#0 (8192KB)               //L3 cache，对应4个物理core，8个HT
        L2 L#0 (512KB) + L1d L#0 (32KB) + L1i L#0 (64KB) + Core L#0 // L1/L2
          PU L#0 (P#0)
          PU L#1 (P#64)
        L2 L#1 (512KB) + L1d L#1 (32KB) + L1i L#1 (64KB) + Core L#1
          PU L#2 (P#1)
          PU L#3 (P#65)
        L2 L#2 (512KB) + L1d L#2 (32KB) + L1i L#2 (64KB) + Core L#2
          PU L#4 (P#2)
          PU L#5 (P#66)
        L2 L#3 (512KB) + L1d L#3 (32KB) + L1i L#3 (64KB) + Core L#3
          PU L#6 (P#3)
          PU L#7 (P#67)
      L3 L#1 (8192KB)
      L3 L#2 (8192KB)
      L3 L#3 (8192KB)
      L3 L#4 (8192KB)
      L3 L#5 (8192KB)
      L3 L#6 (8192KB)
      L3 L#7 (8192KB)
    HostBridge L#0
      PCIBridge
        PCIBridge
          PCI 1a03:2000
            GPU L#0 "controlD64"
            GPU L#1 "card0"
      PCIBridge
        PCI 1d94:7901
          Block(Disk) L#2 "sdm"   //ssd系统盘，接在Node0上，绑核有优势
    HostBridge L#4
      PCIBridge
        PCI 1000:0097
      PCIBridge
        PCI 1c5f:000d
      PCIBridge
        PCI 1c5f:000d
    HostBridge L#8
      PCIBridge
        PCI 15b3:1015
          Net L#3 "p1p1"      //万兆网卡接在Node0上
        PCI 15b3:1015
          Net L#4 "p1p2"
    HostBridge L#10
      PCIBridge
        PCI 8086:1521
          Net L#5 "em1"       //千兆网卡接在Node0上
        PCI 8086:1521
          Net L#6 "em2"
  NUMANode L#1 (P#1 251GB)    //另外一个socket
    Package L#1
      L3 L#8 (8192KB)
        L2 L#32 (512KB) + L1d L#32 (32KB) + L1i L#32 (64KB) + Core L#32

----------- FT2500 两路共128core
#lstopo-no-graphics --logical
Machine (503GB total)
  Package L#0 + L3 L#0 (64MB)
    NUMANode L#0 (P#0 31GB)
      L2 L#0 (2048KB)         //4个物理core共享2M 
        L1d L#0 (32KB) + L1i L#0 (32KB) + Core L#0 + PU L#0 (P#0)
        L1d L#1 (32KB) + L1i L#1 (32KB) + Core L#1 + PU L#1 (P#1)
        L1d L#2 (32KB) + L1i L#2 (32KB) + Core L#2 + PU L#2 (P#2)
        L1d L#3 (32KB) + L1i L#3 (32KB) + Core L#3 + PU L#3 (P#3)
      L2 L#1 (2048KB)
        L1d L#4 (32KB) + L1i L#4 (32KB) + Core L#4 + PU L#4 (P#4)
        L1d L#5 (32KB) + L1i L#5 (32KB) + Core L#5 + PU L#5 (P#5)
        L1d L#6 (32KB) + L1i L#6 (32KB) + Core L#6 + PU L#6 (P#6)
        L1d L#7 (32KB) + L1i L#7 (32KB) + Core L#7 + PU L#7 (P#7)
      HostBridge L#0
        PCIBridge
          PCIBridge
            PCIBridge
              PCI 1000:00ac
                Block(Disk) L#0 "sdh"
                Block(Disk) L#1 "sdf"  // 磁盘挂在Node0上
            PCIBridge
              PCI 8086:1521
                Net L#13 "eth0"
              PCI 8086:1521
                Net L#14 "eth1"       //网卡挂在node0上
        PCIBridge
          PCIBridge
            PCI 1a03:2000
              GPU L#15 "controlD64"
              GPU L#16 "card0"
    NUMANode L#1 (P#1 31GB)
    NUMANode L#2 (P#2 31GB)
    NUMANode L#3 (P#3 31GB)
    NUMANode L#4 (P#4 31GB)
    NUMANode L#5 (P#5 31GB)
    NUMANode L#6 (P#6 31GB)
    NUMANode L#7 (P#7 31GB)
      L2 L#14 (2048KB)
        L1d L#56 (32KB) + L1i L#56 (32KB) + Core L#56 + PU L#56 (P#56)
        L1d L#57 (32KB) + L1i L#57 (32KB) + Core L#57 + PU L#57 (P#57)
        L1d L#58 (32KB) + L1i L#58 (32KB) + Core L#58 + PU L#58 (P#58)
        L1d L#59 (32KB) + L1i L#59 (32KB) + Core L#59 + PU L#59 (P#59)
      L2 L#15 (2048KB)
        L1d L#60 (32KB) + L1i L#60 (32KB) + Core L#60 + PU L#60 (P#60)
        L1d L#61 (32KB) + L1i L#61 (32KB) + Core L#61 + PU L#61 (P#61)
        L1d L#62 (32KB) + L1i L#62 (32KB) + Core L#62 + PU L#62 (P#62)
        L1d L#63 (32KB) + L1i L#63 (32KB) + Core L#63 + PU L#63 (P#63)
  Package L#1 + L3 L#1 (64MB)   //socket2
    NUMANode L#8 (P#8 31GB)
      L2 L#16 (2048KB)
        L1d L#64 (32KB) + L1i L#64 (32KB) + Core L#64 + PU L#64 (P#64)
        L1d L#65 (32KB) + L1i L#65 (32KB) + Core L#65 + PU L#65 (P#65)
        L1d L#66 (32KB) + L1i L#66 (32KB) + Core L#66 + PU L#66 (P#66)
        L1d L#67 (32KB) + L1i L#67 (32KB) + Core L#67 + PU L#67 (P#67)
      L2 L#17 (2048KB)
        L1d L#68 (32KB) + L1i L#68 (32KB) + Core L#68 + PU L#68 (P#68)
        L1d L#69 (32KB) + L1i L#69 (32KB) + Core L#69 + PU L#69 (P#69)
        L1d L#70 (32KB) + L1i L#70 (32KB) + Core L#70 + PU L#70 (P#70)
        L1d L#71 (32KB) + L1i L#71 (32KB) + Core L#71 + PU L#71 (P#71)
      HostBridge L#7
        PCIBridge
          PCIBridge
            PCIBridge
              PCI 15b3:1015
                Net L#17 "eth2"   //node8 上的网卡，eth2、eth3做了bonding
              PCI 15b3:1015
                Net L#18 "eth3"
            PCIBridge
              PCI 144d:a808
            PCIBridge
              PCI 144d:a808

 ---鲲鹏920 每路48core 2路共4node，网卡插在node0，磁盘插在node2
 #lstopo-no-graphics
Machine (755GB total)
  Package L#0
    NUMANode L#0 (P#0 188GB)
      L3 L#0 (24MB)
        L2 L#0 (512KB) + L1d L#0 (64KB) + L1i L#0 (64KB) + Core L#0 + PU L#0 (P#0)
        L2 L#1 (512KB) + L1d L#1 (64KB) + L1i L#1 (64KB) + Core L#1 + PU L#1 (P#1)
        L2 L#22 (512KB) + L1d L#22 (64KB) + L1i L#22 (64KB) + Core L#22 + PU L#22 (P#22)
        L2 L#23 (512KB) + L1d L#23 (64KB) + L1i L#23 (64KB) + Core L#23 + PU L#23 (P#23)
      HostBridge L#0
        PCIBridge
          PCI 15b3:1017
            Net L#0 "enp2s0f0"
          PCI 15b3:1017
            Net L#1 "eth1"
        PCIBridge
          PCI 19e5:1711
            GPU L#2 "controlD64"
            GPU L#3 "card0"
      HostBridge L#3
        2 x { PCI 19e5:a230 }
        PCI 19e5:a235
          Block(Disk) L#4 "sda"
      HostBridge L#4
        PCIBridge
          PCI 19e5:a222
            Net L#5 "enp125s0f0"
          PCI 19e5:a221
            Net L#6 "enp125s0f1"
          PCI 19e5:a222
            Net L#7 "enp125s0f2"
          PCI 19e5:a221
            Net L#8 "enp125s0f3"
    NUMANode L#1 (P#1 189GB) + L3 L#1 (24MB)
      L2 L#24 (512KB) + L1d L#24 (64KB) + L1i L#24 (64KB) + Core L#24 + PU L#24 (P#24)
  Package L#1
    NUMANode L#2 (P#2 189GB)
      L3 L#2 (24MB)
        L2 L#48 (512KB) + L1d L#48 (64KB) + L1i L#48 (64KB) + Core L#48 + PU L#48 (P#48)
      HostBridge L#6
        PCIBridge
          PCI 19e5:3714
        PCIBridge
          PCI 19e5:3714
        PCIBridge
          PCI 19e5:3714
        PCIBridge
          PCI 19e5:3714
      HostBridge L#11
        PCI 19e5:a230
        PCI 19e5:a235
        PCI 19e5:a230
    NUMANode L#3 (P#3 189GB) + L3 L#3 (24MB)
      L2 L#72 (512KB) + L1d L#72 (64KB) + L1i L#72 (64KB) + Core L#72 + PU L#72 (P#72)
  Misc(MemoryModule)
```

如果cpu core太多, interrupts 没法看的话，通过cut只看其中一部分core

```
cat /proc/interrupts | grep -i 'eth4\|CPU' | cut -c -8,865-995,1425-
```

## 参考资料

[高斯在鲲鹏下跑TPCC的优化](https://www.modb.pro/db/29135)



Reference:

