<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="到底一台服务器上最多能创建多少个TCP连接 经常听到有同学说一台机器最多能创建65535个TCP连接，这其实是错误的理解，为什么会有这个错误的理解呢？  port range我们都知道linux下本地随机端口范围由参数控制，也就是listen、connect时候如果没有指定本地端口，那么就从下面的port range 中随机取一个可用的 12# cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_">
<meta property="og:type" content="article">
<meta property="og:title" content="到底一台服务器上最多能创建多少个TCP连接">
<meta property="og:url" content="https://plantegg.github.io/2020/11/30/%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="到底一台服务器上最多能创建多少个TCP连接 经常听到有同学说一台机器最多能创建65535个TCP连接，这其实是错误的理解，为什么会有这个错误的理解呢？  port range我们都知道linux下本地随机端口范围由参数控制，也就是listen、connect时候如果没有指定本地端口，那么就从下面的port range 中随机取一个可用的 12# cat &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20220224103024676.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220721100246598.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220721100421130.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220721100446521.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220721093116395.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-8259033.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20221112211814567.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image7.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-9840722.jpeg">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220627154822263.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220627154931495.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20220413134252639">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20240522103927360.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20240522103407831.png">
<meta property="article:published_time" content="2020-11-30T02:30:03.000Z">
<meta property="article:modified_time" content="2025-11-16T11:58:49.575Z">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="SO_REUSEADDR">
<meta property="article:tag" content="TCP">
<meta property="article:tag" content="ip_local_port_range">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20220224103024676.png">


<link rel="canonical" href="https://plantegg.github.io/2020/11/30/%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://plantegg.github.io/2020/11/30/%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5/","path":"2020/11/30/一台机器上最多能创建多少个TCP连接/","title":"到底一台服务器上最多能创建多少个TCP连接"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>到底一台服务器上最多能创建多少个TCP连接 | plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">plantegg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%88%B0%E5%BA%95%E4%B8%80%E5%8F%B0%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5"><span class="nav-number">1.</span> <span class="nav-text">到底一台服务器上最多能创建多少个TCP连接</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#port-range"><span class="nav-number">1.1.</span> <span class="nav-text">port range</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%B0%E5%BA%95%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5"><span class="nav-number">1.2.</span> <span class="nav-text">到底一台机器上最多能创建多少个TCP连接</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TCP-SO-REUSEADDR"><span class="nav-number">1.3.</span> <span class="nav-text">TCP SO_REUSEADDR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TCP-SO-REUSEPORT"><span class="nav-number">1.4.</span> <span class="nav-text">TCP SO_REUSEPORT</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Ephemeral-Port-Range"><span class="nav-number">1.5.</span> <span class="nav-text">The Ephemeral Port Range</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#linux-%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9Ephemeral-Port"><span class="nav-number">1.6.</span> <span class="nav-text">linux 如何选择Ephemeral Port</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-10-0-327-ali2017-alios7-x86-64"><span class="nav-number">1.6.1.</span> <span class="nav-text">3.10.0-327.ali2017.alios7.x86_64</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-19-91-19-1-al7-x86-64"><span class="nav-number">1.6.2.</span> <span class="nav-text">4.19.91-19.1.al7.x86_64</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tcp-max-tw-buckets"><span class="nav-number">1.7.</span> <span class="nav-text">tcp_max_tw_buckets</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SO-LINGER"><span class="nav-number">1.8.</span> <span class="nav-text">SO_LINGER</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#NIO%E4%B8%8B%E8%AE%BE%E7%BD%AE-SO-LINGER-%E7%9A%84%E9%94%99%E8%AF%AF%E6%A1%88%E4%BE%8B"><span class="nav-number">1.8.1.</span> <span class="nav-text">NIO下设置 SO_LINGER 的错误案例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%9C%89-time-wait-%E7%8A%B6%E6%80%81"><span class="nav-number">1.9.</span> <span class="nav-text">为什么要有 time_wait 状态</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%B1Nginx-SY-CPU%E9%AB%98%E8%B4%9F%E8%BD%BD%E5%BC%95%E5%8F%91%E5%86%85%E6%A0%B8%E6%8E%A2%E7%B4%A2%E4%B9%8B%E6%97%85"><span class="nav-number">1.10.</span> <span class="nav-text">由Nginx SY CPU高负载引发内核探索之旅</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8ESTGW%E6%B5%81%E9%87%8F%E4%B8%8B%E9%99%8D%E6%8E%A2%E7%A7%98%E5%86%85%E6%A0%B8%E6%94%B6%E5%8C%85%E6%9C%BA%E5%88%B6"><span class="nav-number">1.11.</span> <span class="nav-text">从STGW流量下降探秘内核收包机制</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9F%AD%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BC%80%E9%94%80"><span class="nav-number">1.12.</span> <span class="nav-text">短连接的开销</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E6%9D%A1%E8%BF%9E%E6%8E%A5%E7%9A%84%E5%BC%80%E9%94%80"><span class="nav-number">1.13.</span> <span class="nav-text">一条连接的开销</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E8%BF%9B%E7%A8%8B%E4%BD%BF%E7%94%A8%E7%9B%B8%E5%90%8C%E7%AB%AF%E5%8F%A3%EF%BC%8C%E8%AE%BE%E7%BD%AESO-REUSEADDR%E5%90%8E%E8%A2%ABbind-%E5%AF%BC%E8%87%B4%E5%8F%AF%E7%94%A8-local-port-%E4%B8%8D%E5%A4%9F"><span class="nav-number">1.14.</span> <span class="nav-text">不同进程使用相同端口，设置SO_REUSEADDR后被bind  导致可用 local port 不够</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%AA%8C%E8%AF%81%E7%AB%AF%E5%8F%A3%E8%A2%ABconnect-%E5%92%8C-listen-%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8"><span class="nav-number">1.14.1.</span> <span class="nav-text">验证端口被connect 和 listen 同时使用</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">1.15.</span> <span class="nav-text">结论</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.16.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">276</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/30/%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="到底一台服务器上最多能创建多少个TCP连接 | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          到底一台服务器上最多能创建多少个TCP连接
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-30 10:30:03" itemprop="dateCreated datePublished" datetime="2020-11-30T10:30:03+08:00">2020-11-30</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="到底一台服务器上最多能创建多少个TCP连接"><a href="#到底一台服务器上最多能创建多少个TCP连接" class="headerlink" title="到底一台服务器上最多能创建多少个TCP连接"></a>到底一台服务器上最多能创建多少个TCP连接</h1><blockquote>
<p>经常听到有同学说一台机器最多能创建65535个TCP连接，这其实是错误的理解，为什么会有这个错误的理解呢？</p>
</blockquote>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地随机端口范围由参数控制，也就是listen、connect时候如果没有指定本地端口，那么就从下面的port range 中随机取一个可用的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </span><br><span class="line">2000	65535</span><br></pre></td></tr></table></figure>

<p>port range的上限是65535，所以也经常看到这个<strong>误解</strong>：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>先说<strong>结论</strong>：在内存、文件句柄足够的话可以创建的连接是<strong>没有限制</strong>的（每个TCP连接至少要消耗一个文件句柄）。</p>
<p>那么&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>核心规则：<strong>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一</strong></p>
<p>后面所讲都遵循这个规则，所以在心里反复默念：<strong>四元组唯一</strong> 五个大字，就能分析出来到底能创建多少TCP连接了。</p>
<p>比如如下这个机器上的TCP连接实际状态：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 18089</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </span><br><span class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</span><br></pre></td></tr></table></figure>

<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。他们的四元组均不相同。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<p>**那么为什么大家有这样的误解呢？**我总结了下，大概是以下两个原因让大家误解了：</p>
<ul>
<li>如果是listen服务，那么肯定端口不能重复使用，这样就跟我们的误解对应上了，一个服务器上最多能监听65535个端口。比如nginx监听了80端口，那么tomcat就没法再监听80端口了，这里的80端口只能监听一次。</li>
<li>另外如果我们要连的server只有一个，比如：1.1.1.1:80 ，同时本机只有一个ip的话，那么这个时候即使直接调connect 也只能创建出65535个连接，因为四元组中的三个是固定的了。</li>
</ul>
<p>我们在创建连接前，经常会先调bind，bind后可以调 listen当做服务端监听，也可以直接调connect当做client来连服务端。</p>
<p>bind(ip,port&#x3D;0) 的时候是让系统绑定到某个网卡和自动分配的端口，此时系统没有办法确定接这个socket 是要去connect还是listen. 如果是listen的话，那么肯定是不能出现端口冲突的(得local port 唯一)，如果是connect的话，只要满足4元组唯一即可。在这种情况下，系统只能尽可能满足更强的要求，就是先要求端口不能冲突，即使之后去connect的时候四元组是唯一的。</p>
<p>比如 Nginx HaProxy envoy这些软件在创建到upstream的连接时，都会用 bind(0) 的方式, 导致到不同目的的连接无法复用同一个src port，这样后端的最大连接数受限于local_port_range。 nginx的修改 <a target="_blank" rel="noopener" href="http://hg.nginx.org/nginx/rev/2c7b488a61fb">http://hg.nginx.org/nginx/rev/2c7b488a61fb</a></p>
<blockquote>
<p>Linux 4.2后的内核增加了IP_BIND_ADDRESS_NO_PORT 这个socket option来解决这个问题，将src port的选择延后到connect的时候</p>
<p><a target="_blank" rel="noopener" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=90c337da1524863838658078ec34241f45d8394d">IP_BIND_ADDRESS_NO_PORT (since Linux 4.2)</a><br>              Inform the kernel to not reserve an ephemeral port when using bind(2) with a port number of 0.  The port will later be automatically chosen at connect(2) time, in a way that allows sharing a source port as long as the 4-tuple is unique.</p>
</blockquote>
<p>但如果我只是个client端，只需要连接server建立连接，也就不需要bind，直接调connect就可以了，这个时候只要保证四元组唯一就行。</p>
<p>bind()的时候内核是还不知道四元组的，只知道src_ip、src_port，所以这个时候单网卡下src_port是没法重复的，但是connect()的时候已经知道了四元组的全部信息，所以只要保证四元组唯一就可以了，那么这里的src_port完全是可以重复使用的。</p>
<p><img src="/images/951413iMgBlog/640-20220224103024676.png" alt="Image"></p>
<p><strong>是不是加上了 SO_REUSEADDR、SO_REUSEPORT 就能重用端口了呢？</strong></p>
<h2 id="TCP-SO-REUSEADDR"><a href="#TCP-SO-REUSEADDR" class="headerlink" title="TCP SO_REUSEADDR"></a>TCP SO_REUSEADDR</h2><p>文档描述：</p>
<blockquote>
<p>SO_REUSEADDR      Indicates that the rules used in validating addresses supplied in a bind(2) call should allow reuse of local addresses.  For AF_INET sockets this means that a socket may bind, except when there is an active listening socket bound to the address. When the listening socket is bound to INADDR_ANY with a specific port then it is not possible to bind to this port for any local address.  Argument is an integer boolean flag.</p>
</blockquote>
<p>从这段文档中我们可以知道三个事：</p>
<ol>
<li>使用这个参数后，bind操作是可以重复使用local address的，注意，这里说的是local address，即ip加端口组成的本地地址，如果机器有两个本地ip，那么任意ip或端口部分不一样，它们本身就是可以共存的，不需要使用这个参数。</li>
<li>当local address被一个处于listen状态的socket使用时，加上该参数也不能重用这个地址。</li>
<li>当处于listen状态的socket监听的本地地址的ip部分是INADDR_ANY，即表示监听本地的所有ip，即使使用这个参数，也不能再bind包含这个端口的任意本地地址，这个和 2 中描述的其实是一样的。</li>
</ol>
<p>&#x3D;&#x3D;SO_REUSEADDR 可以用本地相同的(sip, sport) 去连connect 远程的不同的（dip、dport）&#x2F;&#x2F;而 SO_REUSEPORT主要是解决Server端的port重用&#x3D;&#x3D;</p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ">SO_REUSEADDR 还可以重用TIME_WAIT状态的port</a>, 在程序崩溃后之前的TCP连接会进入到TIME_WAIT状态，需要一段时间才能释放，如果立即重启就会抛出<u>Address Already in use</u>的错误导致启动失败。这时候可以通过在调用bind函数之前设置SO_REUSEADDR来解决。</p>
<blockquote>
<p>What exactly does SO_REUSEADDR do?</p>
<p>This socket option tells the kernel that even if this port is busy (in the TIME_WAIT state), go ahead and reuse it anyway. If it is busy, but with another state, you will still get an address already in use error. It is useful if your server has been shut down, and then restarted right away while sockets are still active on its port. You should be aware that if any unexpected data comes in, it may confuse your server, but while this is possible, it is not likely.</p>
<p>It has been pointed out that “A socket is a 5 tuple (proto, local addr, local port, remote addr, remote port). SO_REUSEADDR just says that you can reuse local addresses. The 5 tuple still must be unique!” This is true, and this is why it is very unlikely that unexpected data will ever be seen by your server. The danger is that such a 5 tuple is still floating around on the net, and while it is bouncing around, a new connection from the same client, on the same system, happens to get the same remote port. </p>
</blockquote>
<p>By setting <code>SO_REUSEADDR</code> user informs the kernel of an intention to share the bound port with anyone else, but only if it doesn’t cause a conflict on the protocol layer. There are at least three situations when this flag is useful:</p>
<ol>
<li>Normally after binding to a port and stopping a server it’s neccesary to wait for a socket to time out before another server can bind to the same port. With <code>SO_REUSEADDR</code> set it’s possible to rebind immediately, even if the socket is in a <code>TIME_WAIT</code> state.</li>
<li>When one server binds to <code>INADDR_ANY</code>, say <code>0.0.0.0:1234</code>, it’s impossible to have another server binding to a specific address like <code>192.168.1.21:1234</code>. With <code>SO_REUSEADDR</code> flag this behaviour is allowed.</li>
<li>When using the bind before connect trick only a single connection can use a single outgoing source port. With this flag, it’s possible for many connections to reuse the same source port, given that they connect to different destination addresses.</li>
</ol>
<h2 id="TCP-SO-REUSEPORT"><a href="#TCP-SO-REUSEPORT" class="headerlink" title="TCP SO_REUSEPORT"></a>TCP SO_REUSEPORT</h2><p>SO_REUSEPORT主要用来解决惊群、性能等问题。通过多个进程、线程来监听同一端口，进来的连接通过内核来hash分发做到负载均衡，避免惊群。</p>
<blockquote>
<p>SO_REUSEPORT is also useful for eliminating the try-10-times-to-bind hack in ftpd’s data connection setup routine.  Without SO_REUSEPORT, only one ftpd thread can bind to TCP (lhost, lport, INADDR_ANY, 0) in preparation for connecting back to the client.  Under conditions of heavy load, there are more threads colliding here than the try-10-times hack can accomodate.  With SO_REUSEPORT, things  work nicely and the hack becomes unnecessary.</p>
</blockquote>
<p>SO_REUSEPORT使用场景：linux kernel 3.9 引入了最新的SO_REUSEPORT选项，使得多进程或者多线程创建多个绑定同一个ip:port的监听socket，提高服务器的接收链接的并发能力,程序的扩展性更好；此时需要设置SO_REUSEPORT（<strong>注意所有进程都要设置才生效</strong>）。</p>
<p>setsockopt(listenfd, SOL_SOCKET, SO_REUSEPORT,(const void *)&amp;reuse , sizeof(int));</p>
<p>目的：每一个进程有一个独立的监听socket，并且bind相同的ip:port，独立的listen()和accept()；提高接收连接的能力。（例如nginx多进程同时监听同一个ip:port）</p>
<blockquote>
<p>(a) on Linux SO_REUSEPORT is meant to be used <em>purely</em> for load balancing multiple incoming UDP packets or incoming TCP connection requests across multiple sockets belonging to the same app.  ie. it’s a work around for machines with a lot of cpus, handling heavy load, where a single listening socket becomes a bottleneck because of cross-thread contention on the in-kernel socket lock (and state).</p>
<p>(b) set IP_BIND_ADDRESS_NO_PORT socket option for tcp sockets before binding to a specific source ip<br>with port 0 if you’re going to use the socket for connect() rather then listen() this allows the kernel<br>to delay allocating the source port until connect() time at which point it is much cheaper</p>
</blockquote>
<h2 id="The-Ephemeral-Port-Range"><a href="#The-Ephemeral-Port-Range" class="headerlink" title="The Ephemeral Port Range"></a><a target="_blank" rel="noopener" href="http://www.ncftp.com/ncftpd/doc/misc/ephemeral_ports.html">The Ephemeral Port Range</a></h2><p>Ephemeral Port Range就是我们前面所说的Port Range（&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range）</p>
<blockquote>
<p>A TCP&#x2F;IPv4 connection consists of two endpoints, and each endpoint consists of an IP address and a port number. Therefore, when a client user connects to a server computer, an established connection can be thought of as the 4-tuple of (server IP, server port, client IP, client port).</p>
<p>Usually three of the four are readily known – client machine uses its own IP address and when connecting to a remote service, the server machine’s IP address and service port number are required.</p>
<p>What is not immediately evident is that when a connection is established that the client side of the connection uses a port number. Unless a client program explicitly requests a specific port number, the port number used is an ephemeral port number.</p>
<p>Ephemeral ports are temporary ports assigned by a machine’s IP stack, and are assigned from a designated range of ports for this purpose. When the connection terminates, the ephemeral port is available for reuse, although most IP stacks won’t reuse that port number until the entire pool of ephemeral ports have been used.</p>
<p>So, if the client program reconnects, it will be assigned a different ephemeral port number for its side of the new connection.</p>
</blockquote>
<h2 id="linux-如何选择Ephemeral-Port"><a href="#linux-如何选择Ephemeral-Port" class="headerlink" title="linux 如何选择Ephemeral Port"></a>linux 如何选择Ephemeral Port</h2><p>有资料说是随机从Port Range选择port，有的说是顺序选择，那么实际验证一下。</p>
<p>如下测试代码：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;      // printf</span><br><span class="line">#include &lt;stdlib.h&gt;     // atoi</span><br><span class="line">#include &lt;unistd.h&gt;     // close</span><br><span class="line">#include &lt;arpa/inet.h&gt;  // ntohs</span><br><span class="line">#include &lt;sys/socket.h&gt; // connect, socket</span><br><span class="line"></span><br><span class="line">void sample() &#123;</span><br><span class="line">    // Create socket</span><br><span class="line">    int sockfd;</span><br><span class="line">    if (sockfd = socket(AF_INET, SOCK_STREAM, 0), -1 == sockfd) &#123;</span><br><span class="line">        perror(&quot;socket&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Connect to remote. This does NOT actually send a packet.</span><br><span class="line">    const struct sockaddr_in raddr = &#123;</span><br><span class="line">        .sin_family = AF_INET,</span><br><span class="line">        .sin_port   = htons(8080),     // arbitrary remote port</span><br><span class="line">        .sin_addr   = htonl(INADDR_ANY)  // arbitrary remote host</span><br><span class="line">    &#125;;</span><br><span class="line">    if (-1 == connect(sockfd, (const struct sockaddr *)&amp;raddr, sizeof(raddr))) &#123;</span><br><span class="line">        perror(&quot;connect&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    // Display selected ephemeral port</span><br><span class="line">    const struct sockaddr_in laddr;</span><br><span class="line">    socklen_t laddr_len = sizeof(laddr);</span><br><span class="line">    if (-1 == getsockname(sockfd, (struct sockaddr *)&amp;laddr, &amp;laddr_len)) &#123;</span><br><span class="line">        perror(&quot;getsockname&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">    printf(&quot;local port: %i\n&quot;, ntohs(laddr.sin_port));</span><br><span class="line"></span><br><span class="line">    // Close socket</span><br><span class="line">    close(sockfd);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main() &#123;</span><br><span class="line">    for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">        sample();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>bind逻辑测试代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;netinet/in.h&gt;</span><br><span class="line">#include &lt;arpa/inet.h&gt;</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line">#include &lt;unistd.h&gt;</span><br><span class="line">#include &lt;errno.h&gt;</span><br><span class="line">#include &lt;string.h&gt;</span><br><span class="line">#include &lt;sys/types.h&gt;</span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line"></span><br><span class="line">void test_bind()&#123;</span><br><span class="line">    int listenfd = 0, connfd = 0;</span><br><span class="line">    struct sockaddr_in serv_addr;</span><br><span class="line">    char sendBuff[1025];</span><br><span class="line">    time_t ticks;</span><br><span class="line">	  socklen_t len;</span><br><span class="line"></span><br><span class="line">    listenfd = socket(AF_INET, SOCK_STREAM, 0);</span><br><span class="line">    memset(&amp;serv_addr, &#x27;0&#x27;, sizeof(serv_addr));</span><br><span class="line">    memset(sendBuff, &#x27;0&#x27;, sizeof(sendBuff));</span><br><span class="line"></span><br><span class="line">    serv_addr.sin_family = AF_INET;</span><br><span class="line">    serv_addr.sin_addr.s_addr = htonl(INADDR_ANY);</span><br><span class="line">    serv_addr.sin_port = htons(0);</span><br><span class="line"></span><br><span class="line">    bind(listenfd, (struct sockaddr*)&amp;serv_addr, sizeof(serv_addr));</span><br><span class="line"></span><br><span class="line">  	len = sizeof(serv_addr);</span><br><span class="line">	  if (getsockname(listenfd, (struct sockaddr *)&amp;serv_addr, &amp;len) == -1) &#123;</span><br><span class="line">		      perror(&quot;getsockname&quot;);</span><br><span class="line">			    return;</span><br><span class="line">	  &#125;</span><br><span class="line">	  printf(&quot;port number %d\n&quot;, ntohs(serv_addr.sin_port)); //只是挑选到了port，在系统层面保留，tcp连接还没有，netstat是看不到的</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main(int argc, char *argv[])</span><br><span class="line">&#123;</span><br><span class="line">	    for (int i = 0; i &lt; 5; i++) &#123;</span><br><span class="line">			         test_bind();</span><br><span class="line">					     &#125;</span><br><span class="line">		    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="3-10-0-327-ali2017-alios7-x86-64"><a href="#3-10-0-327-ali2017-alios7-x86-64" class="headerlink" title="3.10.0-327.ali2017.alios7.x86_64"></a>3.10.0-327.ali2017.alios7.x86_64</h3><p>编译后，执行(3.10.0-327.ali2017.alios7.x86_64)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</span><br><span class="line">Fri Nov 27 10:52:52 CST 2020</span><br><span class="line">local port: 17448</span><br><span class="line">local port: 17449</span><br><span class="line">local port: 17451</span><br><span class="line">local port: 17452</span><br><span class="line">local port: 17453</span><br><span class="line">+++++++</span><br><span class="line">local port: 17455</span><br><span class="line">local port: 17456</span><br><span class="line">local port: 17457</span><br><span class="line">local port: 17458</span><br><span class="line">local port: 17460</span><br><span class="line">-------</span><br><span class="line">local port: 17475</span><br><span class="line">local port: 17476</span><br><span class="line">local port: 17477</span><br><span class="line">local port: 17478</span><br><span class="line">local port: 17479</span><br><span class="line">Fri Nov 27 10:53:02 CST 2020</span><br><span class="line">local port: 17997</span><br><span class="line">local port: 17998</span><br><span class="line">local port: 17999</span><br><span class="line">local port: 18000</span><br><span class="line">local port: 18001</span><br><span class="line">+++++++</span><br><span class="line">local port: 18002</span><br><span class="line">local port: 18003</span><br><span class="line">local port: 18004</span><br><span class="line">local port: 18005</span><br><span class="line">local port: 18006</span><br><span class="line">******</span><br><span class="line">local port: 18010</span><br><span class="line">local port: 18011</span><br><span class="line">local port: 18012</span><br><span class="line">local port: 18013</span><br><span class="line">local port: 18014</span><br></pre></td></tr></table></figure>

<p>从测试看起来linux下端口选择跟时间有关系，起始端口肯定是顺序增加，起始端口应该是在Ephemeral Port范围内并且和时间戳绑定的某个值（也是递增的），即使没有使用任何端口，起始端口也会随时间增加而增加。</p>
<h3 id="4-19-91-19-1-al7-x86-64"><a href="#4-19-91-19-1-al7-x86-64" class="headerlink" title="4.19.91-19.1.al7.x86_64"></a>4.19.91-19.1.al7.x86_64</h3><p>换个内核版本编译后，执行(4.19.91-19.1.al7.x86_64)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 ; echo &quot;-------&quot; &amp;&amp; ./client &amp;&amp; sleep 10; date; ./client &amp;&amp; echo &quot;+++++++&quot; ; ./client &amp;&amp; sleep 0.1 &amp;&amp; echo &quot;******&quot;; ./client;</span><br><span class="line">Fri Nov 27 14:10:47 CST 2020</span><br><span class="line">local port: 7890</span><br><span class="line">local port: 7892</span><br><span class="line">local port: 7894</span><br><span class="line">local port: 7896</span><br><span class="line">local port: 7898</span><br><span class="line">+++++++</span><br><span class="line">local port: 7900</span><br><span class="line">local port: 7902</span><br><span class="line">local port: 7904</span><br><span class="line">local port: 7906</span><br><span class="line">local port: 7908</span><br><span class="line">-------</span><br><span class="line">local port: 7910</span><br><span class="line">local port: 7912</span><br><span class="line">local port: 7914</span><br><span class="line">local port: 7916</span><br><span class="line">local port: 7918</span><br><span class="line">Fri Nov 27 14:10:57 CST 2020</span><br><span class="line">local port: 7966</span><br><span class="line">local port: 7968</span><br><span class="line">local port: 7970</span><br><span class="line">local port: 7972</span><br><span class="line">local port: 7974</span><br><span class="line">+++++++</span><br><span class="line">local port: 7976</span><br><span class="line">local port: 7978</span><br><span class="line">local port: 7980</span><br><span class="line">local port: 7982</span><br><span class="line">local port: 7984</span><br><span class="line">******</span><br><span class="line">local port: 7988</span><br><span class="line">local port: 7990</span><br><span class="line">local port: 7992</span><br><span class="line">local port: 7994</span><br><span class="line">local port: 7996</span><br></pre></td></tr></table></figure>

<p>以上测试时的参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">1024    65535</span><br></pre></td></tr></table></figure>

<p>将1024改成1025后，分配出来的都是奇数端口了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">1025    1034</span><br><span class="line"></span><br><span class="line">$./client</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br><span class="line">local port: 1027</span><br><span class="line">local port: 1029</span><br><span class="line">local port: 1031</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br><span class="line">local port: 1027</span><br><span class="line">local port: 1029</span><br><span class="line">local port: 1031</span><br><span class="line">local port: 1033</span><br><span class="line">local port: 1025</span><br></pre></td></tr></table></figure>

<p>之所以都是偶数端口，是因为port_range 从偶数开始, 每次从++变到+2的<a target="_blank" rel="noopener" href="https://github.com/plantegg/linux/commit/1580ab63fc9a03593072cc5656167a75c4f1d173">原因</a>，connect挑选随机端口时都是在起始端口的基础上+2，而bind挑选随机端口的起始端口是系统port_range起始端口+1（这样和connect错开），然后每次仍然尝试+2，这样connect和bind基本一个用偶数另外一个就用奇数，一旦不够了再尝试使用另外一组</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/net/ipv4/ip_local_port_range</span><br><span class="line">1024    1047</span><br><span class="line"></span><br><span class="line">$./bind &amp;  ---bind程序随机挑选5个端口</span><br><span class="line">port number 1039</span><br><span class="line">port number 1043</span><br><span class="line">port number 1045</span><br><span class="line">port number 1041</span><br><span class="line">port number 1047  --用完所有奇数端口</span><br><span class="line"></span><br><span class="line">$./bind &amp;    --继续挑选偶数端口</span><br><span class="line">[8] 4170</span><br><span class="line">port number 1044</span><br><span class="line">port number 1042</span><br><span class="line">port number 1046</span><br><span class="line">port number 0    --实在没有了</span><br><span class="line">port number 0</span><br></pre></td></tr></table></figure>

<p>可见4.19内核下每次port是+2，在3.10内核版本中是+1. 并且都是递增的，同时即使port不使用，也会随着时间的变化这个起始port增大。</p>
<p>Port Range有点像雷达转盘数字，时间就像是雷达上的扫描指针，这个指针不停地旋转，如果这个时候刚好有应用要申请Port，那么就从指针正好指向的Port开始向后搜索可用port</p>
<h2 id="tcp-max-tw-buckets"><a href="#tcp-max-tw-buckets" class="headerlink" title="tcp_max_tw_buckets"></a>tcp_max_tw_buckets</h2><p>tcp_max_tw_buckets: 在 TIME_WAIT 数量等于 tcp_max_tw_buckets 时，新的连接断开不再进入TIME_WAIT阶段，而是直接断开，并打印warnning.</p>
<p>实际测试发现 在 TIME_WAIT 数量等于 tcp_max_tw_buckets 时 新的连接仍然可以不断地创建和断开，这个参数大小不会影响性能，只是影响TIME_WAIT 数量的展示（当然 TIME_WAIT 太多导致local port不够除外）, 这个值设置小一点会避免出现端口不够的情况</p>
<blockquote>
<p>tcp_max_tw_buckets - INTEGER<br>	Maximal number of timewait sockets held by system simultaneously.If this number is exceeded time-wait socket is immediately destroyed and warning is printed. This limit exists only to prevent simple DoS attacks, you <em>must</em> not lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than default value.</p>
</blockquote>
<p>监控指标：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -s | grep TCPTimeWaitOverflow</span><br></pre></td></tr></table></figure>



<h2 id="SO-LINGER"><a href="#SO-LINGER" class="headerlink" title="SO_LINGER"></a><a target="_blank" rel="noopener" href="https://notes.shichao.io/unp/ch7/">SO_LINGER</a></h2><p>SO_LINGER选项<strong>用来设置延迟关闭的时间，等待套接字发送缓冲区中的数据发送完成</strong>。 没有设置该选项时，在调用close() 后，在发送完FIN后会立即进行一些清理工作并返回。 如果设置了SO_LINGER选项，并且等待时间为正值，则在清理之前会等待一段时间。</p>
<p>如果把延时设置为 0  时，Socket就丢弃数据，并向对方发送一个 <code>RST</code> 来终止连接，因为走的是 RST 包，所以就不会有 <code>TIME_WAIT</code> 了。</p>
<blockquote>
<p>This option specifies how the <code>close</code> function operates for a connection-oriented protocol (for TCP, but not for UDP). By default, <code>close</code> returns immediately, but &#x3D;&#x3D;if there is any data still remaining in the socket send buffer, the system will try to deliver the data to the peer&#x3D;&#x3D;.</p>
</blockquote>
<p>SO_LINGER 有三种情况</p>
<ol>
<li>l_onoff 为false（0）， 那么 l_linger 的值没有意义，socket主动调用close时会立即返回，操作系统会将残留在缓冲区中的数据发送到对端，并按照正常流程关闭(交换FIN-ACK），最后连接进入<code>TIME_WAIT</code>状态。<strong>这是默认情况</strong></li>
<li>l_onoff 为true（非0），  l_linger 为0，主动调用close的一方也是立刻返回，但是这时TCP会丢弃发送缓冲中的数据，而且不是按照正常流程关闭连接（不发送FIN包），直接发送<code>RST</code>，连接不会进入 time_wait 状态，对端会收到 <code>java.net.SocketException: Connection reset</code>异常</li>
<li>l_onoff 为true（非0），  l_linger 也为非 0，这表示 <code>SO_LINGER</code>选项生效，并且超时时间大于零，这时调用close的线程被阻塞，TCP会发送缓冲区中的残留数据，这时有两种可能的情况：<ul>
<li>数据发送完毕，收到对方的ACK，然后进行连接的正常关闭（交换FIN-ACK）</li>
<li>超时，未发送完(指没收到对端的 ACK)的数据被丢弃，发送<code>RST</code>进行非正常关闭</li>
</ul>
</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">struct linger &#123;</span><br><span class="line">  int   l_onoff;        /* 0=off, nonzero=on */</span><br><span class="line">  int   l_linger;       /* linger time, POSIX specifies units as seconds */</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>

<h3 id="NIO下设置-SO-LINGER-的错误案例"><a href="#NIO下设置-SO-LINGER-的错误案例" class="headerlink" title="NIO下设置 SO_LINGER 的错误案例"></a>NIO下设置 SO_LINGER 的错误案例</h3><p>在使用NIO时，最好不设置<code>SO_LINGER</code>。比如Tomcat服务端接收到请求创建新连接时，做了这样的设置：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SocketChannel.setOption(SocketOption.SO_LINGER, 1000)</span><br></pre></td></tr></table></figure>

<p><code>SO_LINGER</code>的单位为<code>秒</code>！在网络环境比较好的时候，例如客户端、服务器都部署在同一个机房，close虽然会被阻塞，但时间极短可以忽略。但当网络环境不那么好时，例如存在丢包、较长的网络延迟，buffer中的数据一直无法发送成功，那么问题就出现了：<code>close会被阻塞较长的时间，从而直接或间接引起NIO的IO线程被阻塞</code>，服务器会不响应，不能处理accept、read、write等任何IO事件。也就是应用频繁出现挂起现象。解决方法就是删掉这个设置，close时立即返回，由操作系统接手后面的工作。</p>
<p>被阻塞时会看到如下连接状态：</p>
<p><img src="/images/951413iMgBlog/image-20220721100246598.png" alt="image-20220721100246598"></p>
<p>以及对应的堆栈</p>
<p><img src="/images/951413iMgBlog/image-20220721100421130.png" alt="image-20220721100421130"></p>
<p>查看其中一个IO线程等待的锁，发现锁是被HTTP线程持有。这个线程正在执行<code>preClose0</code>，就是在这里等待连接的关闭<img src="/images/951413iMgBlog/image-20220721100446521.png" alt="image-20220721100446521"></p>
<p>每次HTTP线程在关闭连接被阻塞时，同时持有了<code>SocketChannelImpl</code>的对象锁，而 IO线程在把这个连接移除出它的 selector管理队列时，也要获得同一个<code>SocketChannelImpl</code>的对象锁。IO 线程就这么一次次的被阻塞，悲剧的无以复加。有些 NIO框架会让 IO线程去做close，这时候就更加悲剧了。</p>
<p><strong>总之这里的错误原因有两点：1）网络状态不好；2）错误理解了l_linger 的单位，是秒，不是毫秒。 在这两个原因的共同作用下导致了数据迟迟不能发送完毕，l_linger 超时又需要很久，所以服务会出现一直阻塞的状态。</strong></p>
<h2 id="为什么要有-time-wait-状态"><a href="#为什么要有-time-wait-状态" class="headerlink" title="为什么要有 time_wait 状态"></a>为什么要有 time_wait 状态</h2><blockquote>
<p>TIME-WAIT - represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request.</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20220721093116395.png" alt="alt text"></p>
<h2 id="由Nginx-SY-CPU高负载引发内核探索之旅"><a href="#由Nginx-SY-CPU高负载引发内核探索之旅" class="headerlink" title="由Nginx SY CPU高负载引发内核探索之旅"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/njpdTW5TndO4-H7nbEpXAA">由Nginx SY CPU高负载引发内核探索之旅</a></h2><p>这个案例来自腾讯7层网关团队，网关用的Nginx，请求转发给后面的被代理机器(RS:real server)，发现 sys CPU异常高，CPU都用在搜索可用端口.</p>
<p><img src="/images/951413iMgBlog/640-8259033.png" alt="Image"></p>
<p><img src="/images/951413iMgBlog/640-20221112211814567.png" alt="Image"></p>
<p><img src="/images/951413iMgBlog/image7.png" alt="Figure 4: This is a flame graph of the connect syscall in Linux."> </p>
<p>local port 不够的时候inet_hash_connect 中的spin_lock 会消耗过高的 sys（特别注意4.6内核后 local port 分奇偶数，每次loop+2，所以更容易触发port不够的场景）</p>
<p>核心原因总结: 4.6后内核把本地端口分成奇偶数，奇数给connect, 偶数给listen，本来端口有6万，这样connect只剩下3万，当这3万用完后也不会报找不到本地可用端口的错误(这里报错可能更好)，而是在奇数里找不到就找偶数里的，每次都这样。 没改以前，总共6万端口，用掉3万，不分奇偶的话那么每找两个端口就有一个能用，也就是50%的概率。但是改了新的实现方案后，每次先要找奇数的3万个，全部在用，然后到偶数里继续找到第30001个才是可用的，也就是找到的概率变成了3万分之一，一下子复杂度高了15000倍，不慢才怪</p>
<p>对这个把端口分成奇偶数我的看法：这个做法就是坑爹货，在内核里胡乱搞，为了一个小场景搞崩大多数正常场景，真没必要，当然我这是事后诸葛亮，如果当时这种feature拿给我看我也会认为很不错，想不到这个坑点！</p>
<h2 id="从STGW流量下降探秘内核收包机制"><a href="#从STGW流量下降探秘内核收包机制" class="headerlink" title="从STGW流量下降探秘内核收包机制"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5ODYwMjI2MA==&mid=2649745268&idx=1&sn=f72f164847060d7b19cba272a38485e5&scene=21#wechat_redirect">从STGW流量下降探秘内核收包机制</a></h2><p>listen port search消耗CPU异常高</p>
<p><img src="/images/951413iMgBlog/640-9840722.jpeg" alt="图片"></p>
<p>在正常的情况下，服务器的listen port数量，大概就是几w个这样的量级。这种量级下，一个port对应一个socket，哈希桶大小为32是可以接受的。</p>
<p>然而在内核支持了reuseport并且被广泛使用后，情况就不一样了，**在多进程架构里，listen port对应的socket数量，是会被几十倍的放大的。*<em>以应用层监听了5000个端口，reuseport 使用了50个cpu核心为例，5000</em>50&#x2F;32约等于7812，意味着每次握手包到来时，光是查找listen socket，就需要遍历7800多次。随着机器硬件性能越来越强，应用层使用的cpu数量增多，这个问题还会继续加剧。</p>
<p><strong>正因为上述原因，并且我们现网机器开启了reuseport，在端口数量较多的机器里，inet_lookup_listener的哈希桶大小太小，遍历过程消耗了cpu，导致出现了函数热点。</strong></p>
<h2 id="短连接的开销"><a href="#短连接的开销" class="headerlink" title="短连接的开销"></a>短连接的开销</h2><p>用ab通过短连接走 lo 网卡压本机 nginx，CPU0是 ab 进程，CPU3&#x2F;4 是 Nginx 服务，可以看到 si 非常高，QPS 2.2万</p>
<p><img src="/images/951413iMgBlog/image-20220627154822263.png" alt="image-20220627154822263"></p>
<p>再将 ab 改用长连接来压，可以看到si、sy都有下降，并且 si 下降到短连接的20%，QPS 还能提升到 5.2万</p>
<p><img src="/images/951413iMgBlog/image-20220627154931495.png" alt="image-20220627154931495"></p>
<h2 id="一条连接的开销"><a href="#一条连接的开销" class="headerlink" title="一条连接的开销"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/BwddYkVLSYlkKFNeA-NUVg">一条连接的开销</a></h2><p>主要是内存开销(如图，来源见水印)，另外就是每个连接都会占用一个文件句柄，可以通过参数来设置：fs.nr_open、nofile（其实 nofile 还分 soft 和 hard） 和 fs.file-max</p>
<p><img src="/images/951413iMgBlog/640-20220413134252639" alt="Image"></p>
<p>从上图可以看到：</p>
<ul>
<li><p>没有收发数据的时候收发buffer不用提前分配，3K多点的内存是指一个连接的元信息数据空间，不包含传输数据的内存buffer</p>
</li>
<li><p>客户端发送数据后，会根据数据大小分配send buffer（一般不超过wmem，默认kernel会根据系统内存压力来调整send buffer大小)</p>
</li>
<li><p>server端kernel收到数据后存放在rmem中，应用读走后就会释放对应的rmem</p>
</li>
<li><p>rmem和wmem都不会重用，用时分配用完释放</p>
</li>
</ul>
<p>可见，内核在 socket 内存开销优化上采取了不少方法:</p>
<ul>
<li>内核会尽量及时回收发送缓存区、接收缓存区，但高版本做的更好</li>
<li>发送接收缓存区最小并一定不是 rmem 内核参数里的最小值，实际大部分时间都是0</li>
<li>其它状态下，例如对于TIME_WAIT还会回收非必要的 socket_alloc 等对象</li>
</ul>
<p>或者看这篇分析：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/25241630">https://zhuanlan.zhihu.com/p/25241630</a> </p>
<h2 id="不同进程使用相同端口，设置SO-REUSEADDR后被bind-导致可用-local-port-不够"><a href="#不同进程使用相同端口，设置SO-REUSEADDR后被bind-导致可用-local-port-不够" class="headerlink" title="不同进程使用相同端口，设置SO_REUSEADDR后被bind  导致可用 local port 不够"></a><a target="_blank" rel="noopener" href="https://ata.alibaba-inc.com/articles/251853">不同进程使用相同端口，设置SO_REUSEADDR后被bind  导致可用 local port 不够</a></h2><p>A进程选择某个端口当local port 来connect，并设置了 reuseaddr opt（表示其它进程还能继续用这个端口），这时B进程选了这个端口，并且bind了，B进程用完后把这个bind的端口释放了，但是如果 A 进程一直不释放这个端口对应的连接，那么这个端口会一直在内核中记录被bind用掉了（能bind的端口 是65535个，四元组不重复的连接你理解可以无限多），这样的端口越来越多后，剩下可供 A 进程发起连接的本地随机端口就越来越少了(也就是本来A进程选择端口是按四元组的，但因为前面所说的原因，导致不按四元组了，只按端口本身这个一元组来排重)，这时会造成新建连接的时候这个四元组高概率重复，一般这个时候对端大概率还在 time_wait 状态，会忽略掉握手 syn 包并回复 ack ，进而造成建连接卡顿的现象；超频繁的端口复用在LVS 场景下会产生问题，导致建连异常；或者syn包被 RST 触发1秒钟重传 syn</p>
<p>这个A、B进程共同跑在一台宿主机上很多年了，只因为之前是3.10内核，这次升级到了4.19后因为奇偶数放大了问题</p>
<p>当A进程已经开启了 SO_REUSEADDR 对外建联，此时 B 进程同样开启 SO_REUSEADDR 可以bind 此端口成功，当前端口就被设置为bind 状态，其他非 SO_REUSEADDR 的建联无法选到此端口</p>
<h3 id="验证端口被connect-和-listen-同时使用"><a href="#验证端口被connect-和-listen-同时使用" class="headerlink" title="验证端口被connect 和 listen 同时使用"></a>验证端口被connect 和 listen 同时使用</h3><p>尝试先用 connect 把18181 端口用掉，然后在18181端口上起一个listen 服务，再从其他地方连这个listen的 18181端口</p>
<p><img src="/images/951413iMgBlog/image-20240522103927360.png" alt="image-20240522103927360"></p>
<p>抓包，本机 ip 是 172.17.151.5 ：</p>
<p><img src="/images/951413iMgBlog/image-20240522103407831.png" alt="image-20240522103407831"></p>
<p>抓包里的 stream 1 对应上图的connect to baidu.com:80 </p>
<p>抓包里的 stream 2 对应其它客户端连listen 18181上的服务，对应的netstat 信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#netstat -anpo |grep 18181</span><br><span class="line">0.0.0.0:18181           0.0.0.0:*               LISTEN      2732449/nc           off (0.00/0/0)</span><br><span class="line">172.17.151.5:18181      19.12.59.7:56166        ESTABLISHED 2732449/nc           off (0.00/0/0) (stream2)</span><br><span class="line">172.17.151.5:18181      110.242.68.66:80        ESTABLISHED 2732445/python       keepalive (4.96/0/0)（stream1）</span><br><span class="line">172.17.151.5:18181      10.143.33.49:123        ESTABLISHED 624/chronyd          off (0.00/0/0)</span><br></pre></td></tr></table></figure>

<p>可以得出如下结论：</p>
<ul>
<li><p>两个TCP 连接四元组不一样，互相不干涉</p>
</li>
<li><p>先connect(SO_REUSEADDR) 用掉A端口后，还可以在上面继续使用A 端口来 listen(nc -l 18181)</p>
</li>
<li><p>先 listen 再connect 是不行的，报：Cannot assign requested address</p>
</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>在内存、文件句柄足够的话一台服务器上可以创建的TCP连接数量是没有限制的</li>
<li>SO_REUSEADDR 主要用于快速重用 TIME_WAIT状态的TCP端口，避免服务重启就会抛出Address Already in use的错误</li>
<li>先起一个listen 的端口设置了 SO_REUSEADDR，在其它进程 connect 的时候也不会从 port range 里再被选出来重用</li>
<li>SO_REUSEPORT主要用来解决惊群、性能等问题</li>
<li>全局范围可以用 net.ipv4.tcp_max_tw_buckets &#x3D; 50000 来限制总 time_wait 数量，但是会掩盖问题</li>
<li>local port的选择是递增搜索的，搜索起始port随时间增加也变大</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000002396411">https://segmentfault.com/a/1190000002396411</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a364572/article/details/40628171">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a target="_blank" rel="noopener" href="https://ops.tips/blog/how-linux-tcp-introspection/">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a target="_blank" rel="noopener" href="https://idea.popcount.org/2014-04-03-bind-before-connect/">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/C-Eeoeh9GHxugF4J30fz1A">TCP连接中客户端的端口号是如何确定的？</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/plantegg/linux/commit/9b3312bf18f6873e67f1f51dab3364c95c9dc54c">对应4.19内核代码解析</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.cloudflare.com/how-to-stop-running-out-of-ephemeral-ports-and-start-to-love-long-lived-connections/">How to stop running out of ephemeral ports and start to love long-lived connections</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.cloudflare.com/how-to-stop-running-out-of-ephemeral-ports-and-start-to-love-long-lived-connections/">https://blog.cloudflare.com/how-to-stop-running-out-of-ephemeral-ports-and-start-to-love-long-lived-connections/</a></p>
<p>connect() why you so slow?<a target="_blank" rel="noopener" href="https://blog.cloudflare.com/linux-transport-protocol-port-selection-performance">https://blog.cloudflare.com/linux-transport-protocol-port-selection-performance</a>  <a target="_blank" rel="noopener" href="https://lpc.events/event/17/contributions/1593/attachments/1208/2472/lpc-2023-connect-why-you-so-slow.pdf?file=lpc-2023-connect-why-you-so-slow.pdf">https://lpc.events/event/17/contributions/1593/attachments/1208/2472/lpc-2023-connect-why-you-so-slow.pdf?file=lpc-2023-connect-why-you-so-slow.pdf</a> </p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Linux/" rel="tag"># Linux</a>
              <a href="/tags/TCP/" rel="tag"># TCP</a>
              <a href="/tags/SO-REUSEADDR/" rel="tag"># SO_REUSEADDR</a>
              <a href="/tags/ip-local-port-range/" rel="tag"># ip_local_port_range</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/23/%E4%B8%80%E6%AC%A1%E6%98%A5%E8%8A%82%E5%A4%A7%E4%BF%83%E6%80%A7%E8%83%BD%E5%8E%8B%E6%B5%8B%E4%B8%8D%E8%BE%BE%E6%A0%87%E7%9A%84%E7%93%B6%E9%A2%88%E6%8E%A8%E6%BC%94/" rel="prev" title="一次春节大促性能压测不达标的瓶颈推演">
                  <i class="fa fa-angle-left"></i> 一次春节大促性能压测不达标的瓶颈推演
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/12/25/%E4%B8%80%E4%B8%AA%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E9%97%AE%E9%A2%98/" rel="next" title="一个有意思的问题">
                  一个有意思的问题 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
