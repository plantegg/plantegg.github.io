<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Linux内存–HugePage本系列有如下几篇 [Linux 内存问题汇总](&#x2F;2020&#x2F;01&#x2F;15&#x2F;Linux 内存问题汇总&#x2F;) Linux内存–PageCache Linux内存–管理和碎片 Linux内存–HugePage Linux内存–零拷贝 &#x2F;proc&#x2F;buddyinfo&#x2F;proc&#x2F;buddyi">
<meta property="og:type" content="article">
<meta property="og:title" content="Linux内存--HugePage">
<meta property="og:url" content="https://plantegg.github.io/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="Linux内存–HugePage本系列有如下几篇 [Linux 内存问题汇总](&#x2F;2020&#x2F;01&#x2F;15&#x2F;Linux 内存问题汇总&#x2F;) Linux内存–PageCache Linux内存–管理和碎片 Linux内存–HugePage Linux内存–零拷贝 &#x2F;proc&#x2F;buddyinfo&#x2F;proc&#x2F;buddyi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/false_sharing.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20240116134744487.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/7a26deaf96bdcc07db4db34ae1178641.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/3385ae6ffbd5b48b80efa759f42b8174.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/329769dd1da2ed324ac11b8b922382cd.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20210628144121108.png">
<meta property="article:published_time" content="2020-11-15T08:30:03.000Z">
<meta property="article:modified_time" content="2025-11-16T11:58:49.520Z">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="HugePage">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="Memory">
<meta property="article:tag" content="free">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plantegg.github.io/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png">


<link rel="canonical" href="https://plantegg.github.io/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://plantegg.github.io/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/","path":"2020/11/15/Linux内存--HugePage/","title":"Linux内存--HugePage"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Linux内存--HugePage | plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">plantegg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Linux%E5%86%85%E5%AD%98%E2%80%93HugePage"><span class="nav-number">1.</span> <span class="nav-text">Linux内存–HugePage</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#proc-buddyinfo"><span class="nav-number">1.1.</span> <span class="nav-text">&#x2F;proc&#x2F;buddyinfo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#slabtop%E5%92%8C-proc-slabinfo"><span class="nav-number">1.2.</span> <span class="nav-text">slabtop和&#x2F;proc&#x2F;slabinfo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8Ehugetlb"><span class="nav-number">1.3.</span> <span class="nav-text">关于hugetlb</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HugePage"><span class="nav-number">1.4.</span> <span class="nav-text">HugePage</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E9%A2%84%E7%95%99%E5%A4%A7%E9%A1%B5"><span class="nav-number">1.4.1.</span> <span class="nav-text">1.预留大页</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%8C%82%E8%BD%BDhugetlb%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.4.2.</span> <span class="nav-text">2.挂载hugetlb文件系统</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%98%A0%E5%B0%84hugetbl%E6%96%87%E4%BB%B6"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.映射hugetbl文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-hugepage%E7%BB%9F%E8%AE%A1%E4%BF%A1%E6%81%AF"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.hugepage统计信息</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-hugpage%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-number">1.4.5.</span> <span class="nav-text">5 hugpage优缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B7%A5%E5%85%B7"><span class="nav-number">1.4.6.</span> <span class="nav-text">工具</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A4%A7%E9%A1%B5%E5%92%8C-MySQL-%E6%80%A7%E8%83%BD-case"><span class="nav-number">1.4.7.</span> <span class="nav-text">大页和 MySQL 性能 case</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HugePage-%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-number">1.4.8.</span> <span class="nav-text">HugePage 带来的问题</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#CPU%E5%AF%B9%E5%90%8C%E4%B8%80%E4%B8%AAPage%E6%8A%A2%E5%8D%A0%E5%A2%9E%E5%A4%9A"><span class="nav-number">1.4.8.1.</span> <span class="nav-text">CPU对同一个Page抢占增多</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%BF%9E%E7%BB%AD%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81%E8%B7%A8CPU%E8%AF%BB%E5%8F%96"><span class="nav-number">1.4.8.2.</span> <span class="nav-text">连续数据需要跨CPU读取</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Java%E8%BF%9B%E7%A8%8B%E5%BC%80%E5%90%AFHugePage"><span class="nav-number">1.4.9.</span> <span class="nav-text">Java进程开启HugePage</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9JVM%E5%90%AF%E5%8A%A8%E5%8F%82%E6%95%B0"><span class="nav-number">1.4.9.1.</span> <span class="nav-text">修改JVM启动参数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BF%AE%E6%94%B9%E6%9C%BA%E5%99%A8%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE"><span class="nav-number">1.4.9.2.</span> <span class="nav-text">修改机器系统配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#code-hugepage-%E4%BB%A3%E7%A0%81%E5%A4%A7%E9%A1%B5"><span class="nav-number">1.4.10.</span> <span class="nav-text">code_hugepage 代码大页</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#THP"><span class="nav-number">1.5.</span> <span class="nav-text">THP</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#THP-%E5%8E%9F%E7%90%86"><span class="nav-number">1.5.1.</span> <span class="nav-text">THP 原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#THP%E5%92%8Cperf"><span class="nav-number">1.5.2.</span> <span class="nav-text">THP和perf</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MySQL-%E5%9C%BA%E6%99%AF%E4%B8%8B%E4%BB%A3%E7%A0%81%E5%A4%A7%E9%A1%B5%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">1.6.</span> <span class="nav-text">MySQL 场景下代码大页对性能的影响</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TLAB-miss%E9%AB%98%E7%9A%84%E6%A1%88%E4%BE%8B"><span class="nav-number">1.7.</span> <span class="nav-text">TLAB miss高的案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A2%8E%E7%89%87%E5%8C%96"><span class="nav-number">1.8.</span> <span class="nav-text">碎片化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bpagetypeinfo"><span class="nav-number">1.8.1.</span> <span class="nav-text">查看pagetypeinfo</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E5%AD%98%E7%A2%8E%E7%89%87%E5%8C%96%E5%AF%BC%E8%87%B4rt%E5%8D%87%E9%AB%98%E7%9A%84%E8%AF%8A%E6%96%AD"><span class="nav-number">1.9.</span> <span class="nav-text">内存碎片化导致rt升高的诊断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.10.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">185</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">274</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Linux内存--HugePage | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Linux内存--HugePage
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-15 16:30:03" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">2020-11-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Memory/" itemprop="url" rel="index"><span itemprop="name">Memory</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Linux内存–HugePage"><a href="#Linux内存–HugePage" class="headerlink" title="Linux内存–HugePage"></a>Linux内存–HugePage</h1><p>本系列有如下几篇</p>
<p>[Linux 内存问题汇总](&#x2F;2020&#x2F;01&#x2F;15&#x2F;Linux 内存问题汇总&#x2F;)</p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E7%AE%A1%E7%90%86%E5%92%8C%E7%A2%8E%E7%89%87/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E9%9B%B6%E6%8B%B7%E8%B4%9D/">Linux内存–零拷贝</a></p>
<h2 id="proc-buddyinfo"><a href="#proc-buddyinfo" class="headerlink" title="&#x2F;proc&#x2F;buddyinfo"></a>&#x2F;proc&#x2F;buddyinfo</h2><p>&#x2F;proc&#x2F;buddyinfo记录了内存的详细碎片情况。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/buddyinfo </span><br><span class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </span><br><span class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </span><br><span class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</span><br></pre></td></tr></table></figure>

<p>Normal行的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<h2 id="slabtop和-proc-slabinfo"><a href="#slabtop和-proc-slabinfo" class="headerlink" title="slabtop和&#x2F;proc&#x2F;slabinfo"></a>slabtop和&#x2F;proc&#x2F;slabinfo</h2><p>slabtop和&#x2F;proc&#x2F;slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<h2 id="关于hugetlb"><a href="#关于hugetlb" class="headerlink" title="关于hugetlb"></a>关于hugetlb</h2><p>This is an entry in the TLB that points to a HugePage (a large&#x2F;big page larger than regular 4K and predefined in size). HugePages are implemented via hugetlb entries, i.e. we can say that a HugePage is handled by a “hugetlb page entry”. The ‘hugetlb” term is also (and mostly) used synonymously with a HugePage.</p>
<p> hugetlb 是TLB中指向HugePage的一个entry(通常大于4k或预定义页面大小)。 HugePage 通过hugetlb entries来实现，也可以理解为HugePage 是hugetlb page entry的一个句柄。</p>
<p><strong>Linux下的大页分为两种类型：标准大页（Huge Pages）和透明大页（Transparent Huge Pages）</strong></p>
<p>标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式</p>
<p>目前透明大页与传统HugePages联用会出现一些问题，导致性能问题和系统重启。Oracle 建议禁用透明大页（Transparent Huge Pages）</p>
<p>hugetlbfs比THP要好，开thp的机器碎片化严重（不开THP会有更严重的碎片化问题），最后和没开THP一样 <a target="_blank" rel="noopener" href="https://www.atatech.org/articles/152660">https://www.atatech.org/articles/152660</a></p>
<p>Linux 中的 HugePages 都被锁定在内存中，所以哪怕是在系统内存不足时，它们也不会被 Swap 到磁盘上，这也就能从根源上杜绝了重要内存被频繁换入和换出的可能。</p>
<blockquote>
<p><strong>Transparent Hugepages</strong> are similar to standard <strong>HugePages</strong>. However, while standard <strong>HugePages</strong> allocate memory at startup, <strong>Transparent Hugepages</strong> memory uses the khugepaged thread in the kernel to allocate memory dynamically during runtime, using swappable <strong>HugePages</strong>.</p>
</blockquote>
<p>HugePage要求OS启动的时候提前分配出来，管理难度比较大，所以Enterprise Linux 6增加了一层抽象层来动态创建管理HugePage，这就是THP，而这个THP对应用透明，由khugepaged thread在后台动态将小页组成大页给应用使用，这里会遇上碎片问题导致需要compact才能得到大页，应用感知到的就是SYS CPU飙高，应用卡顿了。</p>
<p>虽然 HugePages 的开启大都需要开发或者运维工程师的额外配置，但是在应用程序中启用 HugePages 却可以在以下几个方面降低内存页面的管理开销：</p>
<ul>
<li>更大的内存页能够减少内存中的页表层级，这不仅可以降低页表的内存占用，也能降低从虚拟内存到物理内存转换的性能损耗；</li>
<li>更大的内存页意味着更高的缓存命中率，CPU 有更高的几率可以直接在 TLB（Translation lookaside buffer）中获取对应的物理地址；</li>
<li>更大的内存页可以减少获取大内存的次数，使用 HugePages 每次可以获取 2MB 的内存，是 4KB 的默认页效率的 512 倍；</li>
</ul>
<h2 id="HugePage"><a href="#HugePage" class="headerlink" title="HugePage"></a>HugePage</h2><p><strong>为什么需要Huge Page</strong> 了解CPU Cache大致架构的话，一定听过TLB Cache。<code>Linux</code>系统中，对程序可见的，可使用的内存地址是<code>Virtual Address</code>。每个程序的内存地址都是从0开始的。而实际的数据访问是要通过<code>Physical Address</code>进行的。因此，每次内存操作，CPU都需要从<code>page table</code>中把<code>Virtual Address</code>翻译成对应的<code>Physical Address</code>，那么对于大量内存密集型程序来说<code>page table</code>的查找就会成为程序的瓶颈。</p>
<p>所以现代CPU中就出现了TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。然而由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p>Huge pages require contiguous areas of memory, so allocating them at boot is the most reliable method since memory has not yet become fragmented. To do so, add the following parameters to the kernel boot command line:</p>
<p><strong>Huge pages kernel options</strong></p>
<ul>
<li><p>hugepages</p>
<p>Defines the number of persistent huge pages configured in the kernel at boot time. The default value is <code>0</code>. It is only possible to allocate (or deallocate) huge pages if there are sufficient physically contiguous free pages in the system. Pages reserved by this parameter cannot be used for other purposes.</p>
<p>Default size huge pages can be dynamically allocated or deallocated by changing the value of the <code>/proc/sys/vm/nr_hugepages</code> file.</p>
<p>In a NUMA system, huge pages assigned with this parameter are divided equally between nodes. You can assign huge pages to specific nodes at runtime by changing the value of the node’s <code>/sys/devices/system/node/node_id/hugepages/hugepages-1048576kB/nr_hugepages</code> file.</p>
<p>For more information, read the relevant kernel documentation, which is installed in <code>/usr/share/doc/kernel-doc-kernel_version/Documentation/vm/hugetlbpage.txt</code> by default. This documentation is available only if the <em>kernel-doc</em> package is installed.</p>
</li>
<li><p>hugepagesz</p>
<p>Defines the size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
<li><p>default_hugepagesz</p>
<p>Defines the default size of persistent huge pages configured in the kernel at boot time. Valid values are 2 MB and 1 GB. The default value is 2 MB.</p>
</li>
</ul>
<p>应用程序想要利用大页优势，需要通过hugetlb文件系统来使用标准大页。<a target="_blank" rel="noopener" href="https://ata.alibaba-inc.com/articles/208718">操作步骤</a></p>
<h4 id="1-预留大页"><a href="#1-预留大页" class="headerlink" title="1.预留大页"></a>1.预留大页</h4><p>echo 20 &gt; &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;hugepages&#x2F;hugepages-2048kB&#x2F;nr_hugepages</p>
<h4 id="2-挂载hugetlb文件系统"><a href="#2-挂载hugetlb文件系统" class="headerlink" title="2.挂载hugetlb文件系统"></a>2.挂载hugetlb文件系统</h4><p>mount hugetlbfs &#x2F;mnt&#x2F;huge -t hugetlbfs</p>
<h4 id="3-映射hugetbl文件"><a href="#3-映射hugetbl文件" class="headerlink" title="3.映射hugetbl文件"></a>3.映射hugetbl文件</h4><p>fd &#x3D; open(“&#x2F;mnt&#x2F;huge&#x2F;test.txt”, O_CREAT|O_RDWR);</p>
<p>addr &#x3D; mmap(0, MAP_LENGTH, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);</p>
<h4 id="4-hugepage统计信息"><a href="#4-hugepage统计信息" class="headerlink" title="4.hugepage统计信息"></a>4.hugepage统计信息</h4><p>通过hugepage提供的sysfs接口，可以了解大页使用情况</p>
<p>HugePages_Total: 预先分配的大页数量</p>
<p>HugePages_Free：空闲大页数量</p>
<p>HugePages_Rsvd: mmap申请大页数量(还没有产生缺页)</p>
<p>HugePages_Surp: 多分配的大页数量(由nr_overcommit_hugepages决定)</p>
<h4 id="5-hugpage优缺点"><a href="#5-hugpage优缺点" class="headerlink" title="5 hugpage优缺点"></a>5 hugpage优缺点</h4><p>缺点:</p>
<p>1.需要提前预估大页使用量，并且预留的大页不能被其他内存分配接口使用。</p>
<p>2.兼容性不好，应用使用标准大页，需要对代码进行重构才能有效的使用标准大页。</p>
<p>优点:因为内存是预留的，缺页延时非常小</p>
<p>针对Hugepage的不足，内核又衍生出了THP大页(<strong>Transparent Huge pages)</strong></p>
<h3 id="工具"><a href="#工具" class="headerlink" title="工具"></a><a target="_blank" rel="noopener" href="https://www.golinuxcloud.com/configure-hugepages-vm-nr-hugepages-red-hat-7/">工具</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum install libhugetlbfs-utils -y</span><br><span class="line"></span><br><span class="line">//列出</span><br><span class="line">hugeadm --pool-list</span><br><span class="line">      Size  Minimum  Current  Maximum  Default</span><br><span class="line">   2097152    12850    12850    12850        *</span><br><span class="line">1073741824        0        0        0</span><br><span class="line"></span><br><span class="line">hugeadm --list-all-mounts</span><br><span class="line">Mount Point          Options</span><br><span class="line">/dev/hugepages       rw,relatime,pagesize=2M</span><br></pre></td></tr></table></figure>

<h3 id="大页和-MySQL-性能-case"><a href="#大页和-MySQL-性能-case" class="headerlink" title="大页和 MySQL 性能 case"></a>大页和 MySQL 性能 case</h3><p>MySQL的页都是16K, 当查询的行不在内存中时需要按照16K为单位从磁盘读取页,而文件系统中的页是4k，也就是一次数据库请求需要有4次磁盘IO，如过查询比较随机，每次只需要一个页中的几行数据，存在很大的读放大。</p>
<p>那么我们是否可以把MySQL的页设置为4K来减少读放大呢？</p>
<p>在5.7里收益不大，因为每次IO存在 fil_system 的锁，导致IO的并发上不去</p>
<p>8.0中总算优化了这个场景，测试细节可以参考<a target="_blank" rel="noopener" href="http://dimitrik.free.fr/blog/archives/2018/05/mysql-performance-1m-iobound-qps-with-80-ga-on-intel-optane-ssd.html">这篇</a></p>
<p>16K VS 4K 性能对比（4K接近翻倍）</p>
<p><img src="/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png" alt="img"></p>
<p>4K会带来的问题：顺序insert慢了10%（因为fsync更多了）；DDL更慢；二级索引更多的场景下4K性能较差；大BP下，刷脏代价大。</p>
<h3 id="HugePage-带来的问题"><a href="#HugePage-带来的问题" class="headerlink" title="HugePage 带来的问题"></a><a target="_blank" rel="noopener" href="http://cenalulu.github.io/linux/huge-page-on-numa/">HugePage 带来的问题</a></h3><h4 id="CPU对同一个Page抢占增多"><a href="#CPU对同一个Page抢占增多" class="headerlink" title="CPU对同一个Page抢占增多"></a>CPU对同一个Page抢占增多</h4><p>对于写操作密集型的应用，Huge Page会大大增加Cache写冲突的发生概率。由于CPU独立Cache部分的写一致性用的是<code>MESI协议</code>，写冲突就意味：</p>
<ul>
<li>通过CPU间的总线进行通讯，造成总线繁忙</li>
<li>同时也降低了CPU执行效率。</li>
<li>CPU本地Cache频繁失效</li>
</ul>
<p>类比到数据库就相当于，原来一把用来保护10行数据的锁，现在用来锁1000行数据了。必然这把锁在线程之间的争抢概率要大大增加。</p>
<h4 id="连续数据需要跨CPU读取"><a href="#连续数据需要跨CPU读取" class="headerlink" title="连续数据需要跨CPU读取"></a>连续数据需要跨CPU读取</h4><p>Page太大，更容易造成Page跨Numa&#x2F;CPU 分布。</p>
<p>从下图我们可以看到，原本在4K小页上可以连续分配，并因为较高命中率而在同一个CPU上实现locality的数据。到了Huge Page的情况下，就有一部分数据为了填充统一程序中上次内存分配留下的空间，而被迫分布在了两个页上。而在所在Huge Page中占比较小的那部分数据，由于在计算CPU亲和力的时候权重小，自然就被附着到了其他CPU上。那么就会造成：本该以热点形式存在于CPU2 L1或者L2 Cache上的数据，不得不通过CPU inter-connect去remote CPU获取数据。 假设我们连续申明两个数组，<code>Array A</code>和<code>Array B</code>大小都是1536K。内存分配时由于第一个Page的2M没有用满，因此<code>Array B</code>就被拆成了两份，分割在了两个Page里。而由于内存的亲和配置，一个分配在Zone 0，而另一个在Zone 1。那么当某个线程需要访问Array B时就不得不通过代价较大的Inter-Connect去获取另外一部分数据。</p>
<p><img src="/images/951413iMgBlog/false_sharing.png" alt="img"></p>
<h3 id="Java进程开启HugePage"><a href="#Java进程开启HugePage" class="headerlink" title="Java进程开启HugePage"></a><a target="_blank" rel="noopener" href="https://kstefanj.github.io/2021/05/19/large-pages-and-java.html">Java进程开启HugePage</a></h3><p>从perf数据来看压满后tlab miss比较高，得想办法降低这个值</p>
<h4 id="修改JVM启动参数"><a href="#修改JVM启动参数" class="headerlink" title="修改JVM启动参数"></a>修改JVM启动参数</h4><p>JVM启动参数增加如下三个(-XX:LargePageSizeInBytes&#x3D;2m, 这个一定要，有些资料没提这个，在我的JDK8.0环境必须要)：</p>
<blockquote>
<p>-XX:+UseLargePages -XX:LargePageSizeInBytes&#x3D;2m -XX:+UseHugeTLBFS</p>
</blockquote>
<h4 id="修改机器系统配置"><a href="#修改机器系统配置" class="headerlink" title="修改机器系统配置"></a>修改机器系统配置</h4><p>设置HugePage的大小</p>
<blockquote>
<p>cat &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;nr_hugepages</p>
</blockquote>
<p>nr_hugepages设置多大参考如下计算方法：</p>
<blockquote>
<p>If you are using the option <code>-XX:+UseSHM</code> or <code>-XX:+UseHugeTLBFS</code>, then specify the number of large pages. In the following example, 3 GB of a 4 GB system are reserved for large pages (assuming a large page size of 2048kB, then 3 GB &#x3D; 3 * 1024 MB &#x3D; 3072 MB &#x3D; 3072 * 1024 kB &#x3D; 3145728 kB and 3145728 kB &#x2F; 2048 kB &#x3D; 1536):</p>
<p>echo 1536 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;nr_hugepages </p>
</blockquote>
<p>透明大页是没有办法减少系统tlab，tlab是对应于进程的，系统分给进程的透明大页还是由物理上的4K page组成。</p>
<p>对于c++来说，他malloc经常会散落得全地址都是，因为会触发各种mmap，冷热区域。所以THP和hugepage都可能导致大量内存被浪费了，进而导致内存紧张，性能下滑。jvm的连续内存布局，加上gc会使得内存密度很紧凑。THP的问题是，他是逻辑页，不是物理页，tlb依旧要N份，所以他的收益来自page fault减少，是一次性的收益。</p>
<p>hugepage的在减少page_fault上和thp效果一样第二个作用是，他只需要一份TLB了，hugepage是真正的大页内存，thp是逻辑上的，物理上还是需要很多小的page。</p>
<p><strong>如果TLB miss，则可能需要额外三次内存读取操作才能将线性地址翻译为物理地址。</strong></p>
<h3 id="code-hugepage-代码大页"><a href="#code-hugepage-代码大页" class="headerlink" title="code_hugepage 代码大页"></a>code_hugepage 代码大页</h3><p>代码大页特性主要为大代码段业务服务，可以降低程序的iTLB miss，从而提升程序性能。针对倚天这一类跨numa访存开销大的芯片有比较好的性能提升效果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 1 表示仅打开二进制和动态库大页  2 仅打开可执行匿名大页 3 相当于1+2，0 表示关闭</span><br><span class="line">echo 1 &gt; /sys/kernel/mm/transparent_hugepage/hugetext_enabled //1 可以改成2/3</span><br></pre></td></tr></table></figure>

<p>是否启用代码大页，可以查看&#x2F;proc&#x2F;&#x2F;smaps中FilePmdMapped字段可确定是否使用了代码大页。 扫描进程代码大页使用数量（<strong>单位KB</strong>）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/&lt;pid&gt;/smaps | grep FilePmdMapped | awk &#x27;&#123;sum+=$2&#125;END&#123;print&quot;Sum= &quot;,sum&#125;&#x27;</span><br></pre></td></tr></table></figure>



<h2 id="THP"><a href="#THP" class="headerlink" title="THP"></a><a target="_blank" rel="noopener" href="https://ata.atatech.org/articles/11000208718">THP</a></h2><p>Linux kernel在2.6.38内核增加了Transparent Huge Pages (THP)特性 ，支持大内存页(2MB)分配，默认开启。当开启时可以降低fork子进程的速度，但fork之后，每个内存页从原来4KB变为2MB，会大幅增加重写期间父进程内存消耗。同时<strong>每次写命令引起的复制内存页单位放大了512倍</strong>，会拖慢写操作的执行时间，导致大量写操作慢查询。例如简单的incr命令也会出现在慢查询中。因此Redis日志中建议将此特性进行禁用。  </p>
<p>THP 的目的是用一个页表项来映射更大的内存（大页），这样可以减少 Page Fault，因为需要的页数少了。当然，这也会提升 TLB（Translation Lookaside Buffer，由存储器管理单元用于改进虚拟地址到物理地址的转译速度） 命中率，因为需要的页表项也少了。如果进程要访问的数据都在这个大页中，那么这个大页就会很热，会被缓存在 Cache 中。而大页对应的页表项也会出现在 TLB 中，从上一讲的存储层次我们可以知道，这有助于性能提升。但是反过来，假设应用程序的数据局部性比较差，它在短时间内要访问的数据很随机地位于不同的大页上，那么大页的优势就会消失。</p>
<p>大页在使用的时候需要清理512个4K页面，再返回给用户，这里的清理动作可能会导致卡顿。另外碎片化严重的时候触发内存整理造成卡顿</p>
<p>大页分配: 在缺页处理函数__handle_mm_fault中判断是否使用大页 大页生成: 主要通过在分配大页内存时是否带__GFP_DIRECT_RECLAIM 标志来控制大页的生成.﻿</p>
<p>1.异步生成大页: 在缺页处理中，把需要异步生成大页的VMA注册到链表，内核态线程k<strong>hugepage</strong>d 动态为vma分配大页(内存回收，内存归整)</p>
<p>2.madvise系统调用只是给VMA加了VM_<strong>HUGEPAGE,用来</strong>标记这段虚拟地址需要使用大页</p>
<p><img src="/images/951413iMgBlog/image-20240116134744487.png" alt="image-20240116134744487"></p>
<h3 id="THP-原理"><a href="#THP-原理" class="headerlink" title="THP 原理"></a>THP 原理</h3><p>大页分配: 在缺页处理函数__handle_mm_fault中判断是否使用大页 大页生成: 主要通过在分配大页内存时是否带__GFP_DIRECT_RECLAIM 标志来控制大页的生成.</p>
<p>1.异步生成大页: 在缺页处理中，把需要异步生成大页的VMA注册到链表，内核态线程k<strong>hugepage</strong>d 动态为vma分配大页(内存回收，内存归整)</p>
<p>2.madvise系统调用只是给VMA加了VM_<strong>HUGEPAGE,用来</strong>标记这段虚拟地址需要使用大页</p>
<p>THP 对redis、mongodb 这种cache类推荐关闭，对drds这种java应用最好打开</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">[always] madvise never</span><br><span class="line"></span><br><span class="line">grep &quot;Huge&quot; /proc/meminfo</span><br><span class="line">AnonHugePages:   1286144 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line"></span><br><span class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &#x27;&#123; if($2&gt;4) print $0&#125; &#x27; |  awk -F &quot;/&quot;  &#x27;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &#x27;</span><br><span class="line"></span><br><span class="line">$grep -e AnonHugePages  /proc/*/smaps | awk  &#x27;&#123; if($2&gt;4) print $0&#125; &#x27; |  awk -F &quot;/&quot;  &#x27;&#123;print $0; system(&quot;ps -fp &quot; $3)&#125; &#x27;</span><br><span class="line"></span><br><span class="line">//查看pagesize（默认4K） </span><br><span class="line">$getconf PAGESIZE</span><br></pre></td></tr></table></figure>

<p>在透明大页功能打开时，造成系统性能下降的主要原因可能是 <code>khugepaged</code> 守护进程。该进程会在（它认为）系统空闲时启动，扫描系统中剩余的空闲内存，并将普通 4k 页转换为大页。该操作会在内存路径中加锁，而该守护进程可能会在错误的时间启动扫描和转换大页的操作，从而影响应用性能。</p>
<p>此外，当缺页异常(page faults)增多时，透明大页会和普通 4k 页一样，产生同步内存压缩(direct compaction)操作，以节省内存。该操作是一个同步的内存整理操作，如果应用程序会短时间分配大量内存，内存压缩操作很可能会被触发，从而会对系统性能造成风险。<a target="_blank" rel="noopener" href="https://yq.aliyun.com/articles/712830">https://yq.aliyun.com/articles/712830</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#查看系统级别的 THP 使用情况，执行下列命令：</span><br><span class="line">cat /proc/meminfo  | grep AnonHugePages</span><br><span class="line">#类似地，查看进程级别的 THP 使用情况，执行下列命令：</span><br><span class="line">cat /proc/1730/smaps | grep AnonHugePages |grep -v &quot;0 kB&quot;</span><br><span class="line">#是否开启了hugepage</span><br><span class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">always [madvise] never</span><br></pre></td></tr></table></figure>

<p><code>/proc/sys/vm/nr_hugepages</code> 中存储的数据就是大页面的数量，虽然在默认情况下它的值都是 0，不过我们可以通过更改该文件的内容申请或者释放操作系统中的大页：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ echo 1 &gt; /proc/sys/vm/nr_hugepages</span><br><span class="line">$ cat /proc/meminfo | grep HugePages_</span><br><span class="line">HugePages_Total:       1</span><br><span class="line">HugePages_Free:        1</span><br></pre></td></tr></table></figure>

<h3 id="THP和perf"><a href="#THP和perf" class="headerlink" title="THP和perf"></a>THP和perf</h3><p>thp on后比off性能稳定好 10-15%，开启THP最显著的指标是 iTLB命中率显著提升了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">//on 419+E5-2682， thp never</span><br><span class="line">9,145,128,732      branch-instructions       #  229.068 M/sec                    (10.65%)</span><br><span class="line">       555,518,878      branch-misses             #    6.07% of all branches          (14.24%)</span><br><span class="line">     3,951,535,475      bus-cycles                #   98.979 M/sec                    (14.29%)</span><br><span class="line">       372,477,068      cache-misses              #    7.733 % of all cache refs      (14.34%)</span><br><span class="line">     4,816,702,013      cache-references          #  120.649 M/sec                    (14.36%)</span><br><span class="line">   114,521,174,305      cpu-cycles                #    2.869 GHz                      (14.36%)</span><br><span class="line">    48,969,565,344      instructions              #    0.43  insn per cycle           (17.93%)</span><br><span class="line">    98,728,666,922      ref-cycles                # 2472.967 M/sec                    (21.52%)</span><br><span class="line">         39,922.47 msec cpu-clock                 #    7.898 CPUs utilized</span><br><span class="line">     1,848,336,574      L1-dcache-load-misses     #   13.31% of all L1-dcache hits    (21.51%)</span><br><span class="line">    13,889,399,043      L1-dcache-loads           #  347.903 M/sec                    (21.51%)</span><br><span class="line">     7,055,617,648      L1-dcache-stores          #  176.730 M/sec                    (21.50%)</span><br><span class="line">     2,017,950,458      L1-icache-load-misses                                         (21.50%)</span><br><span class="line">        88,802,885      LLC-load-misses           #    9.86% of all LL-cache hits     (14.35%)</span><br><span class="line">       900,379,398      LLC-loads                 #   22.553 M/sec                    (14.33%)</span><br><span class="line">       162,711,813      LLC-store-misses          #    4.076 M/sec                    (7.13%)</span><br><span class="line">       419,869,955      LLC-stores                #   10.517 M/sec                    (7.14%)</span><br><span class="line">       553,257,955      branch-load-misses        #   13.858 M/sec                    (10.71%)</span><br><span class="line">     9,195,874,519      branch-loads              #  230.339 M/sec                    (14.29%)</span><br><span class="line">       176,112,524      dTLB-load-misses          #    1.28% of all dTLB cache hits   (14.29%)</span><br><span class="line">    13,739,965,115      dTLB-loads                #  344.160 M/sec                    (14.28%)</span><br><span class="line">        33,087,849      dTLB-store-misses         #    0.829 M/sec                    (14.28%)</span><br><span class="line">     6,992,863,588      dTLB-stores               #  175.158 M/sec                    (14.26%)</span><br><span class="line">       170,555,902      iTLB-load-misses          #  107.90% of all iTLB cache hits   (14.24%)</span><br><span class="line">       158,070,998      iTLB-loads                #    3.959 M/sec                    (14.24%)</span><br><span class="line"></span><br><span class="line">//on 419+E5-2682， thp always</span><br><span class="line">    12,958,974,094      branch-instructions       #  227.392 M/sec                    (10.68%)</span><br><span class="line">       850,468,837      branch-misses             #    6.56% of all branches          (14.27%)</span><br><span class="line">     5,639,495,284      bus-cycles                #   98.957 M/sec                    (14.29%)</span><br><span class="line">       526,744,798      cache-misses              #    7.324 % of all cache refs      (14.32%)</span><br><span class="line">     7,192,328,925      cache-references          #  126.204 M/sec                    (14.34%)</span><br><span class="line">   163,419,436,811      cpu-cycles                #    2.868 GHz                      (14.33%)</span><br><span class="line">    68,638,583,038      instructions              #    0.42  insn per cycle           (17.90%)</span><br><span class="line">   140,882,455,768      ref-cycles                # 2472.076 M/sec                    (21.48%)</span><br><span class="line">         56,987.52 msec cpu-clock                 #    7.932 CPUs utilized</span><br><span class="line">     2,471,392,118      L1-dcache-load-misses     #   12.69% of all L1-dcache hits    (21.47%)</span><br><span class="line">    19,480,914,771      L1-dcache-loads           #  341.833 M/sec                    (21.48%)</span><br><span class="line">    10,059,893,871      L1-dcache-stores          #  176.522 M/sec                    (21.46%)</span><br><span class="line">     3,184,073,065      L1-icache-load-misses                                         (21.46%)</span><br><span class="line">       128,467,945      LLC-load-misses           #   10.83% of all LL-cache hits     (14.31%)</span><br><span class="line">     1,186,653,892      LLC-loads                 #   20.822 M/sec                    (14.30%)</span><br><span class="line">       224,877,539      LLC-store-misses          #    3.946 M/sec                    (7.15%)</span><br><span class="line">       628,574,746      LLC-stores                #   11.030 M/sec                    (7.15%)</span><br><span class="line">       848,830,289      branch-load-misses        #   14.894 M/sec                    (10.71%)</span><br><span class="line">    13,074,297,582      branch-loads              #  229.416 M/sec                    (14.28%)</span><br><span class="line">       109,223,171      dTLB-load-misses          #    0.56% of all dTLB cache hits   (14.27%)</span><br><span class="line">    19,418,657,165      dTLB-loads                #  340.741 M/sec                    (14.29%)</span><br><span class="line">        13,930,402      dTLB-store-misses         #    0.244 M/sec                    (14.28%)</span><br><span class="line">    10,047,511,003      dTLB-stores               #  176.305 M/sec                    (14.28%)</span><br><span class="line">       194,902,860      iTLB-load-misses          #   61.23% of all iTLB cache hits   (14.27%)</span><br><span class="line">       318,292,771      iTLB-loads                #    5.585 M/sec                    (14.26%)</span><br><span class="line"></span><br><span class="line">//on 310+8269 thp never</span><br><span class="line">        90,790,778      dTLB-load-misses          #    0.67% of all dTLB cache hits   (16.66%)</span><br><span class="line">    13,639,069,352      dTLB-loads                                                    (16.66%)</span><br><span class="line">         6,553,693      dTLB-store-misses                                             (16.63%)</span><br><span class="line">     6,494,274,815      dTLB-stores                                                   (20.28%)</span><br><span class="line">        76,175,883      iTLB-load-misses          #   40.53% of all iTLB cache hits   (20.80%)</span><br><span class="line">       187,932,292      iTLB-loads                                                    (20.76%)</span><br><span class="line"></span><br><span class="line">//on 310+8269 thp always</span><br><span class="line">     7,199,483,512      branch-instructions       #  338.269 M/sec                    (11.46%)</span><br><span class="line">        81,893,729      branch-misses             #    1.14% of all branches          (14.95%)</span><br><span class="line">       532,919,206      bus-cycles                #   25.039 M/sec                    (14.85%)</span><br><span class="line">       253,267,167      cache-misses              #   11.507 % of all cache refs      (14.81%)</span><br><span class="line">     2,201,001,946      cache-references          #  103.414 M/sec                    (14.15%)</span><br><span class="line">    63,971,073,336      cpu-cycles                #    3.006 GHz                      (14.55%)</span><br><span class="line">    37,214,341,673      instructions              #    0.58  insns per cycle          (18.09%)</span><br><span class="line">    52,209,823,072      ref-cycles                # 2453.086 M/sec                    (17.23%)</span><br><span class="line">     1,098,964,315      L1-dcache-load-misses     #   10.17% of all L1-dcache hits    (14.22%)</span><br><span class="line">    10,808,109,191      L1-dcache-loads           #  507.820 M/sec                    (14.31%)</span><br><span class="line">     5,092,652,478      L1-dcache-stores          #  239.279 M/sec                    (14.38%)</span><br><span class="line">     4,338,580,209      L1-icache-load-misses     #  203.849 M/sec                    (14.40%)</span><br><span class="line">        60,262,584      LLC-load-misses           #   21.81% of all LL-cache hits     (14.35%)</span><br><span class="line">       276,321,779      LLC-loads                 #   12.983 M/sec                    (14.31%)</span><br><span class="line">        62,982,184      LLC-store-misses          #    2.959 M/sec                    (10.76%)</span><br><span class="line">       105,448,227      LLC-stores                #    4.954 M/sec                    (8.08%)</span><br><span class="line">        81,163,187      branch-load-misses        #    3.813 M/sec                    (11.67%)</span><br><span class="line">     7,111,481,940      branch-loads              #  334.134 M/sec                    (14.37%)</span><br><span class="line">         4,527,406      dTLB-load-misses          #    0.04% of all dTLB cache hits   (14.30%)</span><br><span class="line">    10,726,725,791      dTLB-loads                #  503.997 M/sec                    (17.33%)</span><br><span class="line">         1,066,097      dTLB-store-misses         #    0.050 M/sec                    (17.37%)</span><br><span class="line">     5,090,008,144      dTLB-stores               #  239.155 M/sec                    (17.34%)</span><br><span class="line">        18,715,797      iTLB-load-misses          #   18.97% of all iTLB cache hits   (17.33%)</span><br><span class="line">        98,684,189      iTLB-loads                #    4.637 M/sec                    (14.29%)</span><br><span class="line">       </span><br></pre></td></tr></table></figure>



<h2 id="MySQL-场景下代码大页对性能的影响"><a href="#MySQL-场景下代码大页对性能的影响" class="headerlink" title="MySQL 场景下代码大页对性能的影响"></a><a target="_blank" rel="noopener" href="https://ata.alibaba-inc.com/articles/217859">MySQL 场景下代码大页对性能的影响</a></h2><p>不只是数据可以用HugePage，代码段也可以开启HugePage, 无论在x86还是arm（arm下提升更明显）下，都可以得到大页优于透明大页，透明大页优于正常的4K page</p>
<blockquote>
<p>收益：代码大页 &gt; anon THP &gt; 4k</p>
</blockquote>
<p>arm下，对32core机器用32并发的sysbench来对比，代码大页带来的性能提升大概有11%，iTLB miss下降了10倍左右。</p>
<p>x86下，性能提升只有大概3-5%之间，iTLB miss下降了1.5-3倍左右。</p>
<h2 id="TLAB-miss高的案例"><a href="#TLAB-miss高的案例" class="headerlink" title="TLAB miss高的案例"></a><a target="_blank" rel="noopener" href="https://ata.alibaba-inc.com/articles/152660">TLAB miss高的案例</a></h2><p>程序运行久了之后会变慢大概10%</p>
<p>刚开始运行的时候perf各项数据:</p>
<p><img src="/images/951413iMgBlog/7a26deaf96bdcc07db4db34ae1178641.png" alt="img"></p>
<p>长时间运行后：</p>
<p><img src="/images/951413iMgBlog/3385ae6ffbd5b48b80efa759f42b8174.png" alt="img"></p>
<p>内存的利用以页为单位，当时分析认为，在此4k连续的基础上，页的碎片不应该对64 byte align的cache有什么影响。当时guest和host都没有开THP。</p>
<p>既然无法理解这个结果，那就只有按部就班的查看内核执行路径上各个函数的差别了，祭出ftrace:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">echo kerel_func_name1 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</span><br><span class="line"></span><br><span class="line">echo kerel_func_name2 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</span><br><span class="line"></span><br><span class="line">echo kerel_func_name3 &gt; /sys/kernel/debug/tracing/set_ftrace_filter</span><br><span class="line">echo 1 &gt; /sys/kernel/debug/tracing/function_profile_enabled</span><br></pre></td></tr></table></figure>

<p>在CPU#20上执行代码:</p>
<p>taskset -c 20 .&#x2F;b</p>
<p>代码执行完后:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; /sys/kernel/debug/tracing/function_profile_enabled</span><br><span class="line">cat /sys/kernel/debug/tracing/trace_stat/function20</span><br></pre></td></tr></table></figure>

<p>这个时候就会打印出在各个函数上花费的时间，比如:</p>
<p><img src="/images/951413iMgBlog/329769dd1da2ed324ac11b8b922382cd.png" alt="img"></p>
<p>经过调试后，逐步定位到主要时间差距在  __mem_cgroup_commit_charge() (58%).</p>
<p>在阅读代码的过程中，注意到当前内核使能了CONFIG_SPARSEMEM_VMEMMAP&#x3D;y</p>
<p>原因就是机器运行久了之后内存碎片化严重，导致TLAB Miss严重。</p>
<p>解决：开启THP后，性能稳定</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G&#x2F;8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a target="_blank" rel="noopener" href="https://www.atatech.org/articles/163233">https://www.atatech.org/articles/163233</a> <a target="_blank" rel="noopener" href="https://www.atatech.org/articles/97130">https://www.atatech.org/articles/97130</a></p>
<p>compact: 在进行 compcation 时，线程会从前往后扫描已使用的 movable page，然后从后往前扫描 free page，扫描结束后会把这些 movable page 给迁移到 free page 里，最终规整出一个 2M 的连续物理内存，这样 THP 就可以成功申请内存了。</p>
<p><img src="/images/951413iMgBlog/image-20210628144121108.png" alt="image-20210628144121108"></p>
<p>一次THP compact堆栈：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">java          R  running task        0 144305 144271 0x00000080</span><br><span class="line"> ffff88096393d788 0000000000000086 ffff88096393d7b8 ffffffff81060b13</span><br><span class="line"> ffff88096393d738 ffffea003968ce50 000000000000000e ffff880caa713040</span><br><span class="line"> ffff8801688b0638 ffff88096393dfd8 000000000000fbc8 ffff8801688b0640</span><br><span class="line"></span><br><span class="line">Call Trace:</span><br><span class="line"> [&lt;ffffffff81060b13&gt;] ? perf_event_task_sched_out+0x33/0x70</span><br><span class="line"> [&lt;ffffffff8100bb8e&gt;] ? apic_timer_interrupt+0xe/0x20</span><br><span class="line"> [&lt;ffffffff810686da&gt;] __cond_resched+0x2a/0x40</span><br><span class="line"> [&lt;ffffffff81528300&gt;] _cond_resched+0x30/0x40</span><br><span class="line"> [&lt;ffffffff81169505&gt;] compact_checklock_irqsave+0x65/0xd0</span><br><span class="line"> [&lt;ffffffff81169862&gt;] compaction_alloc+0x202/0x460</span><br><span class="line"> [&lt;ffffffff811748d8&gt;] ? buffer_migrate_page+0xe8/0x130</span><br><span class="line"> [&lt;ffffffff81174b4a&gt;] migrate_pages+0xaa/0x480</span><br><span class="line"> [&lt;ffffffff81169660&gt;] ? compaction_alloc+0x0/0x460                 //compact and migrate</span><br><span class="line"> [&lt;ffffffff8116a1a1&gt;] compact_zone+0x581/0x950</span><br><span class="line"> [&lt;ffffffff8116a81c&gt;] compact_zone_order+0xac/0x100</span><br><span class="line"> [&lt;ffffffff8116a951&gt;] try_to_compact_pages+0xe1/0x120</span><br><span class="line"> [&lt;ffffffff8112f1ba&gt;] __alloc_pages_direct_compact+0xda/0x1b0</span><br><span class="line"> [&lt;ffffffff8112f80b&gt;] __alloc_pages_nodemask+0x57b/0x8d0</span><br><span class="line"> [&lt;ffffffff81167b9a&gt;] alloc_pages_vma+0x9a/0x150</span><br><span class="line"> [&lt;ffffffff8118337d&gt;] do_huge_pmd_anonymous_page+0x14d/0x3b0        //huge page</span><br><span class="line"> [&lt;ffffffff8152a116&gt;] ? rwsem_down_read_failed+0x26/0x30</span><br><span class="line"> [&lt;ffffffff8114b350&gt;] handle_mm_fault+0x2f0/0x300</span><br><span class="line"> [&lt;ffffffff810ae950&gt;] ? wake_futex+0x40/0x60</span><br><span class="line"> [&lt;ffffffff8104a8d8&gt;] __do_page_fault+0x138/0x480</span><br><span class="line"> [&lt;ffffffff810097cc&gt;] ? __switch_to+0x1ac/0x320</span><br><span class="line"> [&lt;ffffffff81527910&gt;] ? thread_return+0x4e/0x76e</span><br><span class="line"> [&lt;ffffffff8152d45e&gt;] do_page_fault+0x3e/0xa0                       //page fault</span><br><span class="line"> [&lt;ffffffff8152a815&gt;] page_fault+0x25/0x30</span><br></pre></td></tr></table></figure>



<h3 id="查看pagetypeinfo"><a href="#查看pagetypeinfo" class="headerlink" title="查看pagetypeinfo"></a>查看pagetypeinfo</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/pagetypeinfo</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    0, zone      DMA, type    Unmovable      1      1      1      0      2      1      1      0      1      0      0</span><br><span class="line">Node    0, zone      DMA, type  Reclaimable      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      1      3</span><br><span class="line">Node    0, zone      DMA, type      Reserve      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, type          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, type    Unmovable     89    144     98     42     21     14      5      2      1      0      1</span><br><span class="line">Node    0, zone    DMA32, type  Reclaimable     28     22      9      8      0      0      0      0      0      1      7</span><br><span class="line">Node    0, zone    DMA32, type      Movable    402     50     21      8    880    924    321     51      4      1    227</span><br><span class="line">Node    0, zone    DMA32, type      Reserve      0      0      0      0      0      0      0      0      0      0      1</span><br><span class="line">Node    0, zone    DMA32, type          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, type    Unmovable  13709  15231   6637   2646    816    181     46      4      4      1      0</span><br><span class="line">Node    0, zone   Normal, type  Reclaimable      1      5      6   3293   1295    128     29      7      5      0      0</span><br><span class="line">Node    0, zone   Normal, type      Movable   6396 1383350 1301956 1007627 670102 366248 160232  54894  13126   1482     37</span><br><span class="line">Node    0, zone   Normal, type      Reserve      0      0      0      2      1      1      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</span><br><span class="line">Node 0, zone      DMA            1            0            7            0            0            0</span><br><span class="line">Node 0, zone    DMA32           24           38          889            1            0            0</span><br><span class="line">Node 0, zone   Normal         1568          795       127683            2            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    1, zone   Normal, type    Unmovable   3938   8735   5469   3221   2097    989    202      6      0      0      0</span><br><span class="line">Node    1, zone   Normal, type  Reclaimable      1      7      7      8      7      2      2      2      1      0      0</span><br><span class="line">Node    1, zone   Normal, type      Movable  18623 1001037 2084894 1261484 631159 276096  87272  17169   1389    797      0</span><br><span class="line">Node    1, zone   Normal, type      Reserve      0      0      0      8      0      0      0      0      0      0      0</span><br><span class="line">Node    1, zone   Normal, type          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</span><br><span class="line">Node 1, zone   Normal         1530          637       128903            2            0            0</span><br></pre></td></tr></table></figure>

<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</span><br><span class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 1, zone   Normal</span><br><span class="line"></span><br><span class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</span><br><span class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3975</span><br><span class="line">        min      20</span><br><span class="line">        low      25</span><br><span class="line">        high     30</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3996</span><br><span class="line">        managed  3975</span><br><span class="line">    nr_free_pages 3975</span><br><span class="line">    nr_alloc_batch 5</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     382873</span><br><span class="line">        min      2335</span><br><span class="line">        low      2918</span><br><span class="line">        high     3502</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  513024</span><br><span class="line">        managed  450639</span><br><span class="line">    nr_free_pages 382873</span><br><span class="line">    nr_alloc_batch 584</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     11105097</span><br><span class="line">        min      61463</span><br><span class="line">        low      76828</span><br><span class="line">        high     92194</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  12058624</span><br><span class="line">        present  12058624</span><br><span class="line">        managed  11859912</span><br><span class="line">    nr_free_pages 11105097</span><br><span class="line">    nr_alloc_batch 12344</span><br><span class="line">    </span><br><span class="line">    low = 5/4 * min</span><br><span class="line">high = 3/2 * min</span><br><span class="line"></span><br><span class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</span><br><span class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &#x27;&#123;print $NF&#125;&#x27;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=499 MB</span><br><span class="line"></span><br><span class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</span><br><span class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &#x27;&#123;print $NF&#125;&#x27;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=624 MB</span><br><span class="line"></span><br><span class="line">[root@jiangyi01.sqa.zmf /home/ahao.mah]</span><br><span class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &#x27;&#123;print $NF&#125;&#x27;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=802 MB</span><br></pre></td></tr></table></figure>

<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand&#x2F;s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;&#x3D; 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义参考 （<a target="_blank" rel="noopener" href="https://man7.org/linux/man-pages/man5/proc.5.html%EF%BC%89%EF%BC%8C%E5%90%8C%E6%A0%B7%E5%85%B3%E6%B3%A8">https://man7.org/linux/man-pages/man5/proc.5.html），同样关注</a> order &gt;&#x3D; 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a target="_blank" rel="noopener" href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt">drsnoop</a>，<a target="_blank" rel="noopener" href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<p>​	</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://www.atatech.org/articles/66885">https://www.atatech.org/articles/66885</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1087455">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiaolincoding/p/13719610.html">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Linux/" rel="tag"># Linux</a>
              <a href="/tags/free/" rel="tag"># free</a>
              <a href="/tags/Memory/" rel="tag"># Memory</a>
              <a href="/tags/HugePage/" rel="tag"># HugePage</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--pagecache/" rel="prev" title="Linux内存--PageCache">
                  <i class="fa fa-angle-left"></i> Linux内存--PageCache
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/11/17/MySQL%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BB%B6%E6%97%B6%E5%8D%A1%E9%A1%BF%E6%8E%92%E6%9F%A5/" rel="next" title="MySQL线程池导致的延时卡顿排查">
                  MySQL线程池导致的延时卡顿排查 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
