<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Linux,free,buddyinfo,碎片,">





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1">






<meta name="description" content="Linux内存–管理和碎片本系列有如下几篇 [Linux 内存问题汇总](&amp;#x2F;2020&amp;#x2F;01&amp;#x2F;15&amp;#x2F;Linux 内存问题汇总&amp;#x2F;) Linux内存–PageCache Linux内存–管理和碎片 Linux内存–HugePage Linux内存–零拷贝 物理结构分析内存从物理结构上面分为：Channel &amp;gt; DIMM（对应物理上售卖的内存条） &amp;">
<meta name="keywords" content="Linux,free,buddyinfo,碎片">
<meta property="og:type" content="article">
<meta property="og:title" content="Linux内存--管理和碎片">
<meta property="og:url" content="https://plantegg.github.io/2020/11/15/Linux内存--管理和碎片/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="Linux内存–管理和碎片本系列有如下几篇 [Linux 内存问题汇总](&amp;#x2F;2020&amp;#x2F;01&amp;#x2F;15&amp;#x2F;Linux 内存问题汇总&amp;#x2F;) Linux内存–PageCache Linux内存–管理和碎片 Linux内存–HugePage Linux内存–零拷贝 物理结构分析内存从物理结构上面分为：Channel &amp;gt; DIMM（对应物理上售卖的内存条） &amp;">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211222135852796.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211107175201297.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211107182305732.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220928160318893.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/tlb_lookup.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1610941678194-bb42451f-b59a-475d-9b8d-b1085c18766d.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108115717113.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108120532370.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108120718732.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-20220323120420806.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-20220323120443853.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118121500859.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108122432263.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/debfe12e-d1b9-49cd-988d-3f7fcba6ecd2.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118120823595.png">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118120851731.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/2e73247c-8a10-43e6-bb0e-49ecfff14268.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/16ebe996-0e3a-4d67-810f-3121b457271e.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/5135d81f-6985-4d6a-8896-e451c0ba20f5.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/0d8a26db-3663-40af-b215-f8601ef23676.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/f16438b744a248d7671d5ac7317b0a98.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/cf58f10a523e1e4f0db443be3f54fc04.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/8bbb5c886dc06196546daec46712ff71.png">
<meta property="og:image" content="https://plantegg.github.io/Users/ren/case/ossimg/8e15e91d4dcc61bbd329e7283c7c7500.png">
<meta property="og:updated_time" content="2024-11-20T10:00:54.143Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Linux内存--管理和碎片">
<meta name="twitter:description" content="Linux内存–管理和碎片本系列有如下几篇 [Linux 内存问题汇总](&amp;#x2F;2020&amp;#x2F;01&amp;#x2F;15&amp;#x2F;Linux 内存问题汇总&amp;#x2F;) Linux内存–PageCache Linux内存–管理和碎片 Linux内存–HugePage Linux内存–零拷贝 物理结构分析内存从物理结构上面分为：Channel &amp;gt; DIMM（对应物理上售卖的内存条） &amp;">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211222135852796.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/2020/11/15/Linux内存--管理和碎片/">





  <title>Linux内存--管理和碎片 | plantegg</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/15/Linux内存--管理和碎片/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Linux内存--管理和碎片</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-11-15T16:30:03+08:00">
                2020-11-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Memory/" itemprop="url" rel="index">
                    <span itemprop="name">Memory</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>次
            </span>
          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Linux内存–管理和碎片"><a href="#Linux内存–管理和碎片" class="headerlink" title="Linux内存–管理和碎片"></a>Linux内存–管理和碎片</h1><p>本系列有如下几篇</p>
<p>[Linux 内存问题汇总](&#x2F;2020&#x2F;01&#x2F;15&#x2F;Linux 内存问题汇总&#x2F;)</p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E7%AE%A1%E7%90%86%E5%92%8C%E7%A2%8E%E7%89%87/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E9%9B%B6%E6%8B%B7%E8%B4%9D/">Linux内存–零拷贝</a></p>
<h2 id="物理结构分析"><a href="#物理结构分析" class="headerlink" title="物理结构分析"></a>物理结构分析</h2><p>内存从物理结构上面分为：<strong>Channel &gt; DIMM（对应物理上售卖的内存条） &gt;Rank &gt; Chip &gt; Bank &gt; Row&#x2F;Column。</strong></p>
<p>Chip就是DRAM芯片，一个chip里面会有很多bank。每个bank就是数据存储的实体，相当于一个二维矩阵，只要声明了column和row就可以从每个bank中取出8bit的数据。</p>
<p>具体可以看如下图，一个通道Channel可以是一个DIMM也可以是两个DIMM，甚至3个DIMM，图中是2个DIMM。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211222135852796.png" alt="image-20211222135852796"></p>
<h2 id="虚拟内存和物理内存"><a href="#虚拟内存和物理内存" class="headerlink" title="虚拟内存和物理内存"></a>虚拟内存和物理内存</h2><p>进程所操作的内存是一个虚拟内存，由OS来将这块虚拟内存映射到实际的物理内存上，这样做的好处是每个进程可以独占 128T 内存，任意地使用，系统上还运行了哪些进程已经与我们完全没有关系了（不需要考虑和其它进程之间的地址会冲突）。为变量和函数分配地址的活，我们交给链接器去自动安排就可以了。这一切都是因为虚拟内存能够提供内存地址空间的隔离，极大地扩展了可用空间。</p>
<p>操作系统管理着这种映射关系，所以你在写代码的时候，就不用再操心物理内存的使用情况了，你看到的内存就是虚拟内存。无论一个进程占用的内存资源有多大，在任一时刻，它需要的物理内存都是很少的。在这个推论的基础上，CPU 为每个进程只需要保留很少的物理内存就可以保证进程的正常执行了。</p>
<p>当程序中使用 malloc 等分配内存的接口时会将内存从待分配状态变成已分配状态，此时这块分配好的内存还没有真正映射到对应的物理内存上，这块内存就是未映射状态，因为它并没有被映射到相应的物理内存，直到对该块内存进行读写时，操作系统才会真正地为它分配物理内存。然后这个页面才能成为正常页面。</p>
<p>i7 处理器的页表也是存储在内存页里的，每个页表项都是 4 字节。所以，人们就将 1024 个页表项组成一张页表。这样一张页表的大小就刚好是 4K，占据一个内存页，这样就更加方便管理。而且，当前市场上主流的处理器也都选择将页大小定为 4K。</p>
<blockquote>
<p>虚拟地址在计算机体系结构里可以评为特优的一项技术；超线程、流水线、多发射只是优；cache则只是良好（成本高）</p>
</blockquote>
<h3 id="CPU-如何找到真实地址"><a href="#CPU-如何找到真实地址" class="headerlink" title="CPU 如何找到真实地址"></a>CPU 如何找到真实地址</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211107175201297.png" alt="image-20211107175201297"></p>
<ul>
<li>第一步是确定页目录基址。每个 CPU 都有一个页目录基址寄存器，最高级页表的基地址就存在这个寄存器里。在 X86 上，这个寄存器是 CR3。每一次计算物理地址时，MMU 都会从 CR3 寄存器中取出页目录所在的物理地址。</li>
<li>第二步是定位页目录项（PDE）。一个 32 位的虚拟地址可以拆成 10 位，10 位和 12 位三段，上一步找到的页目录表基址加上高 10 位的值乘以 4，就是页目录项的位置。这是因为，一个页目录项正好是 4 字节，所以 1024 个页目录项共占据 4096 字节，刚好组成一页，而 1024 个页目录项需要 10 位进行编码。这样，我们就可以通过最高 10 位找到该地址所对应的 PDE 了。</li>
<li>第三步是定位页表项（PTE）。页目录项里记录着页表的位置，CPU 通过页目录项找到页表的位置以后，再用中间 10 位计算页表中的偏移，可以找到该虚拟地址所对应的页表项了。页表项也是 4 字节的，所以一页之内刚好也是 1024 项，用 10 位进行编码。所以计算公式与上一步相似，用页表基址加上中间 10 位乘以 4，可以得到页表项的地址。</li>
<li>最后一步是确定真实的物理地址。上一步 CPU 已经找到页表项了，这里存储着物理地址，这才真正找到该虚拟地址所对应的物理页。虚拟地址的低 12 位，刚好可以对一页内的所有字节进行编码，所以我们用低 12 位来代表页内偏移。计算的公式是物理页的地址直接加上低 12 位。</li>
</ul>
<p>前面我们分析的是 32 位操作系统，那对于 64 位机器是不是有点不同呢？在 64 位的机器上，使用了 48 位的虚拟地址，所以它需要使用 4 级页表。它的结构与 32 位的 3 级页表是相似的，<strong>只是多了一级页目录，定位的过程也从 32 位的 4 步变成了 5 步。</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211107182305732.png" alt="image-20211107182305732"></p>
<p>8086最开始是按不同的作用将内存分为代码段、数据段等，386开始按页开始管理内存（混合有按段管理）。 现代的操作系统都是采用段式管理来做基本的权限管理，而对于内存的分配、回收、调度都是依赖页式管理。</p>
<h3 id="tlab-miss"><a href="#tlab-miss" class="headerlink" title="tlab miss"></a><a href="https://lwn.net/Articles/379748" target="_blank" rel="noopener">tlab miss</a></h3><p>tlb：从各级cache里分配的一块专用空间，用来存放页表(虚拟地址和物理地址的对应关系)–存放在CPU cache里的索引</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#x86info -c</span><br><span class="line">Monitor/Mwait: min/max line size 64/64, ecx bit 0 support, enumeration extension</span><br><span class="line">SVM: revision 1, 32768 ASIDs</span><br><span class="line">Address Size: 48 bits virtual, 48 bits physical</span><br><span class="line">The physical package has 96 of 32768 possible cores implemented.</span><br><span class="line">L1 Data TLB (1G):           Fully associative. 64 entries.</span><br><span class="line">L1 Instruction TLB (1G):    Fully associative. 64 entries.</span><br><span class="line">L1 Data TLB (2M/4M):        Fully associative. 64 entries.</span><br><span class="line">L1 Instruction TLB (2M/4M): Fully associative. 64 entries.</span><br><span class="line">L1 Data TLB (4K):           Fully associative. 64 entries.</span><br><span class="line">L1 Instruction TLB (4K):    Fully associative. 64 entries.</span><br><span class="line">L1 Data cache:</span><br><span class="line">	Size: 32Kb	8-way associative.</span><br><span class="line">	lines per tag=1	line size=64 bytes.</span><br><span class="line">L1 Instruction cache:</span><br><span class="line">	Size: 32Kb	8-way associative.</span><br><span class="line">	lines per tag=1	line size=64 bytes.</span><br><span class="line">L2 Data TLB (1G):           Fully associative. 64 entries.</span><br><span class="line">L2 Instruction TLB (1G):    Disabled. 0 entries.</span><br><span class="line">L2 Data TLB (2M/4M):        4-way associative. 2048 entries.</span><br><span class="line">L2 Instruction TLB (2M/4M): 2-way associative. 512 entries.</span><br><span class="line">L2 Data TLB (4K):           8-way associative. 2048 entries.</span><br><span class="line">L2 Instruction TLB (4K):    4-way associative. 512 entries.</span><br><span class="line">L2 cache:</span><br><span class="line">	Size: 512Kb	8-way associative.</span><br><span class="line">	lines per tag=1	line size=64 bytes.</span><br><span class="line"></span><br><span class="line"> running at an estimated 2.55GHz</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220928160318893.png" alt="image-20220928160318893"></p>
<p>TLB(Translation Lookaside Buffer) Cache用于缓存少量热点内存地址的mapping关系。TLB和L1一样每个core独享，由于制造成本和工艺的限制，响应时间需要控制在CPU Cycle级别的Cache容量只能存储几十个对象。那么TLB Cache在应对大量热点数据<code>Virual Address</code>转换的时候就显得捉襟见肘了。我们来算下按照标准的Linux页大小(page size) 4K，一个能缓存64元素的TLB Cache只能涵盖<code>4K*64 = 256K</code>的热点数据的内存地址，显然离理想非常遥远的。于是Huge Page就产生了。</p>
<p><a href="https://en.wikipedia.org/wiki/Translation_lookaside_buffer" target="_blank" rel="noopener">These are typical performance levels of a TLB</a>:</p>
<ul>
<li>Size: 12 bits – 4,096 entries</li>
<li>Hit time: 0.5 – 1 clock cycle</li>
<li>Miss penalty: 10 – 100 clock cycles</li>
<li>Miss rate: 0.01 – 1% (20–40% for sparse&#x2F;graph applications)</li>
</ul>
<p>TLB也分为iTLB和dTLB, 分别顶在L1i和L1d前面（比L1更小更快，每个core独享tlb）</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/tlb_lookup.png" alt="img"></p>
<p>以intel x86为例，一个cpu也就32到64个tlb, 超出这个范畴，就得去查页表。 每个型号的cpu都不一样，需要查看<a href="https://en.wikichip.org/wiki/WikiChip" target="_blank" rel="noopener">spec</a></p>
<p>进程分配到的不是内存的实际物理地址，而是一个经过映射后的虚拟地址，这么做的原因是为了让每个应用可以独享完整的虚拟地址，而不需要每个进程互相考虑使用内存的协调。</p>
<p>但是虚拟地址到物理地址的映射需要巨大的映射空间，如何用更少的内存消耗来管理庞大的内存（如果没有分级，4G内存对应着4MB的索引空间，一级比如使用4K就够了，多个二级使总共用4M，但是这4M大部分时候不用提前分配），Linux通过四级表项来做虚拟地址到物理地址的映射，这样4Kb就能管理256T内存，4级映射是时间换空间的典型案例。不过一般而言一个进程是远远用不了256T内存的，那么这四级映射大部分时候都是没必要的，所以实际用不了那么大的PageTable。</p>
<p><a href="https://mp.weixin.qq.com/s/dZNjq05q9jMFYhJrjae_LA" target="_blank" rel="noopener">虚拟内存的核心原理</a>是：为每个程序设置一段”连续”的虚拟地址空间，把这个地址空间分割成多个具有连续地址范围的页 (page)，并把这些页和物理内存做映射，在程序运行期间动态映射到物理内存。当程序引用到一段在物理内存的地址空间时，由硬件立刻执行必要的映射；而当程序引用到一段不在物理内存中的地址空间时，由操作系统负责将缺失的部分装入物理内存并重新执行失败的指令：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1610941678194-bb42451f-b59a-475d-9b8d-b1085c18766d.png" alt="img"></p>
<p>在 <strong>内存管理单元（Memory Management Unit，MMU）</strong>进行地址转换时，如果页表项的 “在&#x2F;不在” 位是 0，则表示该页面并没有映射到真实的物理页框，则会引发一个<strong>缺页中断</strong>，CPU 陷入操作系统内核，接着操作系统就会通过页面置换算法选择一个页面将其换出 (swap)，以便为即将调入的新页面腾出位置，如果要换出的页面的页表项里的修改位已经被设置过，也就是被更新过，则这是一个脏页 (dirty page)，需要写回磁盘更新改页面在磁盘上的副本，如果该页面是”干净”的，也就是没有被修改过，则直接用调入的新页面覆盖掉被换出的旧页面即可。</p>
<p>还需要了解的一个概念是<strong>转换检测缓冲器（Translation Lookaside Buffer，TLB，每个core一个TLB，类似L1 cache）</strong>，也叫快表，是用来加速虚拟地址映射的，因为虚拟内存的分页机制，页表一般是保存内存中的一块固定的存储区，导致进程通过 MMU 访问内存比直接访问内存多了一次内存访问，性能至少下降一半，因此需要引入加速机制，即 TLB 快表，TLB 可以简单地理解成页表的高速缓存，保存了最高频被访问的页表项，由于一般是硬件实现的，因此速度极快，MMU 收到虚拟地址时一般会先通过硬件 TLB 查询对应的页表号，若命中且该页表项的访问操作合法，则直接从 TLB 取出对应的物理页框号返回，若不命中则穿透到内存页表里查询，并且会用这个从内存页表里查询到最新页表项替换到现有 TLB 里的其中一个，以备下次缓存命中。</p>
<p>如果没有TLB那么每一次内存映射都需要查表四次然后才是一次真正的内存访问，代价比较高。</p>
<p>有了TLB之后，CPU访问某个虚拟内存地址的过程如下</p>
<ol>
<li>CPU产生一个虚拟地址</li>
<li>MMU从TLB中获取页表，翻译成物理地址</li>
<li>MMU把物理地址发送给L1&#x2F;L2&#x2F;L3&#x2F;内存</li>
<li>L1&#x2F;L2&#x2F;L3&#x2F;内存将地址对应数据返回给CPU</li>
</ol>
<p>由于第2步是类似于寄存器的访问速度，所以<strong>如果TLB能命中，则虚拟地址到物理地址的时间开销几乎可以忽</strong>略。tlab miss比较高的话开启内存大页对性能是有提升的，但是会有一定的内存浪费。</p>
<h2 id="内存布局"><a href="#内存布局" class="headerlink" title="内存布局"></a>内存布局</h2><ul>
<li>代码段：CPU 运行一个程序，实质就是在顺序执行该程序的机器码。一个程序的机器码会被组织到同一个地方。</li>
<li>数据段：程序在运行过程中必然要操作数据。这其中，对于有初值的变量，它的初始值会存放在程序的二进制文件中，而且，这些数据部分也会被装载到内存中，即程序的数据段。数据段存放的是程序中已经初始化且不为 0 的全局变量和静态变量。</li>
<li>BSS 段: 对于未初始化的全局变量和静态变量，因为编译器知道它们的初始值都是 0，因此便不需要再在程序的二进制映像中存放这么多 0 了，只需要记录他们的大小即可，这便是BSS段 。BSS 段这个缩写名字是 Block Started by Symbol，但很多人可能更喜欢把它记作 Better Save Space 的缩写。</li>
<li>堆是程序员可以自由申请的空间，当我们在写程序时要保存数据，优先会选择堆；</li>
<li>栈是函数执行时的活跃记录，这将是我们下一节课要重点分析的内容。</li>
</ul>
<p>数据段和 BSS 段里存放的数据也只能是部分数据，主要是全局变量和静态变量，但程序在运行过程中，仍然需要记录大量的临时变量，以及运行时生成的变量，这里就需要新的内存区域了，即程序的堆空间跟栈空间。与代码段以及数据段不同的是，堆和栈并不是从磁盘中加载，它们都是由程序在运行的过程中申请，在程序运行结束后释放。</p>
<p>总的来说，一个程序想要运行起来所需要的几块基本内存区域：代码段、数据段、BSS 段、堆空间和栈空间。下面就是内存布局的示意图：</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108115717113.png" alt="image-20211108115717113" style="zoom:35%;">

<p>其它内存形态：</p>
<ul>
<li>存放加载的共享库的内存空间：如果一个进程依赖共享库，那对应的，该共享库的代码段、数据段、BSS 段也需要被加载到这个进程的地址空间中。</li>
<li>共享内存段：我们可以通过系统调用映射一块匿名区域作为共享内存，用来进行进程间通信。</li>
<li>内存映射文件：我们也可以将磁盘的文件映射到内存中，用来进行文件编辑或者是类似共享内存的方式进行进程通信。</li>
</ul>
<p>32位 x86机器下，通过 cat &#x2F;proc&#x2F;pid&#x2F;maps 看到的进程所使用的内存分配空间：</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108120532370.png" alt="image-20211108120532370" style="zoom:33%;">

<p>64位 x86机器下，通过 cat &#x2F;proc&#x2F;pid&#x2F;maps 看到的进程所使用的内存分配空间：</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108120718732.png" alt="image-20211108120718732" style="zoom:40%;">

<p>目前的 64 系统下的寻址空间是 2^48(太多用不完，高16位为Canonical空间），即 256TB。而且根据 canonical address 的划分，地址空间天然地被分割成两个区间，分别是 0x0 - 0x00007fffffffffff 和 0xffff800000000000 - 0xffffffffffffffff。这样就直接将低 128T 的空间划分为用户空间，高 128T 划分为内核空间。</p>
<p>brk:内核维护指向堆的顶部</p>
<p>Java程序对应的maps：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/14011/maps</span><br><span class="line">00400000-00401000 r-xp 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</span><br><span class="line">00600000-00601000 rw-p 00000000 08:03 3935494                            /opt/taobao/install/ajdk-8.3.6_fp9-b30/bin/java</span><br><span class="line">ed400000-1001e0000 rw-p 00000000 00:00 0</span><br><span class="line">1001e0000-140000000 ---p 00000000 00:00 0</span><br><span class="line">7f86e8000000-7f8a7fc00000 rw-p 00000000 00:00 0</span><br><span class="line">..</span><br><span class="line">7f8aaecfa000-7f8aaeff8000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aaeff8000-7f8aaf000000 r-xp 00000000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</span><br><span class="line">7f8aaf000000-7f8aaf1ff000 ---p 00008000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</span><br><span class="line">7f8aaf1ff000-7f8aaf200000 rw-p 00007000 08:03 3935973                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libmanagement.so</span><br><span class="line">..</span><br><span class="line">7f8ad8cea000-7f8ad8cec000 r--s 00004000 08:05 7078938                    /home/admin/drds-worker/lib/netty-handler-proxy-4.1.17.Final.jar</span><br><span class="line">7f8ad8cec000-7f8ad8cf5000 r--s 0006f000 08:05 7078952                    /home/admin/drds-worker/lib/log4j-1.2.17.jar</span><br><span class="line">7f8ad8cf5000-7f8ad8cf7000 r--s 00005000 08:05 7078960                    /home/admin/drds-worker/lib/objenesis-1.0.jar</span><br><span class="line">7f8ad8cf7000-7f8ad8cff000 r--s 0004b000 08:05 7078929                    /home/admin/drds-worker/lib/spring-aop-3.2.18.RELEASE.jar</span><br><span class="line">7f8ad8cff000-7f8ad8d00000 ---p 00000000 00:00 0</span><br><span class="line">7f8ad8d00000-7f8ad9000000 rw-p 00000000 00:00 0</span><br><span class="line">7f8ad90e3000-7f8ad90ef000 r--s 000b6000 08:05 7079066                    /home/admin/drds-worker/lib/transmittable-thread-local-2.5.1.jar</span><br><span class="line">7f8ad9dd8000-7f8ad9dfe000 r--s 0026f000 08:05 7078997                    /home/admin/drds-worker/lib/druid-1.1.7-preview_12.jar</span><br><span class="line">7f8ad9dfe000-7f8ad9dff000 ---p 00000000 00:00 0</span><br><span class="line">7f8ad9dff000-7f8ad9eff000 rw-p 00000000 00:00 0</span><br><span class="line">7f8ad9eff000-7f8ad9f00000 ---p 00000000 00:00 0</span><br><span class="line">7f8ad9f00000-7f8ada200000 rw-p 00000000 00:00 0</span><br><span class="line">7f8ada200000-7f8ada202000 r--s 00003000 08:05 7078944                    /home/admin/drds-worker/lib/liberate-rest-1.0.2.jar</span><br><span class="line">7f8ada202000-7f8ada206000 r--s 00036000 08:05 7078912                    /home/admin/drds-worker/lib/jackson-core-lgpl-1.9.6.jar</span><br><span class="line">7f8ada289000-7f8ada28b000 r--s 00001000 08:05 7078998                    /home/admin/drds-worker/lib/opencensus-contrib-grpc-metrics-0.10.0.jar</span><br><span class="line">7f8ada2b5000-7f8ada2b9000 r--s 0003a000 08:05 7079099                    /home/admin/drds-worker/lib/tddl-repo-mysql-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</span><br><span class="line">7f8ada2b9000-7f8ada2c6000 r--s 0007d000 08:05 7078982                    /home/admin/drds-worker/lib/grpc-core-1.9.0.jar</span><br><span class="line">7f8ada2c6000-7f8ada2d6000 r--s 00149000 08:05 7079000                    /home/admin/drds-worker/lib/protobuf-java-3.5.1.jar</span><br><span class="line">7f8ada2d6000-7f8ada2db000 r--s 0002b000 08:05 7078927                    /home/admin/drds-worker/lib/tddl-net-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</span><br><span class="line">7f8ada2db000-7f8ada2e0000 r--s 0002a000 08:05 7078939                    /home/admin/drds-worker/lib/grpc-netty-1.9.0.jar</span><br><span class="line">7f8ada2e0000-7f8ada2ff000 r--s 00150000 08:05 7078965                    /home/admin/drds-worker/lib/mockito-core-1.9.5.jar</span><br><span class="line">7f8ada2ff000-7f8ada300000 ---p 00000000 00:00 0</span><br><span class="line">7f8ada300000-7f8ada600000 rw-p 00000000 00:00 0</span><br><span class="line">7f8ada600000-7f8ada601000 r--s 00003000 08:05 7079089                    /home/admin/drds-worker/lib/ushura-1.0.jar</span><br><span class="line">7f8ae9ba2000-7f8ae9baa000 r-xp 00000000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</span><br><span class="line">7f8ae9baa000-7f8ae9da9000 ---p 00008000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</span><br><span class="line">7f8ae9da9000-7f8ae9daa000 rw-p 00007000 08:03 3935984                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libzip.so</span><br><span class="line">7f8ae9daa000-7f8ae9db6000 r-xp 00000000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8ae9db6000-7f8ae9fb5000 ---p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8ae9fb5000-7f8ae9fb6000 r--p 0000b000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8ae9fb6000-7f8ae9fb7000 rw-p 0000c000 08:03 1837851                    /usr/lib64/libnss_files-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8ae9fb7000-7f8ae9fbd000 rw-p 00000000 00:00 0</span><br><span class="line">7f8ae9fbd000-7f8ae9fe7000 r-xp 00000000 08:03 3935961                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/jre/lib/amd64/libjava.so</span><br><span class="line">7f8aebc03000-7f8aebc05000 r--s 00008000 08:05 7079098                    /home/admin/drds-worker/lib/grpc-stub-1.9.0.jar</span><br><span class="line">7f8aebc05000-7f8aebc07000 r--s 00020000 08:05 7078930                    /home/admin/drds-worker/lib/tddl-group-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</span><br><span class="line">7f8aebc07000-7f8aebc1f000 r--s 001af000 08:05 7079085                    /home/admin/drds-worker/lib/aspectjweaver-1.8.5.jar</span><br><span class="line">7f8aebc1f000-7f8aebc20000 ---p 00000000 00:00 0</span><br><span class="line">7f8aebc20000-7f8aebd20000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aebd20000-7f8aebd35000 r-xp 00000000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</span><br><span class="line">7f8aebd35000-7f8aebf34000 ---p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</span><br><span class="line">7f8aebf34000-7f8aebf35000 r--p 00014000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</span><br><span class="line">7f8aebf35000-7f8aebf36000 rw-p 00015000 08:03 1837234                    /usr/lib64/libgcc_s-4.8.5-20150702.so.1</span><br><span class="line">7f8aebf36000-7f8aec037000 r-xp 00000000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aec037000-7f8aec236000 ---p 00101000 08:03 1837579                    /usr/lib64/libm-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aeca17000-7f8aeca18000 rw-p 0000d000 08:03 3936057                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/jli/libjli.so</span><br><span class="line">7f8aeca18000-7f8aeca2d000 r-xp 00000000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</span><br><span class="line">7f8aeca2d000-7f8aecc2c000 ---p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</span><br><span class="line">7f8aecc2c000-7f8aecc2d000 r--p 00014000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</span><br><span class="line">7f8aecc2d000-7f8aecc2e000 rw-p 00015000 08:03 1836998                    /usr/lib64/libz.so.1.2.7</span><br><span class="line">7f8aecc2e000-7f8aecc45000 r-xp 00000000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aecc45000-7f8aece44000 ---p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aece44000-7f8aece45000 r--p 00016000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aece45000-7f8aece46000 rw-p 00017000 08:03 1836993                    /usr/lib64/libpthread-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aece46000-7f8aece4a000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aece4a000-7f8aecea1000 r-xp 00000000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</span><br><span class="line">7f8aecea1000-7f8aed0a0000 ---p 00057000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</span><br><span class="line">7f8aed0a0000-7f8aed0a3000 rw-p 00056000 08:03 3936059                    /opt/taobao/install/ajdk-8.3.6_fp9-b30/lib/amd64/libjemalloc.so.2</span><br><span class="line">7f8aed0a3000-7f8aed0b5000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aed0b5000-7f8aed0d7000 r-xp 00000000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aed0d7000-7f8aed0dc000 r--s 00038000 08:05 7079012                    /home/admin/drds-worker/lib/org.osgi.core-4.2.0.jar</span><br><span class="line">7f8aed0dc000-7f8aed0e1000 r--s 00038000 08:05 7079018                    /home/admin/drds-worker/lib/commons-beanutils-1.9.3.jar</span><br><span class="line">7f8aed0e1000-7f8aed0e3000 r--s 00001000 08:05 7079033                    /home/admin/drds-worker/lib/j2objc-annotations-1.1.jar</span><br><span class="line">7f8aed0e3000-7f8aed0e8000 r--s 00017000 08:05 7079056                    /home/admin/drds-worker/lib/hibernate-jpa-2.1-api-1.0.0.Final.jar</span><br><span class="line">7f8aed1be000-7f8aed1c6000 rw-s 00000000 08:04 393222                     /tmp/hsperfdata_admin/14011</span><br><span class="line">7f8aed1c6000-7f8aed1ca000 ---p 00000000 00:00 0</span><br><span class="line">7f8aed1ca000-7f8aed2cd000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aed2cd000-7f8aed2ce000 r--s 00005000 08:05 7079029                    /home/admin/drds-worker/lib/jersey-apache-connector-2.26.jar</span><br><span class="line">7f8aed2ce000-7f8aed2d1000 r--s 0000a000 08:05 7079027                    /home/admin/drds-worker/lib/metrics-jvm-1.7.4.jar</span><br><span class="line">7f8aed2d1000-7f8aed2d3000 r--s 00006000 08:05 7078961                    /home/admin/drds-worker/lib/tddl-client-5.2.7-2-EXTEND-HOTMAPPING-SNAPSHOT.jar</span><br><span class="line">7f8aed2d3000-7f8aed2d4000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aed2d4000-7f8aed2d5000 r--p 00000000 00:00 0</span><br><span class="line">7f8aed2d5000-7f8aed2d6000 rw-p 00000000 00:00 0</span><br><span class="line">7f8aed2d6000-7f8aed2d7000 r--p 00021000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aed2d7000-7f8aed2d8000 rw-p 00022000 08:03 1837788                    /usr/lib64/ld-2.17.so;614d5f07 (deleted)</span><br><span class="line">7f8aed2d8000-7f8aed2d9000 rw-p 00000000 00:00 0</span><br><span class="line">7fff087e0000-7fff08801000 rw-p 00000000 00:00 0                          [stack]</span><br><span class="line">7fff089c2000-7fff089c4000 r-xp 00000000 00:00 0                          [vdso]</span><br><span class="line">ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]</span><br></pre></td></tr></table></figure>

<h2 id="内存管理和使用"><a href="#内存管理和使用" class="headerlink" title="内存管理和使用"></a>内存管理和使用</h2><h3 id="malloc"><a href="#malloc" class="headerlink" title="malloc"></a><a href="https://mp.weixin.qq.com/s?__biz=MzAxODI5ODMwOA==&mid=2666566845&idx=1&sn=3d4b181d4928adae408f570ba46c17b3" target="_blank" rel="noopener">malloc</a></h3><p>malloc()分配内存时：</p>
<ul>
<li>如果用户分配的内存小于 128 KB，则通过 brk() 申请内存–在堆顶分配；</li>
<li>如果用户分配的内存大于 128 KB，则通过 mmap()  申请内存–从文件映射区域分配；</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-20220323120420806.png" alt="图片"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-20220323120443853.png" alt="图片"></p>
<p>对于 「malloc 申请的内存，free 释放内存会归还给操作系统吗？」：</p>
<ul>
<li>malloc 通过 <strong>brk()</strong> 方式申请的内存，free 释放内存的时候，<strong>并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用</strong>–小内存分配避免反复调用系统操作导致上下文切换，缺点是没回收容易导致内存碎片进而浪费内存。brk分配出来的内存在maps中显示有heap字样；</li>
<li>malloc 通过 <strong>mmap()</strong> 方式申请的内存，free 释放内存的时候，<strong>会把内存归还给操作系统，内存得到真正的释放</strong>。并且mmap分配的虚拟内存都是缺页状态的。</li>
</ul>
<h3 id="malloc和mmap"><a href="#malloc和mmap" class="headerlink" title="malloc和mmap"></a>malloc和mmap</h3><p>glibc中的malloc&#x2F;free 负责向内核批发内存（不需要每次分配都真正地去调用内核态来分配），分配好的内存按大小分成不同的桶，每次malloc的时候实际到对应的桶上摘取对应的块(slab)就好，用完free的时候挂回去。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118121500859.png" alt="image-20211118121500859"></p>
<p>mmap映射内存</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211108122432263.png" alt="image-20211108122432263" style="zoom:20%;">

<p>私有匿名映射常用于分配内存，也就是申请堆内存</p>
<p>分桶式内存管理比简单算法无论是在算法效率方面，还是在碎片控制方面都有很大的提升。但它的缺陷也很明显：区域内部的使用率不够高和动态扩展能力不够好。例如，4 字节的区域提前消耗完了，但 8 字节的空闲区域还有很多，此时就会面临两难选择，如果直接分配 8 字节的区域，则区域内部浪费就比较多，如果不分配，则明明还有空闲区域，却无法成功分配。</p>
<p>为了解决以上问题所以搞了buddy</p>
<h3 id="node-gt-zone-gt-buddy-gt-slab"><a href="#node-gt-zone-gt-buddy-gt-slab" class="headerlink" title="node-&gt;zone-&gt;buddy-&gt;slab"></a>node-&gt;zone-&gt;buddy-&gt;slab</h3><p><img src="/Users/ren/case/ossimg/debfe12e-d1b9-49cd-988d-3f7fcba6ecd2.png" alt="img"></p>
<p>假如需要分配一块 4 字节大小的空间，但是在 4 字节的 free list 上找不到空闲区域，系统就会往上找，假如 8 字节和 16 字节的 free list 中也没有空闲区域，就会一直向上找到 32 字节的 free list。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118120823595.png" alt="image-20211118120823595"></p>
<p>伙伴系统不会直接把 32 的空闲区域分配出去，因为这样做的话，会带来巨大的浪费。它会先把 32 字节分成两个 16 字节，把后边一个挂入到 16 字节的 free list 中。然后继续拆分前一半。前一半继续拆成两个 8 字节，再把后一半挂入到 8 字节的 free list，最后，把前一半 8 字节拿去分配，当然这里也要继续拆分成两个 4 字节的空闲区域，其中一个用于本次 malloc 分配，另一个则挂入到 4 字节的 free list。分配后的内存的状态如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211118120851731.png" alt="image-20211118120851731"></p>
<h3 id="查看zone"><a href="#查看zone" class="headerlink" title="查看zone"></a><a href="https://mp.weixin.qq.com/s/Cn-oX0W5DrI2PivaWLDpPw" target="_blank" rel="noopener">查看zone</a></h3><p><a href="https://utcc.utoronto.ca/~cks/space/blog/linux/KernelMemoryZones" target="_blank" rel="noopener">The zones are</a>:</p>
<ul>
<li><code>DMA</code> is the low 16 MBytes of memory. At this point it exists for historical reasons; once upon what is now a long time ago, there was hardware that could only do DMA into this area of physical memory.</li>
<li><code>DMA32</code> exists only in 64-bit Linux; it is the low 4 GBytes of memory, more or less. It exists because the transition to large memory 64-bit machines has created a class of hardware that can only do DMA to the low 4 GBytes of memory.(This is where people mutter about everything old being new again.)</li>
<li><strong><code>Normal</code></strong> is different on 32-bit and 64-bit machines. On 64-bit machines, it is all RAM from 4GB or so on upwards. On 32-bit machines it is all RAM from 16 MB to 896 MB for complex and somewhat historical reasons. Note that this implies that machines with a 64-bit kernel can have very small amounts of Normal memory unless they have significantly more than 4GB of RAM. For example, a 2 GB machine running a 64-bit kernel will have no Normal memory at all while a 4 GB machine will have only a tiny amount of it.</li>
<li><code>HighMem</code> exists only on 32-bit Linux; it is all RAM above 896 MB, including RAM above 4 GB on sufficiently large machines.</li>
</ul>
<p>每个zone下很多pages(大小为4K)，buddy就是这些Pages的组织管理者</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"># cat /proc/zoneinfo |grep Node -A10</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3972</span><br><span class="line">        min      0</span><br><span class="line">        low      0</span><br><span class="line">        high     0</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3993</span><br><span class="line">        managed  3972</span><br><span class="line">    nr_free_pages 3972</span><br><span class="line">    nr_alloc_batch 0</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     361132</span><br><span class="line">        min      30</span><br><span class="line">        low      37</span><br><span class="line">        high     45</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  430773</span><br><span class="line">        managed  361133</span><br><span class="line">    nr_free_pages 361132</span><br><span class="line">    nr_alloc_batch 8</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     96017308</span><br><span class="line">        min      16864</span><br><span class="line">        low      21080</span><br><span class="line">        high     25296</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  200736768</span><br><span class="line">        present  200736768</span><br><span class="line">        managed  197571780</span><br><span class="line">    nr_free_pages 96017308</span><br><span class="line">    nr_alloc_batch 3807</span><br><span class="line">    </span><br><span class="line"># free -g</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            755         150         367           3         236         589</span><br><span class="line">Swap:             0           0           0</span><br></pre></td></tr></table></figure>

<p>每个页面大小是4K，很容易可以计算出每个 zone 的大小。比如对于上面 Node0 的 Normal， 197571780 * 4K&#x2F;(1024*1024) &#x3D; 753 GB。</p>
<p>dmidecode 可以查看到服务器上插着的所有内存条，也可以看到它是和哪个CPU直接连接的。每一个CPU以及和他直连的内存条组成了一个 <strong>node（节点）</strong></p>
<h3 id="x2F-proc-x2F-buddyinfo"><a href="#x2F-proc-x2F-buddyinfo" class="headerlink" title="&#x2F;proc&#x2F;buddyinfo"></a>&#x2F;proc&#x2F;buddyinfo</h3><p>&#x2F;proc&#x2F;buddyinfo记录了<strong>可用内存</strong>的情况。</p>
<p>Normal那行之后的第二列表示：  643847*2^1*Page_Size(4K) ;  第三列表示：  357451*2^2*Page_Size(4K)  ，高阶内存指的是2^3及更大的内存块。</p>
<p>应用申请大块连续内存（高阶内存，一般之4阶及以上, 也就是64K以上–2^4*4K）时，容易导致卡顿。这是因为大块连续内存确实系统需要触发回收或者碎片整理，需要一定的时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/buddyinfo </span><br><span class="line">Node 0, zone      DMA      1      1      1      0      2      1      1      0      1      1      3 </span><br><span class="line">Node 0, zone    DMA32      2      5      3      6      2      0      4      4      2      2    404 </span><br><span class="line">Node 0, zone   Normal 243430 643847 357451  32531   9508   6159   3917   2960  17172   2633  22854</span><br><span class="line"></span><br><span class="line">如果是多node机器：</span><br><span class="line">#cat /proc/buddyinfo</span><br><span class="line">Node 0, zone      DMA      4      6      3      2      3      3      1      1      2      3      1</span><br><span class="line">Node 0, zone    DMA32   1607   1619   1552   1520   1370   1065    827    576    284    105     13</span><br><span class="line">Node 0, zone   Normal  38337 145731 222145 199776 151452  91969  38086  10037   1762    104      1</span><br><span class="line">Node 1, zone   Normal  21521 147637 299185 245533 172451  81459  19451   7198    579      3      0</span><br><span class="line">Node 2, zone   Normal  68427 538670 446906 229138 123555  62539  21161   4407   1122    166    274</span><br><span class="line">Node 3, zone   Normal  27353  54601 114355 123568 101892  79098  48610  21036   5021    475      6</span><br><span class="line">Node 4, zone   Normal  45802  42758   8573 184548 148397  70540  20772   4147    381    148    109</span><br><span class="line">Node 5, zone   Normal  19514  39583 140493 167901 134774  61888  22998   6326    457     32      0</span><br><span class="line">Node 6, zone   Normal 104493 378362 355158  93138  12928   2248   1019    663    172     40    121</span><br><span class="line">Node 7, zone   Normal  34185 256886 249560  95547  54526  51022  28180   9757   2038   1351    280</span><br><span class="line"></span><br><span class="line">[root@hygon8 15:50 /root]</span><br><span class="line">#numactl -H</span><br><span class="line">available: 8 nodes (0-7)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 64 65 66 67 68 69 70 71</span><br><span class="line">node 0 size: 64083 MB</span><br><span class="line">node 0 free: 49838 MB</span><br><span class="line">node 1 cpus: 8 9 10 11 12 13 14 15 72 73 74 75 76 77 78 79</span><br><span class="line">node 1 size: 64480 MB</span><br><span class="line">node 1 free: 43596 MB</span><br><span class="line">node 2 cpus: 16 17 18 19 20 21 22 23 80 81 82 83 84 85 86 87</span><br><span class="line">node 2 size: 64507 MB</span><br><span class="line">node 2 free: 44216 MB</span><br><span class="line">node 3 cpus: 24 25 26 27 28 29 30 31 88 89 90 91 92 93 94 95</span><br><span class="line">node 3 size: 64507 MB</span><br><span class="line">node 3 free: 51095 MB</span><br><span class="line">node 4 cpus: 32 33 34 35 36 37 38 39 96 97 98 99 100 101 102 103</span><br><span class="line">node 4 size: 64507 MB</span><br><span class="line">node 4 free: 32877 MB</span><br><span class="line">node 5 cpus: 40 41 42 43 44 45 46 47 104 105 106 107 108 109 110 111</span><br><span class="line">node 5 size: 64507 MB</span><br><span class="line">node 5 free: 33430 MB</span><br><span class="line">node 6 cpus: 48 49 50 51 52 53 54 55 112 113 114 115 116 117 118 119</span><br><span class="line">node 6 size: 64507 MB</span><br><span class="line">node 6 free: 14233 MB</span><br><span class="line">node 7 cpus: 56 57 58 59 60 61 62 63 120 121 122 123 124 125 126 127</span><br><span class="line">node 7 size: 63483 MB</span><br><span class="line">node 7 free: 36577 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3   4   5   6   7</span><br><span class="line">  0:  10  16  16  16  28  28  22  28</span><br><span class="line">  1:  16  10  16  16  28  28  28  22</span><br><span class="line">  2:  16  16  10  16  22  28  28  28</span><br><span class="line">  3:  16  16  16  10  28  22  28  28</span><br><span class="line">  4:  28  28  22  28  10  16  16  16</span><br><span class="line">  5:  28  28  28  22  16  10  16  16</span><br><span class="line">  6:  22  28  28  28  16  16  10  16</span><br><span class="line">  7:  28  22  28  28  16  16  16  10</span><br><span class="line">  </span><br><span class="line">[root@hygon8 15:51 /root]</span><br><span class="line">#cat /proc/pagetypeinfo</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    0, zone      DMA, type    Unmovable      1      2      1      1      3      2      0      0      1      0      0</span><br><span class="line">Node    0, zone      DMA, type      Movable      0      0      0      0      0      0      0      0      0      3      1</span><br><span class="line">Node    0, zone      DMA, type  Reclaimable      3      4      2      1      0      1      1      1      1      0      0</span><br><span class="line">Node    0, zone      DMA, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, type    Unmovable    151    164    162    165    140     78     19      8      0      0      0</span><br><span class="line">Node    0, zone    DMA32, type      Movable   1435   1430   1374   1335   1214    974    798    563    281     98     12</span><br><span class="line">Node    0, zone    DMA32, type  Reclaimable     21     25     16     20     16     13     10      5      3      7      1</span><br><span class="line">Node    0, zone    DMA32, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, type    Unmovable   4849   6607   4133   1629    654    121     15      3      0      0      0</span><br><span class="line">Node    0, zone   Normal, type      Movable  21088 &gt;100000 &gt;100000 &gt;100000 &gt;100000  90231  37197   9379   1552     83      1</span><br><span class="line">Node    0, zone   Normal, type  Reclaimable    153    139   3012   3113   2437   1617    874    655    210     21      0</span><br><span class="line">Node    0, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 0, zone      DMA            1            6            1            0            0</span><br><span class="line">Node 0, zone    DMA32           27          974           15            0            0</span><br><span class="line">Node 0, zone   Normal          856        30173          709            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    1, zone   Normal, type    Unmovable    842   2898   2495   1316    490    102     23      1      2      0      0</span><br><span class="line">Node    1, zone   Normal, type      Movable  22484 &gt;100000 &gt;100000 &gt;100000 &gt;100000  80084  18922   6889     48      4      0</span><br><span class="line">Node    1, zone   Normal, type  Reclaimable      1   2022   3850   3534   2582   1273    506    308    529      0      0</span><br><span class="line">Node    1, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    1, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 1, zone   Normal          810        31221          737            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    2, zone   Normal, type    Unmovable   2833   6802   3888   1636    329      3      1      2      0      0      0</span><br><span class="line">Node    2, zone   Normal, type      Movable  72017 &gt;100000 &gt;100000 &gt;100000 &gt;100000  61710  20764   4242    841     55    239</span><br><span class="line">Node    2, zone   Normal, type  Reclaimable    114      8   2056   2221   1544    826    396    163    281    111     35</span><br><span class="line">Node    2, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    2, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 2, zone   Normal         1066        31063          639            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    3, zone   Normal, type    Unmovable   2508   6171   3802   1502    365     93     30      1      2      0      0</span><br><span class="line">Node    3, zone   Normal, type      Movable  23396  48450 &gt;100000 &gt;100000  99802  77850  47910  20587   4796    428      5</span><br><span class="line">Node    3, zone   Normal, type  Reclaimable     10      0    609   2111   1726   1155    670    448    223     46      1</span><br><span class="line">Node    3, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    3, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 3, zone   Normal          768        31425          575            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    4, zone   Normal, type    Unmovable   3817   3739   1716    992    261     39      4      1      0      0      1</span><br><span class="line">Node    4, zone   Normal, type      Movable  27857  39138   6875 &gt;100000 &gt;100000  70501  20752   4115    362     49    104</span><br><span class="line">Node    4, zone   Normal, type  Reclaimable      1      8      3      5      0      0     16     31     19     97      4</span><br><span class="line">Node    4, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    4, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 4, zone   Normal          712        31706          350            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    5, zone   Normal, type    Unmovable   4875   4728   3165   1202    464     67      3      0      0      0      0</span><br><span class="line">Node    5, zone   Normal, type      Movable  18382  34874 &gt;100000 &gt;100000 &gt;100000  61296  22711   6235    348     32      0</span><br><span class="line">Node    5, zone   Normal, type  Reclaimable     16      0      1      7      2    525    284     91    109      0      0</span><br><span class="line">Node    5, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    5, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 5, zone   Normal          736        31716          316            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    6, zone   Normal, type    Unmovable  10489   6842   2821    434    257     22      1      1      1      3      0</span><br><span class="line">Node    6, zone   Normal, type      Movable  90841 &gt;100000 &gt;100000  92129  11336   1526    704    552    141     34    118</span><br><span class="line">Node    6, zone   Normal, type  Reclaimable    434     41      0    576   1338    700    314    110     30      5      3</span><br><span class="line">Node    6, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    6, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 6, zone   Normal          807        31686          275            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate type at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    7, zone   Normal, type    Unmovable   1516   1894   2285    908    633    121     16      7      4      2      0</span><br><span class="line">Node    7, zone   Normal, type      Movable  18209 &gt;100000 &gt;100000  93283  52811  50349  27973   9703   2026   1341    248</span><br><span class="line">Node    7, zone   Normal, type  Reclaimable      0      1      0   1341   1082    552    191     47      8      8     32</span><br><span class="line">Node    7, zone   Normal, type   HighAtomic      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    7, zone   Normal, type      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks type     Unmovable      Movable  Reclaimable   HighAtomic      Isolate</span><br><span class="line">Node 7, zone   Normal         1262        31265          241            0            0</span><br></pre></td></tr></table></figure>

<h3 id="x2F-proc-x2F-pagetypeinfo"><a href="#x2F-proc-x2F-pagetypeinfo" class="headerlink" title="&#x2F;proc&#x2F;pagetypeinfo"></a>&#x2F;proc&#x2F;pagetypeinfo</h3><p><code>cat /proc/pagetypeinfo</code>, 你可以看到当前系统里伙伴系统里各个尺寸的可用连续内存块数量。unmovable pages是不可以被迁移的，比如slab等kmem都不可以被迁移，因为内核里面对这些内存很多情况下是通过指针来访问的，而不是通过页表，如果迁移的话，就会导致原来的指针访问出错。</p>
<p><img src="/Users/ren/case/ossimg/2e73247c-8a10-43e6-bb0e-49ecfff14268.png" alt="img"></p>
<p><strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong></p>
<p>alloc_pages分配内存的时候就到上面对应大小的free_area的链表上寻找可用连续页面。<code>alloc_pages</code>是怎么工作的呢？我们举个简单的小例子。假如要申请8K-连续两个页框的内存。为了描述方便，我们先暂时忽略UNMOVEABLE、RELCLAIMABLE等不同类型</p>
<p><img src="/Users/ren/case/ossimg/16ebe996-0e3a-4d67-810f-3121b457271e.png" alt="img"></p>
<p>基于伙伴系统的内存分配中，有可能需要将大块内存拆分成两个小伙伴。在释放中，可能会将两个小伙伴合并再次组成更大块的连续内存。</p>
<blockquote>
<p>伙伴系统中的伙伴指的是两个内存块，大小相同，地址连续，同属于一个大块区域。</p>
</blockquote>
<p>对于应用来说基本分配单位是4K(开启大页后一般是2M)，对于内核来说4K有点浪费。所以内核又专门给自己定制了一个更精细的内存管理系统slab。</p>
<h3 id="slab"><a href="#slab" class="headerlink" title="slab"></a>slab</h3><p>对于内核运行中实际使用的对象来说，多大的对象都有。有的对象有1K多，但有的对象只有几百、甚至几十个字节。如果都直接分配一个 4K的页面 来存储的话也太浪费了，所以伙伴系统并不能直接使用。</p>
<p>在伙伴系统之上，<strong>内核又给自己搞了一个专用的内存分配器， 叫slab</strong>。</p>
<p>这个分配器最大的特点就是，一个slab内只分配特定大小、甚至是特定的对象。这样当一个对象释放内存后，另一个同类对象可以直接使用这块内存。通过这种办法极大地降低了碎片发生的几率。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/meminfo</span><br><span class="line">Slab:             102076 kB</span><br><span class="line">SReclaimable:      70816 kB</span><br><span class="line">SUnreclaim:        31260 kB</span><br></pre></td></tr></table></figure>

<ul>
<li>Slab 内核通过slab分配管理的内存总数。</li>
<li>SReclaimable 内核通过slab分配的可回收的内存（例如dentry），通过echo 2 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches回收。</li>
<li>SUnreclaim 内核通过slab分配的不可回收的内存。</li>
</ul>
<h4 id="kmem-cache"><a href="#kmem-cache" class="headerlink" title="kmem cache"></a>kmem cache</h4><p>slabtop和&#x2F;proc&#x2F;slabinfo 查看cached使用情况 主要是：pagecache（页面缓存）， dentries（目录缓存）， inodes</p>
<p>通过查看 <code>/proc/slabinfo</code> 我们可以查看到所有的 kmem cache。</p>
<p><img src="/Users/ren/case/ossimg/5135d81f-6985-4d6a-8896-e451c0ba20f5.png" alt="img"></p>
<p>slabtop 按占用内存从大往小进行排列。用来分析 slab 内存开销。</p>
<p><img src="/Users/ren/case/ossimg/0d8a26db-3663-40af-b215-f8601ef23676.png" alt="0d8a26db-3663-40af-b215-f8601ef23676.png (1388×1506)"></p>
<p>无论是 <code>/proc/slabinfo</code>，还是 slabtop 命令的输出。里面都包含了每个 cache 中 slab的如下几个关键属性。</p>
<ul>
<li><p>objsize：每个对象的大小</p>
</li>
<li><p>objperslab：一个 slab 里存放的对象的数量</p>
</li>
<li><p>pagesperslab：一个slab 占用的页面的数量，每个页面4K，这样也就能算出每个 slab 占用的内存大小。</p>
</li>
</ul>
<p>比如如下TCP slabinfo中可以看到每个slab占用8(pagesperslab)个Page(8*4096&#x3D;32768)，每个对象的大小是1984(objsize)，每个slab存放了16(objperslab)个对象. 那么1984 *16&#x3D;31744，现在的空间基本用完，剩下接近1K，又放不下一个1984大小的对象，算是额外开销了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/slabinfo |grep -E &quot;active_objs|TCP&quot;</span><br><span class="line"># name            &lt;active_objs&gt; &lt;num_objs&gt; &lt;objsize&gt; &lt;objperslab&gt; &lt;pagesperslab&gt; : tunables &lt;limit&gt; &lt;batchcount&gt; &lt;sharedfactor&gt; : slabdata &lt;active_slabs&gt; &lt;num_slabs&gt; &lt;sharedavail&gt;</span><br><span class="line">tw_sock_TCP         5372   5728    256   32    2 : tunables    0    0    0 : slabdata    179    179      0</span><br><span class="line">TCP                 6090   6144   1984   16    8 : tunables    0    0    0 : slabdata    384    384      0</span><br></pre></td></tr></table></figure>

<h2 id="内存分配和延迟"><a href="#内存分配和延迟" class="headerlink" title="内存分配和延迟"></a>内存分配和延迟</h2><p>内存不够、脏页太多、碎片太多，都会导致分配失败，从而触发回收，导致卡顿。</p>
<h3 id="系统中脏页过多引起-load-飙高"><a href="#系统中脏页过多引起-load-飙高" class="headerlink" title="系统中脏页过多引起 load 飙高"></a>系统中脏页过多引起 load 飙高</h3><p>直接回收过程中，如果存在较多脏页就可能涉及在回收过程中进行回写，这可能会造成非常大的延迟，而且因为这个过程本身是阻塞式的，所以又可能进一步导致系统中处于 D 状态的进程数增多，最终的表现就是系统的 load 值很高。</p>
<img src="/Users/ren/case/ossimg/f16438b744a248d7671d5ac7317b0a98.png" alt="image.png" style="zoom: 50%;">

<p>可以通过 sar -r 来观察系统中的脏页个数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sar -r 1</span><br><span class="line">07:30:01 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</span><br><span class="line">09:20:01 PM   5681588   2137312     27.34         0   1807432    193016      2.47    534416   1310876         4</span><br><span class="line">09:30:01 PM   5677564   2141336     27.39         0   1807500    204084      2.61    539192   1310884        20</span><br><span class="line">09:40:01 PM   5679516   2139384     27.36         0   1807508    196696      2.52    536528   1310888        20</span><br><span class="line">09:50:01 PM   5679548   2139352     27.36         0   1807516    196624      2.51    536152   1310892        24</span><br></pre></td></tr></table></figure>

<p>kbdirty 就是系统中的脏页大小，它同样也是对 &#x2F;proc&#x2F;vmstat 中 nr_dirty 的解析。你可以通过调小如下设置来将系统脏页个数控制在一个合理范围:</p>
<blockquote>
<p>vm.dirty_background_bytes &#x3D; 0</p>
<p>vm.dirty_background_ratio &#x3D; 10</p>
<p>vm.dirty_bytes &#x3D; 0</p>
<p>vm.dirty_expire_centisecs &#x3D; 3000</p>
<p>vm.dirty_ratio &#x3D; 20</p>
</blockquote>
<p>至于这些值调整大多少比较合适，也是因系统和业务的不同而异，我的建议也是一边调整一边观察，将这些值调整到业务可以容忍的程度就可以了，即在调整后需要观察业务的服务质量 (SLA)，要确保 SLA 在可接受范围内。调整的效果你可以通过 &#x2F;proc&#x2F;vmstat 来查看：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#grep &quot;nr_dirty_&quot; /proc/vmstat</span><br><span class="line">nr_dirty_threshold 3071708</span><br><span class="line">nr_dirty_background_threshold 1023902</span><br></pre></td></tr></table></figure>

<p>在4.20的内核并且sar 的版本为12.3.3可以看到PSI（Pressure-Stall Information）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">some avg10=45.49 avg60=10.23 avg300=5.41 total=76464318</span><br><span class="line">full avg10=40.87 avg60=9.05 avg300=4.29 total=58141082</span><br></pre></td></tr></table></figure>

<p>你需要重点关注 avg10 这一列，它表示最近 10s 内存的平均压力情况，如果它很大（比如大于 40）那 load 飙高大概率是由于内存压力，尤其是 Page Cache 的压力引起的。</p>
<p><img src="/Users/ren/case/ossimg/cf58f10a523e1e4f0db443be3f54fc04.png" alt="image.png"></p>
<h3 id="容器中的内存回收"><a href="#容器中的内存回收" class="headerlink" title="容器中的内存回收"></a>容器中的内存回收</h3><blockquote>
<p>kswapd线程(每个node一个kswapd进程，负责本node）回收内存时，可以先对脏页进行回写（writeback）再进行回收，而直接内存回收只回收干净页。也叫同步回收.</p>
<p>直接内存回收是在当前进程的上下文中进行的，要等内存回收完成才能继续尝试进行分配，所以是阻塞了当前进程的执行，会导致响应延迟增加</p>
</blockquote>
<p>如果是在容器里，也就是在某个子memory cgroup 中，那么在分配内存后，还有一个记账（charge）的步骤，就是要把这次分配的内存页记在某个memory cgroup的账上，这样才能控制这个容器里的进程所能使用的内存数量。</p>
<p>在开源社区的linux代码中，如果charge 失败，也就是说，当新分配的内存加上原先的usage超过了limit，就会触发内存回收，try_to_free_mem_cgroup_pages，这个也是同步回收，等同于直接内存回收（发生在当前进程的上下文忠），所以会对应用的响应造成影响（表现为卡顿）。</p>
<h2 id="碎片化"><a href="#碎片化" class="headerlink" title="碎片化"></a>碎片化</h2><p>内存碎片严重的话会导致系统hang很久(回收、压缩内存）</p>
<p>尽量让系统的free多一点(比例高一点）可以调整 vm.min_free_kbytes(128G 以内 2G，256G以内 4G&#x2F;8G), 线上机器直接修改vm.min_free_kbytes<strong>会触发回收，导致系统hang住</strong> <a href="https://www.atatech.org/articles/163233" target="_blank" rel="noopener">https://www.atatech.org/articles/163233</a> <a href="https://www.atatech.org/articles/97130" target="_blank" rel="noopener">https://www.atatech.org/articles/97130</a></p>
<p>每个zone都有自己的min low high,如下，但是单位是page, 计算案例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/zoneinfo  |grep &quot;Node&quot;</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">Node 1, zone   Normal</span><br><span class="line"></span><br><span class="line">#cat /proc/zoneinfo  |grep &quot;Node 0, zone&quot; -A10</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3975</span><br><span class="line">        min      20</span><br><span class="line">        low      25</span><br><span class="line">        high     30</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  4095</span><br><span class="line">        present  3996</span><br><span class="line">        managed  3975</span><br><span class="line">    nr_free_pages 3975</span><br><span class="line">    nr_alloc_batch 5</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     382873</span><br><span class="line">        min      2335</span><br><span class="line">        low      2918</span><br><span class="line">        high     3502</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  1044480</span><br><span class="line">        present  513024</span><br><span class="line">        managed  450639</span><br><span class="line">    nr_free_pages 382873</span><br><span class="line">    nr_alloc_batch 584</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     11105097</span><br><span class="line">        min      61463</span><br><span class="line">        low      76828</span><br><span class="line">        high     92194</span><br><span class="line">        scanned  0</span><br><span class="line">        spanned  12058624</span><br><span class="line">        present  12058624</span><br><span class="line">        managed  11859912</span><br><span class="line">    nr_free_pages 11105097</span><br><span class="line">    nr_alloc_batch 12344</span><br><span class="line">    </span><br><span class="line">    low = 5/4 * min</span><br><span class="line">high = 3/2 * min</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#T=min;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=499 MB</span><br><span class="line"></span><br><span class="line">#T=low;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=624 MB</span><br><span class="line"></span><br><span class="line">#T=high;sum=0;for i in `cat /proc/zoneinfo  |grep $T | awk &apos;&#123;print $NF&#125;&apos;`;do sum=`echo &quot;$sum+$i&quot; |bc`;done;sum=`echo &quot;$sum*4/1024&quot; |bc`;echo &quot;sum=$&#123;sum&#125; MB&quot;</span><br><span class="line">sum=802 MB</span><br></pre></td></tr></table></figure>

<h2 id="内存碎片化导致rt升高的诊断"><a href="#内存碎片化导致rt升高的诊断" class="headerlink" title="内存碎片化导致rt升高的诊断"></a>内存碎片化导致rt升高的诊断</h2><p>判定方法如下：</p>
<ol>
<li>运行 sar -B 观察 pgscand&#x2F;s，其含义为每秒发生的直接内存回收次数，当在一段时间内持续大于 0 时，则应继续执行后续步骤进行排查；</li>
<li>运行 <code>cat /sys/kernel/debug/extfrag/extfrag_index</code> 观察内存碎片指数，重点关注 order &gt;&#x3D; 3 的碎片指数，当接近 1.000 时，表示碎片化严重，当接近 0 时表示内存不足；</li>
<li>运行 <code>cat /proc/buddyinfo, cat /proc/pagetypeinfo</code> 查看内存碎片情况， 指标含义<a href="https://man7.org/linux/man-pages/man5/proc.5.html" target="_blank" rel="noopener">参考</a> ，同样关注 order &gt;&#x3D; 3 的剩余页面数量，pagetypeinfo 相比 buddyinfo 展示的信息更详细一些，根据迁移类型 （伙伴系统通过迁移类型实现反碎片化）进行分组，需要注意的是，<strong>当迁移类型为 Unmovable 的页面都聚集在 order &lt; 3 时，说明内核 slab 碎片化严重</strong>，我们需要结合其他工具来排查具体原因，在本文就不做过多介绍了；</li>
<li>对于 CentOS 7.6 等支持 BPF 的 kernel 也可以运行我们研发的 <a href="https://github.com/iovisor/bcc/blob/master/tools/drsnoop_example.txt" target="_blank" rel="noopener">drsnoop</a>，<a href="https://github.com/iovisor/bcc/blob/master/tools/compactsnoop_example.txt" target="_blank" rel="noopener">compactsnoop</a> 工具对延迟进行定量分析，使用方法和解读方式请参考对应文档；</li>
<li>(Opt) 使用 ftrace 抓取 mm_page_alloc_extfrag 事件，观察因内存碎片从备用迁移类型“盗取”页面的信息。</li>
</ol>
<h2 id="一个阿里云ECS-因为宿主机碎片导致性能衰退的案例"><a href="#一个阿里云ECS-因为宿主机碎片导致性能衰退的案例" class="headerlink" title="一个阿里云ECS 因为宿主机碎片导致性能衰退的案例"></a>一个阿里云ECS 因为宿主机碎片导致性能衰退的案例</h2><p>LVS后面三个RS在同样压力流量下，其中一个节点CPU非常高，通过top看起来是所有操作都很慢，像是CPU被降频了一样，但是直接跑CPU Prime性能又没有问题</p>
<p><img src="/Users/ren/case/ossimg/8bbb5c886dc06196546daec46712ff71.png" alt="image.png"></p>
<p>原因：ECS所在的宿主机内存碎片比较严重，导致分配到的内存主要是4K Page，在ECS中大页场景下会慢很多</p>
<p>通过 <strong>openssl speed aes-256-ige 能稳定重现</strong> 在大块的加密上慢很多</p>
<p><img src="/Users/ren/case/ossimg/8e15e91d4dcc61bbd329e7283c7c7500.png" alt="image.png"></p>
<p>小块上性能一致，这也就是为什么算Prime性能没问题。导致慢只涉及到大块内存分配的场景，这里需要映射到宿主机，但是碎片多分配慢导致了问题。</p>
<p>如果reboot ECS的话实际只是就地重启ECS，仍然使用的reboot前分配好的宿主机内存，不会解决问题。重启ECS中的进程也不会解决问题，只有将ECS迁移到别的物理机（也就是通过控制台重启，会重新选择物理机）才有可能解决这个问题。</p>
<p>或者购买新的ECS机型（比如第6代之后ECS）能避免这个问题。</p>
<p>ECS内部没法查看到这个碎片，只能在宿主机上通过命令查看大页情况：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">有问题NC上buddyinfo信息</span><br><span class="line">$cat /proc/buddyinfo</span><br><span class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</span><br><span class="line">Node 0, zone    DMA32     23     23     17     15     13      9      8      8      4      3    367</span><br><span class="line">Node 0, zone   Normal 295291 298652 286048 266597 218191 156837  93156  45930  25856      0      0</span><br><span class="line"></span><br><span class="line">最新建的vm，大页不多</span><br><span class="line">$sudo cat /proc/9550/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</span><br><span class="line">210944</span><br><span class="line">------------------------</span><br><span class="line">第一台正常ECS所在的NC</span><br><span class="line">$cat /proc/buddyinfo</span><br><span class="line">Node 0, zone      DMA      1      1      0      0      2      1      1      0      1      1      3</span><br><span class="line">Node 0, zone    DMA32      7      5      5      9      8      4      6     10      5      5    366</span><br><span class="line">Node 0, zone   Normal 203242 217888 184465 176280 148612 102122  55787  26642  24824      0      0</span><br><span class="line"></span><br><span class="line">早期的vm，大页充足</span><br><span class="line">$sudo cat /proc/87369/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</span><br><span class="line">8275968</span><br><span class="line"></span><br><span class="line">近期的vm，大页不够</span><br><span class="line">$sudo cat /proc/22081/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</span><br><span class="line">251904</span><br><span class="line"></span><br><span class="line">$sudo cat /proc/44073/smaps |grep AnonHuge |awk &apos;&#123;sum+=$2&#125;END&#123;print sum&#125;&apos;</span><br><span class="line">10240</span><br></pre></td></tr></table></figure>

<h2 id="内存使用分析"><a href="#内存使用分析" class="headerlink" title="内存使用分析"></a>内存使用分析</h2><h3 id="pmap"><a href="#pmap" class="headerlink" title="pmap"></a>pmap</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">pmap -x 24282 | less</span><br><span class="line">24282:   /usr/sbin/rsyslogd -n</span><br><span class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</span><br><span class="line">000055ce1a99f000     596     580       0 r-x-- rsyslogd</span><br><span class="line">000055ce1ac34000      12      12      12 r---- rsyslogd</span><br><span class="line">000055ce1ac37000      28      28      28 rw--- rsyslogd</span><br><span class="line">000055ce1ac3e000       4       4       4 rw---   [ anon ]</span><br><span class="line">000055ce1c1f1000     364     204     204 rw---   [ anon ]</span><br><span class="line">00007fff8b5a4000     132      20      20 rw---   [ stack ]</span><br><span class="line">00007fff8b5e6000      12       0       0 r----   [ anon ]</span><br><span class="line">00007fff8b5e9000       8       4       0 r-x--   [ anon ]</span><br><span class="line">---------------- ------- ------- -------</span><br><span class="line">total kB          620060   17252    3304</span><br></pre></td></tr></table></figure>

<ul>
<li>Address：占用内存的文件的内存起始地址。</li>
<li>Kbytes：占用内存的字节数。</li>
<li>RSS：实际占用内存大小。</li>
<li>Dirty：脏页大小。</li>
<li>Mapping：占用内存的文件，[anon] 为已分配的内存，[stack] 为程序堆栈</li>
</ul>
<h3 id="x2F-proc-x2F-pid-x2F"><a href="#x2F-proc-x2F-pid-x2F" class="headerlink" title="&#x2F;proc&#x2F;pid&#x2F;"></a>&#x2F;proc&#x2F;pid&#x2F;</h3><p><code>/proc/[pid]/</code> 下面与进程内存相关的文件主要有<code>maps , smaps, status</code>。<br>maps： 文件可以查看某个进程的代码段、栈区、堆区、动态库、内核区对应的虚拟地址<br>smaps: 显示每个分区更详细的内存占用数据，能看到一个动态库被共享了几次<br>status: 包含了所有CPU活跃的信息，该文件中的所有值都是从系统启动开始累计到当前时刻</p>
<h2 id="Java内存使用分析"><a href="#Java内存使用分析" class="headerlink" title="Java内存使用分析"></a><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/troubleshoot/tooldescr007.html" target="_blank" rel="noopener">Java内存使用分析</a></h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">创建<span class="number">1000</span>个线程，ss为<span class="number">2</span>M</span><br><span class="line">java -XX:NativeMemoryTracking=detail -Xms10g -Xmx10g -Xmn5g -XX:ReservedCodeCacheSize=<span class="number">512</span>m -XX:MetaspaceSize=<span class="number">512</span>m -XX:MaxMetaspaceSize=<span class="number">512</span>m -XX:MaxDirectMemorySize=<span class="number">1</span>g -Xss2048K ThreadPoolExample</span><br><span class="line"></span><br><span class="line">分析结果：</span><br><span class="line">#jcmd 81849 VM.native_memory summary</span><br><span class="line"><span class="number">81849</span>:</span><br><span class="line"></span><br><span class="line">Native Memory Tracking:</span><br><span class="line"></span><br><span class="line">Total: reserved=<span class="number">14737064</span>KB, committed=<span class="number">13157168</span>KB</span><br><span class="line">-                 <span class="function">Java <span class="title">Heap</span> <span class="params">(reserved=<span class="number">10485760</span>KB, committed=<span class="number">10485760</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(mmap: reserved=<span class="number">10485760</span>KB, committed=<span class="number">10485760</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                     <span class="title">Class</span> <span class="params">(reserved=<span class="number">1102016</span>KB, committed=<span class="number">50112</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(classes #<span class="number">416</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">45248</span>KB #<span class="number">1420</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(mmap: reserved=<span class="number">1056768</span>KB, committed=<span class="number">4864</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"><span class="comment">//committed 已向OS提请分配，实际要到使用时page fault才会实际分配物理内存并在RSS中反应出来</span></span></span><br><span class="line"><span class="function">-                    <span class="title">Thread</span> <span class="params">(reserved=<span class="number">2134883</span>KB, committed=<span class="number">2134883</span>KB)</span><span class="comment">//reserved还没分配,不能访问</span></span></span><br><span class="line"><span class="function">                            <span class="params">(thread #<span class="number">1070</span>)</span> <span class="comment">//1000个应用线程，加70个JVM native线程</span></span></span><br><span class="line"><span class="function">                            <span class="params">(stack: reserved=<span class="number">2128820</span>KB, committed=<span class="number">2128820</span>KB)</span> <span class="comment">//需要2G多点</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">3500</span>KB #<span class="number">5390</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(arena=<span class="number">2563</span>KB #<span class="number">2138</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                      <span class="title">Code</span> <span class="params">(reserved=<span class="number">532612</span>KB, committed=<span class="number">4620</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">132</span>KB #<span class="number">528</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(mmap: reserved=<span class="number">532480</span>KB, committed=<span class="number">4488</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                        <span class="title">GC</span> <span class="params">(reserved=<span class="number">430421</span>KB, committed=<span class="number">430421</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">50737</span>KB #<span class="number">235</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(mmap: reserved=<span class="number">379684</span>KB, committed=<span class="number">379684</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                  <span class="title">Compiler</span> <span class="params">(reserved=<span class="number">137</span>KB, committed=<span class="number">137</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">6</span>KB #<span class="number">53</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(arena=<span class="number">131</span>KB #<span class="number">3</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                  <span class="title">Internal</span> <span class="params">(reserved=<span class="number">48901</span>KB, committed=<span class="number">48901</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">48869</span>KB #<span class="number">14030</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(mmap: reserved=<span class="number">32</span>KB, committed=<span class="number">32</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-                    <span class="title">Symbol</span> <span class="params">(reserved=<span class="number">1479</span>KB, committed=<span class="number">1479</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">959</span>KB #<span class="number">110</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(arena=<span class="number">520</span>KB #<span class="number">1</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-    Native Memory <span class="title">Tracking</span> <span class="params">(reserved=<span class="number">608</span>KB, committed=<span class="number">608</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">193</span>KB #<span class="number">2556</span>)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(tracking overhead=<span class="number">415</span>KB)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">-               Arena <span class="title">Chunk</span> <span class="params">(reserved=<span class="number">248</span>KB, committed=<span class="number">248</span>KB)</span></span></span><br><span class="line"><span class="function">                            <span class="params">(malloc=<span class="number">248</span>KB)</span></span></span><br></pre></td></tr></table></figure>

<p>We can see two types of memory:</p>
<ul>
<li><strong>Reserved</strong> — the size which is guaranteed to be available by a host’s OS (but still not allocated and cannot be accessed by JVM) — it’s just a promise</li>
<li><strong>Committed</strong> — already taken, accessible, and allocated by JVM</li>
</ul>
<h3 id="page-fault"><a href="#page-fault" class="headerlink" title="page fault"></a>page fault</h3><p>内核给用户态申请的内存，默认都只是一段虚拟地址空间而已，并没有分配真正的物理内存。在第一次读写的时候才触发物理内存的分配，这个过程叫做page fault。那么，为了访问到真正的物理内存，page fault的时候，就需要更新对应的page table了。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.atatech.org/articles/66885" target="_blank" rel="noopener">https://www.atatech.org/articles/66885</a></p>
<p><a href="https://cloud.tencent.com/developer/article/1087455" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/13719610.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>
<p><a href="https://blog.csdn.net/fanren224/article/details/103991748" target="_blank" rel="noopener">rsyslog占用内存高</a></p>
<p><a href="https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html" target="_blank" rel="noopener">https://sunsea.im/rsyslogd-systemd-journald-high-memory-solution.html</a></p>
<p><a href="https://wizardforcel.gitbooks.io/vbird-linux-basic-4e/content/160.html" target="_blank" rel="noopener">鸟哥 journald 介绍</a></p>
<p><a href="https://mp.weixin.qq.com/s/OR2XB4J76haGc1THeq7WQg" target="_blank" rel="noopener">说出来你可能不信，内核这家伙在内存的使用上给自己开了个小灶！</a></p>
<p><a href="https://zhengheng.me/2018/08/27/socket-use-slab-dentry/" target="_blank" rel="noopener">socket 与 slab dentry</a></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Linux/" rel="tag"># Linux</a>
          
            <a href="/tags/free/" rel="tag"># free</a>
          
            <a href="/tags/buddyinfo/" rel="tag"># buddyinfo</a>
          
            <a href="/tags/碎片/" rel="tag"># 碎片</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/11/15/Linux内存--HugePage/" rel="next" title="Linux内存--HugePage">
                <i class="fa fa-chevron-left"></i> Linux内存--HugePage
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/11/15/Linux内存--零拷贝/" rel="prev" title="Linux内存--零拷贝">
                Linux内存--零拷贝 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="twitter @plantegg">
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">186</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">275</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Linux内存–管理和碎片"><span class="nav-number">1.</span> <span class="nav-text">Linux内存–管理和碎片</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#物理结构分析"><span class="nav-number">1.1.</span> <span class="nav-text">物理结构分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#虚拟内存和物理内存"><span class="nav-number">1.2.</span> <span class="nav-text">虚拟内存和物理内存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CPU-如何找到真实地址"><span class="nav-number">1.2.1.</span> <span class="nav-text">CPU 如何找到真实地址</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#tlab-miss"><span class="nav-number">1.2.2.</span> <span class="nav-text">tlab miss</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存布局"><span class="nav-number">1.3.</span> <span class="nav-text">内存布局</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存管理和使用"><span class="nav-number">1.4.</span> <span class="nav-text">内存管理和使用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#malloc"><span class="nav-number">1.4.1.</span> <span class="nav-text">malloc</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#malloc和mmap"><span class="nav-number">1.4.2.</span> <span class="nav-text">malloc和mmap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#node-gt-zone-gt-buddy-gt-slab"><span class="nav-number">1.4.3.</span> <span class="nav-text">node-&gt;zone-&gt;buddy-&gt;slab</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#查看zone"><span class="nav-number">1.4.4.</span> <span class="nav-text">查看zone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#x2F-proc-x2F-buddyinfo"><span class="nav-number">1.4.5.</span> <span class="nav-text">/proc/buddyinfo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#x2F-proc-x2F-pagetypeinfo"><span class="nav-number">1.4.6.</span> <span class="nav-text">/proc/pagetypeinfo</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#slab"><span class="nav-number">1.4.7.</span> <span class="nav-text">slab</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#kmem-cache"><span class="nav-number">1.4.7.1.</span> <span class="nav-text">kmem cache</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存分配和延迟"><span class="nav-number">1.5.</span> <span class="nav-text">内存分配和延迟</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#系统中脏页过多引起-load-飙高"><span class="nav-number">1.5.1.</span> <span class="nav-text">系统中脏页过多引起 load 飙高</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#容器中的内存回收"><span class="nav-number">1.5.2.</span> <span class="nav-text">容器中的内存回收</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#碎片化"><span class="nav-number">1.6.</span> <span class="nav-text">碎片化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存碎片化导致rt升高的诊断"><span class="nav-number">1.7.</span> <span class="nav-text">内存碎片化导致rt升高的诊断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#一个阿里云ECS-因为宿主机碎片导致性能衰退的案例"><span class="nav-number">1.8.</span> <span class="nav-text">一个阿里云ECS 因为宿主机碎片导致性能衰退的案例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#内存使用分析"><span class="nav-number">1.9.</span> <span class="nav-text">内存使用分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pmap"><span class="nav-number">1.9.1.</span> <span class="nav-text">pmap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#x2F-proc-x2F-pid-x2F"><span class="nav-number">1.9.2.</span> <span class="nav-text">/proc/pid/</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Java内存使用分析"><span class="nav-number">1.10.</span> <span class="nav-text">Java内存使用分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#page-fault"><span class="nav-number">1.10.1.</span> <span class="nav-text">page fault</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考资料"><span class="nav-number">1.11.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
