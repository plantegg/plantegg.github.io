# 一次POC项目的总结和复盘

## 项目背景

某国产化POC项目，多家公司一起PK

## 业务架构

![image-20210917102658752](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210917102658752.png)

我介入前的一些问题描述，其实核心在TPS到了10000附近就上不去了，下图右下角：

![image-20210917103052855](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210917103052855.png)

## 瓶颈定位

整个项目只能压到10000 TPS 附近，但是看资源应该没跑满。

### 完美的10000 TPS

因为压力起不来，所以从前往后撸，先爬上PTS压力机看了下连接状态，发现==发压力居然是用的短连接，也就是12台PTS机器，每台机器的port range是10000-60000，也就是5万可用端口，12台总共60万可用端口，一个端口默认是60秒timewait，那么TPS就是 60万除以60秒，正好10000 TPS。==

比如下面这个TCP连接状态：

![image-20210927214800904](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210927214800904.png)

赶紧改成长连接继续压。

### TPS 仍然上不去

2000并发，DRDS的状态，SQL QPS能到15-16万之间，RT 在6ms左右，代表DRDS+后面的RDS还可以没到瓶颈。

![image-20210917130220166](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210917130220166.png)

3000并发：

pts（290ms）-> MHPT（225ms）->(10次mysql=9*10=90ms) + 56ms+56ms（注意下图的54.76秒那个调用是重复的，采集展示问题）

![image-20210917132508479](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210917132508479.png)

5000并发：

pts（499ms TPS 9856）-> MHPT（195ms），到ingress抓包统计 ingress会给slb的RT是 381ms，ingress到MHPT之间走kubernetes网络，这里嫌疑比较大

<img src="/Users/ren/src/blog/951413iMgBlog/image-20210917173610865.png" alt="image-20210917173610865" style="zoom: 33%;" />

![image-20210917173533063](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210917173533063.png)

到这里的关键点是从3000到5000并发后，DRDS RT基本没变化（废话，整体TPS没变），==整体TPS没变，RT增加明显，同时可以看到RT增加部分在PTS发压力端到应用端之间，所以毫无疑问这个时候瓶颈点在他们之间。==

他们之间是Ingress（10台nginx转发http请求）和 slb，这两个都是千锤百炼的产品，但是问题一定在你们之间，当然这是没有人愿意承认这个问题的，没有办法的时候就抓包，好在没有人怀疑过抓包的数据。

### 抓包分析

ingress上抓包分析结果：

![image-20210927214604206](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210927214604206.png)

==抓包发现Ingress 响应RT非常不稳，正常情况下都是200ms左右，但是又10%的response响应时间能飙到5-10秒，而且RT不是正常的正态分布。==

### 去掉ingress

下图是去掉Ingress前后两次的TPS对比

![image-20210919021634458](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210919021634458.png)

去掉ingress后，TPS终于上去了，RT抖动也更小了，压测对照抓包几个结论：

> 1.  巅峰 TPS能到13000，维持两分钟，这个时候MHPT的RT也从之前1万TPS的 287 ms上升到了 373ms，说明如果 TPS跑到13000，MHPT（包括后面所有一大坨）才会出来瓶颈；
> 1.  去掉ingress对比之前性能提升30%；
> 1.  ==13000维持2分钟后，TPS掉下去是因为打到MHPT上的请求数不够，为什么我有这个判断是因为TPS往下掉的时候 MHPT 的RT也在降，RT下降只能是压力小了，TPS和RT不会一起下降的，这里还需要看下pts到MHPT之间为啥流量维持不住；==


![image-20210918154645882](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210918154645882.png)

下图中 TPS 13511时RT 380， 2分钟后 TPS一路走低，RT一路走高。pts RT走高过程中，MHPT的RT是走低的。 TPS * RT 这个面积是一个等值才对，如果两个值都减小只能说是压力没过来

![image-20210918163039941](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210918163039941.png)

对比分析去掉ingres前后PTS上的RT时间

![image-20210918160221555](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210918160221555.png)

### MHPT应用做了调整后TPS能稳定了

主要是绑核和业务的Druid连接池设置比较小，容易导致连接阻塞，压力打不到后端数据库上

看起来13000 TPS 在目前配置下就是个天花板

![image-20210919085330792](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210919085330792.png)

应用MHPT容器做了一些绑核操作后：

![image-20210919030532229](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210919030532229.png)

下图红框中的节点RT明显偏高，偏高的倍数基本上和跨路内存延迟增高基本一致，应该是这些ECS正好跨路了![image-20210919030701908](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210919030701908.png)

<img src="/Users/ren/src/blog/951413iMgBlog/image-20210920140357137.png" alt="image-20210920140357137" style="zoom:50%;" />

<img src="/Users/ren/src/blog/951413iMgBlog/image-20210920140453363.png" alt="image-20210920140453363" style="zoom:50%;" />

性能瓶颈定位原理：

-   死盯RT，加并发要么TPS上去要么RT上去，TPS上不去RT上去了瓶颈就一定是RT增加的环节
-   到瓶颈后 TPS*RT 效能面积是稳定的，一个增加另一个就一定减小，同时减小那就是压力不够
-   只有解决了最小的那个瓶颈，后面的瓶颈才会展现出来

## 阶段结果

<table>
<tr class="header">
<th><strong>时间</strong></th>
<th><strong>目标</strong></th>
<th>TPS</th>
<th>RT (ms)</th>
<th><strong>核心优化项</strong></th>
</tr>
<tr class="odd">
<td>2021.9.5</td>
<td>并发 : 5000 TPS : 5000 RT : 500</td>
<td>5344</td>
<td>904</td>
<td>应用，数据库绑核优化</td>
</tr>
<tr class="even">
<td>2021.9.12</td>
<td>并发 : 5000 TPS : 8500 RT : 400</td>
<td>8221</td>
<td>585</td>
<td>系统内核参数优化</td>
</tr>
<tr class="odd">
<td>2021.9.15</td>
<td>并发 : 5000 TPS : 10000 RT : 400</td>
<td>10074</td>
<td>460</td>
<td>应用参数优化</td>
</tr>
<tr class="even">
<td>2021.9.18</td>
<td>并发 : 5000 TPS : 12000 RT : 350</td>
<td>12679</td>
<td>338</td>
<td>网络延时优化 ，去掉Ingress</td>
</tr>
<tr class="odd">
<td>2021.9.19</td>
<td>并发 : 8000 TPS : 12000 RT : 500</td>
<td>12487</td>
<td>567</td>
<td>数据库负载优化</td>
</tr>
<tr class="even">
<td>2021.9.22</td>
<td>并发 : 6000 TPS : 15000 RT : 360</td>
<td><strong>14742</strong></td>
<td>356</td>
<td>应用容量调整</td>
</tr>
</table>

![image-20210924093534204](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210924093534204.png)

## 对国产CPU的性能验证和优化

接下来就是要验证这款国产CPU的性能，在上面的测试过程中ECS团队发现他们把ECS绑到socket0上性能最好，MySQL团队发现把MySQL绑到socket1上性能最好。还有团队发现关掉numa性能更好，怎么可能，这些统统跟我所了解的理论不符。

但是我坚信都是一样好，因为网卡、磁盘所在node的不同会有微小的差异。

所以接下来需要做一些对比验证

### 开关numa 对ECS进行绑核对比

现场给出了以下未开numa的测试数据， 数据肯定是对的，原因就要针对CPU的了解来分析了

![image-20210926171854861](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210926171854861.png)

以及开启numa后的测试数据

![image-20210927222053797](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210927222053797.png)

从数据来看未开numa socket0性能要比socket1好10倍，开了numa后反过来了。

我的分析是这样的，性能差10倍的2个原因：

1.  kvm分配出来的内存仍然在socket1, 这里内存延时慢了3倍；
1.  cpu没压起来，只用了30%，没压起来是在1）的条件下要增加druid连接数来弥补（也许还有线程池也要调大）

2个原因导致各差了3倍，3*3就9倍差距了

开启numa后只要绑核意味着默认绑了对应core最近的内存

所以让现场创建ECS的时候在创建的配置文件中指定绑核方案后，socket0上7个ECS性能到了3500+，从如下内存分布也可以看到每个ECS使用的内存都是精确地在他们所属的node上

```
#numastat -c qemu-kvm

Per-node process memory usage (in MBs)
PID              Node 0 Node 1 Node 2 Node 3 Node 4 Node 5 Node 6 Node 7 Node 8
---------------  ------ ------ ------ ------ ------ ------ ------ ------ ------
111105 (qemu-kvm     33  11840      0      0      0     17      0      0      0
111305 (qemu-kvm     32      0  11872      0      0     17      0      0      0
111763 (qemu-kvm     35      0      0  11964      0     16      0      0      0
112182 (qemu-kvm     33      0      0      0  11871     16      0      0      0
112504 (qemu-kvm     26      0      0      0      0  10702      0      0      0
112679 (qemu-kvm     32      0      0      0      0     16  10736      0      0
112837 (qemu-kvm     36      0      0      0      0     16      0  10808      0
---------------  ------ ------ ------ ------ ------ ------ ------ ------ ------
Total               228  11840  11872  11965  11871  10799  10736  10808      0

PID              Node 9 Node 10 Node 11 Node 12 Node 13 Node 14 Node 15 Total
---------------  ------ ------- ------- ------- ------- ------- ------- -----
111105 (qemu-kvm      0       0       0       0       0       0       0 11891
111305 (qemu-kvm      0       0       0       0       0       0       0 11921
111763 (qemu-kvm      0       0       0       0       0       0       0 12017
112182 (qemu-kvm      0       0       0       0       0       0       1 11921
112504 (qemu-kvm      0       0       0       0       0       0       0 10728
112679 (qemu-kvm      0       0       0       0       0       0       1 10785
112837 (qemu-kvm      0       0       0       0       0       0       1 10861
---------------  ------ ------- ------- ------- ------- ------- ------- -----
Total                 0       0       0       0       1       1       3 80124
```

### 理论测试的性能差异数据

用不同的core 访问 node0 的内存延迟，得到的时延和node距离完全一致

<table>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<tr class="header">
<th></th>
<th>RT变化</th>
</tr>
<tr class="odd">
<td>国产CPU(16 numa node，2 socket)</td>
<td>==core:0== 149.976<br/>==core:8== 168.805<br/>==core:16== 191.415<br/>==core:24== 178.283<br/>==core:32== 170.814<br/>==core:40== 185.699<br/>==core:48== 212.281<br/>==core:56== 202.479<br/>==core:64== 426.176<br/>==core:72== 444.367<br/>==core:80== 465.894<br/>==core:88== 452.245<br/>==core:96== 448.352<br/>==core:104== 460.603<br/>==core:112== 485.989<br/>==core:120== 490.402</td>
</tr>
</table>

可以看到跨socket延迟能到420-490之间，平均大概是450，就近访问内存的话是149，所以这里差了三倍。

只有有了这种理论数据以及对CPU架构有足够的了解之后，我看到socket0 性能比socket1好，或者有的团队说socket1比socket0好，理论告诉我这里搞错了，他们基本是一样好。之所以你感觉好是内存就近了，你感觉不好是内存远了。

另外理论告诉我性能最多差3倍，出来差了10倍的数据是别的地方出现了瓶颈（比如线程池不够了）。

![image-20210928094107229](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210928094107229.png)

![image-20210928094125708](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20210928094125708.png)

### 不同socket性能不一样

测试发现node0，tps 1200， node9上 tps 700，尝试将网卡、磁盘都在socket1上没有任何变化

![image-20211009171656787](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20211009171656787.png)

![image-20211009171740308](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20211009171740308.png)

#### 比较磁盘性能

0路读写磁盘性能要好，/home/mysql/data3002/ren 在0路上的nvme下![image-20211009122534966](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20211009122534966.png)

测试1路上的sda性能，这次是1路要好了![image-20211009122509900](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20211009122509900.png)

![image-20211009125202715](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_blog_cee8f3b4/金税4赛龙舟总结复盘/image-20211009125202715.png)

虽然磁盘看着有影响，实际将磁盘拔下来重新插到另外的socket上，问题仍然存在，所以看起来在MySQL这个场景下磁盘、网卡在哪里影响没那么大。

继续分析，内核团队提供了重要信息，可能是mysqld二进制代码在不同的socket上导致指令load到cache中的时候代价不一样（因为执行的时候都做了绑核），所以需要停掉mysqld进程,然后人为将mysqld 加载到指定的node上对应的内存中

#### mysqld page cache跨socket

```shell
#vmtouch -e /u01/mysql80_20201126/bin/mysqld
           Files: 1
     Directories: 0
   Evicted Pages: 289708 (1G)
         Elapsed: 0.22982 seconds

[root@d21h05024.cloud.h05.amtest154 /root/ren]
#taskset -c 8 md5sum /u01/mysql80_20201126/bin/mysqld
887a0c9d545ad780944b6875d766efbe  /u01/mysql80_20201126/bin/mysqld

#numactl -N 9 -m 9 /u01/mysql80_20201126/bin/mysqld --defaults-file=/apsarapangu/rdstest/my.cnf  --basedir=/u01/mysql80_20201126 --datadir=/apsarapangu/rdstest/dbs3011 --plugin-dir=/u01/mysql80_20201126/lib/plugin --user=mysql --log-error=/apsarapangu/rdstest/mysql/master-error.log --open-files-limit=65535 --pid-file=d21h05024.cloud.h05.amtest154.pid --socket=/apsarapangu/rdstest/tmp/mysql.sock --port=3011

#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -p `pidof mysqld`
^C
 Performance counter stats for process id '77436':

         289322622      branch-misses             #    9.207 M/sec                    (40.09%)
       65128331086      bus-cycles                # 2072.451 M/sec                    (37.29%)
         352781660      cache-misses              #    4.242 % of all cache refs      (37.30%)
        8316035206      cache-references          #  264.625 M/sec                    (37.70%)
       65050959640      cpu-cycles                #    2.070 GHz                      (44.00%)
       22372156742      instructions              #    0.34  insns per cycle          (44.26%)
                 0      alignment-faults          #    0.000 K/sec
             53038      context-switches          #    0.002 M/sec
      31412.355860      cpu-clock (msec)
              3455      cpu-migrations            #    0.110 K/sec
                 0      dummy                     #    0.000 K/sec
                 0      emulation-faults          #    0.000 K/sec
                 0      major-faults              #    0.000 K/sec
                81      minor-faults              #    0.003 K/sec
                81      page-faults               #    0.003 K/sec
      31425.747140      task-clock (msec)         #    7.481 CPUs utilized
         351332897      L1-dcache-load-misses     #    4.21% of all L1-dcache hits    (44.35%)
        8336541107      L1-dcache-loads           #  265.277 M/sec                    (44.18%)
         351147432      L1-dcache-store-misses    #   11.174 M/sec                    (44.16%)
        8341156650      L1-dcache-stores          #  265.424 M/sec                    (44.28%)
         647602225      L1-icache-load-misses     #   20.607 M/sec                    (41.71%)
       11820259122      L1-icache-loads           #  376.133 M/sec                    (37.93%)
         289037486      branch-load-misses        #    9.197 M/sec                    (37.73%)
        5168873526      branch-loads              #  164.479 M/sec                    (37.58%)
         233503642      dTLB-load-misses          #    7.430 M/sec                    (37.18%)
         103448489      iTLB-load-misses          #    3.292 M/sec                    (36.70%)
```

#### mysqld page cache不跨socket

```shell
#vmtouch -e /u01/mysql80_20201126/bin/mysqld
           Files: 1
     Directories: 0
   Evicted Pages: 289708 (1G)
         Elapsed: 0.33737 seconds

[root@d21h05024.cloud.h05.amtest154 /root/ren]
#taskset -c 73 md5sum /u01/mysql80_20201126/bin/mysqld
887a0c9d545ad780944b6875d766efbe  /u01/mysql80_20201126/bin/mysqld

#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses -p `pidof mysqld`
 Performance counter stats for process id '85328':

         351570149      branch-misses             #   14.802 M/sec                    (39.92%)
       49080435884      bus-cycles                # 2066.445 M/sec                    (37.21%)
         421666769      cache-misses              #    4.218 % of all cache refs      (37.69%)
        9997967048      cache-references          #  420.947 M/sec                    (37.79%)
       49047801768      cpu-cycles                #    2.065 GHz                      (44.15%)
       26087934054      instructions              #    0.53  insns per cycle          (45.10%)
                 0      alignment-faults          #    0.000 K/sec
             71310      context-switches          #    0.003 M/sec
      23700.444900      cpu-clock (msec)
              4177      cpu-migrations            #    0.176 K/sec
                 0      dummy                     #    0.000 K/sec
                 0      emulation-faults          #    0.000 K/sec
                 0      major-faults              #    0.000 K/sec
                51      minor-faults              #    0.002 K/sec
                51      page-faults               #    0.002 K/sec
      23751.141460      task-clock (msec)         #    7.457 CPUs utilized
         422174697      L1-dcache-load-misses     #    4.23% of all L1-dcache hits    (44.89%)
        9985335408      L1-dcache-loads           #  420.415 M/sec                    (44.84%)
         422452973      L1-dcache-store-misses    #   17.787 M/sec                    (45.01%)
       10005223452      L1-dcache-stores          #  421.252 M/sec                    (43.97%)
         793749727      L1-icache-load-misses     #   33.419 M/sec                    (41.84%)
       14036146764      L1-icache-loads           #  590.967 M/sec                    (37.29%)
         350598270      branch-load-misses        #   14.761 M/sec                    (37.25%)
        6075373516      branch-loads              #  255.793 M/sec                    (36.83%)
         274431028      dTLB-load-misses          #   11.554 M/sec                    (36.94%)
         124474151      iTLB-load-misses          #    5.241 M/sec                    (36.84%)
```

如上对比可以看到perf 数据基本一样，IPC差异很大这是符合预期的，看起来就是mysqld代码所在的socket不同导致了IPC的差异，比较有意思的是load代码慢基本从perf里面没有指标能体现出来，比如这个case的icache load/miss 比例基本是一致的，只是miss后load代价不一样，perf体现不了iload的延时。

## 总结

其实不想总结这个过程了，但是这个项目对我最大的感受就是相信原理和理论的力量，我从来没有去到现场，但是现场多次给我测试结果基本就能退出来数据有错或者瓶颈在哪里，这是最有价值的，希望能对你也有所帮助。

多了解点理论可以省掉很多无用功



Reference:

