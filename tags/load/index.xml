<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Load - 标签 - plantegg</title>
        <link>https://plantegg.github.io/tags/load/</link>
        <description>Load - 标签 - plantegg</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-Hans</language><lastBuildDate>Wed, 26 Sep 2018 16:30:03 &#43;0000</lastBuildDate><atom:link href="https://plantegg.github.io/tags/load/" rel="self" type="application/rss+xml" /><item>
    <title>Load很高，CPU使用率很低</title>
    <link>https://plantegg.github.io/posts/high_load/</link>
    <pubDate>Wed, 26 Sep 2018 16:30:03 &#43;0000</pubDate><author>
        <name>作者</name>
    </author><guid>https://plantegg.github.io/posts/high_load/</guid>
    <description><![CDATA[<h1 id="load很高cpu使用率很低" class="headerLink">
    <a href="#load%e5%be%88%e9%ab%98cpu%e4%bd%bf%e7%94%a8%e7%8e%87%e5%be%88%e4%bd%8e" class="header-mark"></a>Load很高，CPU使用率很低</h1><blockquote>
  <p>第一次碰到这种Case：物理机的Load很高，CPU使用率很低</p>

</blockquote><h3 id="先看cpuload情况" class="headerLink">
    <a href="#%e5%85%88%e7%9c%8bcpuload%e6%83%85%e5%86%b5" class="header-mark"></a>先看CPU、Load情况</h3><p>如图一：
vmstat显示很有多任务等待排队执行（r）top都能看到Load很高，但是CPU idle 95%以上
<img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/046077102b3a0fd89e53f62cf32874c0.png   alt="image.png"  >
<img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/d905abc4576e0c6ac952c71005696131.png   alt="image.png"  ></p>
<p>这个现象不太合乎常规，也许是在等磁盘IO、也许在等网络返回会导致CPU利用率很低而Load很高</p>
<p>贴个vmstat 说明文档（图片来源于网络N年了，找不到出处）
<img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/9a0c040b24699d4128bbecae1af08b1d.png   alt="image.png"  ></p>
<h3 id="检查磁盘状态很正常vmstat-第二列也一直为0" class="headerLink">
    <a href="#%e6%a3%80%e6%9f%a5%e7%a3%81%e7%9b%98%e7%8a%b6%e6%80%81%e5%be%88%e6%ad%a3%e5%b8%b8vmstat-%e7%ac%ac%e4%ba%8c%e5%88%97%e4%b9%9f%e4%b8%80%e7%9b%b4%e4%b8%ba0" class="header-mark"></a>检查磁盘状态，很正常（vmstat 第二列也一直为0）</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/19d7d02c9472ddb2b057a4d09b497463.png   alt="image.png"  ></p>
<h3 id="再看load是在5号下午1550突然飙起来的" class="headerLink">
    <a href="#%e5%86%8d%e7%9c%8bload%e6%98%af%e5%9c%a85%e5%8f%b7%e4%b8%8b%e5%8d%881550%e7%aa%81%e7%84%b6%e9%a3%99%e8%b5%b7%e6%9d%a5%e7%9a%84" class="header-mark"></a>再看Load是在5号下午15：50突然飙起来的：</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/71127256e8e33a716770f74cb563a1b6.png   alt="image.png"  ></p>
<h3 id="同一时间段的网络流量tcp连接相关数据很平稳" class="headerLink">
    <a href="#%e5%90%8c%e4%b8%80%e6%97%b6%e9%97%b4%e6%ae%b5%e7%9a%84%e7%bd%91%e7%bb%9c%e6%b5%81%e9%87%8ftcp%e8%bf%9e%e6%8e%a5%e7%9b%b8%e5%85%b3%e6%95%b0%e6%8d%ae%e5%be%88%e5%b9%b3%e7%a8%b3" class="header-mark"></a>同一时间段的网络流量、TCP连接相关数据很平稳：</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/8f7ff0bf2f313409f521f6863f2375aa.png   alt="image.png"  ></p>
<p>所以分析到此，可以得出：<strong>Load高跟磁盘、网络、压力都没啥关系</strong></p>
<h3 id="物理机上是跑的docker分析了一下cpuset情况" class="headerLink">
    <a href="#%e7%89%a9%e7%90%86%e6%9c%ba%e4%b8%8a%e6%98%af%e8%b7%91%e7%9a%84docker%e5%88%86%e6%9e%90%e4%ba%86%e4%b8%80%e4%b8%8bcpuset%e6%83%85%e5%86%b5" class="header-mark"></a>物理机上是跑的Docker，分析了一下CPUSet情况：</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/e7996a82da2c140594835e3264c6ef4b.png   alt="image.png"  ></p>
<p><strong>发现基本上所有容器都绑定在CPU1上（感谢 @辺客 发现这个问题）</strong></p>
<h3 id="进而检查top每个核的状态果然cpu1-的idle一直为0" class="headerLink">
    <a href="#%e8%bf%9b%e8%80%8c%e6%a3%80%e6%9f%a5top%e6%af%8f%e4%b8%aa%e6%a0%b8%e7%9a%84%e7%8a%b6%e6%80%81%e6%9e%9c%e7%84%b6cpu1-%e7%9a%84idle%e4%b8%80%e7%9b%b4%e4%b8%ba0" class="header-mark"></a>进而检查top每个核的状态，果然CPU1 的idle一直为0</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/2b32adb2071b3fdb334e0735db899a2e.png   alt="image.png"  ></p>
<p>看到这里大致明白了，虽然CPU整体很闲但是因为很多进程都绑定在CPU1上，导致CPU1上排队很长，看前面tsar的&ndash;load负载截图的 等待运行进程排队长度（runq）确实也很长。</p>
<blockquote>
  <p>物理机有32个核，如果100个任务同时进来，Load大概是3，这是正常的。如果这100个任务都跑在CPU1上，Load还是3（因为Load是所有核的平均值）。但是如果有源源不断的100个任务进来，前面100个还没完后面又来了100个，这个时候CPU1前面队列很长，其它31个核没事做，这个时候整体Load就是6了，时间一长很快Load就能到几百。</p>
<p>这是典型的瓶颈导致积压进而高Load。</p>

</blockquote><h3 id="为什么会出现这种情况" class="headerLink">
    <a href="#%e4%b8%ba%e4%bb%80%e4%b9%88%e4%bc%9a%e5%87%ba%e7%8e%b0%e8%bf%99%e7%a7%8d%e6%83%85%e5%86%b5" class="header-mark"></a>为什么会出现这种情况</h3><p>检查Docker系统日志，发现同一时间点所有物理机同时批量执行docker update 把几百个容器都绑定到CPU1上，导致这个核忙死了，其它核闲得要死（所以看到整体CPU不忙，最忙的那个核被平均掩盖掉了），但是Load高（CPU1上排队太长，即使平均到32个核，这个队列还是长，这就是瓶颈啊）。</p>
<p>如下Docker日志，Load飙升的那个时间点有人批量调docker update 把所有容器都绑定到CPU1上：
<img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/f4925c698c9fd4edb56fcfc2ebb9f625.png   alt="image.png"  ></p>
<p>检查Docker集群Swarm的日志，发现Swarm没有发起这样的update操作，似乎是每个Docker Daemon自己的行为，谁触发了这个CPU的绑定过程的原因还没找到，求指点。</p>
<h3 id="手动执行docker-update-把容器打散到不同的cpu核上恢复正常" class="headerLink">
    <a href="#%e6%89%8b%e5%8a%a8%e6%89%a7%e8%a1%8cdocker-update-%e6%8a%8a%e5%ae%b9%e5%99%a8%e6%89%93%e6%95%a3%e5%88%b0%e4%b8%8d%e5%90%8c%e7%9a%84cpu%e6%a0%b8%e4%b8%8a%e6%81%a2%e5%a4%8d%e6%ad%a3%e5%b8%b8" class="header-mark"></a>手动执行docker update, 把容器打散到不同的cpu核上，恢复正常：</h3><p><img class="tw-inline" loading="lazy" src=https://plantegg.github.io/Users/ren/case/ossimg/9e1adae472cf0b4f95af83390adaead9.png   alt="image.png"  ></p>
<h2 id="关于这个case的总结" class="headerLink">
    <a href="#%e5%85%b3%e4%ba%8e%e8%bf%99%e4%b8%aacase%e7%9a%84%e6%80%bb%e7%bb%93" class="header-mark"></a>关于这个Case的总结</h2><ul>
<li>技术拓展商业边界，同样技能、熟练能力能拓展解决问题的能力。 开始我注意到了Swarm集群显示的CPU绑定过多，同时也发现有些容器绑定在CPU1上。所以我尝试通过API： GET /containers/json 拿到了所有容器的参数，然后搜索里面的CPUSet，结果这个API返回来的参数不包含CPUSet，那我只能挨个 GET /containers/id/json, 要写个循环，偷懒没写，所以没发现这个问题。</li>
<li>这种多个进程绑定到同一个核然后导致Load过高的情况确实很少见，也算是个教训</li>
<li>自己观察top 单核的时候不够仔细，只是看到CPU1 的US 60%，没留意idle，同时以为这个60%就是偶尔一个进程在跑，耐心不够（主要也是没意识到这种极端情况，疏忽了）</li>
</ul>
<h2 id="关于load高的总结" class="headerLink">
    <a href="#%e5%85%b3%e4%ba%8eload%e9%ab%98%e7%9a%84%e6%80%bb%e7%bb%93" class="header-mark"></a>关于Load高的总结</h2><ul>
<li>Load高一般对应着CPU高，就是CPU负载过大，检查CPU具体执行任务是否合理</li>
<li>如果Load高，CPU使用率不高的检查一下IO、网络等是否比较慢</li>
<li>如果是虚拟机，检查是否物理机超卖或者物理机其它ECS抢占CPU、IO导致的</li>
<li>如果两台一样的机器一样的流量，Load有一台偏高的话检查硬件信息，比如CPU被降频了，QPI，内存效率等等（https://www.atatech.org/articles/12201），这个时候可能需要硬件相关同学加入一起排查了，当然牛逼的工程师能把这块也Cover了排查效率自然更高</li>
<li>load计算是看TASK_RUNNING(R)或者TASK_UNINTERRUPTIBLE(D&ndash;不可中断的睡眠进程)的数量，R肯定会占用CPU，但是D一般就不占用CPU了</li>
</ul>
<p>Linux 下load 高主要是因为<a href="http://oliveryang.net/2017/12/linux-high-loadavg-analysis-1" target="_blank" rel="noopener noreferrer">R/D 两个状态的线程多了</a>，排查套路：</p>
<p><img class="tw-inline" loading="lazy" src=https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/format,webp-1273209.   alt="img"  ></p>
<h2 id="参考文章" class="headerLink">
    <a href="#%e5%8f%82%e8%80%83%e6%96%87%e7%ab%a0" class="header-mark"></a>参考文章</h2><p><a href="http://oliveryang.net/2017/12/linux-high-loadavg-analysis-1" target="_blank" rel="noopener noreferrer">浅谈 Linux 高负载的系统化分析</a></p>
]]></description>
</item></channel>
</rss>
