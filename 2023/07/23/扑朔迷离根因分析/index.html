<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="扑朔迷离的根因分析原则追着RT 跑，不断加压力，到瓶颈前随着并发的增加RT很稳定。 但是你要对你的RT怎么来的，包含哪些环节的消耗，这样才不会出错。 如下图左边是QPS不停地增加，每一次台阶(增加20%流量)都是一次加压过程，右边是对应的 RT，可以看到在绿线阶段几乎是平稳的，直到最后的红色箭头 RT略微有一点点提升，但是整体也还好，说明基本没到瓶颈  当然这个图是经过长时间调优的结果，来之不易，">
<meta property="og:type" content="article">
<meta property="og:title" content="扑朔迷离的根因分析">
<meta property="og:url" content="https://plantegg.github.io/2023/07/23/%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="扑朔迷离的根因分析原则追着RT 跑，不断加压力，到瓶颈前随着并发的增加RT很稳定。 但是你要对你的RT怎么来的，包含哪些环节的消耗，这样才不会出错。 如下图左边是QPS不停地增加，每一次台阶(增加20%流量)都是一次加压过程，右边是对应的 RT，可以看到在绿线阶段几乎是平稳的，直到最后的红色箭头 RT略微有一点点提升，但是整体也还好，说明基本没到瓶颈  当然这个图是经过长时间调优的结果，来之不易，">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230520101758175.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230517113148916.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230516150350614.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230516150812485.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/6f6862dec810933f34b7793018cfb0da.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230516150950383.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519170255718.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519172225890.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519172519802.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519165825977.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519203620163.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230520111102697.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20240523084559029.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230721170334688.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519174633172.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519175029396.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519191317489.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230520092224080.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20250306153320704.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519181628114.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230519181451384.png">
<meta property="article:published_time" content="2023-07-23T04:30:03.000Z">
<meta property="article:modified_time" content="2025-11-29T07:19:06.379Z">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="RT">
<meta property="article:tag" content="druid">
<meta property="article:tag" content="network">
<meta property="article:tag" content="performance">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230520101758175.png">


<link rel="canonical" href="https://plantegg.github.io/2023/07/23/%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://plantegg.github.io/2023/07/23/%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90/","path":"2023/07/23/扑朔迷离根因分析/","title":"扑朔迷离的根因分析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>扑朔迷离的根因分析 | plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">plantegg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E7%9A%84%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90"><span class="nav-number">1.</span> <span class="nav-text">扑朔迷离的根因分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8E%9F%E5%88%99"><span class="nav-number">1.1.</span> <span class="nav-text">原则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%9A%E5%8A%A1%E7%BB%93%E6%9E%84"><span class="nav-number">1.2.</span> <span class="nav-text">业务结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E6%95%B0%E6%8D%AE"><span class="nav-number">1.3.</span> <span class="nav-text">监控数据</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86"><span class="nav-number">1.3.1.</span> <span class="nav-text">补充知识</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Tomcat-CPU-%E7%9B%91%E6%8E%A7"><span class="nav-number">1.3.2.</span> <span class="nav-text">Tomcat CPU 监控</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%97%AE%E9%A2%98"><span class="nav-number">1.4.</span> <span class="nav-text">问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B2-yy"><span class="nav-number">1.5.</span> <span class="nav-text">案例2 yy</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E6%9E%90"><span class="nav-number">1.6.</span> <span class="nav-text">分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Druid%E5%88%86%E6%9E%90"><span class="nav-number">1.6.1.</span> <span class="nav-text">Druid分析</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Druid-%E6%8A%A5%E9%94%991"><span class="nav-number">1.6.1.1.</span> <span class="nav-text">Druid 报错1</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Druid-%E6%8A%A5%E9%94%992"><span class="nav-number">1.6.1.2.</span> <span class="nav-text">Druid 报错2</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Druid-%E6%8A%A5%E9%94%993"><span class="nav-number">1.6.1.3.</span> <span class="nav-text">Druid 报错3</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Druid-%E6%8A%A5%E9%94%99-4"><span class="nav-number">1.6.2.</span> <span class="nav-text">Druid 报错 4</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E7%89%87%E9%80%BB%E8%BE%91"><span class="nav-number">1.6.3.</span> <span class="nav-text">分片逻辑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E7%BB%93"><span class="nav-number">1.7.</span> <span class="nav-text">总结</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">276</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/07/23/%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="扑朔迷离的根因分析 | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          扑朔迷离的根因分析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-07-23 12:30:03" itemprop="dateCreated datePublished" datetime="2023-07-23T12:30:03+08:00">2023-07-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/performance/" itemprop="url" rel="index"><span itemprop="name">performance</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="扑朔迷离的根因分析"><a href="#扑朔迷离的根因分析" class="headerlink" title="扑朔迷离的根因分析"></a>扑朔迷离的根因分析</h1><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><p>追着RT 跑，不断加压力，到瓶颈前随着并发的增加RT很稳定。</p>
<p>但是你要对你的RT怎么来的，包含哪些环节的消耗，这样才不会出错。</p>
<p>如下图左边是QPS不停地增加，每一次台阶(增加20%流量)都是一次加压过程，右边是对应的 RT，可以看到在绿线阶段几乎是平稳的，直到最后的红色箭头 RT略微有一点点提升，但是整体也还好，说明基本没到瓶颈</p>
<p><img src="/images/951413iMgBlog/image-20230520101758175.png" alt="image-20230520101758175"></p>
<p>当然这个图是经过长时间调优的结果，来之不易，是理想的期望系统状态，但在这之前是长时间的痛苦分析和瓶颈在哪里的定位过程。</p>
<p>凡是复杂的实际业务总是有很多干扰项出现在你的理论图上，你得很好地识别他们</p>
<h2 id="业务结构"><a href="#业务结构" class="headerlink" title="业务结构"></a>业务结构</h2><p><img src="/images/951413iMgBlog/image-20230517113148916.png" alt="image-20230517113148916"></p>
<p>概念说明：</p>
<p>黑色&#x3D;Database&#x3D;被依赖业务&#x3D;物理</p>
<p>蓝色&#x3D;Tomcat&#x3D;上游业务&#x3D;逻辑</p>
<p>上游响应时间&#x3D;下游业务响应时间+网络时间+上游自身处理耗时</p>
<p>响应时间&#x3D;RT&#x3D;耗时监控</p>
<p>tcprt：从内核网络取Database的响应时间</p>
<p>实际很魔幻的是同样流量有时候压测很稳定，有时候又不稳定，性能上不去(稳定时可能是压测数据、没有触发Database雪崩之类的问题)，所以导致问题</p>
<p><strong>所有压测过程中肯定是没有任何资源上的瓶颈(CPU、内存、网络带宽、磁盘等等)</strong></p>
<h2 id="监控数据"><a href="#监控数据" class="headerlink" title="监控数据"></a>监控数据</h2><p>如图，蓝线表示Tomcat，黑线表示Database被调用方，可以看到每次黑色 RT上升QPS下跌很猛(符合预期)，奇怪的是黑色RT很快降下来后蓝色RT还会维持较高一段时间，监控频率每5秒采集一次，以下所有监控图时间范围是一样的，但采集频率不一样</p>
<p><img src="/images/951413iMgBlog/image-20230516150350614.png" alt="image-20230516150350614"></p>
<p>(图1)</p>
<p>上图的两个 RT 监控数据都是Tomcat的业务代码记录下来的，比如Database的响应时间就包含网络+Database的处理时间</p>
<p>如下图通过网络监控看响应时间(tcprt <a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/181331.html">阿里云文档</a>，从OS 内核中取到网络包的响应时间)，蓝线表示Tomcat，紫线表示Database，监控力度每1分钟采集一次，有被平均</p>
<p><img src="/images/951413iMgBlog/image-20230516150812485.png" alt="image-20230516150812485"></p>
<p>以上两个监控图的矛盾点：如果从网络层面记录的Database RT 可以看到上升幅度不明显，但是Tomcat 的RT上升很明显，但是Tomcat记录的RT则又是Database 上升明显。</p>
<h4 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a><a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/181331.html">补充知识</a></h4><p>tcprt和tomcat业务进程记录的 Database rt差异，tcprt记录到的是RDS&#x2F;Database的响应时间+网络时间，tomcat在这个基础上还要加入自己进程调出处理时间，比如tomcat进程取到数据库连接的时候连接需要排队等待1秒钟(后面有分析)，那么这个一秒钟对tcprt来说是不会记录进去的，但是客户端感知到的这次调用是1秒以上。当然业务记录的Database 还可以更精准，比如在连接池Druid(或者其它连接池的实现)内取记录，但是无论如何从业务进程到OS内核这里的差距总是存在的。</p>
<p><img src="/images/951413iMgBlog/6f6862dec810933f34b7793018cfb0da.png" alt="image.png"></p>
<h3 id="Tomcat-CPU-监控"><a href="#Tomcat-CPU-监控" class="headerlink" title="Tomcat CPU 监控"></a>Tomcat CPU 监控</h3><p><img src="/images/951413iMgBlog/image-20230516150950383.png" alt="image-20230516150950383"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>可以很清楚看到 QPS 下降是因为 RT上升，那么究竟是Database的RT上升导致的还是Tomcat的RT上升导致的。</p>
<p>但是我们从监控图也能看到Database RT降下来后Tomcat RT还维持高水位，所以有点迷惑了。</p>
<p>继续看另外案例</p>
<h2 id="案例2-yy"><a href="#案例2-yy" class="headerlink" title="案例2 yy"></a>案例2 yy</h2><p>两次压测监控数据，左右两个图标是同一时间的QPS和RT，蓝线表示Tomcat，黑线表示Database被调用方</p>
<p><img src="/images/951413iMgBlog/image-20230519170255718.png" alt="image-20230519170255718"></p>
<p><img src="/images/951413iMgBlog/image-20230519172225890.png" alt="image-20230519172225890"></p>
<p>从两个图来看，随着并发加高(QPS加高) 黑色RT增加明显，但是跑着跑着降下去了，可以理解成突发流量导致黑色RT增加，但是很快黑色RT稳住了阵脚，降回去了，但是蓝色 RT没降，所以表面看起来是蓝色(Tomcat)处理能力到了瓶颈</p>
<p>上图时间点内核监控的tcprt，可以看到还是Database 处理耗时增加，和上图的黑色RT下降根本不匹配，上图黑色RT最后在2.96ms，下图内核监控到的Database的tcprt在8.49，差异矛盾点</p>
<p><img src="/images/951413iMgBlog/image-20230519172519802.png" alt="image-20230519172519802"></p>
<p>第三次压测图</p>
<p><img src="/images/951413iMgBlog/image-20230519165825977.png" alt="image-20230519165825977"></p>
<p>从第一个图来看，随着并发加高(QPS加高) 黑色RT增加明显，蓝色 RT去掉黑色部分也有增加，并且黑色、蓝色都没降回去，看起来主要是黑色(Database)处理能力到了瓶颈</p>
<p>纠结的时候就在Tomcat上抓包确认一下，如下图黑色 Database服务端口是5493，可以看到Tomcat 发request总是很快，但是Database 响应都是几十毫秒(下图红色框)，和监控一致。其实监控可以说明问题，但是我担心业务记录时间不准，以及建连接时间都考虑在内，所以想抓包微观上印证一下，这种项目牵扯到很多人你搞错了方向丢人不说，大家合作联调浪费很大，所以必须稳！</p>
<p><img src="/images/951413iMgBlog/image-20230519203620163.png" alt="image-20230519203620163"></p>
<p>如果说问题在Database上，那为什么会有Database RT忽上忽下，Database RT降下去了Tomcat RT不降？我们要继续分析一下 Tomcat RT以及Database RT是怎么记录和实现的</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>问题解决后的原因分析以及数据整理</p>
<p>这个时候我们再把Tomcat部分的业务调用和RT记录再细化一下，如下图：</p>
<p><img src="/images/951413iMgBlog/image-20230520111102697.png" alt="image-20230520111102697"></p>
<h3 id="Druid分析"><a href="#Druid分析" class="headerlink" title="Druid分析"></a><a target="_blank" rel="noopener" href="https://github.com/alibaba/druid">Druid分析</a></h3><p>创建和回收逻辑：<a target="_blank" rel="noopener" href="https://exceting.github.io/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89Druid%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/#%E4%B9%9D%E3%80%81%E4%B8%BB%E6%B5%81%E7%A8%8B5%EF%BC%9A%E5%9B%9E%E6%94%B6%E8%BF%9E%E6%8E%A5">https://exceting.github.io/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89Druid%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/#%E4%B9%9D%E3%80%81%E4%B8%BB%E6%B5%81%E7%A8%8B5%EF%BC%9A%E5%9B%9E%E6%94%B6%E8%BF%9E%E6%8E%A5</a></p>
<p>作为Tomcat和Database的连接点、枢纽点搞清楚Druid的逻辑对理解Tomcat和Database之间的问题的理解很关键。</p>
<p><img src="/images/951413iMgBlog/image-20240523084559029.png" alt="image-20240523084559029"></p>
<p>比如以下要说的三个Druid 错误状态如果你不放到一起比较，看到这个错误你最多反应就是连接池不够了，什么原因不知道。但是如果放到一次比较一次后你以后对详细错误提示会积极敏感，进而发现第四、第五种错误提示</p>
<p>这就是综合比较、总结的好处。</p>
<p>Druid 最核心的类是 DruidDataSource，连接的构建，入池，获取，收缩，销毁，以及核心监控数据都在这个类维护</p>
<p><img src="/images/951413iMgBlog/image-20230721170334688.png" alt="image-20230721170334688"></p>
<p>连接池初始化流程：初始化驱动实例 -&gt; 加锁 -&gt; 初始化属性 -&gt; 初始化过滤器 -&gt; 校验参数 -&gt; <strong>创建初始化连接并校验后加入池中</strong> -&gt; 创建logStatsThread、createConnectionThread和destroyConnectionThread -&gt; 注册MBean，用于支持JMX -&gt; 如果设置了keepAlive，通知createConnectionThread创建连接对象 -&gt; 解锁</p>
<p>Druid 创建连接的堆栈如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&quot;Druid-ConnectionPool-Create-1647809146&quot; #265 daemon prio=5 os_prio=0 tid=0x00007fbcdfd5f000 nid=0x1a0 runnable [0x00007fbcdf9fd000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">        at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">        at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">        - locked &lt;0x00000000e71ca648&gt; (a com.mysql.jdbc.util.ReadAheadInputStream)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3001)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.readPacket(MysqlIO.java:567)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1018)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2253)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2284)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2083)</span><br><span class="line">        - locked &lt;0x00000000e7f898f0&gt; (a com.mysql.jdbc.JDBC4Connection)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:806)</span><br><span class="line">        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)</span><br><span class="line">        at sun.reflect.GeneratedConstructorAccessor196.newInstance(Unknown Source)</span><br><span class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">        at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:410)</span><br><span class="line">        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:328)</span><br><span class="line">        at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1558)</span><br><span class="line">        at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1623)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2468)       </span><br><span class="line">        </span><br><span class="line">&quot;Druid-ConnectionPool-Create-1823047135&quot; #160 daemon prio=5 os_prio=0 tid=0x00007fbd60cf0000 nid=0x142 waiting on condition [0x00007fbd043fe000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">        at sun.misc.Unsafe.park0(Native Method)</span><br><span class="line">        - parking to wait for  &lt;0x00000000c448b348&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">        at sun.misc.Unsafe.park(Unsafe.java:1036)</span><br><span class="line">        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:176)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2047)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2443)        </span><br></pre></td></tr></table></figure>



<h4 id="Druid-报错1"><a href="#Druid-报错1" class="headerlink" title="Druid 报错1"></a>Druid 报错1</h4><p>获取连接排队是基本不消耗CPU，下图右上角是获取失败的日志打堆栈消耗，可以看到异常非常多。</p>
<p><img src="/images/951413iMgBlog/image-20230519174633172.png" alt="image-20230519174633172"></p>
<p><img src="/images/951413iMgBlog/image-20230519175029396.png" alt="image-20230519175029396"></p>
<p>Druid最大连接数默认是30，多次调大，30-&gt;60-&gt;120-&gt;160，一直调下去对调大能解决问题都没有信心了，总是报错</p>
<blockquote>
<p>maxWaitThreadCount 30, current wait Thread count 0 </p>
</blockquote>
<p>调大到160后的报错堆栈，<a target="_blank" rel="noopener" href="https://sourcegraph.com/github.com/alibaba/druid/-/blob/core/src/main/java/com/alibaba/druid/pool/DruidDataSource.java?L1733:92">对应源码 </a> 这个报错说明报错时已经有160个线程在等连接了，别等了，先快速报错返回吧</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.sql.SQLException: maxWaitThreadCount 160, current wait Thread count 0</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1620)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1404)</span><br><span class="line">        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:5059)</span><br><span class="line">        at com.alibaba.druid.filter.FilterAdapter.dataSource_getConnection(FilterAdapter.java:2756)</span><br><span class="line">        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:5055)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1382)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1374)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:98)</span><br><span class="line"></span><br><span class="line">对应代码：</span><br><span class="line">                   final Lock lock = this.lock;</span><br><span class="line">                    lock.lock();</span><br><span class="line">                    try &#123;</span><br><span class="line">                        if (activeCount &lt; maxActive) &#123;</span><br><span class="line">                            activeCount++;</span><br><span class="line">                            holder.active = true;</span><br><span class="line">                            if (activeCount &gt; activePeak) &#123;</span><br><span class="line">                                activePeak = activeCount;</span><br><span class="line">                                activePeakTime = System.currentTimeMillis();</span><br><span class="line">                            &#125;</span><br><span class="line">                            break;</span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            discard = true;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; finally &#123;</span><br><span class="line">                        lock.unlock();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    if (discard) &#123;</span><br><span class="line">                        JdbcUtils.close(pyConnInfo.getPhysicalConnection());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            final ReentrantLock lock = this.lock;</span><br><span class="line">            try &#123;</span><br><span class="line">                lock.lockInterruptibly();</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                connectErrorCountUpdater.incrementAndGet(this);</span><br><span class="line">                throw new SQLException(&quot;interrupt&quot;, e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            try &#123;</span><br><span class="line">                if (maxWaitThreadCount &gt; 0</span><br><span class="line">                        &amp;&amp; notEmptyWaitThreadCount &gt;= maxWaitThreadCount) &#123;</span><br><span class="line">                    connectErrorCountUpdater.incrementAndGet(this);</span><br><span class="line">                    throw new SQLException(&quot;maxWaitThreadCount &quot; + maxWaitThreadCount + &quot;, current wait Thread count &quot;</span><br><span class="line">                            + lock.getQueueLength());//bug? lock.getQueueLength()永远为0，应该改成：lock.getWaitQueueLength(notEmpty)</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure>



<p>以下两个Druid 报错这次压测没有出现但是可以放一起比较一下，其它项目场景经常出现</p>
<h4 id="Druid-报错2"><a href="#Druid-报错2" class="headerlink" title="Druid 报错2"></a>Druid 报错2</h4><p>Druid类似报错，明显是等了5秒最大等待时间还没有获取到连接：<img src="/images/951413iMgBlog/image-20230519191317489.png" alt="image-20230519191317489"></p>
<p>红色错误信息表示等了5006毫秒（设置的5000毫秒超时）还没有取到连接，所以超时了，然后抛出错误堆栈。</p>
<p>红色信息还提示我们当前连接池最大10，目前 active 0, 说明不是连接池满了取不到，而是连接池里一直是空的。</p>
<p>看到这个错误不能说明数据库、访问数据库有啥问题，只能说明Druid 连接池取不到连接，要继续分析Druid创建连接的线程栈。或者比如Druid 参数设置不合理，可以把min、init、max 连接数设置为相同的值，避免压力高峰期再去创建连接。</p>
<p>Druid通过另外一个task（thread）异步给连接池补充连接，也就是这里可能是Druid创建连接失败，比如密码错误、比如连不上数据库，比如创建的thread卡死了、报其他异常了</p>
<p><strong>Druid创建 连接 和业务取连接是两个线程，所以业务取连接报错是看不到创建连接报错的堆栈和原因的</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#grep CreateConnectionThread.run stack4.log</span><br><span class="line">	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2818)</span><br><span class="line">	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2813)</span><br></pre></td></tr></table></figure>



<h4 id="Druid-报错3"><a href="#Druid-报错3" class="headerlink" title="Druid 报错3"></a>Druid 报错3</h4><p><img src="/images/951413iMgBlog/image-20230520092224080.png" alt="image-20230520092224080"></p>
<p>借出连接为0(active 0)，creating也是0，没有新连接正在创建。</p>
<p>分析方法：</p>
<ol>
<li>dump Java应用内存，用MAT内存分析工具打开dump文件</li>
<li>使用OQL，select * from com.alibaba.druid.pool.DruidDataSource where createTaskCount&#x3D;3</li>
<li>选出来的DruidDataSource即为有问题的datasource</li>
</ol>
<p>原因</p>
<p>Druid中有个计数器createTaskCount，用来记录每个连接池当前正在创建连接的任务数，默认不能超过3。Druid中，在keepAlive&#x3D;true的情况下，这个计数器有bug，存在加了但没减的情况，导致这个值涨到3之后没有减回去，从而无法提交新的创建连接任务。</p>
<p> 注意，进入这个状态后的连接池，是无法自动恢复的。Druid升级到1.1.24可以修复这个问题。</p>
<h3 id="Druid-报错-4"><a href="#Druid-报错-4" class="headerlink" title="Druid 报错 4"></a>Druid 报错 4</h3><p>下图这个堆栈很有意思，栈顶是连接不够触发 Druid 创建新连接，但是建新连接报错了，于是需要记录日志，但是有些场景(比如任务流) 要求，错误信息要往数据库存，于是记录日志触发取连接，然后就死锁了：</p>
<p><img src="/images/951413iMgBlog/image-20250306153320704.png" alt="image-20250306153320704"></p>
<h3 id="分片逻辑"><a href="#分片逻辑" class="headerlink" title="分片逻辑"></a>分片逻辑</h3><p>因为数据量太大，一台Database存放不下，自然会分片，或者说单表几千万之后也是建议分片。</p>
<p>分片逻辑是取业务id最后两位的字符去取string hashcode，再对16个Database分片</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">如果对id后两位字符(从00-99供100个数字，因为不排除id里面有字符，但实际主要是0-9的数字)的ascii码取hash然后按16取模的结果：</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9  --开始不正常，10-14号分片没有直接跳到15号分片</span><br><span class="line">15</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">……</span><br><span class="line"></span><br><span class="line">//分片求模代码</span><br><span class="line">for(int i=0;i&lt;10;++i) //0的ascii码是48，依此类推</span><br><span class="line">	for(int j=0;j&lt;10;++j)</span><br><span class="line">		 int value=((48+i)*31+j) mod 16;</span><br></pre></td></tr></table></figure>

<p>补充个小八卦</p>
<blockquote>
<p>为什么取某几位尾数来求模？比如很多业务按user_id拆分片，然后希望这个用户的所有订单拆分也落在一个分片内。于是他们想到的办法是在订单id最后几位上追加进去下单人的user_id后几位，对订单拆分会取订单id后几位hash，这样同一个用户肯定到同一个分片</p>
<p>这样查询某个用户的所有订单时(高频需求)就只需要查一个分片，否则就要扫描所有分片。</p>
<p>掏出你的某宝、某东看看你的订单后几位</p>
</blockquote>
<p>分片后的数据，明显两头的多中间的少，这必然导致后面的 Database 负载不均衡：</p>
<p><img src="/images/951413iMgBlog/image-20230519181628114.png" alt="image-20230519181628114"></p>
<p>Java源码：</p>
<p><img src="/images/951413iMgBlog/image-20230519181451384.png" alt="image-20230519181451384"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>问题的根本原因？</p>
<p>多个Database中的某几个瓶颈，为什么会这样见数据分布部分的分析</p>
<p>为什么Database RT监控能降下来？</p>
<p>业务Tomcat 帮Database拦截了流量，一旦Database响应慢 Druid 连接就会不够，请求都堵在Tomcat中，导致Tomcat RT升高(包含等待连接时间)——替人堵了枪眼，很好，Tomcat crash总比 Database crash要好，但是业务要清楚这是替人挨枪子，该往哪里去查瓶颈。</p>
<p>比如加流量20%，开始Database RT升高，很快连接不可用，可能有接近20%的流量被Tomcat拦截，这个时候Database RT能稳定，也有可能拦截的不够多，这个时候Database RT还是很高，但Tomcat RT更高，进入一种平衡状态</p>
<p>为什么有时候压测能过？</p>
<p>应该是数据分布比较巧，刚好压测流里面的数据分布没那么不均衡，没触发数据库雪崩</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/performance/" rel="tag"># performance</a>
              <a href="/tags/network/" rel="tag"># network</a>
              <a href="/tags/RT/" rel="tag"># RT</a>
              <a href="/tags/druid/" rel="tag"># druid</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2023/06/30/%E5%AE%9E%E6%88%98%E7%93%B6%E9%A2%88%E5%AE%9A%E4%BD%8D-%E6%88%91%E7%9A%84MySQL%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8E%8B%E4%B8%8D%E4%B8%8A%E5%8E%BB--%E5%86%99%E5%9C%BA%E6%99%AF/" rel="prev" title="实战瓶颈定位-我的MySQL为什么压不上去--写场景">
                  <i class="fa fa-angle-left"></i> 实战瓶颈定位-我的MySQL为什么压不上去--写场景
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2023/07/23/%E6%89%91%E6%9C%94%E8%BF%B7%E7%A6%BB%E7%9A%84%E6%A0%B9%E5%9B%A0%E5%88%86%E6%9E%90--%E6%8A%96%E5%8A%A8%E5%92%8C%E5%B9%B6%E5%8F%91/" rel="next" title="扑朔迷离的根因分析--抖动和并发">
                  扑朔迷离的根因分析--抖动和并发 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
