<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/5/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="技术,编程,博客">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://plantegg.github.io/page/5/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/5/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">plantegg</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">186</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">276</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/07/03/MySQL8.0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%95%B0%E6%8D%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/07/03/MySQL8.0%E7%9A%84%E4%B8%80%E4%BA%9B%E6%95%B0%E6%8D%AE/" class="post-title-link" itemprop="url">MySQL 8.0新特性和性能数据</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-07-03 17:30:03" itemprop="dateCreated datePublished" datetime="2022-07-03T17:30:03+08:00">2022-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:11:18" itemprop="dateModified" datetime="2025-11-29T15:11:18+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MySQL-8-0新特性和性能数据"><a href="#MySQL-8-0新特性和性能数据" class="headerlink" title="MySQL 8.0新特性和性能数据"></a>MySQL 8.0新特性和性能数据</h1><h2 id="MySQL-8-0带来了很多新特性"><a href="#MySQL-8-0带来了很多新特性" class="headerlink" title="MySQL 8.0带来了很多新特性"></a>MySQL 8.0带来了很多新特性</h2><p>针对性能方面介绍全在这个PPT（ <a target="_blank" rel="noopener" href="http://dimitrik.free.fr/Presentations/MySQL_Perf-OOW2018-dim.pdf%EF%BC%89%E9%87%8C%E9%9D%A2%E4%BA%86%EF%BC%9A">http://dimitrik.free.fr/Presentations/MySQL_Perf-OOW2018-dim.pdf）里面了：</a></p>
<p>IO_Bound 下性能提升简直非常明显，之前主要是fil_system的锁导致IO的并发上不去，见图1。</p>
<p>因为优化了redo的写入模式，采用了事件的模型，所以写入场景有较好的提升 。</p>
<p>utf8mb4在点查询场景优势不明显，在distinct range查询下有30%提升。</p>
<p>内存只读场景略有提升。</p>
<p>还有傲腾对SSD的数据，不过Intel都放弃了，就不说了。</p>
<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><h3 id="page-size"><a href="#page-size" class="headerlink" title="page size"></a>page size</h3><p>MySQL的页都是16K, 当查询的行不在内存中时需要按照16K为单位从磁盘读取页,而文件系统中的页是4k，也就是一次数据库请求需要有4次磁盘IO，如过查询比较随机，每次只需要一个页中的几行数据，存在很大的读放大。</p>
<p>那么我们是否可以把MySQL的页设置为4K来减少读放大呢？</p>
<p>在5.7里收益不大，因为每次IO存在 fil_system 的锁，导致IO的并发上不去</p>
<p>8.0中总算优化了这个场景，测试细节可以参考<a target="_blank" rel="noopener" href="http://dimitrik.free.fr/blog/archives/2018/05/mysql-performance-1m-iobound-qps-with-80-ga-on-intel-optane-ssd.html">这篇</a></p>
<p>16K VS 4K 性能对比（4K接近翻倍）</p>
<p><img src="/images/951413iMgBlog/1547605552845-d406952d-9857-462d-a666-1694b19fbedb.png" alt="img"></p>
<p>4K会带来的问题：顺序insert慢了10%（因为fsync更多了）；DDL更慢；二级索引更多的场景下4K性能较差；大BP下，刷脏代价大。</p>
<h3 id="REDO的优化"><a href="#REDO的优化" class="headerlink" title="REDO的优化"></a><strong>REDO的优化</strong></h3><p>redo的优化似乎是8.0读写性能优于以往的主要原因</p>
<p>redo的模型改成了事件驱动，而不是通过争抢锁实现，专用的flush线程刷完IO后通知用户线程，并且会根据IO的rt自动调整每次flush的data大小，如果io延迟很低，就大量小IO，如果IO延迟高，就用大io刷，也就说redo的刷写能力完全取决于IO的吞吐</p>
<p>但是事件驱动的方式在小并发下性能没有单线程锁的方式高效，这块已经优化了，需要自己测下效果</p>
<p><img src="/images/951413iMgBlog/image-20220810150929638.png" alt="image-20220810150929638"></p>
<h2 id="Innodb-相关数据"><a href="#Innodb-相关数据" class="headerlink" title="Innodb 相关数据"></a>Innodb 相关数据</h2><p><strong>innodb_row_read</strong>：行读，点查峰值大约在800W左右，列表查大约在1200W左右。<br><strong>innodb_buffer_pool_read_requests</strong>：逻辑读，峰值800W左右。<br><strong>innodb_bp_hit</strong>：innodb bp缓存命中率，比较优秀的命中率一般在99.8%+。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>MySQL 8.0优化总结，从官方给出的数据来看，可以总结如下</p>
<ul>
<li>只读场景没有什么优化</li>
<li><a target="_blank" rel="noopener" href="https://yuque.antfin-inc.com/frodo/lyul32/qcggx4#b329a99a">utf8mb4的性能提升比较明显</a></li>
<li>优化了fil_system，<a target="_blank" rel="noopener" href="https://yuque.antfin-inc.com/frodo/lyul32/qcggx4#26583664">MySQL 可以尝试使用4K的页</a></li>
<li>8.0使用新硬件能够获得较好的收益，多socket, optane</li>
<li>由于redo的优化以及<a target="_blank" rel="noopener" href="https://mysqlserverteam.com/contention-aware-transaction-scheduling-arriving-in-innodb-to-boost-performance/">新的热点检查算法</a>，关闭binlog下，读写混合的场景性能比5.7好很多，但是生产环境无法关闭binlog，默认的字符集也不是latin，所以具体的数据需要单独测试，官方数据只能参考</li>
<li>Double Write的问题需要在高并发，低命中率下才会触发，生产环境遇到的不多，该问题预计下个版本就修复了</li>
<li>生产环境需要关闭UNDO Auto-Truncate </li>
<li>binlog的问题在8.0比较明显，暂时没有解法</li>
<li>另外innodb_flush_method&#x3D;O_DIRECT_NO_FSYNC 在8.0.14版本后可以保障应用的稳定性了</li>
</ul>
<blockquote>
<p>Prior to 8.0.14, the <code>O_DIRECT_NO_FSYNC</code> setting is not recommended for use on Linux systems. It may cause the operating system to hang due to file system metadata becoming unsynchronized. As of MySQL 8.0.14, <code>InnoDB</code> calls <code>fsync()</code> after creating a new file, after increasing file size, and after closing a file, which permits <code>O_DIRECT_NO_FSYNC</code> mode to be safely used on EXT4 and XFS file systems. The <code>fsync()</code> system call is still skipped after each write operation.</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/06/05/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2%E5%BC%80%E9%94%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/06/05/%E4%B8%8A%E4%B8%8B%E6%96%87%E5%88%87%E6%8D%A2%E5%BC%80%E9%94%80/" class="post-title-link" itemprop="url">上下文切换的代价</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-05 17:30:00" itemprop="dateCreated datePublished" datetime="2022-06-05T17:30:00+08:00">2022-06-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/performance/" itemprop="url" rel="index"><span itemprop="name">performance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="上下文切换的代价"><a href="#上下文切换的代价" class="headerlink" title="上下文切换的代价"></a>上下文切换的代价</h1><h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><p>进程切换、软中断、内核态用户态切换、CPU超线程切换</p>
<p>内核态用户态切换：还是在一个线程中，只是由用户态进入内核态为了安全等因素需要更多的指令，系统调用具体多做了啥请看：<a target="_blank" rel="noopener" href="https://github.com/torvalds/linux/blob/v5.2/arch/x86/entry/entry_64.S#L145">https://github.com/torvalds/linux/blob/v5.2/arch/x86/entry/entry_64.S#L145</a></p>
<p>软中断：比如网络包到达，触发ksoftirqd(每个核一个)进程来处理，是进程切换的一种</p>
<p>进程切换是里面最重的，少不了上下文切换，代价还有进程阻塞唤醒调度。另外进程切换有主动让出CPU的切换、也有时间片用完后被切换</p>
<p>CPU超线程切换：最轻，发生在CPU内部，OS、应用都无法感知</p>
<p>多线程调度下的热点火焰图：</p>
<p><img src="/images/951413iMgBlog/7ece6c553c78927c7886f70c09d7e15b.png" alt="image.png"></p>
<p>上下文切换后还会因为调度的原因导致线程卡顿更久</p>
<p>Linux 内核进程调度时间片一般是HZ的倒数，HZ在编译的时候一般设置为1000，倒数也就是1ms，也就是每个进程的时间片是1ms（早年是10ms–HZ 为100的时候），如果进程1阻塞让出CPU进入调度队列，这个时候调度队列前还有两个进程2&#x2F;3在排队，也就是最差会在2ms后才轮到1被调度执行。负载决定了排队等待调度队列的长短，如果轮到调度的进程已经ready那么性能没有浪费，反之如果轮到被调度但是没有ready（比如网络回包没到达）相当浪费了一次调度</p>
<blockquote>
<p><code>sched_min_granularity_ns</code> is the most prominent setting. In the original <a target="_blank" rel="noopener" href="https://elixir.bootlin.com/linux/v2.6.25/source/Documentation/scheduler/sched-design-CFS.txt#L82">sched-design-CFS.txt</a> this was described as the only “tunable” setting, “to tune the scheduler from ‘desktop’ (low latencies) to ‘server’ (good batching) workloads.”</p>
<p>In other words, we can change this setting to reduce overheads from context-switching, and therefore improve throughput at the cost of responsiveness (“latency”).</p>
<p>The CFS setting as mimicking the previous build-time setting, <a target="_blank" rel="noopener" href="https://elixir.bootlin.com/linux/v2.6.25/source/kernel/Kconfig.hz">CONFIG_HZ</a>. In the first version of the CFS code, the default value was 1 ms, equivalent to 1000 Hz for “desktop” usage. Other supported values of CONFIG_HZ were 250 Hz (the default), and 100 Hz for the “server” end. 100 Hz was also useful when running Linux on very slow CPUs, this was one of the reasons given <a target="_blank" rel="noopener" href="https://lwn.net/Articles/56378/">when CONFIG_HZ was first added as an build setting on X86</a>.</p>
</blockquote>
<p>或者参数调整：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#sysctl -a |grep -i sched_ |grep -v cpu</span><br><span class="line">kernel.sched_autogroup_enabled = 0</span><br><span class="line">kernel.sched_cfs_bandwidth_slice_us = 5000</span><br><span class="line">kernel.sched_cfs_bw_burst_enabled = 1</span><br><span class="line">kernel.sched_cfs_bw_burst_onset_percent = 0</span><br><span class="line">kernel.sched_child_runs_first = 0</span><br><span class="line">kernel.sched_latency_ns = 24000000</span><br><span class="line">kernel.sched_migration_cost_ns = 500000</span><br><span class="line">kernel.sched_min_granularity_ns = 3000000</span><br><span class="line">kernel.sched_nr_migrate = 32</span><br><span class="line">kernel.sched_rr_timeslice_ms = 100</span><br><span class="line">kernel.sched_rt_period_us = 1000000</span><br><span class="line">kernel.sched_rt_runtime_us = 950000</span><br><span class="line">kernel.sched_schedstats = 1</span><br><span class="line">kernel.sched_tunable_scaling = 1</span><br><span class="line">kernel.sched_wakeup_granularity_ns = 4000000</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="How-long-does-it-take-to-make-a-context-switch"><a href="#How-long-does-it-take-to-make-a-context-switch" class="headerlink" title="How long does it take to make a context switch?"></a><a target="_blank" rel="noopener" href="https://blog.tsunanet.net/2010/11/how-long-does-it-take-to-make-context.html">How long does it take to make a context switch?</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">model name : Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</span><br><span class="line">2 physical CPUs, 26 cores/CPU, 2 hardware threads/core = 104 hw threads total</span><br><span class="line">-- No CPU affinity --</span><br><span class="line">10000000 system calls in 1144720626ns (114.5ns/syscall)</span><br><span class="line">2000000 process context switches in 6280519812ns (3140.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 6417846724ns (3208.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 147035970ns (73.5ns/ctxsw)</span><br><span class="line">-- With CPU affinity --</span><br><span class="line">10000000 system calls in 1109675081ns (111.0ns/syscall)</span><br><span class="line">2000000 process context switches in 4204573541ns (2102.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 2740739815ns (1370.4ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 474815006ns (237.4ns/ctxsw)</span><br><span class="line">-- With CPU affinity to CPU 0 --</span><br><span class="line">10000000 system calls in 1039827099ns (104.0ns/syscall)</span><br><span class="line">2000000 process context switches in 5622932975ns (2811.5ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 5697704164ns (2848.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 143474146ns (71.7ns/ctxsw)</span><br><span class="line">----------</span><br><span class="line">model name : Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</span><br><span class="line">2 physical CPUs, 16 cores/CPU, 2 hardware threads/core = 64 hw threads total</span><br><span class="line">-- No CPU affinity --</span><br><span class="line">10000000 system calls in 772827735ns (77.3ns/syscall)</span><br><span class="line">2000000 process context switches in 4009838007ns (2004.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 5234823470ns (2617.4ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 193276269ns (96.6ns/ctxsw)</span><br><span class="line">-- With CPU affinity --</span><br><span class="line">10000000 system calls in 746578449ns (74.7ns/syscall)</span><br><span class="line">2000000 process context switches in 3598569493ns (1799.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 2475733882ns (1237.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 381484302ns (190.7ns/ctxsw)</span><br><span class="line">-- With CPU affinity to CPU 0 --</span><br><span class="line">10000000 system calls in 746674401ns (74.7ns/syscall)</span><br><span class="line">2000000 process context switches in 4129856807ns (2064.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 4226458450ns (2113.2ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 193047255ns (96.5ns/ctxsw)</span><br><span class="line">---------</span><br><span class="line">model name : Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</span><br><span class="line">2 physical CPUs, 24 cores/CPU, 2 hardware threads/core = 96 hw threads total</span><br><span class="line">-- No CPU affinity --</span><br><span class="line">10000000 system calls in 765013680ns (76.5ns/syscall)</span><br><span class="line">2000000 process context switches in 5906908170ns (2953.5ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 6741875538ns (3370.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 173271254ns (86.6ns/ctxsw)</span><br><span class="line">-- With CPU affinity --</span><br><span class="line">10000000 system calls in 764139687ns (76.4ns/syscall)</span><br><span class="line">2000000 process context switches in 4040915457ns (2020.5ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 2327904634ns (1164.0ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 378847082ns (189.4ns/ctxsw)</span><br><span class="line">-- With CPU affinity to CPU 0 --</span><br><span class="line">10000000 system calls in 762375921ns (76.2ns/syscall)</span><br><span class="line">2000000 process context switches in 5827318932ns (2913.7ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 6360562477ns (3180.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 173019064ns (86.5ns/ctxsw)</span><br><span class="line">--------ECS</span><br><span class="line">model name : Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</span><br><span class="line">1 physical CPUs, 2 cores/CPU, 2 hardware threads/core = 4 hw threads total</span><br><span class="line">-- No CPU affinity --</span><br><span class="line">10000000 system calls in 561242906ns (56.1ns/syscall)</span><br><span class="line">2000000 process context switches in 3025706345ns (1512.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 3333843503ns (1666.9ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 145410372ns (72.7ns/ctxsw)</span><br><span class="line">-- With CPU affinity --</span><br><span class="line">10000000 system calls in 586742944ns (58.7ns/syscall)</span><br><span class="line">2000000 process context switches in 2369203084ns (1184.6ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 1929627973ns (964.8ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 335827569ns (167.9ns/ctxsw)</span><br><span class="line">-- With CPU affinity to CPU 0 --</span><br><span class="line">10000000 system calls in 630259940ns (63.0ns/syscall)</span><br><span class="line">2000000 process context switches in 3027444795ns (1513.7ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 3172677638ns (1586.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 144168251ns (72.1ns/ctxsw)</span><br><span class="line">---------kupeng 920</span><br><span class="line">2 physical CPUs, 96 cores/CPU, 1 hardware threads/core = 192 hw threads total</span><br><span class="line">-- No CPU affinity --</span><br><span class="line">10000000 system calls in 1216730780ns (121.7ns/syscall)</span><br><span class="line">2000000 process context switches in 4653366132ns (2326.7ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 4689966324ns (2345.0ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 167871167ns (83.9ns/ctxsw)</span><br><span class="line">-- With CPU affinity --</span><br><span class="line">10000000 system calls in 1220106854ns (122.0ns/syscall)</span><br><span class="line">2000000 process context switches in 3420506934ns (1710.3ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 2962106029ns (1481.1ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 543325133ns (271.7ns/ctxsw)</span><br><span class="line">-- With CPU affinity to CPU 0 --</span><br><span class="line">10000000 system calls in 1216466158ns (121.6ns/syscall)</span><br><span class="line">2000000 process context switches in 2797948549ns (1399.0ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 3119316050ns (1559.7ns/ctxsw)</span><br><span class="line">2000000  thread context switches in 167728516ns (83.9ns/ctxsw)</span><br></pre></td></tr></table></figure>



<p>测试代码仓库：<a target="_blank" rel="noopener" href="https://github.com/tsuna/contextswitch">https://github.com/tsuna/contextswitch</a></p>
<p>Source code: <a target="_blank" rel="noopener" href="https://github.com/tsuna/contextswitch/blob/master/timectxsw.c">timectxsw.c</a> Results:</p>
<ul>
<li>Intel 5150: ~4300ns&#x2F;context switch</li>
<li>Intel E5440: ~3600ns&#x2F;context switch</li>
<li>Intel E5520: ~4500ns&#x2F;context switch</li>
<li>Intel X5550: ~3000ns&#x2F;context switch</li>
<li>Intel L5630: ~3000ns&#x2F;context switch</li>
<li>Intel E5-2620: ~3000ns&#x2F;context switch</li>
</ul>
<p>如果绑核后上下文切换能提速在66-45%之间</p>
<p>系统调用代价</p>
<p>Source code: <a target="_blank" rel="noopener" href="https://github.com/tsuna/contextswitch/blob/master/timesyscall.c">timesyscall.c</a> Results:</p>
<ul>
<li>Intel 5150: 105ns&#x2F;syscall</li>
<li>Intel E5440: 87ns&#x2F;syscall</li>
<li>Intel E5520: 58ns&#x2F;syscall</li>
<li>Intel X5550: 52ns&#x2F;syscall</li>
<li>Intel L5630: 58ns&#x2F;syscall</li>
<li>Intel E5-2620: 67ns&#x2F;syscall</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/uq5s5vwk5vtPOZ30sfNsOg">https://mp.weixin.qq.com/s/uq5s5vwk5vtPOZ30sfNsOg</a> 进程&#x2F;线程切换究竟需要多少开销？</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">创建两个进程并在它们之间传送一个令牌。其中一个进程在读取令牌时就会引起阻塞。另一个进程发送令牌后等待其返回时也处于阻塞状态。如此往返传送一定的次数，然后统计他们的平均单次切换时间开销</span></span><br><span class="line"><span class="comment">代码来自：https://www.jianshu.com/p/be3250786a91</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/time.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sched.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/types.h&gt;</span>  </span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span>      <span class="comment">//pipe()  </span></span></span><br><span class="line">  </span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span>  </span><br><span class="line">&#123;  </span><br><span class="line">    <span class="type">int</span> x, i, fd[<span class="number">2</span>], p[<span class="number">2</span>];  </span><br><span class="line">    <span class="type">char</span> send    = <span class="string">&#x27;s&#x27;</span>;  </span><br><span class="line">    <span class="type">char</span> receive;  </span><br><span class="line">    pipe(fd);  </span><br><span class="line">    pipe(p);  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">timeval</span> <span class="title">tv</span>;</span>  </span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">sched_param</span> <span class="title">param</span>;</span>  </span><br><span class="line">    param.sched_priority = <span class="number">0</span>;  </span><br><span class="line">  </span><br><span class="line">    <span class="keyword">while</span> ((x = fork()) == <span class="number">-1</span>); </span><br><span class="line">    <span class="keyword">if</span> (x==<span class="number">0</span>) &#123;  </span><br><span class="line">        sched_setscheduler(getpid(), SCHED_FIFO, &amp;param);  </span><br><span class="line">        gettimeofday(&amp;tv, <span class="literal">NULL</span>);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Before Context Switch Time%u s, %u us\n&quot;</span>, tv.tv_sec, tv.tv_usec);  </span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i++) &#123;  </span><br><span class="line">            read(fd[<span class="number">0</span>], &amp;receive, <span class="number">1</span>);  </span><br><span class="line">            write(p[<span class="number">1</span>], &amp;send, <span class="number">1</span>);  </span><br><span class="line">        &#125;  </span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">0</span>);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">else</span> &#123;  </span><br><span class="line">        sched_setscheduler(getpid(), SCHED_FIFO, &amp;param);  </span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; <span class="number">10000</span>; i++) &#123;  </span><br><span class="line">            write(fd[<span class="number">1</span>], &amp;send, <span class="number">1</span>);  </span><br><span class="line">            read(p[<span class="number">0</span>], &amp;receive, <span class="number">1</span>);  </span><br><span class="line">        &#125;  </span><br><span class="line">        gettimeofday(&amp;tv, <span class="literal">NULL</span>);  </span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;After Context SWitch Time%u s, %u us\n&quot;</span>, tv.tv_sec, tv.tv_usec);  </span><br><span class="line">    &#125;  </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;  </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure>

<p>平均每次上下文切换耗时3.5us左右</p>
<h2 id="软中断开销计算"><a href="#软中断开销计算" class="headerlink" title="软中断开销计算"></a>软中断开销计算</h2><p>下面的计算方法比较糙，仅供参考。压力越大，一次软中断需要处理的网络包数量就越多，消耗的时间越长。如果包数量太少那么测试干扰就太严重了，数据也不准确。</p>
<p>测试机将收发队列设置为1，让所有软中断交给一个core来处理。</p>
<p>无压力时 interrupt大概4000，然后故意跑压力，CPU跑到80%，通过vmstat和top查看：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$vmstat 1 </span><br><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line">19  0      0 174980 151840 3882800    0    0     0    11    1    1  1  0 99  0  0</span><br><span class="line">11  0      0 174820 151844 3883668    0    0     0     0 30640 113918 59 22 20  0  0</span><br><span class="line"> 9  0      0 175952 151852 3884576    0    0     0   224 29611 108549 57 22 21  0  0</span><br><span class="line">11  0      0 171752 151852 3885636    0    0     0  3452 30682 113874 57 22 21  0  0</span><br></pre></td></tr></table></figure>

<p>top看到 si% 大概为20%，也就是一个核25000个interrupt需要消耗 20% 的CPU, 说明这些软中断消耗了200毫秒</p>
<p>200*1000微秒&#x2F;25000&#x3D;200&#x2F;25&#x3D;8微秒，8000纳秒 – 偏高</p>
<p>降低压力CPU 跑到55% si消耗12%</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 6  0      0 174180 152076 3884360    0    0     0     0 25314 119681 40 17 43  0  0</span><br><span class="line"> 1  0      0 172600 152080 3884308    0    0     0   252 24971 116407 40 17 43  0  0</span><br><span class="line"> 4  0      0 174664 152080 3884540    0    0     0  3536 25164 118175 39 18 42  0  0</span><br></pre></td></tr></table></figure>

<p>120*1000微秒&#x2F;(21000)&#x3D;5.7微秒， 5700纳秒 – 偏高</p>
<p>降低压力（4核CPU只压到15%）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 0  0      0 183228 151788 3876288    0    0     0     0 15603 42460  6  3 91  0  0</span><br><span class="line"> 0  0      0 181312 151788 3876032    0    0     0     0 15943 43129  7  2 91  0  0</span><br><span class="line"> 1  0      0 181728 151788 3876544    0    0     0  3232 15790 42409  7  3 90  0  0</span><br><span class="line"> 0  0      0 181584 151788 3875956    0    0     0     0 15728 42641  7  3 90  0  0</span><br><span class="line"> 1  0      0 179276 151792 3876848    0    0     0   192 15862 42875  6  3 91  0  0</span><br><span class="line"> 0  0      0 179508 151796 3876424    0    0     0     0 15404 41899  7  2 91  0  0</span><br></pre></td></tr></table></figure>

<p>单核11000 interrupt，对应 si CPU 2.2%</p>
<p>22*1000&#x2F;11000&#x3D; 2微秒 2000纳秒 略微靠谱</p>
<h2 id="超线程切换开销"><a href="#超线程切换开销" class="headerlink" title="超线程切换开销"></a>超线程切换开销</h2><p>最小，基本可以忽略，1ns以内</p>
<h2 id="lmbench测试工具"><a href="#lmbench测试工具" class="headerlink" title="lmbench测试工具"></a>lmbench测试工具</h2><p>lmbench的lat_ctx等，单位是微秒，压力小的时候一次进程的上下文是1540纳秒</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[root@plantegg 13:19 /root/lmbench3]</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">taskset -c 4 ./bin/lat_ctx -P 2 -W warmup -s 64 2  //CPU 打满</span></span><br><span class="line">&quot;size=64k ovr=3.47</span><br><span class="line">2 7.88</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">taskset -c 4 ./bin/lat_ctx -P 1 -W warmup -s 64 2</span></span><br><span class="line">&quot;size=64k ovr=3.46</span><br><span class="line">2 1.54</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">taskset -c 4-5 ./bin/lat_ctx  -W warmup -s 64 2</span></span><br><span class="line">&quot;size=64k ovr=3.44</span><br><span class="line">2 3.11</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">taskset -c 4-7 ./bin/lat_ctx -P 2 -W warmup -s 64 2  //CPU 打到50%</span></span><br><span class="line">&quot;size=64k ovr=3.48</span><br><span class="line">2 3.14</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">taskset -c 4-15 ./bin/lat_ctx -P 3 -W warmup -s 64 2</span></span><br><span class="line">&quot;size=64k ovr=3.46</span><br><span class="line">2 3.18</span><br></pre></td></tr></table></figure>

<h2 id="协程对性能的影响"><a href="#协程对性能的影响" class="headerlink" title="协程对性能的影响"></a>协程对性能的影响</h2><p>将WEB服务改用协程调度后，TPS提升50%（30000提升到45000），而contextswitch数量从11万降低到8000（无压力的cs也有4500）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----</span><br><span class="line"> r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st</span><br><span class="line"> 5  0      0 3831480 153136 3819244    0    0     0     0 23599 6065 79 19  2  0  0</span><br><span class="line"> 4  0      0 3829208 153136 3818824    0    0     0   160 23324 7349 80 18  2  0  0</span><br><span class="line"> 4  0      0 3833320 153140 3818672    0    0     0     0 24567 8213 80 19  2  0  0</span><br><span class="line"> 4  0      0 3831880 153140 3818532    0    0     0     0 24339 8350 78 20  2  0  0</span><br><span class="line"> </span><br><span class="line">[  99s] threads: 60, tps: 0.00, reads/s: 44609.77, writes/s: 0.00, response time: 2.05ms (95%)</span><br><span class="line">[ 100s] threads: 60, tps: 0.00, reads/s: 46538.27, writes/s: 0.00, response time: 1.99ms (95%)</span><br><span class="line">[ 101s] threads: 60, tps: 0.00, reads/s: 46061.84, writes/s: 0.00, response time: 2.01ms (95%)</span><br><span class="line">[ 102s] threads: 60, tps: 0.00, reads/s: 46961.05, writes/s: 0.00, response time: 1.94ms (95%)</span><br><span class="line">[ 103s] threads: 60, tps: 0.00, reads/s: 46224.15, writes/s: 0.00, response time: 2.00ms (95%)</span><br><span class="line">[ 104s] threads: 60, tps: 0.00, reads/s: 46556.93, writes/s: 0.00, response time: 1.98ms (95%)</span><br><span class="line">[ 105s] threads: 60, tps: 0.00, reads/s: 45965.12, writes/s: 0.00, response time: 1.97ms (95%)</span><br><span class="line">[ 106s] threads: 60, tps: 0.00, reads/s: 46369.96, writes/s: 0.00, response time: 2.01ms (95%)</span><br><span class="line"></span><br><span class="line">//4core 机器下</span><br><span class="line"> PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND</span><br><span class="line"> 11588 admin     20   0   12.9g   6.9g  22976 R 95.7 45.6   0:33.07 Root-Worke //四个协程把CPU基本跑满</span><br><span class="line"> 11586 admin     20   0   12.9g   6.9g  22976 R 93.7 45.6   0:34.29 Root-Worke</span><br><span class="line"> 11587 admin     20   0   12.9g   6.9g  22976 R 93.7 45.6   0:32.58 Root-Worke</span><br><span class="line"> 11585 admin     20   0   12.9g   6.9g  22976 R 92.0 45.6   0:33.25 Root-Worke</span><br></pre></td></tr></table></figure>

<p>没开协程CPU有20%闲置打不上去，开了协程后CPU 跑到95%</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul>
<li>进程上下文切换需要几千纳秒（不同CPU型号会有差异）</li>
<li>如果做taskset 那么上下文切换会减少50%的时间（避免了L1、L2 Miss等）</li>
<li>线程比进程上下文切换略快10%左右</li>
<li>测试数据和实际运行场景相关很大，比较难以把控，CPU竞争太激烈容易把等待调度时间计入；如果CPU比较闲体现不出cache miss等导致的时延加剧</li>
<li>系统调用相对进程上下文切换就很轻了，大概100ns以内</li>
<li>函数调用更轻，大概几个ns，压栈跳转</li>
<li>CPU的超线程调度和函数调用差不多，都是几个ns可以搞定</li>
</ul>
<p>看完这些数据再想想协程是在做什么、为什么效率高就很自然的了</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/05/05/Netty%E5%92%8CDisruptor%E7%9A%84cache_line%E5%AF%B9%E9%BD%90%E5%AE%9E%E8%B7%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/05/05/Netty%E5%92%8CDisruptor%E7%9A%84cache_line%E5%AF%B9%E9%BD%90%E5%AE%9E%E8%B7%B5/" class="post-title-link" itemprop="url">Netty和Disruptor的cache_line对齐实践</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-05-05 18:20:03" itemprop="dateCreated datePublished" datetime="2022-05-05T18:20:03+08:00">2022-05-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Netty和Disruptor的cache-line对齐实践"><a href="#Netty和Disruptor的cache-line对齐实践" class="headerlink" title="Netty和Disruptor的cache_line对齐实践"></a>Netty和Disruptor的cache_line对齐实践</h1><p>原理先看这篇：<a href="https://plantegg.github.io/2021/05/16/CPU_Cache_Line%E5%92%8C%E6%80%A7%E8%83%BD/">CPU 性能和Cache Line</a></p>
<p>写这篇文章的起因是这个 <a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/vkCskOVSpzxt3Umzc_GYrQ">记一次 Netty PR 的提交</a>，然后我去看了下这次提交，发现Netty的这部分代码有问题、这次提交也有问题</p>
<h2 id="什么是-cache-line"><a href="#什么是-cache-line" class="headerlink" title="什么是 cache_line"></a>什么是 cache_line</h2><p>CPU从内存中读取数据的时候是一次读一个cache_line到 cache中以提升效率，一般情况下cache_line的大小是64 byte，也就是每次读取64byte到CPU cache中，按照热点逻辑这个cache line中的数据大概率会被访问到。</p>
<h3 id="cache-失效"><a href="#cache-失效" class="headerlink" title="cache 失效"></a>cache 失效</h3><p>假设CPU的两个核 A 和 B, 都在各自本地 Cache Line 里有同一个变量1的拷贝时，此时该 Cache Line 处于 Shared 状态。当 核A 在本地修改了变量2，除去把本地变量所属的 Cache Line 置为 Modified 状态以外，还必须在另一个 核B 读另一个变量2前，对该变量所在的 B 处理器本地 Cache Line 发起 Invaidate 操作，标记 B 处理器的那条 Cache Line 为 Invalidate 状态。随后，若处理器 B 在对变量做读写操作时，如果遇到这个标记为 Invalidate 的状态的 Cache Line，即会引发 Cache Miss，从而将内存中最新的数据拷贝到 Cache Line 里，然后处理器 B 再对此 Cache Line 对变量做读写操作。</p>
<p>上面这个过程也叫false-share, 即伪共享，因为变量1、2不是真的关联共享，本来变量1失效不应该导致变量2失效，但是因为cache line机制的存在导致 变量2也失效了，所以这里变量1、2叫false-share</p>
<h2 id="Disruptor中对cache-line的使用"><a href="#Disruptor中对cache-line的使用" class="headerlink" title="Disruptor中对cache_line的使用"></a>Disruptor中对cache_line的使用</h2><p>Disruptor中为了保护下面的那几个final 成员变量，前后都加了 p1-p7就是为了避免这4个final成员不要和别的变量放到同一个cache line中。</p>
<p>重点留意下面代码中的p1-p7这几个没有用的long变量，实际使用来占位，占住实际变量前后的位置，这样避免这些变量被其他变量的修改而失效。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">abstract class RingBufferPad</span><br><span class="line">&#123;</span><br><span class="line">    protected long p1, p2, p3, p4, p5, p6, p7;</span><br><span class="line">&#125;</span><br><span class="line">  </span><br><span class="line">abstract class RingBufferFields&lt;E&gt; extends RingBufferPad</span><br><span class="line">&#123;</span><br><span class="line">    ......    </span><br><span class="line">    private final long indexMask;</span><br><span class="line">    private final Object[] entries;</span><br><span class="line">    protected final int bufferSize;</span><br><span class="line">    protected final Sequencer sequencer;</span><br><span class="line">    ......    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public final class RingBuffer&lt;E&gt; extends RingBufferFields&lt;E&gt; implements Cursored, EventSequencer&lt;E&gt;, EventSink&lt;E&gt;</span><br><span class="line">&#123;</span><br><span class="line">    ......    </span><br><span class="line">    protected long p1, p2, p3, p4, p5, p6, p7;</span><br><span class="line">    ......</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>结果如下图所示绿色部分很好地被保护起来一定是独占一个cache line，本来绿色部分都是final，也就是你理解成只读的，不会更改了，这样不会因为共享cache line的变量被修改导致他们所在的cache失效（完全没必要）</p>
<p><img src="/images/951413iMgBlog/1620984677390-81694fd0-0323-4052-98d1-32be39a02248-4505908.png" alt="image.png"></p>
<p>队列大部分时候都是空的（head挨着tail），也就导致head 和 tail在一个cache line中，读和写会造成没必要的cache ping-pong，一般可以通过将head 和 tail 中间填充其它内容来实现错开到不同的cache line中</p>
<p><img src="/images/951413iMgBlog/1577093636588-6b58c36c-1617-4f2c-aba9-156c52972689-1744256.png" alt="image"></p>
<p>数组(RingBuffer)基本能保证元素在内存中是连续的，但是Queue（链表）就不一定了，连续的话更利于CPU cache</p>
<h2 id="Netty中cache-line的对齐"><a href="#Netty中cache-line的对齐" class="headerlink" title="Netty中cache line的对齐"></a>Netty中cache line的对齐</h2><p><a target="_blank" rel="noopener" href="https://github.com/arthur-zhang/netty/blob/e8250372cafe4cf5435a1dbc4c8e400072fb9791/common/src/main/java/io/netty/util/internal/InternalThreadLocalMap.java">注意下图12行</a>的代码，重点也请注意下11行的注释</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">// String-related thread-locals</span><br><span class="line">private StringBuilder stringBuilder;</span><br><span class="line">private Map&lt;Charset, CharsetEncoder&gt; charsetEncoderCache;</span><br><span class="line">private Map&lt;Charset, CharsetDecoder&gt; charsetDecoderCache;</span><br><span class="line"></span><br><span class="line">// ArrayList-related thread-locals</span><br><span class="line">private ArrayList&lt;Object&gt; arrayList;</span><br><span class="line"></span><br><span class="line">private BitSet cleanerFlags;</span><br><span class="line"></span><br><span class="line">/** @deprecated These padding fields will be removed in the future. */</span><br><span class="line">public long rp1, rp2, rp3, rp4, rp5, rp6, rp7, rp8;</span><br><span class="line"></span><br><span class="line">static &#123;</span><br><span class="line">    STRING_BUILDER_INITIAL_SIZE =</span><br><span class="line">            SystemPropertyUtil.getInt(&quot;io.netty.threadLocalMap.stringBuilder.initialSize&quot;, 1024);</span><br><span class="line">    logger.debug(&quot;-Dio.netty.threadLocalMap.stringBuilder.initialSize: &#123;&#125;&quot;, STRING_BUILDER_INITIAL_SIZE);</span><br><span class="line"></span><br><span class="line">    STRING_BUILDER_MAX_SIZE = SystemPropertyUtil.getInt(&quot;io.netty.threadLocalMap.stringBuilder.maxSize&quot;, 1024 * 4);</span><br><span class="line">    logger.debug(&quot;-Dio.netty.threadLocalMap.stringBuilder.maxSize: &#123;&#125;&quot;, STRING_BUILDER_MAX_SIZE);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>一看这里也和Disruptor一样想保护某个变量尽量少失效，可是这个实现我看不出来想要保护哪个变量，因为这种保护办法只对齐了一边，还有一边是和别的变量共享cache line。</p>
<p>另外这个代码之前是9个long rp来对齐，这个<a target="_blank" rel="noopener" href="https://github.com/netty/netty/pull/12309">PR</a>改成了8个，9个就实在是迷惑了（9个long占72bytes了）对齐也是64bytes就好了</p>
<p>还是按照11行注释所说去掉这个对齐的rp吧，要不明确要保护哪些变量，前后夹击真正保护起来，并且做好对比测试</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Netty的这段代码纸上谈兵更多一点，Donald E. Knuth 告诉我们不要提前优化</p>
<h2 id="系列文章"><a href="#系列文章" class="headerlink" title="系列文章"></a>系列文章</h2><p><a href="/2021/06/01/CPU%E7%9A%84%E5%88%B6%E9%80%A0%E5%92%8C%E6%A6%82%E5%BF%B5/">CPU的制造和概念</a></p>
<p>[Perf IPC以及CPU性能](&#x2F;2021&#x2F;05&#x2F;16&#x2F;Perf IPC以及CPU利用率&#x2F;)</p>
<p>[CPU 性能和Cache Line](&#x2F;2021&#x2F;05&#x2F;16&#x2F;CPU Cache Line 和性能&#x2F;)</p>
<p><a href="/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p>[Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的](&#x2F;2019&#x2F;12&#x2F;16&#x2F;Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的&#x2F;)</p>
<p><a href="/2021/06/18/%E5%87%A0%E6%AC%BECPU%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/03/07/%E4%B8%80%E6%AC%A1%E6%B5%B7%E5%85%89%E7%89%A9%E7%90%86%E6%9C%BA%E8%B5%84%E6%BA%90%E7%AB%9E%E4%BA%89%E5%8E%8B%E6%B5%8B%E7%9A%84%E8%AE%B0%E5%BD%95/">一次海光物理机资源竞争压测的记录</a></p>
<p><a href="/2021/05/15/%E9%A3%9E%E8%85%BEARM%E8%8A%AF%E7%89%87-FT2500%E7%9A%84%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/">飞腾ARM芯片(FT2500)的性能测试</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/03/15/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%90%AC%E9%A3%8E%E6%89%87%E5%A3%B0%E9%9F%B3%E6%9D%A5%E5%AE%9A%E4%BD%8D%E6%80%A7%E8%83%BD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/03/15/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%90%AC%E9%A3%8E%E6%89%87%E5%A3%B0%E9%9F%B3%E6%9D%A5%E5%AE%9A%E4%BD%8D%E6%80%A7%E8%83%BD/" class="post-title-link" itemprop="url">听风扇声音来定位性能瓶颈</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-03-15 17:30:03" itemprop="dateCreated datePublished" datetime="2022-03-15T17:30:03+08:00">2022-03-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="记一次听风扇声音来定位性能瓶颈"><a href="#记一次听风扇声音来定位性能瓶颈" class="headerlink" title="记一次听风扇声音来定位性能瓶颈"></a>记一次听风扇声音来定位性能瓶颈</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在一次POC测试过程中，测试机构提供了两台Intel压力机来压我们的集群</p>
<ul>
<li>压力机1：两路共72core intel 5XXX系列 CPU，主频2.2GHz， 128G内存</li>
<li>压力机2：四路共196core intel 8XXX系列 CPU，主频2.5GHz， 256G内存 （8系列比5系列 CPU的性能要好、要贵）</li>
</ul>
<p>从CPU硬件指标来看压力机2都是碾压压力机1，但是实际测试是压力机2只能跑到接近压力机1的能力，两台机器CPU基本都跑满，并且都是压测进程消耗了90%以上的CPU，内核态消耗不到5%CPU</p>
<p>所以接下来需要在调试我们集群性能前先把测试机优化好，才能把压力打上来。</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>测试机构提供的机器上没有任何工具来评估CPU性能，也无法安装，只能<strong>仔细听196core机器的CPU风扇声音更小，说明196core的CPU出工不出力，大概是流水线在频繁地Stall</strong>（不管你信不信反正我是信的）</p>
<p>进一步分析，首先看到 业务消耗了90%以上的CPU，内核态消耗不到5%CPU，两台机器都是这样，这说明 196core 只跑出了 72core的水平，一定是CPU效率出了问题，top看到的CPU占用率不完全是全力在运算，其实cpu 流水线stall也是占用CPU的。</p>
<p>这个分析理论请参考我的文章<a href="https://plantegg.github.io/2021/05/16/Perf%20IPC%E4%BB%A5%E5%8F%8ACPU%E5%88%A9%E7%94%A8%E7%8E%87/">《Perf IPC以及CPU性能》</a></p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>通过stream测试读写内存的带宽和时延，得到如下数据：</p>
<p>72core机器，  本路时延1.1，跨路时延1.4，因为是2路所以有50%的概率跨路，性能下降30%</p>
<p>196core机器，本路时延1.2，跨路时延1.85，因为是4路所以有75%的概率跨路，性能下降50%</p>
<p>从以上测试数据可以明显看到虽然196core机器拥有更强的单核能力以及更多的核数，但是因为访问内存太慢严重拖累了CPU运算能力，导致大部分时间CPU都在等待内存，这里CPU和内存的速度差了2个数量级，所以内存延时才是整体的瓶颈。</p>
<p>测试数据和方法请参考我的文章<a href="https://plantegg.github.io/2021/06/18/%E5%87%A0%E6%AC%BECPU%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">《AMD Zen CPU 架构以及不同CPU性能大PK》</a></p>
<p>有了这个数据心里非常有底问题在哪里了，但是还要想清楚怎么解释给测试机构他们才会信服，因为第一次解释他们直接说不可能，怎么会196core打不过72core呢，再说从来没有集群是测试机构196core压力机打不满的，这台压力机用了几年从来没有人说过这个问题 :(</p>
<h2 id="内存信息"><a href="#内存信息" class="headerlink" title="内存信息"></a>内存信息</h2><p>接下来需要拿到更详细的硬件信息来说服测试机构了。</p>
<p>通过dmidecode 获取两台机器内存的速度，分别是2100（196core） VS 2900（72core），同时系统也吐出了内存延时分别是 0.5ns VS 0.3 ns，这两个时间对比很直观，普通人也能看懂。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">//以下硬件信息是从家里机器上获取，并非测试机构提供的机器，测试机构提供的机器不让拍照和采集</span><br><span class="line">#dmidecode -t memory</span><br><span class="line"># dmidecode 3.2</span><br><span class="line">Getting SMBIOS data from sysfs.</span><br><span class="line">SMBIOS 3.2.1 present.</span><br><span class="line"># SMBIOS implementations newer than version 3.2.0 are not</span><br><span class="line"># fully supported by this version of dmidecode.</span><br><span class="line"></span><br><span class="line">Handle 0x0033, DMI type 16, 23 bytes </span><br><span class="line">Physical Memory Array</span><br><span class="line">	Location: System Board Or Motherboard</span><br><span class="line">	Use: System Memory</span><br><span class="line">	Error Correction Type: Multi-bit ECC</span><br><span class="line">	Maximum Capacity: 2 TB  //最大支持2T</span><br><span class="line">	Error Information Handle: 0x0032</span><br><span class="line">	Number Of Devices: 32   //32个插槽</span><br><span class="line">	</span><br><span class="line">	Handle 0x0041, DMI type 17, 84 bytes</span><br><span class="line">Memory Device</span><br><span class="line">	Array Handle: 0x0033</span><br><span class="line">	Error Information Handle: 0x0040</span><br><span class="line">	Total Width: 72 bits</span><br><span class="line">	Data Width: 64 bits</span><br><span class="line">	Size: 32 GB</span><br><span class="line">	Form Factor: DIMM</span><br><span class="line">	Set: None</span><br><span class="line">	Locator: CPU0_DIMMA0</span><br><span class="line">	Bank Locator: P0 CHANNEL A</span><br><span class="line">	Type: DDR4</span><br><span class="line">	Type Detail: Synchronous Registered (Buffered)</span><br><span class="line">	Speed: 2933 MT/s                    //dmmi 内存插槽支持最大速度 ?</span><br><span class="line">	Manufacturer: SK Hynix</span><br><span class="line">	Serial Number: 220F9EC0</span><br><span class="line">	Asset Tag: Not Specified</span><br><span class="line">	Part Number: HMAA4GR7AJR8N-WM</span><br><span class="line">	Rank: 2</span><br><span class="line">	Configured Memory Speed: 2100 MT/s  //内存实际运行速度</span><br><span class="line">	Minimum Voltage: 1.2 V</span><br><span class="line">	Maximum Voltage: 1.2 V</span><br><span class="line">	Configured Voltage: 1.2 V</span><br><span class="line">	Memory Technology: DRAM</span><br><span class="line">	Memory Operating Mode Capability: Volatile memory</span><br><span class="line">	Module Manufacturer ID: Bank 1, Hex 0xAD</span><br><span class="line">	Non-Volatile Size: None</span><br><span class="line">	Volatile Size: 32 GB</span><br><span class="line">	</span><br><span class="line">	#lshw</span><br><span class="line">	*-bank:19  //主板插槽槽位</span><br><span class="line">             description: DIMM DDR4 Synchronous Registered (Buffered) 2933 MHz (0.3 ns) </span><br><span class="line">             product: HMAA4GR7AJR8N-WM</span><br><span class="line">             vendor: SK Hynix</span><br><span class="line">             physical id: 13</span><br><span class="line">             serial: 220F9F63</span><br><span class="line">             slot: CPU1_DIMMB0</span><br><span class="line">             size: 32GiB  //实际所插内存大小</span><br><span class="line">             width: 64 bits</span><br><span class="line">             clock: 2933MHz (0.3ns)</span><br></pre></td></tr></table></figure>

<blockquote>
<p>In <code>dmidecode</code>’s output for memory, “Speed” is the highest speed supported by the DIMM, as determined by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/JEDEC">JEDEC</a> SPD information. “Configured Clock Speed” is the speed at which it is currently running (as set up during boot).</p>
</blockquote>
<p>Dimm（双列直插式存储模块（dual In-line memory module））： DIMM是内存条印刷电路板正反面均有金手指与主板上的内存条槽接触，这种结构被称为DIMM。于是内存条也有人叫DIMM条，主板上的内存槽也有人称为DIMM槽。</p>
<p>大多数主板设计为易于用户安装和更换DIMM，通常只需打开侧边卡扣，将DIMM垂直插入插槽，然后关闭卡扣即可固定内存模块。正确安装DIMM时通常会有轻微的“点击”声，表示模块已经正确位于插槽中。</p>
<p>DIMM 代表物理上的一根内存条，下图中三根内存条共享一个channel连到 CPU</p>
<p><img src="/images/951413iMgBlog/05-05_DPC_Bandwidth_Impact.svg" alt="05-05_DPC_Bandwidth_Impact"></p>
<p><img src="/images/951413iMgBlog/image-20220705104403314.png" alt="image-20220705104403314"></p>
<p><img src="/images/951413iMgBlog/8f04a1f57fe07692327b9269ba484ce4.jpg" alt="img"></p>
<h2 id="最终的运行方案"><a href="#最终的运行方案" class="headerlink" title="最终的运行方案"></a>最终的运行方案</h2><p>给196core的机器换上新的2933 MHz (0.3 ns)的内存条，速度一下子就上去了。</p>
<p>然后在196core的机器上起4个压力进程，每个进程分担25%的压力，避免跨路访问内存导致时延从1.2掉到1.8，实际测试也是只用196core中的48core性能和用全部196core是一样的，所以这里一定要起多个进程做内存亲和性绑定，充分使用全部196core。</p>
<p><strong>最终整机196core机器的打压能力达到了原来的3.6倍左右。</strong></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>程序员要保护好听力，关键时刻可能会用上 :)</p>
<p>你说196core机器用了这么强的CPU但是为什么搭配那么差的内存以及主板，我也不知道，大概是有人拿回扣吧。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://frankdenneman.nl/2016/07/13/numa-deep-dive-4-local-memory-optimization/">NUMA DEEP DIVE PART 4: LOCAL MEMORY OPTIMIZATION</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/25/ssd_san%E5%92%8Csas%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/25/ssd_san%E5%92%8Csas%E7%A3%81%E7%9B%98%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/" class="post-title-link" itemprop="url">ssd/san/sas/磁盘/光纤性能比较</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-25 17:30:03" itemprop="dateCreated datePublished" datetime="2022-01-25T17:30:03+08:00">2022-01-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/performance/" itemprop="url" rel="index"><span itemprop="name">performance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ssd-san-sas-磁盘-光纤-RAID性能比较"><a href="#ssd-san-sas-磁盘-光纤-RAID性能比较" class="headerlink" title="ssd&#x2F;san&#x2F;sas&#x2F;磁盘&#x2F;光纤&#x2F;RAID性能比较"></a>ssd&#x2F;san&#x2F;sas&#x2F;磁盘&#x2F;光纤&#x2F;RAID性能比较</h1><p>本文汇总HDD、SSD、SAN、LVM、软RAID等一些性能数据</p>
<h2 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h2><p>正好有机会用到一个san存储设备，跑了一把性能数据，记录一下</p>
<p><img src="/images/oss/d57a004c846e193126ca01398e394319.png" alt="image.png"></p>
<p>所使用的测试命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -size=1000G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br></pre></td></tr></table></figure>

<p>ssd（Solid State Drive）和san的比较是在同一台物理机上，所以排除了其他因素的干扰。</p>
<p>简要的结论： </p>
<ul>
<li><p>本地ssd性能最好、sas机械盘(RAID10)性能最差</p>
</li>
<li><p>san存储走特定的光纤网络，不是走tcp的san（至少从网卡看不到san的流量），性能居中</p>
</li>
<li><p>从rt来看 ssd:san:sas 大概是 1:3:15</p>
</li>
<li><p>san比本地sas机械盘性能要好，这也许取决于san的网络传输性能和san存储中的设备（比如用的ssd而不是机械盘）</p>
</li>
</ul>
<h2 id="NVMe-SSD-和-HDD的性能比较"><a href="#NVMe-SSD-和-HDD的性能比较" class="headerlink" title="NVMe SSD 和 HDD的性能比较"></a>NVMe SSD 和 HDD的性能比较</h2><p><img src="/images/oss/d64a0f78ebf471ac69d447ecb46d90f1.png" alt="image.png"></p>
<p>表中性能差异比上面测试还要大，SSD 的随机 IO 延迟比传统硬盘快百倍以上，一般在微妙级别；IO 带宽也高很多倍，可以达到每秒几个 GB；随机 IOPS 更是快了上千倍，可以达到几十万。</p>
<p><strong>HDD只有一个磁头，并发没有意义，但是SSD支持高并发写入读取。SSD没有磁头、不需要旋转，所以随机读取和顺序读取基本没有差别。</strong></p>
<p><img src="/images/951413iMgBlog/1ab661ee2d3a71f54bae3ecf62982e7e.png" alt="img"></p>
<p>从上图可以看出如果是随机读写HDD性能极差，但是如果是顺序读写HDD和SDD、内存差异就不那么大了。</p>
<h2 id="磁盘类型查看"><a href="#磁盘类型查看" class="headerlink" title="磁盘类型查看"></a>磁盘类型查看</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$cat /sys/block/vda/queue/rotational //但是对于虚拟机就不一定对</span><br><span class="line">1  //1表示旋转，非ssd，0表示ssd</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">lsblk -d -o name,rota,size,label,uuid</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">$sudo smartctl -a /dev/sdm | grep &quot;Rotation Rate&quot;</span><br><span class="line">Rotation Rate:    7200 rpm //机械盘</span><br><span class="line"></span><br><span class="line">[shuguang-35E@c27c02021.cloud.c02.amtest35 /apsarapangu/disk10]</span><br><span class="line">$sudo smartctl -a /dev/sdn | grep &quot;Rotation Rate&quot;</span><br><span class="line">Rotation Rate:    Solid State Device //ssd</span><br></pre></td></tr></table></figure>

<h2 id="fio测试"><a href="#fio测试" class="headerlink" title="fio测试"></a>fio测试</h2><p>以下是两块测试的SSD磁盘测试前的基本情况</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">/dev/sda	240.06G  SSD_SATA  //sata</span><br><span class="line">/dev/sfd0n1	3200G	 SSD_PCIE  //PCIE</span><br><span class="line"></span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/sda3        49G   29G   18G  63% / </span><br><span class="line">/dev/sfdv0n1p1  2.0T  803G  1.3T  40% /data</span><br><span class="line"></span><br><span class="line"># cat /sys/block/sda/queue/rotational </span><br><span class="line">0</span><br><span class="line"># cat /sys/block/sfdv0n1/queue/rotational </span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">#测试前的iostat状态</span><br><span class="line"># iostat -d sfdv0n1 sda3 1 -x</span><br><span class="line">Linux 3.10.0-957.el7.x86_64 (nu4d01142.sqa.nu8) 	2021年02月23日 	_x86_64_	(104 CPU)</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda3              0.00    10.67    1.24   18.78     7.82   220.69    22.83     0.03    1.64    1.39    1.66   0.08   0.17</span><br><span class="line">sfdv0n1           0.00     0.21    9.91  841.42   128.15  8237.10    19.65     0.93    0.04    0.25    0.04   1.05  89.52</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda3              0.00    15.00    0.00   17.00     0.00   136.00    16.00     0.03    2.00    0.00    2.00   1.29   2.20</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 11158.00     0.00 54448.00     9.76     1.03    0.02    0.00    0.02   0.09 100.00</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda3              0.00     5.00    0.00   18.00     0.00   104.00    11.56     0.01    0.61    0.00    0.61   0.61   1.10</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 10970.00     0.00 53216.00     9.70     1.02    0.03    0.00    0.03   0.09 100.10</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda3              0.00     0.00    0.00   24.00     0.00   100.00     8.33     0.01    0.58    0.00    0.58   0.08   0.20</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 11206.00     0.00 54476.00     9.72     1.03    0.03    0.00    0.03   0.09  99.90</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda3              0.00    14.00    0.00   21.00     0.00   148.00    14.10     0.01    0.48    0.00    0.48   0.33   0.70</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 10071.00     0.00 49028.00     9.74     1.02    0.03    0.00    0.03   0.10  99.80</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="NVMe-SSD测试数据"><a href="#NVMe-SSD测试数据" class="headerlink" title="NVMe SSD测试数据"></a>NVMe SSD测试数据</h3><p>对一块ssd进行如下测试(挂载在 &#x2F;data 目录 libaio 会导致测数据好几倍，可以去掉对比一下，去掉后更像 MySQL innodb 的场景 )</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file (1 file / 16384MiB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=63.8MiB/s][r=0,w=16.3k IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=258871: Tue Feb 23 14:12:23 2021</span><br><span class="line">  write: IOPS=18.9k, BW=74.0MiB/s (77.6MB/s)(4441MiB/60001msec)</span><br><span class="line">    slat (usec): min=4, max=6154, avg=48.82, stdev=56.38</span><br><span class="line">    clat (nsec): min=1049, max=12360k, avg=3326362.62, stdev=920683.43</span><br><span class="line">     lat (usec): min=68, max=12414, avg=3375.52, stdev=928.97</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[ 1483],  5.00th=[ 1811], 10.00th=[ 2114], 20.00th=[ 2376],</span><br><span class="line">     | 30.00th=[ 2704], 40.00th=[ 3130], 50.00th=[ 3523], 60.00th=[ 3785],</span><br><span class="line">     | 70.00th=[ 3949], 80.00th=[ 4080], 90.00th=[ 4293], 95.00th=[ 4490],</span><br><span class="line">     | 99.00th=[ 5604], 99.50th=[ 5997], 99.90th=[ 7111], 99.95th=[ 7832],</span><br><span class="line">     | 99.99th=[ 9634]</span><br><span class="line">   bw (  KiB/s): min=61024, max=118256, per=99.98%, avg=75779.58, stdev=12747.95, samples=120</span><br><span class="line">   iops        : min=15256, max=29564, avg=18944.88, stdev=3186.97, samples=120</span><br><span class="line">  lat (usec)   : 2=0.01%, 100=0.01%, 250=0.01%, 500=0.01%, 750=0.02%</span><br><span class="line">  lat (usec)   : 1000=0.06%</span><br><span class="line">  lat (msec)   : 2=7.40%, 4=66.19%, 10=26.32%, 20=0.01%</span><br><span class="line">  cpu          : usr=5.23%, sys=46.71%, ctx=846953, majf=0, minf=6</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=0,1136905,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=74.0MiB/s (77.6MB/s), 74.0MiB/s-74.0MiB/s (77.6MB/s-77.6MB/s), io=4441MiB (4657MB), run=60001-60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sfdv0n1: ios=0/1821771, merge=0/7335, ticks=0/39708, in_queue=78295, util=100.00%</span><br></pre></td></tr></table></figure>

<p>如上测试iops为：18944，测试期间的iostat，测试中一直有mysql在导入数据，所以测试开始前util就已经100%了，并且w&#x2F;s到了13K左右</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># iostat -d sfdv0n1 3 -x</span><br><span class="line">Linux 3.10.0-957.el7.x86_64 (nu4d01142.sqa.nu8) 	2021年02月23日 	_x86_64_	(104 CPU)</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.18    3.45  769.17   102.83  7885.16    20.68     0.93    0.04    0.26    0.04   1.16  89.46</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 13168.67     0.00 66244.00    10.06     1.05    0.03    0.00    0.03   0.08 100.10</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 12822.67     0.00 65542.67    10.22     1.04    0.02    0.00    0.02   0.08 100.07</span><br><span class="line"></span><br><span class="line">//增加压力</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 27348.33     0.00 214928.00    15.72     1.27    0.02    0.00    0.02   0.04 100.17</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     1.00    0.00 32661.67     0.00 271660.00    16.63     1.32    0.02    0.00    0.02   0.03 100.37</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 31645.00     0.00 265988.00    16.81     1.33    0.02    0.00    0.02   0.03 100.37</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00   574.00    0.00 31961.67     0.00 271094.67    16.96     1.36    0.02    0.00    0.02   0.03 100.13</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sfdv0n1           0.00     0.00    0.00 27656.33     0.00 224586.67    16.24     1.28    0.02    0.00    0.02   0.04 100.37</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>从iostat看出，测试开始前util已经100%（因为ssd，util失去参考意义），w&#x2F;s 13K左右，压力跑起来后w&#x2F;s能到30K，svctm、await均保持稳定</p>
<p>如下测试中direct&#x3D;1和direct&#x3D;0的write avg iops分别为42K、16K</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=16G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=507MiB/s,w=216MiB/s][r=130k,w=55.2k IOPS][eta 00m:00s] </span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=415921: Tue Feb 23 14:34:33 2021</span><br><span class="line">   read: IOPS=99.8k, BW=390MiB/s (409MB/s)(11.2GiB/29432msec)</span><br><span class="line">    slat (nsec): min=1043, max=917837, avg=4273.86, stdev=3792.17</span><br><span class="line">    clat (usec): min=2, max=4313, avg=459.80, stdev=239.61</span><br><span class="line">     lat (usec): min=4, max=4328, avg=464.16, stdev=241.81</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  251],  5.00th=[  277], 10.00th=[  289], 20.00th=[  310],</span><br><span class="line">     | 30.00th=[  326], 40.00th=[  343], 50.00th=[  363], 60.00th=[  400],</span><br><span class="line">     | 70.00th=[  502], 80.00th=[  603], 90.00th=[  750], 95.00th=[  881],</span><br><span class="line">     | 99.00th=[ 1172], 99.50th=[ 1401], 99.90th=[ 3032], 99.95th=[ 3359],</span><br><span class="line">     | 99.99th=[ 3785]</span><br><span class="line">   bw (  KiB/s): min=182520, max=574856, per=99.24%, avg=395975.64, stdev=119541.78, samples=58</span><br><span class="line">   iops        : min=45630, max=143714, avg=98993.90, stdev=29885.42, samples=58</span><br><span class="line">  write: IOPS=42.8k, BW=167MiB/s (175MB/s)(4915MiB/29432msec)</span><br><span class="line">    slat (usec): min=3, max=263, avg= 9.34, stdev= 4.35</span><br><span class="line">    clat (usec): min=14, max=2057, avg=402.26, stdev=140.67</span><br><span class="line">     lat (usec): min=19, max=2070, avg=411.72, stdev=142.67</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  237],  5.00th=[  281], 10.00th=[  293], 20.00th=[  314],</span><br><span class="line">     | 30.00th=[  330], 40.00th=[  343], 50.00th=[  359], 60.00th=[  379],</span><br><span class="line">     | 70.00th=[  404], 80.00th=[  457], 90.00th=[  586], 95.00th=[  717],</span><br><span class="line">     | 99.00th=[  930], 99.50th=[ 1004], 99.90th=[ 1254], 99.95th=[ 1385],</span><br><span class="line">     | 99.99th=[ 1532]</span><br><span class="line">   bw (  KiB/s): min=78104, max=244408, per=99.22%, avg=169671.52, stdev=51142.10, samples=58</span><br><span class="line">   iops        : min=19526, max=61102, avg=42417.86, stdev=12785.51, samples=58</span><br><span class="line">  lat (usec)   : 4=0.01%, 10=0.01%, 20=0.01%, 50=0.02%, 100=0.04%</span><br><span class="line">  lat (usec)   : 250=1.02%, 500=73.32%, 750=17.28%, 1000=6.30%</span><br><span class="line">  lat (msec)   : 2=1.83%, 4=0.19%, 10=0.01%</span><br><span class="line">  cpu          : usr=15.84%, sys=83.31%, ctx=13765, majf=0, minf=7</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=2936000,1258304,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=390MiB/s (409MB/s), 390MiB/s-390MiB/s (409MB/s-409MB/s), io=11.2GiB (12.0GB), run=29432-29432msec</span><br><span class="line">  WRITE: bw=167MiB/s (175MB/s), 167MiB/s-167MiB/s (175MB/s-175MB/s), io=4915MiB (5154MB), run=29432-29432msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sfdv0n1: ios=795793/1618341, merge=0/11, ticks=218710/27721, in_queue=264935, util=100.00%</span><br><span class="line">[root@nu4d01142 data]# </span><br><span class="line">[root@nu4d01142 data]# fio -ioengine=libaio -bs=4k -direct=0 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=6G -filename=/data/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=124MiB/s,w=53.5MiB/s][r=31.7k,w=13.7k IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=437523: Tue Feb 23 14:37:54 2021</span><br><span class="line">   read: IOPS=38.6k, BW=151MiB/s (158MB/s)(4300MiB/28550msec)</span><br><span class="line">    slat (nsec): min=1205, max=1826.7k, avg=13253.36, stdev=17173.87</span><br><span class="line">    clat (nsec): min=236, max=5816.8k, avg=1135969.25, stdev=337142.34</span><br><span class="line">     lat (nsec): min=1977, max=5831.2k, avg=1149404.84, stdev=341232.87</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  461],  5.00th=[  627], 10.00th=[  717], 20.00th=[  840],</span><br><span class="line">     | 30.00th=[  938], 40.00th=[ 1029], 50.00th=[ 1123], 60.00th=[ 1221],</span><br><span class="line">     | 70.00th=[ 1319], 80.00th=[ 1434], 90.00th=[ 1565], 95.00th=[ 1680],</span><br><span class="line">     | 99.00th=[ 1893], 99.50th=[ 1975], 99.90th=[ 2671], 99.95th=[ 3261],</span><br><span class="line">     | 99.99th=[ 3851]</span><br><span class="line">   bw (  KiB/s): min=119304, max=216648, per=100.00%, avg=154273.07, stdev=29925.10, samples=57</span><br><span class="line">   iops        : min=29826, max=54162, avg=38568.25, stdev=7481.30, samples=57</span><br><span class="line">  write: IOPS=16.5k, BW=64.6MiB/s (67.7MB/s)(1844MiB/28550msec)</span><br><span class="line">    slat (usec): min=3, max=3565, avg=21.07, stdev=22.23</span><br><span class="line">    clat (usec): min=14, max=9983, avg=1164.21, stdev=459.66</span><br><span class="line">     lat (usec): min=21, max=10011, avg=1185.57, stdev=463.28</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  498],  5.00th=[  619], 10.00th=[  709], 20.00th=[  832],</span><br><span class="line">     | 30.00th=[  930], 40.00th=[ 1020], 50.00th=[ 1123], 60.00th=[ 1237],</span><br><span class="line">     | 70.00th=[ 1336], 80.00th=[ 1450], 90.00th=[ 1598], 95.00th=[ 1713],</span><br><span class="line">     | 99.00th=[ 2311], 99.50th=[ 3851], 99.90th=[ 5932], 99.95th=[ 6456],</span><br><span class="line">     | 99.99th=[ 7701]</span><br><span class="line">   bw (  KiB/s): min=50800, max=92328, per=100.00%, avg=66128.47, stdev=12890.64, samples=57</span><br><span class="line">   iops        : min=12700, max=23082, avg=16532.07, stdev=3222.66, samples=57</span><br><span class="line">  lat (nsec)   : 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%</span><br><span class="line">  lat (usec)   : 2=0.01%, 4=0.01%, 10=0.01%, 20=0.02%, 50=0.03%</span><br><span class="line">  lat (usec)   : 100=0.04%, 250=0.18%, 500=1.01%, 750=11.05%, 1000=25.02%</span><br><span class="line">  lat (msec)   : 2=61.87%, 4=0.62%, 10=0.14%</span><br><span class="line">  cpu          : usr=10.87%, sys=61.98%, ctx=218415, majf=0, minf=7</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=1100924,471940,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=151MiB/s (158MB/s), 151MiB/s-151MiB/s (158MB/s-158MB/s), io=4300MiB (4509MB), run=28550-28550msec</span><br><span class="line">  WRITE: bw=64.6MiB/s (67.7MB/s), 64.6MiB/s-64.6MiB/s (67.7MB/s-67.7MB/s), io=1844MiB (1933MB), run=28550-28550msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sfdv0n1: ios=536103/822037, merge=0/1442, ticks=66507/17141, in_queue=99429, util=100.00%</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="SATA-SSD测试数据"><a href="#SATA-SSD测试数据" class="headerlink" title="SATA SSD测试数据"></a>SATA SSD测试数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># cat /sys/block/sda/queue/rotational </span><br><span class="line">0</span><br><span class="line"># lsblk -d -o name,rota</span><br><span class="line">NAME     ROTA</span><br><span class="line">sda         0</span><br><span class="line">sfdv0n1     0</span><br></pre></td></tr></table></figure>

<p>-direct&#x3D;0 -buffered&#x3D;0读写iops分别为15.8K、6.8K 比ssd差了不少（都是direct&#x3D;0），如果direct、buffered都是1的话，ESSD性能很差，读写iops分别为4312、1852</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line"># fio -ioengine=libaio -bs=4k -direct=0 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file (1 file / 2048MiB)</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=68.7MiB/s,w=29.7MiB/s][r=17.6k,w=7594 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=13261: Tue Feb 23 14:42:41 2021</span><br><span class="line">   read: IOPS=15.8k, BW=61.8MiB/s (64.8MB/s)(1432MiB/23172msec)</span><br><span class="line">    slat (nsec): min=1266, max=7261.0k, avg=7101.88, stdev=20655.54</span><br><span class="line">    clat (usec): min=167, max=27670, avg=2832.68, stdev=1786.18</span><br><span class="line">     lat (usec): min=175, max=27674, avg=2839.93, stdev=1784.42</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  437],  5.00th=[  668], 10.00th=[  873], 20.00th=[  988],</span><br><span class="line">     | 30.00th=[ 1401], 40.00th=[ 2442], 50.00th=[ 2835], 60.00th=[ 3195],</span><br><span class="line">     | 70.00th=[ 3523], 80.00th=[ 4047], 90.00th=[ 5014], 95.00th=[ 5866],</span><br><span class="line">     | 99.00th=[ 8160], 99.50th=[ 9372], 99.90th=[13829], 99.95th=[15008],</span><br><span class="line">     | 99.99th=[23725]</span><br><span class="line">   bw (  KiB/s): min=44183, max=149440, per=99.28%, avg=62836.17, stdev=26590.84, samples=46</span><br><span class="line">   iops        : min=11045, max=37360, avg=15709.02, stdev=6647.72, samples=46</span><br><span class="line">  write: IOPS=6803, BW=26.6MiB/s (27.9MB/s)(616MiB/23172msec)</span><br><span class="line">    slat (nsec): min=1566, max=11474k, avg=8460.17, stdev=38221.51</span><br><span class="line">    clat (usec): min=77, max=24047, avg=2789.68, stdev=2042.55</span><br><span class="line">     lat (usec): min=80, max=24054, avg=2798.29, stdev=2040.85</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  265],  5.00th=[  433], 10.00th=[  635], 20.00th=[  840],</span><br><span class="line">     | 30.00th=[  979], 40.00th=[ 2212], 50.00th=[ 2671], 60.00th=[ 3130],</span><br><span class="line">     | 70.00th=[ 3523], 80.00th=[ 4228], 90.00th=[ 5342], 95.00th=[ 6456],</span><br><span class="line">     | 99.00th=[ 9241], 99.50th=[10421], 99.90th=[13960], 99.95th=[15533],</span><br><span class="line">     | 99.99th=[23725]</span><br><span class="line">   bw (  KiB/s): min=18435, max=63112, per=99.26%, avg=27012.57, stdev=11299.42, samples=46</span><br><span class="line">   iops        : min= 4608, max=15778, avg=6753.11, stdev=2824.87, samples=46</span><br><span class="line">  lat (usec)   : 100=0.01%, 250=0.23%, 500=3.14%, 750=5.46%, 1000=15.27%</span><br><span class="line">  lat (msec)   : 2=11.47%, 4=43.09%, 10=20.88%, 20=0.44%, 50=0.01%</span><br><span class="line">  cpu          : usr=3.53%, sys=18.08%, ctx=47448, majf=0, minf=6</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=366638,157650,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=61.8MiB/s (64.8MB/s), 61.8MiB/s-61.8MiB/s (64.8MB/s-64.8MB/s), io=1432MiB (1502MB), run=23172-23172msec</span><br><span class="line">  WRITE: bw=26.6MiB/s (27.9MB/s), 26.6MiB/s-26.6MiB/s (27.9MB/s-27.9MB/s), io=616MiB (646MB), run=23172-23172msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sda: ios=359202/155123, merge=299/377, ticks=946305/407820, in_queue=1354596, util=99.61%</span><br><span class="line">  </span><br><span class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=0 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][95.5%][r=57.8MiB/s,w=25.7MiB/s][r=14.8k,w=6568 IOPS][eta 00m:01s] </span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=26167: Tue Feb 23 14:44:40 2021</span><br><span class="line">   read: IOPS=16.9k, BW=65.9MiB/s (69.1MB/s)(1432MiB/21730msec)</span><br><span class="line">    slat (nsec): min=1312, max=4454.2k, avg=8489.99, stdev=15763.97</span><br><span class="line">    clat (usec): min=201, max=18856, avg=2679.38, stdev=1720.02</span><br><span class="line">     lat (usec): min=206, max=18860, avg=2688.03, stdev=1717.19</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  635],  5.00th=[  832], 10.00th=[  914], 20.00th=[  971],</span><br><span class="line">     | 30.00th=[ 1090], 40.00th=[ 2114], 50.00th=[ 2704], 60.00th=[ 3064],</span><br><span class="line">     | 70.00th=[ 3392], 80.00th=[ 3851], 90.00th=[ 4817], 95.00th=[ 5735],</span><br><span class="line">     | 99.00th=[ 7767], 99.50th=[ 8979], 99.90th=[13698], 99.95th=[15139],</span><br><span class="line">     | 99.99th=[16581]</span><br><span class="line">   bw (  KiB/s): min=45168, max=127528, per=100.00%, avg=67625.19, stdev=26620.82, samples=43</span><br><span class="line">   iops        : min=11292, max=31882, avg=16906.28, stdev=6655.20, samples=43</span><br><span class="line">  write: IOPS=7254, BW=28.3MiB/s (29.7MB/s)(616MiB/21730msec)</span><br><span class="line">    slat (nsec): min=1749, max=3412.2k, avg=9816.22, stdev=14501.05</span><br><span class="line">    clat (usec): min=97, max=23473, avg=2556.02, stdev=1980.53</span><br><span class="line">     lat (usec): min=107, max=23477, avg=2566.01, stdev=1977.65</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  277],  5.00th=[  486], 10.00th=[  693], 20.00th=[  824],</span><br><span class="line">     | 30.00th=[  881], 40.00th=[ 1205], 50.00th=[ 2442], 60.00th=[ 2868],</span><br><span class="line">     | 70.00th=[ 3326], 80.00th=[ 3949], 90.00th=[ 5080], 95.00th=[ 6128],</span><br><span class="line">     | 99.00th=[ 8717], 99.50th=[10159], 99.90th=[14484], 99.95th=[15926],</span><br><span class="line">     | 99.99th=[18744]</span><br><span class="line">   bw (  KiB/s): min=19360, max=55040, per=100.00%, avg=29064.05, stdev=11373.59, samples=43</span><br><span class="line">   iops        : min= 4840, max=13760, avg=7266.00, stdev=2843.41, samples=43</span><br><span class="line">  lat (usec)   : 100=0.01%, 250=0.17%, 500=1.66%, 750=3.74%, 1000=22.57%</span><br><span class="line">  lat (msec)   : 2=12.66%, 4=40.62%, 10=18.20%, 20=0.38%, 50=0.01%</span><br><span class="line">  cpu          : usr=4.17%, sys=22.27%, ctx=14314, majf=0, minf=7</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=366638,157650,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=65.9MiB/s (69.1MB/s), 65.9MiB/s-65.9MiB/s (69.1MB/s-69.1MB/s), io=1432MiB (1502MB), run=21730-21730msec</span><br><span class="line">  WRITE: bw=28.3MiB/s (29.7MB/s), 28.3MiB/s-28.3MiB/s (29.7MB/s-29.7MB/s), io=616MiB (646MB), run=21730-21730msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sda: ios=364744/157621, merge=779/473, ticks=851759/352008, in_queue=1204024, util=99.61%</span><br><span class="line"></span><br><span class="line"># fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randrw -rwmixread=70 -size=2G -filename=/var/lib/docker/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60 </span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=15.9MiB/s,w=7308KiB/s][r=4081,w=1827 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=31560: Tue Feb 23 14:46:10 2021</span><br><span class="line">   read: IOPS=4312, BW=16.8MiB/s (17.7MB/s)(1011MiB/60001msec)</span><br><span class="line">    slat (usec): min=63, max=14320, avg=216.76, stdev=430.61</span><br><span class="line">    clat (usec): min=5, max=778861, avg=10254.92, stdev=22345.40</span><br><span class="line">     lat (usec): min=1900, max=782277, avg=10472.16, stdev=22657.06</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    6],  5.00th=[    6], 10.00th=[    6], 20.00th=[    7],</span><br><span class="line">     | 30.00th=[    7], 40.00th=[    7], 50.00th=[    7], 60.00th=[    7],</span><br><span class="line">     | 70.00th=[    8], 80.00th=[    8], 90.00th=[    8], 95.00th=[   11],</span><br><span class="line">     | 99.00th=[  107], 99.50th=[  113], 99.90th=[  132], 99.95th=[  197],</span><br><span class="line">     | 99.99th=[  760]</span><br><span class="line">   bw (  KiB/s): min=  168, max=29784, per=100.00%, avg=17390.92, stdev=10932.90, samples=119</span><br><span class="line">   iops        : min=   42, max= 7446, avg=4347.71, stdev=2733.21, samples=119</span><br><span class="line">  write: IOPS=1852, BW=7410KiB/s (7588kB/s)(434MiB/60001msec)</span><br><span class="line">    slat (usec): min=3, max=666432, avg=23.59, stdev=2745.39</span><br><span class="line">    clat (msec): min=3, max=781, avg=10.14, stdev=20.50</span><br><span class="line">     lat (msec): min=3, max=781, avg=10.16, stdev=20.72</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    6],  5.00th=[    6], 10.00th=[    6], 20.00th=[    7],</span><br><span class="line">     | 30.00th=[    7], 40.00th=[    7], 50.00th=[    7], 60.00th=[    7],</span><br><span class="line">     | 70.00th=[    7], 80.00th=[    8], 90.00th=[    8], 95.00th=[   11],</span><br><span class="line">     | 99.00th=[  107], 99.50th=[  113], 99.90th=[  131], 99.95th=[  157],</span><br><span class="line">     | 99.99th=[  760]</span><br><span class="line">   bw (  KiB/s): min=   80, max=12328, per=100.00%, avg=7469.53, stdev=4696.69, samples=119</span><br><span class="line">   iops        : min=   20, max= 3082, avg=1867.34, stdev=1174.19, samples=119</span><br><span class="line">  lat (usec)   : 10=0.01%</span><br><span class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=94.64%, 20=1.78%, 50=0.11%</span><br><span class="line">  lat (msec)   : 100=1.80%, 250=1.63%, 500=0.01%, 750=0.02%, 1000=0.01%</span><br><span class="line">  cpu          : usr=2.51%, sys=10.98%, ctx=260210, majf=0, minf=7</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=258768,111147,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=16.8MiB/s (17.7MB/s), 16.8MiB/s-16.8MiB/s (17.7MB/s-17.7MB/s), io=1011MiB (1060MB), run=60001-60001msec</span><br><span class="line">  WRITE: bw=7410KiB/s (7588kB/s), 7410KiB/s-7410KiB/s (7588kB/s-7588kB/s), io=434MiB (455MB), run=60001-60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sda: ios=258717/89376, merge=0/735, ticks=52540/564186, in_queue=616999, util=90.07%</span><br></pre></td></tr></table></figure>

<h3 id="ESSD磁盘测试数据"><a href="#ESSD磁盘测试数据" class="headerlink" title="ESSD磁盘测试数据"></a>ESSD磁盘测试数据</h3><p>这是一块虚拟的阿里云网络盘，不能算完整意义的SSD（承诺IOPS 4200），数据仅供参考，磁盘概况：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$df -lh</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda1        99G   30G   65G  32% /</span><br><span class="line"></span><br><span class="line">$cat /sys/block/vda/queue/rotational</span><br><span class="line">1</span><br></pre></td></tr></table></figure>

<p>测试数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line">$fio -ioengine=libaio -bs=4k -direct=1 -buffered=1  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=10.8MiB/s,w=11.2MiB/s][r=2757,w=2876 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25641: Tue Feb 23 16:35:19 2021</span><br><span class="line">   read: IOPS=2136, BW=8545KiB/s (8750kB/s)(501MiB/60001msec)</span><br><span class="line">    slat (usec): min=190, max=830992, avg=457.20, stdev=3088.80</span><br><span class="line">    clat (nsec): min=1792, max=1721.3M, avg=14657528.60, stdev=63188988.75</span><br><span class="line">     lat (usec): min=344, max=1751.1k, avg=15115.20, stdev=65165.80</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</span><br><span class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   17], 99.50th=[   53], 99.90th=[ 1028], 99.95th=[ 1167],</span><br><span class="line">     | 99.99th=[ 1653]</span><br><span class="line">   bw (  KiB/s): min=   56, max=12648, per=100.00%, avg=8598.92, stdev=5289.40, samples=118</span><br><span class="line">   iops        : min=   14, max= 3162, avg=2149.73, stdev=1322.35, samples=118</span><br><span class="line">  write: IOPS=2137, BW=8548KiB/s (8753kB/s)(501MiB/60001msec)</span><br><span class="line">    slat (usec): min=2, max=181, avg= 6.67, stdev= 7.22</span><br><span class="line">    clat (usec): min=628, max=1721.1k, avg=14825.32, stdev=65017.66</span><br><span class="line">     lat (usec): min=636, max=1721.1k, avg=14832.10, stdev=65018.10</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</span><br><span class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   17], 99.50th=[   53], 99.90th=[ 1045], 99.95th=[ 1200],</span><br><span class="line">     | 99.99th=[ 1687]</span><br><span class="line">   bw (  KiB/s): min=   72, max=13304, per=100.00%, avg=8602.99, stdev=5296.31, samples=118</span><br><span class="line">   iops        : min=   18, max= 3326, avg=2150.75, stdev=1324.08, samples=118</span><br><span class="line">  lat (usec)   : 2=0.01%, 500=0.01%, 750=0.01%</span><br><span class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=37.85%, 20=61.53%, 50=0.10%</span><br><span class="line">  lat (msec)   : 100=0.06%, 250=0.03%, 500=0.01%, 750=0.03%, 1000=0.25%</span><br><span class="line">  lat (msec)   : 2000=0.14%</span><br><span class="line">  cpu          : usr=0.70%, sys=4.01%, ctx=135029, majf=0, minf=4</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=128180,128223,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=8545KiB/s (8750kB/s), 8545KiB/s-8545KiB/s (8750kB/s-8750kB/s), io=501MiB (525MB), run=60001-60001msec</span><br><span class="line">  WRITE: bw=8548KiB/s (8753kB/s), 8548KiB/s-8548KiB/s (8753kB/s-8753kB/s), io=501MiB (525MB), run=60001-60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vda: ios=127922/87337, merge=0/237, ticks=55122/4269885, in_queue=2209125, util=94.29%</span><br><span class="line"></span><br><span class="line">$fio -ioengine=libaio -bs=4k -direct=1 -buffered=0  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=9680KiB/s,w=9712KiB/s][r=2420,w=2428 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25375: Tue Feb 23 16:33:03 2021</span><br><span class="line">   read: IOPS=2462, BW=9849KiB/s (10.1MB/s)(577MiB/60011msec)</span><br><span class="line">    slat (nsec): min=1558, max=10663k, avg=5900.28, stdev=46286.64</span><br><span class="line">    clat (usec): min=290, max=93493, avg=13054.57, stdev=4301.89</span><br><span class="line">     lat (usec): min=332, max=93497, avg=13060.60, stdev=4301.68</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[ 1844],  5.00th=[10159], 10.00th=[10290], 20.00th=[10421],</span><br><span class="line">     | 30.00th=[10552], 40.00th=[10552], 50.00th=[10683], 60.00th=[10814],</span><br><span class="line">     | 70.00th=[18482], 80.00th=[19006], 90.00th=[19006], 95.00th=[19268],</span><br><span class="line">     | 99.00th=[19530], 99.50th=[19792], 99.90th=[29492], 99.95th=[30278],</span><br><span class="line">     | 99.99th=[43779]</span><br><span class="line">   bw (  KiB/s): min= 9128, max=30392, per=100.00%, avg=9850.12, stdev=1902.00, samples=120</span><br><span class="line">   iops        : min= 2282, max= 7598, avg=2462.52, stdev=475.50, samples=120</span><br><span class="line">  write: IOPS=2465, BW=9864KiB/s (10.1MB/s)(578MiB/60011msec)</span><br><span class="line">    slat (usec): min=2, max=10586, avg= 6.92, stdev=67.34</span><br><span class="line">    clat (usec): min=240, max=69922, avg=12902.33, stdev=4307.92</span><br><span class="line">     lat (usec): min=244, max=69927, avg=12909.37, stdev=4307.03</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[ 1729],  5.00th=[10159], 10.00th=[10290], 20.00th=[10290],</span><br><span class="line">     | 30.00th=[10421], 40.00th=[10421], 50.00th=[10552], 60.00th=[10683],</span><br><span class="line">     | 70.00th=[18220], 80.00th=[18744], 90.00th=[19006], 95.00th=[19006],</span><br><span class="line">     | 99.00th=[19268], 99.50th=[19530], 99.90th=[21103], 99.95th=[35390],</span><br><span class="line">     | 99.99th=[50594]</span><br><span class="line">   bw (  KiB/s): min= 8496, max=31352, per=100.00%, avg=9862.92, stdev=1991.48, samples=120</span><br><span class="line">   iops        : min= 2124, max= 7838, avg=2465.72, stdev=497.87, samples=120</span><br><span class="line">  lat (usec)   : 250=0.01%, 500=0.03%, 750=0.02%, 1000=0.02%</span><br><span class="line">  lat (msec)   : 2=1.70%, 4=0.41%, 10=1.25%, 20=96.22%, 50=0.34%</span><br><span class="line">  lat (msec)   : 100=0.01%</span><br><span class="line">  cpu          : usr=0.89%, sys=4.09%, ctx=206337, majf=0, minf=4</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=147768,147981,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=9849KiB/s (10.1MB/s), 9849KiB/s-9849KiB/s (10.1MB/s-10.1MB/s), io=577MiB (605MB), run=60011-60011msec</span><br><span class="line">  WRITE: bw=9864KiB/s (10.1MB/s), 9864KiB/s-9864KiB/s (10.1MB/s-10.1MB/s), io=578MiB (606MB), run=60011-60011msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vda: ios=147515/148154, merge=0/231, ticks=1922378/1915751, in_queue=3780605, util=98.46%</span><br><span class="line">  </span><br><span class="line">$fio -ioengine=libaio -bs=4k -direct=0 -buffered=1  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=132KiB/s,w=148KiB/s][r=33,w=37 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=25892: Tue Feb 23 16:37:41 2021</span><br><span class="line">   read: IOPS=1987, BW=7949KiB/s (8140kB/s)(467MiB/60150msec)</span><br><span class="line">    slat (usec): min=192, max=599873, avg=479.26, stdev=2917.52</span><br><span class="line">    clat (usec): min=15, max=1975.6k, avg=16004.22, stdev=76024.60</span><br><span class="line">     lat (msec): min=5, max=2005, avg=16.48, stdev=78.00</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</span><br><span class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   19], 99.50th=[  317], 99.90th=[ 1133], 99.95th=[ 1435],</span><br><span class="line">     | 99.99th=[ 1871]</span><br><span class="line">   bw (  KiB/s): min=   32, max=12672, per=100.00%, avg=8034.08, stdev=5399.63, samples=119</span><br><span class="line">   iops        : min=    8, max= 3168, avg=2008.52, stdev=1349.91, samples=119</span><br><span class="line">  write: IOPS=1984, BW=7937KiB/s (8127kB/s)(466MiB/60150msec)</span><br><span class="line">    slat (usec): min=2, max=839634, avg=18.39, stdev=2747.10</span><br><span class="line">    clat (msec): min=5, max=1975, avg=15.64, stdev=73.06</span><br><span class="line">     lat (msec): min=5, max=1975, avg=15.66, stdev=73.28</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    8],  5.00th=[    9], 10.00th=[    9], 20.00th=[   10],</span><br><span class="line">     | 30.00th=[   10], 40.00th=[   11], 50.00th=[   11], 60.00th=[   11],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   18], 99.50th=[  153], 99.90th=[ 1116], 99.95th=[ 1435],</span><br><span class="line">     | 99.99th=[ 1921]</span><br><span class="line">   bw (  KiB/s): min=   24, max=13160, per=100.00%, avg=8021.18, stdev=5405.12, samples=119</span><br><span class="line">   iops        : min=    6, max= 3290, avg=2005.29, stdev=1351.28, samples=119</span><br><span class="line">  lat (usec)   : 20=0.01%</span><br><span class="line">  lat (msec)   : 10=36.51%, 20=62.63%, 50=0.21%, 100=0.12%, 250=0.05%</span><br><span class="line">  lat (msec)   : 500=0.02%, 750=0.02%, 1000=0.19%, 2000=0.26%</span><br><span class="line">  cpu          : usr=0.62%, sys=4.04%, ctx=125974, majf=0, minf=3</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=119533,119347,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=7949KiB/s (8140kB/s), 7949KiB/s-7949KiB/s (8140kB/s-8140kB/s), io=467MiB (490MB), run=60150-60150msec</span><br><span class="line">  WRITE: bw=7937KiB/s (8127kB/s), 7937KiB/s-7937KiB/s (8127kB/s-8127kB/s), io=466MiB (489MB), run=60150-60150msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vda: ios=119533/108186, merge=0/214, ticks=54093/4937255, in_queue=2525052, util=93.99%</span><br><span class="line">  </span><br><span class="line">$fio -ioengine=libaio -bs=4k -direct=0 -buffered=0  -thread -rw=randrw  -size=4G -filename=/home/admin/fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.1</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=9644KiB/s,w=9792KiB/s][r=2411,w=2448 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=26139: Tue Feb 23 16:39:43 2021</span><br><span class="line">   read: IOPS=2455, BW=9823KiB/s (10.1MB/s)(576MiB/60015msec)</span><br><span class="line">    slat (nsec): min=1619, max=18282k, avg=5882.81, stdev=71214.52</span><br><span class="line">    clat (usec): min=281, max=64630, avg=13055.68, stdev=4233.17</span><br><span class="line">     lat (usec): min=323, max=64636, avg=13061.69, stdev=4232.79</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[ 2040],  5.00th=[10290], 10.00th=[10421], 20.00th=[10421],</span><br><span class="line">     | 30.00th=[10552], 40.00th=[10552], 50.00th=[10683], 60.00th=[10814],</span><br><span class="line">     | 70.00th=[18220], 80.00th=[19006], 90.00th=[19006], 95.00th=[19268],</span><br><span class="line">     | 99.00th=[19530], 99.50th=[20055], 99.90th=[28967], 99.95th=[29754],</span><br><span class="line">     | 99.99th=[30540]</span><br><span class="line">   bw (  KiB/s): min= 8776, max=27648, per=100.00%, avg=9824.29, stdev=1655.78, samples=120</span><br><span class="line">   iops        : min= 2194, max= 6912, avg=2456.05, stdev=413.95, samples=120</span><br><span class="line">  write: IOPS=2458, BW=9835KiB/s (10.1MB/s)(576MiB/60015msec)</span><br><span class="line">    slat (usec): min=2, max=10681, avg= 6.79, stdev=71.30</span><br><span class="line">    clat (usec): min=221, max=70411, avg=12909.50, stdev=4312.40</span><br><span class="line">     lat (usec): min=225, max=70414, avg=12916.40, stdev=4312.05</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[ 1909],  5.00th=[10159], 10.00th=[10290], 20.00th=[10290],</span><br><span class="line">     | 30.00th=[10421], 40.00th=[10421], 50.00th=[10552], 60.00th=[10683],</span><br><span class="line">     | 70.00th=[18220], 80.00th=[18744], 90.00th=[19006], 95.00th=[19006],</span><br><span class="line">     | 99.00th=[19268], 99.50th=[19530], 99.90th=[28705], 99.95th=[40109],</span><br><span class="line">     | 99.99th=[60031]</span><br><span class="line">   bw (  KiB/s): min= 8568, max=28544, per=100.00%, avg=9836.03, stdev=1737.29, samples=120</span><br><span class="line">   iops        : min= 2142, max= 7136, avg=2458.98, stdev=434.32, samples=120</span><br><span class="line">  lat (usec)   : 250=0.01%, 500=0.03%, 750=0.02%, 1000=0.02%</span><br><span class="line">  lat (msec)   : 2=1.03%, 4=1.10%, 10=0.98%, 20=96.43%, 50=0.38%</span><br><span class="line">  lat (msec)   : 100=0.01%</span><br><span class="line">  cpu          : usr=0.82%, sys=4.32%, ctx=212008, majf=0, minf=4</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwt: total=147386,147564,0, short=0,0,0, dropped=0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=9823KiB/s (10.1MB/s), 9823KiB/s-9823KiB/s (10.1MB/s-10.1MB/s), io=576MiB (604MB), run=60015-60015msec</span><br><span class="line">  WRITE: bw=9835KiB/s (10.1MB/s), 9835KiB/s-9835KiB/s (10.1MB/s-10.1MB/s), io=576MiB (604MB), run=60015-60015msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vda: ios=147097/147865, merge=0/241, ticks=1916703/1915836, in_queue=3791443, util=98.68%</span><br></pre></td></tr></table></figure>

<p>各类型云盘的性能比较如下表所示。</p>
<table>
<thead>
<tr>
<th align="left">性能类别</th>
<th align="left">ESSD AutoPL云盘（邀测）</th>
<th align="left">ESSD PL-X云盘（邀测）</th>
<th align="left">ESSD云盘 PL3</th>
<th align="left">ESSD云盘 PL0</th>
<th align="left">ESSD云盘 PL1</th>
<th align="left">ESSD云盘 PL0</th>
<th>SSD云盘</th>
<th>高效云盘</th>
<th>普通云盘</th>
</tr>
</thead>
<tbody><tr>
<td align="left">单盘容量范围（GiB）</td>
<td align="left">40~32,768</td>
<td align="left">40~32,768</td>
<td align="left">1261~32,768</td>
<td align="left">461~32,768</td>
<td align="left">20~32,768</td>
<td align="left">40~32,768</td>
<td>20~32,768</td>
<td>20~32,768</td>
<td>5~2,000</td>
</tr>
<tr>
<td align="left">最大IOPS</td>
<td align="left">100,000</td>
<td align="left">3,000,000</td>
<td align="left">1,000,000</td>
<td align="left">100,000</td>
<td align="left">50,000</td>
<td align="left">10,000</td>
<td>25,000</td>
<td>5,000</td>
<td>数百</td>
</tr>
<tr>
<td align="left">最大吞吐量（MB&#x2F;s）</td>
<td align="left">1,131</td>
<td align="left">12,288</td>
<td align="left">4,000</td>
<td align="left">750</td>
<td align="left">350</td>
<td align="left">180</td>
<td>300</td>
<td>140</td>
<td>30~40</td>
</tr>
<tr>
<td align="left">单盘IOPS性能计算公式</td>
<td align="left">min{1,800+50*容量, 50,000}</td>
<td align="left">预配置IOPS</td>
<td align="left">min{1,800+50*容量, 1,000,000}</td>
<td align="left">min{1,800+50*容量, 100,000}</td>
<td align="left">min{1,800+50*容量, 50,000}</td>
<td align="left">min{ 1,800+12*容量, 10,000 }</td>
<td>min{1,800+30*容量, 25,000}</td>
<td>min{1,800+8*容量, 5,000}</td>
<td>无</td>
</tr>
<tr>
<td align="left">单盘吞吐量性能计算公式（MB&#x2F;s）</td>
<td align="left">min{120+0.5*容量, 350}</td>
<td align="left">4 KB*预配置IOPS&#x2F;1024</td>
<td align="left">min{120+0.5*容量, 4,000}</td>
<td align="left">min{120+0.5*容量, 750}</td>
<td align="left">min{120+0.5*容量, 350}</td>
<td align="left">min{100+0.25*容量, 180}</td>
<td>min{120+0.5*容量, 300}</td>
<td>min{100+0.15*容量, 140}</td>
<td>无</td>
</tr>
<tr>
<td align="left">单路随机写平均时延（ms），Block Size&#x3D;4K</td>
<td align="left">0.2</td>
<td align="left">0.03</td>
<td align="left">0.2</td>
<td align="left">0.2</td>
<td align="left">0.2</td>
<td align="left">0.3~0.5</td>
<td>0.5~2</td>
<td>1~3</td>
<td>5~10</td>
</tr>
<tr>
<td align="left">API参数取值</td>
<td align="left">cloud_auto</td>
<td align="left">cloud_plx</td>
<td align="left">cloud_essd</td>
<td align="left">cloud_essd</td>
<td align="left">cloud_essd</td>
<td align="left">cloud_essd</td>
<td>cloud_ssd</td>
<td>cloud_efficiency</td>
<td>cloud</td>
</tr>
</tbody></table>
<h4 id="ESSD-PL3-测试"><a href="#ESSD-PL3-测试" class="headerlink" title="ESSD(PL3) 测试"></a>ESSD(PL3) 测试</h4><blockquote>
<p>阿里云ESSD（Enhanced SSD）云盘结合25 GE网络和RDMA技术，为您提供单盘高达100万的随机读写能力和单路低时延性能。本文介绍了ESSD云盘的性能级别、适用场景及性能上限，提供了选择不同ESSD云盘性能级别时的参考信息。</p>
</blockquote>
<p>测试结论：读能力非常差(不到写的10%)，写能力能符合官方标称的IOPS，但是写IOPS抖动极大，会长时间IOPS 跌0，但最终IOPS还是会达到目标IOPS。</p>
<p>测试命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -ioengine=libaio -bs=4k -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=160G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br></pre></td></tr></table></figure>

<p>ESSD 是aliyun 购买的 ESSD PL3，LVM是海光物理机下两块本地NVMe SSD做的LVM，测试基于ext4文件系统，阿里云官方提供ESSD的 IOPS 性能数据是裸盘（不含文件系统的）</p>
<table>
<thead>
<tr>
<th></th>
<th>本地LVM</th>
<th>ESSD PL3</th>
<th>PL2+倚天</th>
</tr>
</thead>
<tbody><tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -buffered&#x3D;1 read</td>
<td>bw&#x3D;36636KB&#x2F;s, iops&#x3D;9159<br/>nvme0n1:util&#x3D;42.31%<br/>nvme1n1: util&#x3D;41.63%</td>
<td>IOPS&#x3D;3647, BW&#x3D;14.2MiB&#x2F;s<br/>util&#x3D;88.08%</td>
<td>IOPS&#x3D;458k, BW&#x3D;1789MiB&#x2F;s<br/>util&#x3D;96.69%</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -buffered&#x3D;1 randwrite</td>
<td>bw&#x3D;383626KB&#x2F;s, iops&#x3D;95906<br/>nvme0n1:util&#x3D;37.16%<br/>nvme1n1: util&#x3D;33.58%</td>
<td>IOPS&#x3D;104k, BW&#x3D;406MiB&#x2F;s<br/>util&#x3D;39.06%</td>
<td>IOPS&#x3D;37.4k, BW&#x3D;146MiB&#x2F;s<br/>util&#x3D;94.03%</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -buffered&#x3D;1 randrw rwmixread&#x3D;70</td>
<td>write: bw&#x3D;12765KB&#x2F;s, iops&#x3D;3191<br/>read : bw&#x3D;29766KB&#x2F;s, iops&#x3D;7441<br/>nvme0n1:util&#x3D;35.18%<br/>nvme1n1: util&#x3D;35.04%</td>
<td>write:IOPS&#x3D;1701, BW&#x3D;6808KiB&#x2F;s<br/>read: IOPS&#x3D;3962, BW&#x3D;15.5MiB&#x2F;s<br/> nvme7n1: util&#x3D;99.35%</td>
<td>write:IOPS&#x3D;1826, BW&#x3D;7306KiB&#x2F;s<br/>read:IOPS&#x3D;4254, BW&#x3D;16.6MiB&#x2F;s<br/>util&#x3D;98.99%</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0 read</td>
<td>bw&#x3D;67938KB&#x2F;s, iops&#x3D;16984<br/>nvme0n1:util&#x3D;43.17%<br/>nvme1n1: util&#x3D;39.18%</td>
<td>IOPS&#x3D;4687, BW&#x3D;18.3MiB&#x2F;s<br/>util&#x3D;99.75%</td>
<td>read: IOPS&#x3D;145k, BW&#x3D;565MiB&#x2F;s<br/>util&#x3D;98.88%</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0 write</td>
<td>bw&#x3D;160775KB&#x2F;s, iops&#x3D;40193<br/>nvme0n1:util&#x3D;28.66%<br/>nvme1n1: util&#x3D;21.67%</td>
<td>IOPS&#x3D;7153, BW&#x3D;27.9MiB&#x2F;s<br/>util&#x3D;99.85%</td>
<td>write: IOPS&#x3D;98.0k, BW&#x3D;387MiB&#x2F;s<br/>util&#x3D;99.88%</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0 randrw rwmixread&#x3D;70</td>
<td>write: bw&#x3D;23087KB&#x2F;s, iops&#x3D;5771<br/>read : bw&#x3D;53849KB&#x2F;s, iops&#x3D;13462</td>
<td>write:IOPS&#x3D;1511, BW&#x3D;6045KiB&#x2F;s<br/>read: IOPS&#x3D;3534, BW&#x3D;13.8MiB&#x2F;s</td>
<td>write: IOPS&#x3D;29.4k, BW&#x3D;115MiB&#x2F;s<br/>read: IOPS&#x3D;68.6k, BW&#x3D;268MiB&#x2F;s<br/>util&#x3D;99.88%</td>
</tr>
</tbody></table>
<p>结论：</p>
<ul>
<li>ESSD只要有随机读性能就很差,纯读是本地盘（LVM）的40%，纯写和本地盘差不多</li>
<li>direct 读是本地盘的四分之一</li>
<li>direct 写是本地盘的六分之一，写16K Page差距缩小到五分之一（5749&#x2F;25817）</li>
<li>intel direct 写本地intel SSDPE2KX040T8 iops&#x3D;55826（比海光好40%，海光是memblaze）</li>
<li>ESSD 带 buffer 读写抖动很大</li>
<li>ESSD 出现过多次卡死，表现就是磁盘不响应任何操作，大概N分钟后恢复，原因未知</li>
</ul>
<p>PL3单盘IOPS性能计算公式  min{1800+50*容量, 1000000}</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br></pre></td><td class="code"><pre><span class="line">[essd_pl3]# fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=160G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=566MiB/s][r=0,w=145k IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=2416234: Thu Apr  7 17:03:07 2022</span><br><span class="line">  write: IOPS=96.2k, BW=376MiB/s (394MB/s)(22.0GiB/60000msec)</span><br><span class="line">    slat (usec): min=2, max=530984, avg= 8.27, stdev=1104.96</span><br><span class="line">    clat (usec): min=2, max=944103, avg=599.25, stdev=9230.93</span><br><span class="line">     lat (usec): min=7, max=944111, avg=607.60, stdev=9308.81</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   392],  5.00th=[   400], 10.00th=[   404], 20.00th=[   408],</span><br><span class="line">     | 30.00th=[   412], 40.00th=[   416], 50.00th=[   420], 60.00th=[   424],</span><br><span class="line">     | 70.00th=[   433], 80.00th=[   441], 90.00th=[   457], 95.00th=[   482],</span><br><span class="line">     | 99.00th=[   627], 99.50th=[   766], 99.90th=[  1795], 99.95th=[  4228],</span><br><span class="line">     | 99.99th=[488637]</span><br><span class="line">   bw (  KiB/s): min=  168, max=609232, per=100.00%, avg=422254.17, stdev=257181.75, samples=108</span><br><span class="line">   iops        : min=   42, max=152308, avg=105563.63, stdev=64295.48, samples=108</span><br><span class="line">  lat (usec)   : 4=0.01%, 10=0.01%, 50=0.01%, 100=0.01%, 250=0.01%</span><br><span class="line">  lat (usec)   : 500=96.35%, 750=3.11%, 1000=0.26%</span><br><span class="line">  lat (msec)   : 2=0.19%, 4=0.03%, 10=0.02%, 250=0.01%, 500=0.03%</span><br><span class="line">  lat (msec)   : 750=0.01%, 1000=0.01%</span><br><span class="line">  cpu          : usr=13.56%, sys=60.78%, ctx=1455, majf=0, minf=9743</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=0,5771972,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=376MiB/s (394MB/s), 376MiB/s-376MiB/s (394MB/s-394MB/s), io=22.0GiB (23.6GB), run=60000-60000msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vdb: ios=0/1463799, merge=0/7373, ticks=0/2011879, in_queue=2011879, util=27.85%</span><br><span class="line">  </span><br><span class="line">[essd_pl3]# fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randread -rwmixread=70 -size=160G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randread, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [r(1)][100.0%][r=15.9MiB/s,w=0KiB/s][r=4058,w=0 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=2441598: Thu Apr  7 17:05:10 2022</span><br><span class="line">   read: IOPS=3647, BW=14.2MiB/s (14.9MB/s)(855MiB/60001msec)</span><br><span class="line">    slat (usec): min=183, max=10119, avg=239.01, stdev=110.20</span><br><span class="line">    clat (usec): min=2, max=54577, avg=15170.17, stdev=1324.10</span><br><span class="line">     lat (usec): min=237, max=55110, avg=15409.34, stdev=1338.09</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[13960],  5.00th=[14091], 10.00th=[14222], 20.00th=[14484],</span><br><span class="line">     | 30.00th=[14615], 40.00th=[14746], 50.00th=[14877], 60.00th=[15139],</span><br><span class="line">     | 70.00th=[15270], 80.00th=[15533], 90.00th=[16057], 95.00th=[16712],</span><br><span class="line">     | 99.00th=[20317], 99.50th=[22152], 99.90th=[26346], 99.95th=[30802],</span><br><span class="line">     | 99.99th=[52691]</span><br><span class="line">   bw (  KiB/s): min= 6000, max=17272, per=100.00%, avg=16511.28, stdev=1140.64, samples=105</span><br><span class="line">   iops        : min= 1500, max= 4318, avg=4127.81, stdev=285.16, samples=105</span><br><span class="line">  lat (usec)   : 4=0.01%, 250=0.01%, 500=0.01%, 750=0.01%, 1000=0.01%</span><br><span class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=0.01%, 20=98.91%, 50=1.05%</span><br><span class="line">  lat (msec)   : 100=0.02%</span><br><span class="line">  cpu          : usr=0.18%, sys=17.18%, ctx=219041, majf=0, minf=4215</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=218835,0,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=14.2MiB/s (14.9MB/s), 14.2MiB/s-14.2MiB/s (14.9MB/s-14.9MB/s), io=855MiB (896MB), run=60001-60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vdb: ios=218343/7992, merge=0/8876, ticks=50566/3749, in_queue=54315, util=88.08%  </span><br><span class="line"> </span><br><span class="line">[essd_pl3]# fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randrw -rwmixread=70 -size=160G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randrw, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=libaio, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [m(1)][100.0%][r=15.7MiB/s,w=7031KiB/s][r=4007,w=1757 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=2641414: Thu Apr  7 17:21:10 2022</span><br><span class="line">   read: IOPS=3962, BW=15.5MiB/s (16.2MB/s)(929MiB/60001msec)</span><br><span class="line">    slat (usec): min=182, max=7194, avg=243.23, stdev=116.87</span><br><span class="line">    clat (usec): min=2, max=235715, avg=11020.01, stdev=3366.61</span><br><span class="line">     lat (usec): min=253, max=235991, avg=11263.40, stdev=3375.49</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    9],  5.00th=[   10], 10.00th=[   10], 20.00th=[   11],</span><br><span class="line">     | 30.00th=[   11], 40.00th=[   11], 50.00th=[   11], 60.00th=[   12],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   16], 99.50th=[   18], 99.90th=[   31], 99.95th=[   36],</span><br><span class="line">     | 99.99th=[  234]</span><br><span class="line">   bw (  KiB/s): min=10808, max=17016, per=100.00%, avg=15977.89, stdev=895.35, samples=118</span><br><span class="line">   iops        : min= 2702, max= 4254, avg=3994.47, stdev=223.85, samples=118</span><br><span class="line">  write: IOPS=1701, BW=6808KiB/s (6971kB/s)(399MiB/60001msec)</span><br><span class="line">    slat (usec): min=3, max=221631, avg=10.16, stdev=693.59</span><br><span class="line">    clat (usec): min=486, max=235772, avg=11029.42, stdev=3590.93</span><br><span class="line">     lat (usec): min=493, max=235780, avg=11039.67, stdev=3659.04</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    9],  5.00th=[   10], 10.00th=[   10], 20.00th=[   11],</span><br><span class="line">     | 30.00th=[   11], 40.00th=[   11], 50.00th=[   11], 60.00th=[   12],</span><br><span class="line">     | 70.00th=[   12], 80.00th=[   12], 90.00th=[   13], 95.00th=[   14],</span><br><span class="line">     | 99.00th=[   16], 99.50th=[   18], 99.90th=[   31], 99.95th=[   37],</span><br><span class="line">     | 99.99th=[  234]</span><br><span class="line">   bw (  KiB/s): min= 4480, max= 7728, per=100.00%, avg=6862.60, stdev=475.79, samples=118</span><br><span class="line">   iops        : min= 1120, max= 1932, avg=1715.64, stdev=118.97, samples=118</span><br><span class="line">  lat (usec)   : 4=0.01%, 500=0.01%, 750=0.01%</span><br><span class="line">  lat (msec)   : 2=0.01%, 4=0.01%, 10=20.77%, 20=78.89%, 50=0.31%</span><br><span class="line">  lat (msec)   : 100=0.01%, 250=0.02%</span><br><span class="line">  cpu          : usr=0.65%, sys=7.20%, ctx=239089, majf=0, minf=8292</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=237743,102115,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=15.5MiB/s (16.2MB/s), 15.5MiB/s-15.5MiB/s (16.2MB/s-16.2MB/s), io=929MiB (974MB), run=60001-60001msec</span><br><span class="line">  WRITE: bw=6808KiB/s (6971kB/s), 6808KiB/s-6808KiB/s (6971kB/s-6971kB/s), io=399MiB (418MB), run=60001-60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vdb: ios=237216/118960, merge=0/8118, ticks=55191/148225, in_queue=203416, util=99.35%</span><br><span class="line">  </span><br><span class="line">[essd_pl3]# fio  -bs=4k -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=30</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=psync, iodepth=64</span><br><span class="line">fio-3.7</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)][100.0%][r=0KiB/s,w=28.3MiB/s][r=0,w=7249 IOPS][eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=2470117: Fri Apr  8 15:35:20 2022</span><br><span class="line">  write: IOPS=7222, BW=28.2MiB/s (29.6MB/s)(846MiB/30001msec)</span><br><span class="line">    clat (usec): min=115, max=7155, avg=137.29, stdev=68.48</span><br><span class="line">     lat (usec): min=115, max=7156, avg=137.36, stdev=68.49</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  121],  5.00th=[  123], 10.00th=[  125], 20.00th=[  126],</span><br><span class="line">     | 30.00th=[  127], 40.00th=[  129], 50.00th=[  130], 60.00th=[  133],</span><br><span class="line">     | 70.00th=[  135], 80.00th=[  139], 90.00th=[  149], 95.00th=[  163],</span><br><span class="line">     | 99.00th=[  255], 99.50th=[  347], 99.90th=[  668], 99.95th=[  947],</span><br><span class="line">     | 99.99th=[ 3589]</span><br><span class="line">   bw (  KiB/s): min=23592, max=30104, per=99.95%, avg=28873.29, stdev=1084.49, samples=59</span><br><span class="line">   iops        : min= 5898, max= 7526, avg=7218.32, stdev=271.12, samples=59</span><br><span class="line">  lat (usec)   : 250=98.95%, 500=0.81%, 750=0.17%, 1000=0.03%</span><br><span class="line">  lat (msec)   : 2=0.02%, 4=0.02%, 10=0.01%</span><br><span class="line">  cpu          : usr=0.72%, sys=5.08%, ctx=216767, majf=0, minf=148</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=0,216677,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: bw=28.2MiB/s (29.6MB/s), 28.2MiB/s-28.2MiB/s (29.6MB/s-29.6MB/s), io=846MiB (888MB), run=30001-30001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  vdb: ios=0/219122, merge=0/3907, ticks=0/29812, in_queue=29812, util=99.52% </span><br><span class="line">  </span><br><span class="line">[root@hygon8 14:44 /polarx/lvm]</span><br><span class="line">#fio  -bs=4k -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=30</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/157.2MB/0KB /s] [0/40.3K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=3486352: Fri Apr  8 14:45:43 2022</span><br><span class="line">  write: io=4710.4MB, bw=160775KB/s, iops=40193, runt= 30001msec</span><br><span class="line">    clat (usec): min=18, max=4164, avg=22.05, stdev= 7.33</span><br><span class="line">     lat (usec): min=19, max=4165, avg=22.59, stdev= 7.36</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   20],  5.00th=[   20], 10.00th=[   21], 20.00th=[   21],</span><br><span class="line">     | 30.00th=[   21], 40.00th=[   21], 50.00th=[   21], 60.00th=[   22],</span><br><span class="line">     | 70.00th=[   22], 80.00th=[   22], 90.00th=[   23], 95.00th=[   25],</span><br><span class="line">     | 99.00th=[   36], 99.50th=[   40], 99.90th=[   62], 99.95th=[   99],</span><br><span class="line">     | 99.99th=[  157]</span><br><span class="line">    bw (KB  /s): min=147568, max=165400, per=100.00%, avg=160803.12, stdev=2704.22</span><br><span class="line">    lat (usec) : 20=0.08%, 50=99.70%, 100=0.17%, 250=0.04%, 500=0.01%</span><br><span class="line">    lat (usec) : 750=0.01%, 1000=0.01%</span><br><span class="line">    lat (msec) : 2=0.01%, 10=0.01%</span><br><span class="line">  cpu          : usr=6.95%, sys=31.18%, ctx=1205994, majf=0, minf=1573</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=1205849/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=4710.4MB, aggrb=160774KB/s, minb=160774KB/s, maxb=160774KB/s, mint=30001msec, maxt=30001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-2: ios=0/1204503, merge=0/0, ticks=0/15340, in_queue=15340, util=50.78%, aggrios=0/603282, aggrmerge=0/463, aggrticks=0/8822, aggrin_queue=0, aggrutil=28.66%</span><br><span class="line">  nvme0n1: ios=0/683021, merge=0/474, ticks=0/9992, in_queue=0, util=28.66%</span><br><span class="line">  nvme1n1: ios=0/523543, merge=0/452, ticks=0/7652, in_queue=0, util=21.67%</span><br><span class="line">  </span><br><span class="line">[root@x86.170 /polarx/lvm]</span><br><span class="line">#/usr/sbin/nvme list</span><br><span class="line">Node             SN                   Model                                    Namespace Usage                      Format           FW Rev</span><br><span class="line">---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------</span><br><span class="line">/dev/nvme0n1     BTLJ932205P44P0DGN   INTEL SSDPE2KX040T8                      1           3.84  TB /   3.84  TB    512   B +  0 B   VDV10131</span><br><span class="line">/dev/nvme1n1     BTLJ932207H04P0DGN   INTEL SSDPE2KX040T8                      1           3.84  TB /   3.84  TB    512   B +  0 B   VDV10131</span><br><span class="line">/dev/nvme2n1     BTLJ932205AS4P0DGN   INTEL SSDPE2KX040T8                      1           3.84  TB /   3.84  TB    512   B +  0 B   VDV10131</span><br><span class="line">[root@x86.170 /polarx/lvm]</span><br><span class="line">#fio  -bs=4k  -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=30</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/240.2MB/0KB /s] [0/61.5K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=11516: Fri Apr  8 15:44:36 2022</span><br><span class="line">  write: io=7143.3MB, bw=243813KB/s, iops=60953, runt= 30001msec</span><br><span class="line">    clat (usec): min=10, max=818, avg=14.96, stdev= 4.14</span><br><span class="line">     lat (usec): min=10, max=818, avg=15.14, stdev= 4.15</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   11],  5.00th=[   12], 10.00th=[   12], 20.00th=[   14],</span><br><span class="line">     | 30.00th=[   15], 40.00th=[   15], 50.00th=[   15], 60.00th=[   15],</span><br><span class="line">     | 70.00th=[   15], 80.00th=[   16], 90.00th=[   16], 95.00th=[   16],</span><br><span class="line">     | 99.00th=[   20], 99.50th=[   32], 99.90th=[   78], 99.95th=[   84],</span><br><span class="line">     | 99.99th=[  105]</span><br><span class="line">    bw (KB  /s): min=236768, max=246424, per=99.99%, avg=243794.17, stdev=1736.82</span><br><span class="line">    lat (usec) : 20=98.96%, 50=0.73%, 100=0.29%, 250=0.01%, 500=0.01%</span><br><span class="line">    lat (usec) : 750=0.01%, 1000=0.01%</span><br><span class="line">  cpu          : usr=10.65%, sys=42.66%, ctx=1828699, majf=0, minf=7</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=1828662/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=7143.3MB, aggrb=243813KB/s, minb=243813KB/s, maxb=243813KB/s, mint=30001msec, maxt=30001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=0/1823575, merge=0/0, ticks=0/13666, in_queue=13667, util=45.56%, aggrios=0/609558, aggrmerge=0/2, aggrticks=0/4280, aggrin_queue=4198, aggrutil=14.47%</span><br><span class="line">  nvme0n1: ios=0/609144, merge=0/6, ticks=0/4438, in_queue=4353, util=14.47%</span><br><span class="line">  nvme1n1: ios=0/609470, merge=0/0, ticks=0/4186, in_queue=4109, util=13.65%</span><br><span class="line">  nvme2n1: ios=0/610060, merge=0/0, ticks=0/4216, in_queue=4134, util=13.74% </span><br></pre></td></tr></table></figure>

<h3 id="倚天-PL3-VS-SSD"><a href="#倚天-PL3-VS-SSD" class="headerlink" title="倚天 PL3 VS SSD"></a>倚天 PL3 VS SSD</h3><p>测试环境倚天裸金属，4.18 CentOS fio-3.7</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>参数</th>
<th>nvme SSD单盘</th>
<th>PL3+倚天裸金属</th>
</tr>
</thead>
<tbody><tr>
<td>randread</td>
<td>fio -bs&#x3D;4k -buffered&#x3D;1</td>
<td>IOPS&#x3D;17.7K</td>
<td>IOPS&#x3D;2533</td>
</tr>
<tr>
<td>randread</td>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0</td>
<td>IOPS&#x3D;269k</td>
<td>IOPS&#x3D;24k</td>
</tr>
<tr>
<td>randwrite</td>
<td>fio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0</td>
<td>IOPS&#x3D;68.5k</td>
<td>IOPS&#x3D;3275</td>
</tr>
<tr>
<td>randwrite</td>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -buffered&#x3D;1</td>
<td>IOPS&#x3D;253k</td>
<td>IOPS&#x3D;250k</td>
</tr>
<tr>
<td>randrw</td>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -buffered&#x3D;1 rwmixread&#x3D;70</td>
<td>write:IOPS&#x3D;8815, read:IOPS&#x3D;20.5K</td>
<td>write:IOPS&#x3D;1059，read:IOPS&#x3D;2482</td>
</tr>
<tr>
<td>randrw</td>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0 rwmixread&#x3D;70</td>
<td>write:IOPS&#x3D;8754, read: IOPS&#x3D;20.4K</td>
<td>write: IOPS&#x3D;940, read: IOPS&#x3D;2212</td>
</tr>
</tbody></table>
<p>测试命令</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -ioengine=libaio -bs=4k -buffered=1  -thread -rw=randrw -rwmixread=70  -size=16G -filename=./fio.test -name=&quot;essd-pl3&quot; -iodepth=64 -runtime=30</span><br></pre></td></tr></table></figure>

<h3 id="HDD性能测试数据"><a href="#HDD性能测试数据" class="headerlink" title="HDD性能测试数据"></a>HDD性能测试数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">$sudo fio -iodepth=10 -ioengine=libaio -direct=1 -rw=randread -bs=32k -size=1G -numjobs=1 -runtime=60 -group_reporting -filename=./io.test -name=Read_Testing</span><br><span class="line">Jobs: 1 (f=1): [r(1)][100.0%][r=15.0MiB/s][r=478 IOPS][eta 00m:00s]</span><br><span class="line">Read_Testing: (groupid=0, jobs=1): err= 0: pid=104187: Mon Jan 20 09:16:00 2025</span><br><span class="line">  read: IOPS=487, BW=15.2MiB/s (16.0MB/s)(914MiB/60050msec)</span><br><span class="line">    slat (usec): min=2, max=336, avg= 7.62, stdev= 5.36</span><br><span class="line">    clat (usec): min=137, max=261017, avg=20515.50, stdev=24929.14</span><br><span class="line">     lat (usec): min=141, max=261022, avg=20523.12, stdev=24929.38</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   194],  5.00th=[   635], 10.00th=[  1565], 20.00th=[  3916],</span><br><span class="line">     | 30.00th=[  6128], 40.00th=[  8225], 50.00th=[ 10814], 60.00th=[ 15664],</span><br><span class="line">     | 70.00th=[ 22152], 80.00th=[ 32375], 90.00th=[ 51643], 95.00th=[ 71828],</span><br><span class="line">     | 99.00th=[116917], 99.50th=[139461], 99.90th=[185598], 99.95th=[200279],</span><br><span class="line">     | 99.99th=[221250]</span><br><span class="line">   bw (  KiB/s): min= 4288, max=18752, per=100.00%, avg=15597.87, stdev=2572.58, samples=120</span><br><span class="line">   iops        : min=  134, max=  586, avg=487.43, stdev=80.39, samples=120</span><br><span class="line">  lat (usec)   : 250=2.35%, 500=1.08%, 750=3.23%, 1000=0.52%</span><br><span class="line">  lat (msec)   : 2=4.40%, 4=8.71%, 10=26.46%, 20=20.74%, 50=21.68%</span><br><span class="line">  lat (msec)   : 100=8.97%, 250=1.85%, 500=0.01%</span><br><span class="line">  cpu          : usr=0.15%, sys=0.57%, ctx=29254, majf=0, minf=91</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=100.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.1%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued rwts: total=29255,0,0,0 short=0,0,0,0 dropped=0,0,0,0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=10</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: bw=15.2MiB/s (16.0MB/s), 15.2MiB/s-15.2MiB/s (16.0MB/s-16.0MB/s), io=914MiB (959MB), run=60050-60050msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">  sdm: ios=29639/657, merge=0/622, ticks=633013/17071, in_queue=655713, util=99.00%</span><br><span class="line">  </span><br><span class="line">$cat /sys/block/sdm/queue/rotational</span><br><span class="line">1  </span><br></pre></td></tr></table></figure>



<p><img src="/images/951413iMgBlog/0868d560-067f-4302-bc60-bffc3d4460ed.png" alt="img"></p>
<p>从上图可以看到这个磁盘的IOPS 读 935 写 400，读rt 10731nsec 大约10us, 写 17us。如果IOPS是1000的话，rt应该是1ms，实际比1ms小两个数量级，<del>应该是cache、磁盘阵列在起作用。</del></p>
<p>SATA硬盘，10K转</p>
<p>万转机械硬盘组成RAID5阵列，在顺序条件最好的情况下，带宽可以达到1GB&#x2F;s以上，平均延时也非常低，最低只有20多us。但是在随机IO的情况下，机械硬盘的短板就充分暴露了，零点几兆的带宽，将近5ms的延迟，IOPS只有200左右。其原因是因为</p>
<ul>
<li>随机访问直接让RAID卡缓存成了个摆设</li>
<li>磁盘不能并行工作，因为我的机器RAID宽度Strip Size为128 KB</li>
<li>机械轴也得在各个磁道之间跳来跳去。</li>
</ul>
<p>理解了磁盘顺序IO时候的几十M甚至一个GB的带宽，随机IO这个真的是太可怜了。</p>
<p>从上面的测试数据中我们看到了机械硬盘在顺序IO和随机IO下的巨大性能差异。在顺序IO情况下，磁盘是最擅长的顺序IO,再加上Raid卡缓存命中率也高。这时带宽表现有几十、几百M，最好条件下甚至能达到1GB。IOPS这时候能有2-3W左右。到了随机IO的情形下，机械轴也被逼的跳来跳去寻道，RAID卡缓存也失效了。带宽跌到了1MB以下，最低只有100K，IOPS也只有可怜巴巴的200左右。</p>
<h3 id="开关-libaio-对比"><a href="#开关-libaio-对比" class="headerlink" title="开关 libaio 对比"></a>开关 libaio 对比</h3><p>启用和禁用 libaio 进行对比，尤其要注意 libaio 要配合 -iodepth&#x3D;N 使用才能发挥作用</p>
<p>MySQL 8.0 里 innodb_parallel_read_threads 默认是开 4 个线程并行读，这就很像 libaio+iodepth 了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">#fio -ioengine=libaio -direct=1 -iodepth=32 -rw=randread -bs=32k -size=16G -numjobs=1 -runtime=200 -group_reporting -filename=/polarx/ren.test -name=Read_Testing</span><br><span class="line">Read_Testing: (g=0): rw=randread, bs=32K-32K/32K-32K/32K-32K, ioengine=libaio, iodepth=32</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 process</span><br><span class="line">Jobs: 1 (f=1): [r(1)] [100.0% done] [4092MB/0KB/0KB /s] [131K/0/0 iops] [eta 00m:00s]</span><br><span class="line">Read_Testing: (groupid=0, jobs=1): err= 0: pid=125428: Thu Jan 16 19:01:46 2025</span><br><span class="line">  read : io=16384MB, bw=4089.9MB/s, iops=130875, runt=  4006msec</span><br><span class="line">    slat (usec): min=4, max=68, avg= 6.60, stdev= 1.31</span><br><span class="line">    clat (usec): min=102, max=846, avg=237.22, stdev=45.76</span><br><span class="line">     lat (usec): min=108, max=854, avg=243.92, stdev=45.78</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  163],  5.00th=[  179], 10.00th=[  189], 20.00th=[  203],</span><br><span class="line">     | 30.00th=[  213], 40.00th=[  221], 50.00th=[  229], 60.00th=[  239],</span><br><span class="line">     | 70.00th=[  251], 80.00th=[  266], 90.00th=[  294], 95.00th=[  322],</span><br><span class="line">     | 99.00th=[  390], 99.50th=[  418], 99.90th=[  494], 99.95th=[  532],</span><br><span class="line">     | 99.99th=[  588]</span><br><span class="line">    bw (MB  /s): min= 4078, max= 4104, per=100.00%, avg=4090.47, stdev= 7.59</span><br><span class="line">    lat (usec) : 250=69.08%, 500=30.83%, 750=0.09%, 1000=0.01%</span><br><span class="line">  cpu          : usr=12.36%, sys=87.62%, ctx=20, majf=0, minf=267</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=100.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.1%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=524288/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=32</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: io=16384MB, aggrb=4089.9MB/s, minb=4089.9MB/s, maxb=4089.9MB/s, mint=4006msec, maxt=4006msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=1020690/0, merge=0/0, ticks=140356/0, in_queue=142279, util=98.70%, aggrios=349525/0, aggrmerge=0/0, aggrticks=47694/0, aggrin_queue=47893, aggrutil=96.88%</span><br><span class="line">  nvme0n1: ios=349526/0, merge=0/0, ticks=47435/0, in_queue=47527, util=96.81%</span><br><span class="line">  nvme2n1: ios=349523/0, merge=0/0, ticks=47970/0, in_queue=48069, util=96.88%</span><br><span class="line">  nvme1n1: ios=349527/0, merge=0/0, ticks=47677/0, in_queue=48084, util=96.88%</span><br><span class="line"></span><br><span class="line">[root@phy /polarx]</span><br><span class="line">#fio -direct=1 -iodepth=32 -rw=randread  -bs=32k -size=16G -numjobs=1 -runtime=200 -group_reporting -filename=/polarx/ren.test -name=Read_Testing</span><br><span class="line">Read_Testing: (g=0): rw=randread, bs=32K-32K/32K-32K/32K-32K, ioengine=sync, iodepth=32</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 process</span><br><span class="line">Jobs: 1 (f=1): [r(1)] [100.0% done] [321.3MB/0KB/0KB /s] [10.3K/0/0 iops] [eta 00m:00s]</span><br><span class="line">Read_Testing: (groupid=0, jobs=1): err= 0: pid=125665: Thu Jan 16 19:02:49 2025</span><br><span class="line">  read : io=16384MB, bw=327539KB/s, iops=10235, runt= 51222msec</span><br><span class="line">    clat (usec): min=73, max=168, avg=96.75, stdev= 3.64</span><br><span class="line">     lat (usec): min=73, max=169, avg=96.83, stdev= 3.64</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   81],  5.00th=[   94], 10.00th=[   95], 20.00th=[   96],</span><br><span class="line">     | 30.00th=[   97], 40.00th=[   97], 50.00th=[   97], 60.00th=[   98],</span><br><span class="line">     | 70.00th=[   98], 80.00th=[   98], 90.00th=[   99], 95.00th=[  100],</span><br><span class="line">     | 99.00th=[  101], 99.50th=[  102], 99.90th=[  104], 99.95th=[  107],</span><br><span class="line">     | 99.99th=[  131]</span><br><span class="line">    bw (KB  /s): min=326208, max=329792, per=100.00%, avg=327565.80, stdev=726.96</span><br><span class="line">    lat (usec) : 100=94.83%, 250=5.17%</span><br><span class="line">  cpu          : usr=1.64%, sys=8.76%, ctx=524293, majf=0, minf=16</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=524288/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=32</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">   READ: io=16384MB, aggrb=327539KB/s, minb=327539KB/s, maxb=327539KB/s, mint=51222msec, maxt=51222msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=1047582/0, merge=0/0, ticks=90196/0, in_queue=90742, util=92.36%, aggrios=349525/0, aggrmerge=0/0, aggrticks=30421/0, aggrin_queue=29816, aggrutil=60.17%</span><br><span class="line">  nvme0n1: ios=349526/0, merge=0/0, ticks=30465/0, in_queue=30005, util=58.48%</span><br><span class="line">  nvme2n1: ios=349523/0, merge=0/0, ticks=31635/0, in_queue=30871, util=60.17%</span><br><span class="line">  nvme1n1: ios=349527/0, merge=0/0, ticks=29165/0, in_queue=28573, util=55.69%</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">[root@phy /polarx]</span><br><span class="line">#fio -ioengine=libaio -bs=4k -direct=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=/polarx/ren.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/799.5MB/0KB /s] [0/205K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=14877: Thu Jan 16 19:21:21 2025</span><br><span class="line">  write: io=16384MB, bw=811277KB/s, iops=202819, runt= 20680msec</span><br><span class="line">    slat (usec): min=2, max=112, avg= 3.80, stdev= 0.96</span><br><span class="line">    clat (usec): min=11, max=6412, avg=311.05, stdev=55.04</span><br><span class="line">     lat (usec): min=15, max=6416, avg=314.95, stdev=55.04</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  286],  5.00th=[  294], 10.00th=[  298], 20.00th=[  302],</span><br><span class="line">     | 30.00th=[  306], 40.00th=[  310], 50.00th=[  310], 60.00th=[  314],</span><br><span class="line">     | 70.00th=[  314], 80.00th=[  318], 90.00th=[  322], 95.00th=[  326],</span><br><span class="line">     | 99.00th=[  334], 99.50th=[  338], 99.90th=[  740], 99.95th=[ 1224],</span><br><span class="line">     | 99.99th=[ 2704]</span><br><span class="line">    bw (KB  /s): min=789992, max=820936, per=99.99%, avg=811198.63, stdev=7037.24</span><br><span class="line">    lat (usec) : 20=0.04%, 50=0.04%, 100=0.04%, 250=0.19%, 500=99.54%</span><br><span class="line">    lat (usec) : 750=0.04%, 1000=0.03%</span><br><span class="line">    lat (msec) : 2=0.05%, 4=0.01%, 10=0.01%</span><br><span class="line">  cpu          : usr=22.18%, sys=77.54%, ctx=6618, majf=0, minf=1506</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=4194304/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=16384MB, aggrb=811277KB/s, minb=811277KB/s, maxb=811277KB/s, mint=20680msec, maxt=20680msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=0/4189902, merge=0/0, ticks=0/53584, in_queue=53669, util=100.00%, aggrios=0/1398104, aggrmerge=0/1, aggrticks=0/18815, aggrin_queue=17669, aggrutil=58.72%</span><br><span class="line">  nvme0n1: ios=0/1398107, merge=0/1, ticks=0/17693, in_queue=16375, util=55.72%</span><br><span class="line">  nvme2n1: ios=0/1398095, merge=0/1, ticks=0/19587, in_queue=18311, util=57.02%</span><br><span class="line">  nvme1n1: ios=0/1398111, merge=0/1, ticks=0/19166, in_queue=18321, util=58.72%</span><br><span class="line"></span><br><span class="line">[root@phy /polarx]</span><br><span class="line">#fio -bs=4k -direct=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=/polarx/ren.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/229.5MB/0KB /s] [0/58.8K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=16447: Thu Jan 16 19:23:00 2025</span><br><span class="line">  write: io=13666MB, bw=233236KB/s, iops=58309, runt= 60000msec</span><br><span class="line">    clat (usec): min=13, max=1406, avg=16.21, stdev= 1.48</span><br><span class="line">     lat (usec): min=13, max=1406, avg=16.33, stdev= 1.48</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   13],  5.00th=[   14], 10.00th=[   14], 20.00th=[   15],</span><br><span class="line">     | 30.00th=[   16], 40.00th=[   16], 50.00th=[   16], 60.00th=[   17],</span><br><span class="line">     | 70.00th=[   17], 80.00th=[   17], 90.00th=[   18], 95.00th=[   18],</span><br><span class="line">     | 99.00th=[   19], 99.50th=[   20], 99.90th=[   22], 99.95th=[   22],</span><br><span class="line">     | 99.99th=[   24]</span><br><span class="line">    bw (KB  /s): min=222688, max=234992, per=100.00%, avg=233226.35, stdev=1740.79</span><br><span class="line">    lat (usec) : 20=99.49%, 50=0.51%, 100=0.01%, 250=0.01%</span><br><span class="line">    lat (msec) : 2=0.01%</span><br><span class="line">  cpu          : usr=10.76%, sys=29.50%, ctx=3498560, majf=0, minf=1128</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=3498543/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=13666MB, aggrb=233236KB/s, minb=233236KB/s, maxb=233236KB/s, mint=60000msec, maxt=60000msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=0/3494396, merge=0/0, ticks=0/36982, in_queue=36750, util=61.25%, aggrios=0/1166190, aggrmerge=0/3, aggrticks=0/13666, aggrin_queue=11741, aggrutil=20.12%</span><br><span class="line">  nvme0n1: ios=0/1166324, merge=0/3, ticks=0/13514, in_queue=11514, util=19.16%</span><br><span class="line">  nvme2n1: ios=0/1166320, merge=0/3, ticks=0/14245, in_queue=12086, util=20.12%</span><br><span class="line">  nvme1n1: ios=0/1165926, merge=0/3, ticks=0/13240, in_queue=11625, util=19.35% </span><br></pre></td></tr></table></figure>

<p>查看 SSD 的队列数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#cat /sys/block/nvme0n1/queue/nr_requests</span><br><span class="line">1023</span><br><span class="line"></span><br><span class="line"># cat /sys/block/sdd/queue/nr_requests</span><br><span class="line">128</span><br></pre></td></tr></table></figure>



<h4 id="innodb-parallel-read-threads"><a href="#innodb-parallel-read-threads" class="headerlink" title="innodb_parallel_read_threads"></a>innodb_parallel_read_threads</h4><p>加大 innodb_parallel_read_threads 可以看到 count(*) 的速度能和 innodb_parallel_read_threads 匹配增加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">//set global innodb_parallel_read_threads=16</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdb               0.00     0.00 47901.50   42.00 766424.00   402.00    31.99    13.23    0.28    0.28    0.14   0.02 100.15</span><br><span class="line">dm-3              0.00     0.00 47902.50   52.50 766440.00   730.00    32.00    13.13    0.27    0.27    0.16   0.02 100.40</span><br><span class="line">dm-5              0.00     0.00 47902.50   42.50 766440.00   730.00    32.00    13.19    0.27    0.27    0.20   0.02 100.45</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdb               0.00     0.00 47570.00    9.00 761112.00    76.00    32.00    13.22    0.28    0.28    0.22   0.02 100.20</span><br><span class="line">dm-3              0.00     0.00 47569.00   12.00 761104.00   164.00    32.00    13.13    0.27    0.27    0.25   0.02 100.25</span><br><span class="line">dm-5              0.00     0.00 47569.00    9.50 761104.00   164.00    32.00    13.16    0.28    0.28    0.32   0.02 100.20</span><br><span class="line"></span><br><span class="line">//set global innodb_parallel_read_threads=1</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdb               0.00     0.00 3986.00   18.50 63776.00   190.00    31.95     0.83    0.21    0.21    0.08   0.21  82.75</span><br><span class="line">dm-3              0.00     0.00 3986.00   23.00 63776.00   326.00    31.98     0.83    0.21    0.21    0.09   0.21  82.95</span><br><span class="line">dm-5              0.00     0.00 3986.00   19.00 63776.00   326.00    32.01     0.83    0.21    0.21    0.11   0.21  83.10</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sdb               0.00     0.00 4152.50   19.50 66440.00   192.00    31.94     0.83    0.20    0.20    0.15   0.20  82.50</span><br><span class="line">dm-3              0.00     0.00 4152.50   24.00 66440.00   328.00    31.97     0.83    0.20    0.20    0.15   0.20  82.70</span><br><span class="line">dm-5              0.00     0.00 4152.50   20.00 66440.00   328.00    32.00     0.83    0.20    0.20    0.17   0.20  82.85</span><br></pre></td></tr></table></figure>

<p>从上面可以看到一个线程去读的时候 iops 是 4000， 如果 16 个线程并发去读 iops 就是 48000，count 速度也提升了 16 倍</p>
<p>下图是 innodb_parallel_read_threads&#x3D;4 时的 iotop，可以看到单线程读上限就是 52M 左右，相较 1 的时候 count(*)   的性能正好翻了 4 倍</p>
<p><img src="/images/951413iMgBlog/image-20250117173703569.png" alt="image-20250117173703569"></p>
<p>nvme SSD 的吞吐</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">//iodepth=1 时 iops 7324，吞吐 117M</span><br><span class="line">#taskset -c 0 fio -iodepth=10 -ioengine=libaio -direct=1  -rw=randread -bs=32k -size=64G -numjobs=1 -runtime=60 -group_reporting -filename=./ren.test -name=Read_Testing</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     8.50    0.00    2.50     0.00    44.00    35.20     0.00    1.20    0.00    1.20   1.00   0.25</span><br><span class="line">nvme0n1           0.00     0.00 7324.00    0.00 117184.00     0.00    32.00     0.59    0.08    0.08    0.00   0.08  59.50</span><br><span class="line">nvme2n1           0.00     0.00 7271.00    0.00 116336.00     0.00    32.00     0.59    0.08    0.08    0.00   0.08  59.50</span><br><span class="line">nvme1n1           0.00     0.00 7376.50    0.00 118024.00     0.00    32.00     0.60    0.08    0.08    0.00   0.08  59.85</span><br><span class="line">dm-0              0.00     0.00 21972.00    0.00 351552.00     0.00    32.00     1.82    0.08    0.08    0.00   0.04  92.85</span><br><span class="line"></span><br><span class="line">//iodepth=10 时 iops 51434，吞吐 822M</span><br><span class="line">#taskset -c 0 fio -iodepth=10 -ioengine=libaio -direct=1  -rw=randread -bs=32k -size=64G -numjobs=1 -runtime=60 -group_reporting -filename=./ren.test -name=Read_Testing</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme0n1           0.00     0.00 51434.00    0.00 822944.00     0.00    32.00     5.39    0.11    0.11    0.00   0.02 100.25</span><br><span class="line">nvme2n1           0.00     0.00 51584.50    0.00 825352.00     0.00    32.00     5.45    0.11    0.11    0.00   0.02 100.15</span><br><span class="line">nvme1n1           0.00     0.00 51481.00    0.00 823696.00     0.00    32.00     5.50    0.11    0.11    0.00   0.02 100.05</span><br><span class="line">dm-0              0.00     0.00 154499.00    0.00 2471984.00     0.00    32.00    16.45    0.11    0.11    0.00   0.01 100.65</span><br><span class="line"></span><br><span class="line">//iodepth=100 时 iops 89666，吞吐 1434M</span><br><span class="line">#taskset -c 0 fio -iodepth=100 -ioengine=libaio -direct=1  -rw=randread -bs=32k -size=64G -numjobs=1 -runtime=60 -group_reporting -filename=./ren.test -name=Read_Testing</span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme0n1           0.00     0.00 89666.50    0.00 1434664.00     0.00    32.00    12.35    0.14    0.14    0.00   0.01 100.15</span><br><span class="line">nvme2n1           0.00     0.00 89875.50    0.00 1438008.00     0.00    32.00    12.47    0.14    0.14    0.00   0.01 100.20</span><br><span class="line">nvme1n1           0.00     0.00 89802.00    0.00 1436832.00     0.00    32.00    12.63    0.14    0.14    0.00   0.01 100.25</span><br><span class="line">dm-0              0.00     0.00 269342.50    0.00 4309472.00     0.00    32.00    38.53    0.14    0.14    0.00   0.00 102.55</span><br></pre></td></tr></table></figure>

<p>之所以有这么大的差异，是靠 SSD 的多队列，也就是业务层面也要支持多线程同时读写才能发挥出 SSD 的多队列能力，也和目标文件大小相关</p>
<p>从数据上看 %util 对 SSD 参考意义不大，但是 %util 越大越是触摸到 IO 瓶颈了，比如看到 util% 到了 50% 不代表 IO 用到一半了， 50% 代表 1 秒中内有 0.5 秒 SSD 的所有队列都是空闲的</p>
<h2 id="测试数据总结"><a href="#测试数据总结" class="headerlink" title="测试数据总结"></a>测试数据总结</h2><table>
<thead>
<tr>
<th></th>
<th>-direct&#x3D;1 -buffered&#x3D;1</th>
<th>-direct&#x3D;0 -buffered&#x3D;1</th>
<th>-direct&#x3D;1 -buffered&#x3D;0</th>
<th>-direct&#x3D;0 -buffered&#x3D;0</th>
</tr>
</thead>
<tbody><tr>
<td>NVMe SSD</td>
<td>R&#x3D;10.6k W&#x3D;4544</td>
<td>R&#x3D;10.8K W&#x3D;4642</td>
<td>R&#x3D;99.8K W&#x3D;42.8K</td>
<td>R&#x3D;38.6k W&#x3D;16.5k</td>
</tr>
<tr>
<td>SATA SSD</td>
<td>R&#x3D;4312 W&#x3D;1852</td>
<td>R&#x3D;5389 W&#x3D;2314</td>
<td>R&#x3D;16.9k W&#x3D;7254</td>
<td>R&#x3D;15.8k W&#x3D;6803</td>
</tr>
<tr>
<td>ESSD</td>
<td>R&#x3D;2149 W&#x3D;2150</td>
<td>R&#x3D;1987 W&#x3D;1984</td>
<td>R&#x3D;2462 W&#x3D;2465</td>
<td>R&#x3D;2455 W&#x3D;2458</td>
</tr>
</tbody></table>
<p>看起来，<strong>对于SSD如果buffered为1的话direct没啥用，如果buffered为0那么direct为1性能要好很多</strong></p>
<p><strong>SATA SSD的IOPS比NVMe性能差很多</strong>。</p>
<p>SATA SSD当-buffered&#x3D;1参数下SATA SSD的latency在7-10us之间。 </p>
<p>NVMe SSD以及SATA SSD当buffered&#x3D;0的条件下latency均为2-3us,  NVMe SSD latency参考文章第一个表格， 和本次NVMe测试结果一致.  </p>
<p>ESSD的latency基本是13-16us。</p>
<p>以上NVMe SSD测试数据是在测试过程中还有mysql在全力导入数据的情况下，用fio测试所得。所以空闲情况下测试结果会更好。</p>
<h3 id="网上测试数据参考"><a href="#网上测试数据参考" class="headerlink" title="网上测试数据参考"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40497397">网上测试数据参考</a></h3><p>我们来一起看一下具体的数据。首先来看NVＭe如何减小了协议栈本身的时间消耗，我们用<em>blktrace</em>工具来分析一组传输在应用程序层、操作系统层、驱动层和硬件层消耗的时间和占比，来了解AHCI和NVMe协议的性能区别：</p>
<p><img src="/images/951413iMgBlog/v2-8b37f236d5c754efabe17aa9706f99a3_720w.jpg" alt="img"></p>
<p>硬盘HDD作为一个参考基准，它的时延是非常大的，达到14ms，而AHCI SATA为125us，NVMe为111us。我们从图中可以看出，NVMe相对AHCI，协议栈及之下所占用的时间比重明显减小，应用程序层面等待的时间占比很高，这是因为SSD物理硬盘速度不够快，导致应用空转。NVMe也为将来Optane硬盘这种低延迟介质的速度提高留下了广阔的空间。</p>
<h2 id="对比LVM-、RAID0和-一块NVMe-SSD"><a href="#对比LVM-、RAID0和-一块NVMe-SSD" class="headerlink" title="对比LVM 、RAID0和 一块NVMe SSD"></a>对比LVM 、RAID0和 一块NVMe SSD</h2><p>曙光H620-G30A机型下测试</p>
<p>各拿两块nvme，分别作LVM和RAID0，另外单独拿一块nvme直接读写，条带用的是4块nvme做的，然后比较顺序、随机读写，测试结果如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>RAID0（2块盘）</th>
<th>NVMe</th>
<th>LVM</th>
<th>RAID0（4块盘）</th>
<th>线性（4块 linear）</th>
</tr>
</thead>
<tbody><tr>
<td>dd write bs&#x3D;1M count&#x3D;10240 conv&#x3D;fsync</td>
<td>10.9秒</td>
<td>23秒</td>
<td>24.6秒</td>
<td>10.9秒</td>
<td>11.9秒</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k  -buffered&#x3D;1</td>
<td>bw&#x3D;346744KB&#x2F;s, iops&#x3D;86686 <br/> nvme6n1: util&#x3D;38.43%<br/> nvme7n1: util&#x3D;38.96%</td>
<td>bw&#x3D;380816KB&#x2F;s, iops&#x3D;95203<br/>nvme2n1: util&#x3D;68.31%</td>
<td>bw&#x3D;175704KB&#x2F;s, iops&#x3D;43925<br/>nvme0n1:util&#x3D;29.60%<br/>nvme1n1: util&#x3D;25.64%</td>
<td>bw&#x3D;337495KB&#x2F;s, iops&#x3D;84373<br/> nvme6n1: util&#x3D;20.93%<br/> nvme5n1: util&#x3D;21.30%<br/> nvme4n1: util&#x3D;21.12%<br/> nvme7n1: util&#x3D;20.95%</td>
<td>bw&#x3D;329721KB&#x2F;s, iops&#x3D;82430<br/> nvme0n1: util&#x3D;67.22%<br/> nvme3n1: util&#x3D;0%<br/>线性每次只写一块盘</td>
</tr>
<tr>
<td>fio -ioengine&#x3D;libaio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0</td>
<td>bw&#x3D;121556KB&#x2F;s, iops&#x3D;30389 <br/> nvme6n1: util&#x3D;18.70%<br/> nvme7n1: util&#x3D;18.91%</td>
<td>bw&#x3D;126215KB&#x2F;s, iops&#x3D;31553<br/>nvme2n1: util&#x3D;37.27%</td>
<td>bw&#x3D;117192KB&#x2F;s, iops&#x3D;29297<br/>nvme0n1:util&#x3D;21.16%<br/>nvme1n1: util&#x3D;13.35%</td>
<td>bw&#x3D;119145KB&#x2F;s, iops&#x3D;29786<br/> nvme6n1: util&#x3D;9.19%<br/> nvme5n1: util&#x3D;9.45%<br/> nvme4n1: util&#x3D;9.45%<br/> nvme7n1: util&#x3D;9.30%</td>
<td>bw&#x3D;116688KB&#x2F;s, iops&#x3D;29171<br/> nvme0n1: util&#x3D;37.87%<br/> nvme3n1: util&#x3D;0%<br/>线性每次只写一块盘</td>
</tr>
<tr>
<td>fio -bs&#x3D;4k -direct&#x3D;1 -buffered&#x3D;0</td>
<td>bw&#x3D;104107KB&#x2F;s, iops&#x3D;26026 <br/> nvme6n1: util&#x3D;15.55%<br/> nvme7n1: util&#x3D;15.00%</td>
<td>bw&#x3D;105115KB&#x2F;s, iops&#x3D;26278<br/>nvme2n1: util&#x3D;31.25%</td>
<td>bw&#x3D;101936KB&#x2F;s, iops&#x3D;25484<br/>nvme0n1:util&#x3D;17.76%<br/>nvme1n1: util&#x3D;12.07%</td>
<td>bw&#x3D;102517KB&#x2F;s, iops&#x3D;25629<br/> nvme6n1: util&#x3D;8.13%<br/> nvme5n1: util&#x3D;7.65%<br/> nvme4n1: util&#x3D;7.57%<br/> nvme7n1: util&#x3D;7.75%</td>
<td>bw&#x3D;87280KB&#x2F;s, iops&#x3D;21820<br/> nvme0n1: util&#x3D;31.27%<br/> nvme3n1: util&#x3D;0%<br/>线性每次只写一块盘</td>
</tr>
</tbody></table>
<ul>
<li>整体看 nvme 最好(顺序写除外)，raid0性能接近nvme，LVM最差</li>
<li>顺序写raid0是nvme、LVM的两倍</li>
<li>随机读写带buffered的话 nvme最好，raid0略差（猜测是软件消耗），<del>LVM只有前两者的一半</del></li>
<li>关掉buffered 三者性能下降都很大，最终差异变小</li>
<li>raid0下两块盘非常均衡，<del>LVM下两块盘负载差异比较大</del></li>
<li>性能不在单块盘到了瓶颈，当阵列中盘数变多后，软件实现的LVM、RAID性能都有下降</li>
<li>开buffer对性能提升非常大</li>
<li>每次测试前都会echo 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches ; rm -f .&#x2F;fio.test ;测试跑多次，取稳定值</li>
<li>fio 测试里的 iodepth 对应 &#x2F;sys&#x2F;block&#x2F;sdd&#x2F;queue&#x2F;nr_requests， SSD 的队列数越性能越好，但是要配合多线程并发读写</li>
</ul>
<h3 id="顺序读写"><a href="#顺序读写" class="headerlink" title="顺序读写"></a>顺序读写</h3><p>然后同时做dd写入测试</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time taskset -c 0 dd if=/dev/zero of=./tempfile2 bs=1M count=40240 &amp;</span><br></pre></td></tr></table></figure>

<p>下图上面两块nvme做的LVM，下面两块nvme做成RAID0，同时开始测试，可以看到RAID0的两块盘写入速度更快</p>
<p><img src="/images/951413iMgBlog/image-20211231205730735.png" alt="image-20211231205730735"></p>
<p>测试结果</p>
<p><img src="/images/951413iMgBlog/image-20211231205842753.png" alt="image-20211231205842753"></p>
<p>实际单独写一块nvme也比写两块nvme做的LVM要快一倍，对dd这样的顺序读写，软RAID0还是能提升一倍速度的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">[root@hygon33 14:02 /nvme]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./tempfile2 ; time taskset -c 16 dd if=/dev/zero of=./tempfile2 bs=1M count=10240 conv=fsync</span><br><span class="line">记录了10240+0 的读入</span><br><span class="line">记录了10240+0 的写出</span><br><span class="line">10737418240字节（11 GB，10 GiB）已复制，23.0399 s，466 MB/s</span><br><span class="line"></span><br><span class="line">real	0m23.046s</span><br><span class="line">user	0m0.004s</span><br><span class="line">sys	0m8.033s</span><br><span class="line"></span><br><span class="line">[root@hygon33 14:08 /nvme]</span><br><span class="line">#cd ../md0/</span><br><span class="line"></span><br><span class="line">[root@hygon33 14:08 /md0]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./tempfile2 ; time taskset -c 16 dd if=/dev/zero of=./tempfile2 bs=1M count=10240 conv=fsync</span><br><span class="line">记录了10240+0 的读入</span><br><span class="line">记录了10240+0 的写出</span><br><span class="line">10737418240字节（11 GB，10 GiB）已复制，10.9632 s，979 MB/s</span><br><span class="line"></span><br><span class="line">real	0m10.967s</span><br><span class="line">user	0m0.004s</span><br><span class="line">sys	0m10.899s</span><br><span class="line"></span><br><span class="line">[root@hygon33 14:08 /md0]</span><br><span class="line">#cd /polarx/</span><br><span class="line"></span><br><span class="line">[root@hygon33 14:08 /polarx]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./tempfile2 ; time taskset -c 16 dd if=/dev/zero of=./tempfile2 bs=1M count=10240 conv=fsync</span><br><span class="line">记录了10240+0 的读入</span><br><span class="line">记录了10240+0 的写出</span><br><span class="line">10737418240字节（11 GB，10 GiB）已复制，24.6481 s，436 MB/s</span><br><span class="line"></span><br><span class="line">real	0m24.653s</span><br><span class="line">user	0m0.008s</span><br><span class="line">sys	0m24.557s</span><br></pre></td></tr></table></figure>

<h3 id="随机读写"><a href="#随机读写" class="headerlink" title="随机读写"></a>随机读写</h3><p>SSD单独的随机读IOPS大概是随机写IOPS的10%, 应该是因为write有cache</p>
<p>RAID0是使用mdadm做的软raid，系统层面还是有消耗，没法和RAID卡硬件比较</p>
<p>左边是一块nvme，中间是两块nvme做了LVM，右边是两块nvme做RAID0，看起来速度差不多，一块nvme似乎要好一点点</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio -ioengine=libaio -bs=4k -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br></pre></td></tr></table></figure>

<p><img src="/images/951413iMgBlog/image-20220101104145331.png" alt="image-20220101104145331"></p>
<p>从观察来看，RAID0的两块盘读写、iops都非常均衡，LVM的两块盘</p>
<p>三个测试分开跑，独立nvme性能最好，LVM最差并且不均衡</p>
<p><img src="/images/951413iMgBlog/image-20220101110016074.png" alt="image-20220101110016074"></p>
<p>三个测试分开跑，去掉 aio，性能都只有原来的一半</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fio  -bs=4k -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br></pre></td></tr></table></figure>

<p><img src="/images/951413iMgBlog/image-20220101110708888.png" alt="image-20220101110708888"></p>
<p>修改fio参数，用最快的 direct&#x3D;0 buffered&#x3D;1 aio 结论是raid0最快，直接写nvme略慢，LVM只有raid0的一半</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br></pre></td><td class="code"><pre><span class="line">[root@hygon33 13:43 /md0]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./fio.test; fio -ioengine=libaio -bs=4k -direct=0 -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [98.1% done] [0KB/394.3MB/0KB /s] [0/101K/0 iops] [eta 00m:01s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=21016: Sat Jan  1 13:45:25 2022</span><br><span class="line">  write: io=16384MB, bw=329974KB/s, iops=82493, runt= 50844msec</span><br><span class="line">    slat (usec): min=3, max=1496, avg= 9.00, stdev= 2.76</span><br><span class="line">    clat (usec): min=5, max=2272, avg=764.73, stdev=101.63</span><br><span class="line">     lat (usec): min=10, max=2282, avg=774.19, stdev=103.15</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  510],  5.00th=[  612], 10.00th=[  644], 20.00th=[  684],</span><br><span class="line">     | 30.00th=[  700], 40.00th=[  716], 50.00th=[  772], 60.00th=[  820],</span><br><span class="line">     | 70.00th=[  844], 80.00th=[  860], 90.00th=[  884], 95.00th=[  908],</span><br><span class="line">     | 99.00th=[  932], 99.50th=[  940], 99.90th=[  988], 99.95th=[ 1064],</span><br><span class="line">     | 99.99th=[ 1336]</span><br><span class="line">    bw (KB  /s): min=277928, max=490720, per=99.84%, avg=329447.45, stdev=40386.54</span><br><span class="line">    lat (usec) : 10=0.01%, 20=0.01%, 50=0.01%, 100=0.01%, 250=0.01%</span><br><span class="line">    lat (usec) : 500=0.17%, 750=48.67%, 1000=51.08%</span><br><span class="line">    lat (msec) : 2=0.08%, 4=0.01%</span><br><span class="line">  cpu          : usr=17.79%, sys=81.97%, ctx=113, majf=0, minf=5526</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=4194304/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=16384MB, aggrb=329974KB/s, minb=329974KB/s, maxb=329974KB/s, mint=50844msec, maxt=50844msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    md0: ios=0/2883541, merge=0/0, ticks=0/0, in_queue=0, util=0.00%, aggrios=0/1232592, aggrmerge=0/219971, aggrticks=0/44029, aggrin_queue=0, aggrutil=38.91%</span><br><span class="line">  nvme6n1: ios=0/1228849, merge=0/219880, ticks=0/43940, in_queue=0, util=37.19%</span><br><span class="line">  nvme7n1: ios=0/1236335, merge=0/220062, ticks=0/44119, in_queue=0, util=38.91%</span><br><span class="line">  </span><br><span class="line">[root@hygon33 13:46 /nvme]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./fio.test; fio -ioengine=libaio -bs=4k -direct=0 -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/314.3MB/0KB /s] [0/80.5K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=21072: Sat Jan  1 13:47:32 2022</span><br><span class="line">  write: io=16384MB, bw=309554KB/s, iops=77388, runt= 54198msec</span><br><span class="line">    slat (usec): min=3, max=88800, avg= 9.83, stdev=44.88</span><br><span class="line">    clat (usec): min=5, max=89662, avg=815.09, stdev=381.75</span><br><span class="line">     lat (usec): min=27, max=89748, avg=825.38, stdev=385.05</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  470],  5.00th=[  612], 10.00th=[  652], 20.00th=[  684],</span><br><span class="line">     | 30.00th=[  716], 40.00th=[  756], 50.00th=[  796], 60.00th=[  836],</span><br><span class="line">     | 70.00th=[  876], 80.00th=[  932], 90.00th=[ 1012], 95.00th=[ 1096],</span><br><span class="line">     | 99.00th=[ 1272], 99.50th=[ 1368], 99.90th=[ 1688], 99.95th=[ 1912],</span><br><span class="line">     | 99.99th=[ 3920]</span><br><span class="line">    bw (KB  /s): min=247208, max=523840, per=99.99%, avg=309507.85, stdev=34709.01</span><br><span class="line">    lat (usec) : 10=0.01%, 50=0.01%, 100=0.01%, 250=0.01%, 500=1.73%</span><br><span class="line">    lat (usec) : 750=37.71%, 1000=49.60%</span><br><span class="line">    lat (msec) : 2=10.91%, 4=0.03%, 10=0.01%, 100=0.01%</span><br><span class="line">  cpu          : usr=16.00%, sys=79.36%, ctx=138668, majf=0, minf=5522</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=4194304/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=16384MB, aggrb=309554KB/s, minb=309554KB/s, maxb=309554KB/s, mint=54198msec, maxt=54198msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-0: ios=77/1587455, merge=0/0, ticks=184/244940, in_queue=245124, util=98.23%, aggrios=77/1584444, aggrmerge=0/5777, aggrticks=183/193531, aggrin_queue=76, aggrutil=81.60%</span><br><span class="line">  sda: ios=77/1584444, merge=0/5777, ticks=183/193531, in_queue=76, util=81.60%</span><br><span class="line">  </span><br><span class="line">[root@hygon33 13:50 /polarx]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./fio.test; fio -ioengine=libaio -bs=4k -direct=0 -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/293.2MB/0KB /s] [0/75.1K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=22787: Sat Jan  1 13:51:16 2022</span><br><span class="line">  write: io=10270MB, bw=175269KB/s, iops=43817, runt= 60001msec</span><br><span class="line">    slat (usec): min=4, max=2609, avg=19.43, stdev=19.84</span><br><span class="line">    clat (usec): min=4, max=6420, avg=1438.87, stdev=483.15</span><br><span class="line">     lat (usec): min=17, max=6718, avg=1458.80, stdev=490.29</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  700],  5.00th=[  788], 10.00th=[  852], 20.00th=[  964],</span><br><span class="line">     | 30.00th=[ 1080], 40.00th=[ 1208], 50.00th=[ 1368], 60.00th=[ 1560],</span><br><span class="line">     | 70.00th=[ 1752], 80.00th=[ 1944], 90.00th=[ 2128], 95.00th=[ 2224],</span><br><span class="line">     | 99.00th=[ 2416], 99.50th=[ 2480], 99.90th=[ 2672], 99.95th=[ 3248],</span><br><span class="line">     | 99.99th=[ 5088]</span><br><span class="line">    bw (KB  /s): min=109992, max=308016, per=99.40%, avg=174219.83, stdev=56844.59</span><br><span class="line">    lat (usec) : 10=0.01%, 20=0.01%, 50=0.01%, 100=0.01%, 250=0.01%</span><br><span class="line">    lat (usec) : 500=0.01%, 750=2.87%, 1000=20.63%</span><br><span class="line">    lat (msec) : 2=59.43%, 4=17.03%, 10=0.03%</span><br><span class="line">  cpu          : usr=9.11%, sys=57.07%, ctx=762410, majf=0, minf=1769</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=2629079/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=10270MB, aggrb=175269KB/s, minb=175269KB/s, maxb=175269KB/s, mint=60001msec, maxt=60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    dm-2: ios=1/3185487, merge=0/0, ticks=0/86364, in_queue=86364, util=46.24%, aggrios=0/1576688, aggrmerge=0/16344, aggrticks=0/40217, aggrin_queue=0, aggrutil=29.99%</span><br><span class="line">  nvme0n1: ios=0/1786835, merge=0/16931, ticks=0/44447, in_queue=0, util=29.99%</span><br><span class="line">  nvme1n1: ios=1/1366541, merge=0/15758, ticks=0/35987, in_queue=0, util=25.44%</span><br><span class="line">  </span><br></pre></td></tr></table></figure>



<p>将RAID0从两块nvme改成四块后，整体性能略微下降</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#fio  -bs=4k -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=sync, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/99756KB/0KB /s] [0/24.1K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=30608: Sat Jan  1 12:09:29 2022</span><br><span class="line">  write: io=5733.9MB, bw=97857KB/s, iops=24464, runt= 60001msec</span><br><span class="line">    clat (usec): min=29, max=2885, avg=37.95, stdev=12.19</span><br><span class="line">     lat (usec): min=30, max=2886, avg=38.49, stdev=12.20</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[   32],  5.00th=[   33], 10.00th=[   34], 20.00th=[   35],</span><br><span class="line">     | 30.00th=[   36], 40.00th=[   36], 50.00th=[   37], 60.00th=[   37],</span><br><span class="line">     | 70.00th=[   38], 80.00th=[   39], 90.00th=[   40], 95.00th=[   49],</span><br><span class="line">     | 99.00th=[   65], 99.50th=[   76], 99.90th=[  109], 99.95th=[  125],</span><br><span class="line">     | 99.99th=[  203]</span><br><span class="line">    bw (KB  /s): min=92968, max=108344, per=99.99%, avg=97846.18, stdev=2085.73</span><br><span class="line">    lat (usec) : 50=95.20%, 100=4.61%, 250=0.18%, 500=0.01%, 750=0.01%</span><br><span class="line">    lat (usec) : 1000=0.01%</span><br><span class="line">    lat (msec) : 2=0.01%, 4=0.01%</span><br><span class="line">  cpu          : usr=4.67%, sys=56.35%, ctx=1467919, majf=0, minf=1144</span><br><span class="line">  IO depths    : 1=100.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, &gt;=64=0.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=1467872/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=5733.9MB, aggrb=97856KB/s, minb=97856KB/s, maxb=97856KB/s, mint=60001msec, maxt=60001msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    md0: ios=0/1553786, merge=0/0, ticks=0/0, in_queue=0, util=0.00%, aggrios=0/370860, aggrmerge=0/17733, aggrticks=0/6539, aggrin_queue=0, aggrutil=8.41%</span><br><span class="line">  nvme6n1: ios=0/369576, merge=0/17648, ticks=0/6439, in_queue=0, util=7.62%</span><br><span class="line">  nvme5n1: ios=0/370422, merge=0/17611, ticks=0/6600, in_queue=0, util=7.72%</span><br><span class="line">  nvme4n1: ios=0/371559, merge=0/18092, ticks=0/6511, in_queue=0, util=8.41%</span><br><span class="line">  nvme7n1: ios=0/371886, merge=0/17584, ticks=0/6606, in_queue=0, util=8.17%</span><br></pre></td></tr></table></figure>

<h3 id="raid6测试"><a href="#raid6测试" class="headerlink" title="raid6测试"></a>raid6测试</h3><p>raid6开buffer性能比raid0还要好10-20%，实际是将刷盘延迟异步在做，如果用-buffer&#x3D;0 raid6的性能只有raid0的一半</p>
<p><img src="/images/951413iMgBlog/image-20220105173206915.png" alt="image-20220105173206915"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">[root@hygon33 17:19 /md6]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./fio.test; fio -ioengine=libaio -bs=4k -direct=1 -buffered=1 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/424.9MB/0KB /s] [0/109K/0 iops] [eta 00m:00s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=117679: Wed Jan  5 17:21:13 2022</span><br><span class="line">  write: io=16384MB, bw=432135KB/s, iops=108033, runt= 38824msec</span><br><span class="line">    slat (usec): min=4, max=7289, avg= 6.06, stdev= 5.28</span><br><span class="line">    clat (usec): min=3, max=7973, avg=584.23, stdev=45.35</span><br><span class="line">     lat (usec): min=10, max=7986, avg=590.77, stdev=45.75</span><br><span class="line">    clat percentiles (usec):</span><br><span class="line">     |  1.00th=[  548],  5.00th=[  556], 10.00th=[  564], 20.00th=[  572],</span><br><span class="line">     | 30.00th=[  580], 40.00th=[  580], 50.00th=[  580], 60.00th=[  588],</span><br><span class="line">     | 70.00th=[  588], 80.00th=[  596], 90.00th=[  604], 95.00th=[  612],</span><br><span class="line">     | 99.00th=[  636], 99.50th=[  660], 99.90th=[  796], 99.95th=[  820],</span><br><span class="line">     | 99.99th=[  916]</span><br><span class="line">    bw (KB  /s): min=423896, max=455400, per=99.97%, avg=432015.17, stdev=6404.92</span><br><span class="line">    lat (usec) : 4=0.01%, 20=0.01%, 50=0.01%, 100=0.01%, 250=0.01%</span><br><span class="line">    lat (usec) : 500=0.01%, 750=99.78%, 1000=0.21%</span><br><span class="line">    lat (msec) : 2=0.01%, 4=0.01%, 10=0.01%</span><br><span class="line">  cpu          : usr=21.20%, sys=78.56%, ctx=57, majf=0, minf=1769</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=4194304/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=16384MB, aggrb=432135KB/s, minb=432135KB/s, maxb=432135KB/s, mint=38824msec, maxt=38824msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    md6: ios=0/162790, merge=0/0, ticks=0/0, in_queue=0, util=0.00%, aggrios=83058/153522, aggrmerge=1516568/962072, aggrticks=29792/16802, aggrin_queue=2425, aggrutil=44.71%</span><br><span class="line">  nvme0n1: ios=83410/144109, merge=1517412/995022, ticks=31218/16718, in_queue=2416, util=43.62%</span><br><span class="line">  nvme3n1: ios=83301/162626, merge=1517086/927594, ticks=24190/17067, in_queue=2364, util=34.14%</span><br><span class="line">  nvme2n1: ios=81594/144341, merge=1514750/992273, ticks=32204/16646, in_queue=2504, util=44.71%</span><br><span class="line">  nvme1n1: ios=83929/163013, merge=1517025/933399, ticks=31559/16780, in_queue=2416, util=42.83%</span><br><span class="line"></span><br><span class="line">[root@hygon33 17:21 /md6]</span><br><span class="line">#echo 3 &gt; /proc/sys/vm/drop_caches ; rm -f ./fio.test; fio -ioengine=libaio -bs=4k -direct=1 -buffered=0 -thread -rw=randwrite -rwmixread=70 -size=16G -filename=./fio.test -name=&quot;EBS 4K randwrite test&quot; -iodepth=64 -runtime=60</span><br><span class="line">EBS 4K randwrite test: (g=0): rw=randwrite, bs=4K-4K/4K-4K/4K-4K, ioengine=libaio, iodepth=64</span><br><span class="line">fio-2.2.8</span><br><span class="line">Starting 1 thread</span><br><span class="line">EBS 4K randwrite test: Laying out IO file(s) (1 file(s) / 16384MB)</span><br><span class="line">Jobs: 1 (f=0): [w(1)] [22.9% done] [0KB/51034KB/0KB /s] [0/12.8K/0 iops] [eta 03m:25s]</span><br><span class="line">EBS 4K randwrite test: (groupid=0, jobs=1): err= 0: pid=164871: Wed Jan  5 17:25:17 2022</span><br><span class="line">  write: io=3743.6MB, bw=63887KB/s, iops=15971, runt= 60003msec</span><br><span class="line">    slat (usec): min=11, max=123152, avg=29.39, stdev=283.93</span><br><span class="line">    clat (usec): min=261, max=196197, avg=3975.22, stdev=3526.29</span><br><span class="line">     lat (usec): min=300, max=196223, avg=4005.13, stdev=3554.65</span><br><span class="line">    clat percentiles (msec):</span><br><span class="line">     |  1.00th=[    3],  5.00th=[    3], 10.00th=[    4], 20.00th=[    4],</span><br><span class="line">     | 30.00th=[    4], 40.00th=[    4], 50.00th=[    4], 60.00th=[    4],</span><br><span class="line">     | 70.00th=[    5], 80.00th=[    5], 90.00th=[    5], 95.00th=[    6],</span><br><span class="line">     | 99.00th=[    7], 99.50th=[    7], 99.90th=[   39], 99.95th=[   88],</span><br><span class="line">     | 99.99th=[  167]</span><br><span class="line">    bw (KB  /s): min=41520, max=78176, per=100.00%, avg=64093.14, stdev=6896.65</span><br><span class="line">    lat (usec) : 500=0.02%, 750=0.03%, 1000=0.02%</span><br><span class="line">    lat (msec) : 2=0.73%, 4=64.28%, 10=34.72%, 20=0.06%, 50=0.08%</span><br><span class="line">    lat (msec) : 100=0.02%, 250=0.05%</span><br><span class="line">  cpu          : usr=4.11%, sys=48.69%, ctx=357564, majf=0, minf=2653</span><br><span class="line">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span><br><span class="line">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span><br><span class="line">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span><br><span class="line">     issued    : total=r=0/w=958349/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0</span><br><span class="line">     latency   : target=0, window=0, percentile=100.00%, depth=64</span><br><span class="line"></span><br><span class="line">Run status group 0 (all jobs):</span><br><span class="line">  WRITE: io=3743.6MB, aggrb=63886KB/s, minb=63886KB/s, maxb=63886KB/s, mint=60003msec, maxt=60003msec</span><br><span class="line"></span><br><span class="line">Disk stats (read/write):</span><br><span class="line">    md6: ios=0/1022450, merge=0/0, ticks=0/0, in_queue=0, util=0.00%, aggrios=262364/764703, aggrmerge=430291/192464, aggrticks=38687/55432, aggrin_queue=317, aggrutil=42.63%</span><br><span class="line">  nvme0n1: ios=262282/759874, merge=430112/209613, ticks=43304/55197, in_queue=324, util=42.63%</span><br><span class="line">  nvme3n1: ios=260535/771153, merge=430415/176326, ticks=25263/55664, in_queue=280, util=26.11%</span><br><span class="line">  nvme2n1: ios=263663/758974, merge=430349/208189, ticks=42754/55761, in_queue=280, util=42.14%</span><br><span class="line">  nvme1n1: ios=262976/768813, merge=430289/175731, ticks=43430/55109, in_queue=384, util=42.00%</span><br></pre></td></tr></table></figure>

<p>测试完成很久后ssd还维持高水位的读写</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.28    0.00    1.15    0.05    0.00   98.51</span><br><span class="line"></span><br><span class="line">Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz  aqu-sz  %util</span><br><span class="line">dm-0             5.00     56.00     0.00   0.00    0.53    11.20   39.00    292.33     0.00   0.00    0.00     7.50    0.00      0.00     0.00   0.00    0.00     0.00    0.00   0.27</span><br><span class="line">md6              0.00      0.00     0.00   0.00    0.00     0.00   14.00   1794.67     0.00   0.00    0.00   128.19    0.00      0.00     0.00   0.00    0.00     0.00    0.00   0.00</span><br><span class="line">nvme0n1       1164.67 144488.00 34935.33  96.77    0.74   124.06 3203.67  53877.83 10267.00  76.22    0.16    16.82    0.00      0.00     0.00   0.00    0.00     0.00    0.32  32.13</span><br><span class="line">nvme1n1       1172.33 144402.67 34925.00  96.75    0.74   123.18 3888.67  46635.17  7771.33  66.65    0.13    11.99    0.00      0.00     0.00   0.00    0.00     0.00    0.33  29.60</span><br><span class="line">nvme2n1       1166.67 144372.00 34914.00  96.77    0.74   123.75 3263.00  53699.17 10162.67  75.70    0.14    16.46    0.00      0.00     0.00   0.00    0.00     0.00    0.33  27.87</span><br><span class="line">nvme3n1       1157.67 144414.67 34934.33  96.79    0.64   124.75 3894.33  47073.83  7875.00  66.91    0.13    12.09    0.00      0.00     0.00   0.00    0.00     0.00    0.31  20.80</span><br><span class="line">sda              5.00     56.00     0.00   0.00    0.13    11.20   39.00    204.17     0.00   0.00    0.12     5.24    0.00      0.00     0.00   0.00    0.00     0.00    0.00   0.27</span><br></pre></td></tr></table></figure>



<h2 id="fio-结果解读"><a href="#fio-结果解读" class="headerlink" title="fio 结果解读"></a>fio 结果解读</h2><p>slat，异步场景下才有</p>
<blockquote>
<p>其中slat指的是发起IO的时间，在异步IO模式下，发起IO以后，IO会异步完成。例如调用一个异步的write，虽然write返回成功了，但是IO还未完成，slat约等于发起write的耗时；</p>
<p>slat (usec): min&#x3D;4, max&#x3D;6154, avg&#x3D;48.82, stdev&#x3D;56.38： The first latency metric you’ll see is the ‘slat’ or submission latency. It is pretty much what it sounds like, meaning “how long did it take to submit this IO to the kernel for processing?”</p>
</blockquote>
<p>clat</p>
<blockquote>
<p>clat指的是完成时间，从发起IO后到完成IO的时间，在同步IO模式下，clat是指整个写动作完成时间</p>
</blockquote>
<p>lat</p>
<blockquote>
<p>lat是总延迟时间，指的是IO单元创建到完成的总时间，通常这项数据关注较多。同步场景几乎等于clat，异步场景等于clat+slat<br>这项数据需要关注的是max，看看有没有极端的高延迟IO；另外还需要关注stdev，这项数据越大说明，IO响应时间波动越大，反之越小，波动越小</p>
</blockquote>
<p>clat percentiles (usec)：处于某个百分位的io操作时延</p>
<p>cpu          : usr&#x3D;9.11%, sys&#x3D;57.07%, ctx&#x3D;762410, majf&#x3D;0, minf&#x3D;1769  &#x2F;&#x2F;用户和系统的CPU占用时间百分比，线程切换次数，major以及minor页面错误的数量。</p>
<p>direct和buffered参数是冲突的，用一个就行，应该是direct&#x3D;0性能更好，实际不是这样，这里还需要找资料求证下</p>
<blockquote>
<ul>
<li><p><code>direct``=bool</code></p>
<p>If value is true, use non-buffered I&#x2F;O. This is usually O_DIRECT. Note that OpenBSD and ZFS on Solaris don’t support direct I&#x2F;O. On Windows the synchronous ioengines don’t support direct I&#x2F;O. Default: false.</p>
</li>
<li><p><code>buffered``=bool</code></p>
<p>If value is true, use buffered I&#x2F;O. This is the opposite of the <a target="_blank" rel="noopener" href="https://fio.readthedocs.io/en/latest/fio_man.html#cmdoption-arg-direct"><code>direct</code></a> option. Defaults to true.</p>
</li>
</ul>
</blockquote>
<h2 id="iostat-结果解读"><a href="#iostat-结果解读" class="headerlink" title="iostat 结果解读"></a><a href="linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html">iostat 结果解读</a></h2><p> iostat输出的数据来源diskstat (&#x2F;proc&#x2F;diskstats)，推荐：<a target="_blank" rel="noopener" href="https://bean-li.github.io/dive-into-iostat/">https://bean-li.github.io/dive-into-iostat/</a></p>
<p>Dm-0就是lvm</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.32    0.00    3.34    0.13    0.00   96.21</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00    11.40   66.00    7.20  1227.20    74.40    35.56     0.03    0.43    0.47    0.08   0.12   0.88</span><br><span class="line">nvme0n1           0.00  8612.00    0.00 51749.60     0.00 241463.20     9.33     4.51    0.09    0.00    0.09   0.02  78.56</span><br><span class="line">dm-0              0.00     0.00    0.00 60361.80     0.00 241463.20     8.00   152.52    2.53    0.00    2.53   0.01  78.26</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.36    0.00    3.46    0.17    0.00   96.00</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">sda               0.00     8.80    9.20    5.20  1047.20    67.20   154.78     0.01    0.36    0.46    0.19   0.33   0.48</span><br><span class="line">nvme0n1           0.00 11354.20    0.00 50876.80     0.00 248944.00     9.79     5.25    0.10    0.00    0.10   0.02  80.06</span><br><span class="line">dm-0              0.00     0.00    0.00 62231.00     0.00 248944.80     8.00   199.49    3.21    0.00    3.21   0.01  78.86</span><br></pre></td></tr></table></figure>

<p>avgqu_sz，是iostat的一项比较重要的数据。如果队列过长，则表示有大量IO在处理或等待，但是这还不足以说明后端的存储系统达到了处理极限。例如后端存储的并发能力是4096，客户端并发发送了256个IO下去，那么队列长度就是256。即使长时间队列长度是256，也不能说明什么，仅仅表明队列长度是256，有256个IO在处理或者排队。</p>
<p>avgrq-sz：请求是大IO还是小IO</p>
<p>rd_ticks和wr_ticks是把每一个IO消耗时间累加起来，但是硬盘设备一般可以并行处理多个IO，因此，rd_ticks和wr_ticks之和一般会比自然时间（wall-clock time）要大</p>
<p>那么怎么判断IO是在调度队列排队等待，还是在设备上处理呢？iostat有两项数据可以给出一个大致的判断。svctime，这项数据的指的是IO在设备处理中耗费的时间。另外一项数据await，指的是IO从排队到完成的时间，包括了svctime和排队等待的时间。那么通过对比这两项数据，如果两项数据差不多，则说明IO基本没有排队等待，耗费的时间都是设备处理。如果await远大于svctime，则说明有大量的IO在排队，并没有发送给设备处理。</p>
<h2 id="不同厂家SSD性能对比"><a href="#不同厂家SSD性能对比" class="headerlink" title="不同厂家SSD性能对比"></a>不同厂家SSD性能对比</h2><p>国产SSD指的是AliFlash</p>
<p><img src="/images/951413iMgBlog/1638359029693-73b42c13-2649-4f20-9112-a7c4c5dd5432.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/1638358969626-507f34aa-201b-4fd3-91de-66c88c6ce04a.png" alt="img"></p>
<h2 id="rq-affinity"><a href="#rq-affinity" class="headerlink" title="rq_affinity"></a>rq_affinity</h2><p>参考<a target="_blank" rel="noopener" href="https://help.aliyun.com/knowledge_detail/65077.html#title-x10-2c0-yll">aliyun测试文档</a> , rq_affinity增加2的commit： git show 5757a6d76c</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">function RunFio</span><br><span class="line">&#123;</span><br><span class="line"> numjobs=$1   # 实例中的测试线程数，例如示例中的10</span><br><span class="line"> iodepth=$2   # 同时发出I/O数的上限，例如示例中的64</span><br><span class="line"> bs=$3        # 单次I/O的块文件大小，例如示例中的4k</span><br><span class="line"> rw=$4        # 测试时的读写策略，例如示例中的randwrite</span><br><span class="line"> filename=$5  # 指定测试文件的名称，例如示例中的/dev/your_device</span><br><span class="line"> nr_cpus=`cat /proc/cpuinfo |grep &quot;processor&quot; |wc -l`</span><br><span class="line"> if [ $nr_cpus -lt $numjobs ];then</span><br><span class="line">     echo “Numjobs is more than cpu cores, exit!”</span><br><span class="line">     exit -1</span><br><span class="line"> fi</span><br><span class="line"> let nu=$numjobs+1</span><br><span class="line"> cpulist=&quot;&quot;</span><br><span class="line"> for ((i=1;i&lt;10;i++))</span><br><span class="line"> do</span><br><span class="line">     list=`cat /sys/block/your_device/mq/*/cpu_list | awk &#x27;&#123;if(i&lt;=NF) print $i;&#125;&#x27; i=&quot;$i&quot; | tr -d &#x27;,&#x27; | tr &#x27;\n&#x27; &#x27;,&#x27;`</span><br><span class="line">     if [ -z $list ];then</span><br><span class="line">         break</span><br><span class="line">     fi</span><br><span class="line">     cpulist=$&#123;cpulist&#125;$&#123;list&#125;</span><br><span class="line"> done</span><br><span class="line"> spincpu=`echo $cpulist | cut -d &#x27;,&#x27; -f 2-$&#123;nu&#125;`</span><br><span class="line"> echo $spincpu</span><br><span class="line"> fio --ioengine=libaio --runtime=30s --numjobs=$&#123;numjobs&#125; --iodepth=$&#123;iodepth&#125; --bs=$&#123;bs&#125; --rw=$&#123;rw&#125; --filename=$&#123;filename&#125; --time_based=1 --direct=1 --name=test --group_reporting --cpus_allowed=$spincpu --cpus_allowed_policy=split</span><br><span class="line">&#125;</span><br><span class="line">echo 2 &gt; /sys/block/your_device/queue/rq_affinity</span><br><span class="line">sleep 5</span><br><span class="line">RunFio 10 64 4k randwrite filename</span><br></pre></td></tr></table></figure>

<p>对NVME SSD进行测试，左边rq_affinity是2，右边rq_affinity为1，在这个测试参数下rq_affinity为1的性能要好(后许多次测试两者性能差不多)</p>
<p><img src="/images/951413iMgBlog/image-20210607113709945.png" alt="image-20210607113709945"></p>
<h2 id="scheduler-算法"><a href="#scheduler-算法" class="headerlink" title="scheduler 算法"></a>scheduler 算法</h2><p>如下，选择了bfq，ssd的话推荐用none或者mq-deadline</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#cat /sys/block/nvme&#123;0,1,2,3&#125;n1/queue/scheduler</span><br><span class="line">mq-deadline kyber [bfq] none</span><br></pre></td></tr></table></figure>

<p>bfq（Budget Fair Queueing），该调度算法令存储设备公平的对待每个线程，为各个进程服务相同数量的扇区。通常bfq适用于多媒体应用、桌面环境，对于很多IO压力很大的场景，例如IO集中在某些进程上的场景，bfq并不适用。</p>
<p>mq-deadline算法并不限制每个进程的 IO 资源，是一种以提高机械硬盘吞吐量为出发点的调度算法，该算法适用于IO压力大且IO集中在某几个进程的场景，比如大数据、数据库等场景</p>
<p>磁盘队列的主要目的是对磁盘的I&#x2F;O进行合并和排序，以提高磁盘的整理性能，对于传统的机械硬盘而言，由于其读写头需要进行物理寻址，因此请求排序和合并调度是非常必要的。但对于SSD硬盘，由于其不需要进行物理寻址，因此磁盘队列的最用相对于小一点</p>
<h3 id="修改"><a href="#修改" class="headerlink" title="修改"></a>修改</h3><ul>
<li>临时修改全部磁盘的I&#x2F;O调度算法，以mq-deadline为例（临时生效）：</li>
</ul>
<p>echo mq-deadline &gt; &#x2F;sys&#x2F;block&#x2F;sd*&#x2F;queue&#x2F;scheduler</p>
<ul>
<li>永久修改I&#x2F;O调度算法，以mq-deadline为例（重启后生效）：</li>
</ul>
<p>vim &#x2F;lib&#x2F;udev&#x2F;rules.d&#x2F;60-block-scheduler.rules</p>
<p><img src="data:image/png;base64,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" alt="img"></p>
<p>将图中的bfq改为none或者mq-deadline。</p>
<ul>
<li>验证查看磁盘使用的调度算法：</li>
</ul>
<p>使用lsblk -t查看SCHED列。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># lsblk -t</span><br><span class="line">NAME              ALIGNMENT MIN-IO OPT-IO PHY-SEC LOG-SEC ROTA SCHED       RQ-SIZE   RA WSAME</span><br><span class="line">sda                       0    512      0     512     512    0 mq-deadline      64 2048    0B</span><br><span class="line">├─sda1                    0    512      0     512     512    0 mq-deadline      64 2048    0B</span><br><span class="line">├─sda2                    0    512      0     512     512    0 mq-deadline      64 2048    0B</span><br><span class="line">└─sda3                    0    512      0     512     512    0 mq-deadline      64 2048    0B</span><br><span class="line">  ├─klas-root             0    512      0     512     512    0                 128 4096    0B</span><br><span class="line">  ├─klas-swap             0    512      0     512     512    0                 128 4096    0B</span><br><span class="line">  └─klas-backup           0    512      0     512     512    0                 128 4096    0B</span><br><span class="line">nvme3n1                   0    512      0     512     512    0 bfq             256 2048    0B</span><br><span class="line">└─vgpolarx-polarx         0 131072 524288     512     512    0                 128 4096    0B</span><br><span class="line">nvme0n1                   0    512      0     512     512    0 bfq             256 2048    0B</span><br><span class="line">└─vgpolarx-polarx         0 131072 524288     512     512    0                 128 4096    0B</span><br><span class="line">nvme2n1                   0    512      0     512     512    0 bfq             256 2048    0B</span><br><span class="line">└─vgpolarx-polarx         0 131072 524288     512     512    0                 128 4096    0B</span><br><span class="line">nvme1n1                   0    512      0     512     512    0 bfq             256 2048    0B</span><br><span class="line">└─vgpolarx-polarx         0 131072 524288     512     512    0                 128 4096    0B</span><br></pre></td></tr></table></figure>



<h4 id="修改bfq调度器的idle时间（临时生效，重启后失效。）"><a href="#修改bfq调度器的idle时间（临时生效，重启后失效。）" class="headerlink" title="修改bfq调度器的idle时间（临时生效，重启后失效。）"></a>修改bfq调度器的idle时间（临时生效，重启后失效。）</h4><p>bfq的idle时间默认是8ms，<a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100063071/7aa11aeb">将默认值修改为0</a>。</p>
<ol>
<li><p>执行如下命令修改idle值。此处以sdb举例，修改idle为0。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 0 &gt; /sys/block/sdb/queue/iosched/slice_idle</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="none-VS-bfq"><a href="#none-VS-bfq" class="headerlink" title="none VS bfq"></a>none VS bfq</h3><p> 从下图可以看到 iops 减少到 none 的20-40%之间，并且抖动很大</p>
<p><img src="/images/951413iMgBlog/image-20231011090249159.png" alt="image-20231011090249159"></p>
<p>用sysbench write only 场景下 压鲲鹏机器+麒麟(4块nvme做条带LVM )+官方MySQL 也看到了QPS 很差且长期跌0，红框是改成none，红框之前的部分是bfq</p>
<p><img src="/images/951413iMgBlog/lQLPJwdVC35hlSDNBrrNCHawFhZ_xVTrMXIFGKolRUBPAA_2166_1722.png" alt="img"></p>
<p>下图是 sysbench write only 场景不同 scheduler 算法的 QPS，可以看到 bfq 很差，mq-deadline 和 none 几乎差不多</p>
<p><img src="/images/951413iMgBlog/image-20231011095227886.png" alt="image-20231011095227886"></p>
<p>对应的 iotop</p>
<p><img src="/images/951413iMgBlog/image-20231011090609546.png" alt="image-20231011090609546"></p>
<p><img src="/images/951413iMgBlog/image-20231011090625857.png" alt="image-20231011090625857"></p>
<h2 id="磁盘挂载参数"><a href="#磁盘挂载参数" class="headerlink" title="磁盘挂载参数"></a>磁盘挂载参数</h2><p>内核一般配置的脏页回写超时时间是30s，理论上page cache能buffer住所有的脏页，但是ext4文件系统的默认挂载参数开始支持日志（journal），文件的inode被修改后，需要刷到journal里，这样系统crash了文件系统能恢复过来，内核配置默认5s刷一次journal。</p>
<p>ext4还有一个配置项叫挂载方式，有<code>ordered</code>和<code>writeback</code>两个选项，区别是ordered在把inode刷到journal里之前，会把inode的所有脏页先回写到磁盘里，如果不希望inode这么快写回到磁盘则可以用writeback参数。当SSD开始写盘的时候会严重影响SSD读能力</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 编辑/etc/fstab，挂载参数设置为defaults,noatime,nodiratime,delalloc,nobarrier,data=writeback</span><br><span class="line">/dev/lvm1 /data    ext4    defaults,noatime,nodiratime,delalloc,nobarrier,data=writeback 0 0</span><br></pre></td></tr></table></figure>

<p><code>noatime</code> 读取文件时，将禁用对元数据的更新。它还启用了 nodiratime 行为，该行为会在读取目录时禁用对元数据的更新</p>
<p><code>nodelalloc</code> 参数是关闭了ext4的delayed  allocation 特性。所谓delayed allocation 是指，把磁盘block的分配推后到真正要写数据的时候，比如写入文件的时候，先写内存，当数据需要落盘的时候，再由文件系统分配磁盘块，这有利于文件系统做出更佳的磁盘块分配决策，比如可以分配大片连续的磁盘块。显然 nodelalloc 性能要差些</p>
<blockquote>
<p>delalloc吞吐高，但是偶发性延迟抖动，平均延迟略高<br>nodelalloc延迟稳定，但是吞吐会下降，偶发性会延迟剧烈抖动.</p>
</blockquote>
<p><code>nobarrier</code> 参数是不保证先写入文件系统日志然后才写入数据，也就是不保证系统崩溃后文件系统恢复的正确性,但是对写入性能有提升</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">参数说明：</span><br><span class="line">noatime：不更新文件系统上inode访问时间，可以提升性能。</span><br><span class="line">nobarrier：禁用用于文件系统的日志及数据完整性的写入操作，可以提高文件系统的性能。</span><br><span class="line">nodelalloc: 在数据从用户空间copy 到page cache 就分配block </span><br><span class="line">nobarrier: 关闭jbd中的写屏障，可以提升性能</span><br><span class="line">stripe: 条带大小，单位未block</span><br><span class="line">writeback:，则实际的数据下刷不会存在于jbd2的commit路径中，减少由于jbd2 commit transaction产生的延迟</span><br></pre></td></tr></table></figure>



<h3 id="优化case"><a href="#优化case" class="headerlink" title="优化case"></a>优化case</h3><p>10个GB的原始文件里面都是随机数，如何快速建索引支持分页查询top(k,n)场景，机器配置是24核，JVM堆内存限制2.5G，磁盘读写为490-500MB&#x2F;s左右。</p>
<p>最后成绩在22.9s，去掉评测方法引入的1.1s，5次查询含建索引总时间21.8s，因为读10GB文件就需要21.5s时间。当向SSD开始写索引文件后SSD读取性能下降厉害，实际期望的是写出索引到SSD的时候会被PageCache，没触发刷脏。但是这里的刷盘就是ext4挂载参数 ordered 导致了刷盘。</p>
<p>整个方案是：原始文件切割成小分片，喂给24个worker；每个worker读数据，处理数据，定期批量写索引出去；最后查询会去读每个worker生成的所有索引文件，通过跳表快速seek。</p>
<p><img src="/images/951413iMgBlog/586fef765e3f08f6183907f311a76259.png" alt="img"></p>
<h2 id="LVM性能对比"><a href="#LVM性能对比" class="headerlink" title="LVM性能对比"></a>LVM性能对比</h2><p>磁盘信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#lsblk</span><br><span class="line">NAME         MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</span><br><span class="line">sda            8:0    0 223.6G  0 disk</span><br><span class="line">├─sda1         8:1    0     3M  0 part</span><br><span class="line">├─sda2         8:2    0     1G  0 part /boot</span><br><span class="line">├─sda3         8:3    0    96G  0 part /</span><br><span class="line">├─sda4         8:4    0    10G  0 part /tmp</span><br><span class="line">└─sda5         8:5    0 116.6G  0 part /home</span><br><span class="line">nvme0n1      259:4    0   2.7T  0 disk</span><br><span class="line">└─nvme0n1p1  259:5    0   2.7T  0 part</span><br><span class="line">  └─vg1-drds 252:0    0   5.4T  0 lvm  /drds</span><br><span class="line">nvme1n1      259:0    0   2.7T  0 disk</span><br><span class="line">└─nvme1n1p1  259:2    0   2.7T  0 part /u02</span><br><span class="line">nvme2n1      259:1    0   2.7T  0 disk</span><br><span class="line">└─nvme2n1p1  259:3    0   2.7T  0 part</span><br><span class="line">  └─vg1-drds 252:0    0   5.4T  0 lvm  /drds</span><br></pre></td></tr></table></figure>

<p>单块nvme SSD盘跑mysql server，运行sysbench导入测试数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#iostat -x nvme1n1 1</span><br><span class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.32    0.00    0.17    0.07    0.00   99.44</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme1n1           0.00    47.19    0.19  445.15     2.03 43110.89   193.62     0.31    0.70    0.03    0.70   0.06   2.85</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.16    0.00    0.36    0.17    0.00   98.31</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme1n1           0.00   122.00    0.00 3290.00     0.00 271052.00   164.77     1.65    0.50    0.00    0.50   0.05  17.00</span><br><span class="line"></span><br><span class="line">#iostat 1</span><br><span class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.14    0.00    0.13    0.05    0.00   99.67</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda              49.21       554.51      2315.83    1416900    5917488</span><br><span class="line">nvme1n1           5.65         2.34       844.73       5989    2158468</span><br><span class="line">nvme2n1           0.06         1.13         0.00       2896          0</span><br><span class="line">nvme0n1           0.06         1.13         0.00       2900          0</span><br><span class="line">dm-0              0.02         0.41         0.00       1036          0</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.39    0.00    0.23    0.08    0.00   98.30</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda               8.00         0.00        60.00          0         60</span><br><span class="line">nvme1n1         868.00         0.00    132100.00          0     132100</span><br><span class="line">nvme2n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme0n1           0.00         0.00         0.00          0          0</span><br><span class="line">dm-0              0.00         0.00         0.00          0          0</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.44    0.00    0.14    0.09    0.00   98.33</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda               0.00         0.00         0.00          0          0</span><br><span class="line">nvme1n1         766.00         0.00    132780.00          0     132780</span><br><span class="line">nvme2n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme0n1           0.00         0.00         0.00          0          0</span><br><span class="line">dm-0              0.00         0.00         0.00          0          0</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.41    0.00    0.16    0.09    0.00   98.34</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda             105.00         0.00       532.00          0        532</span><br><span class="line">nvme1n1         760.00         0.00    122236.00          0     122236</span><br><span class="line">nvme2n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme0n1           0.00         0.00         0.00          0          0</span><br><span class="line">dm-0              0.00         0.00         0.00          0          0</span><br></pre></td></tr></table></figure>

<p>如果同样写lvm，由两块nvme组成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">nvme0n1           0.00   137.00    0.00 5730.00     0.00 421112.00   146.98     2.95    0.52    0.00    0.52   0.05  27.30</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.17    0.00    0.34    0.19    0.00   98.30</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">nvme0n1           0.00   109.00    0.00 2533.00     0.00 271236.00   214.16     1.08    0.43    0.00    0.43   0.06  15.90</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.38    0.00    0.42    0.20    0.00   98.00</span><br><span class="line"></span><br><span class="line">Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span><br><span class="line">nvme2n1           0.00     0.00    0.00    0.00     0.00     0.00     0.00     0.00    0.00    0.00    0.00   0.00   0.00</span><br><span class="line">nvme0n1           0.00   118.00    0.00 3336.00     0.00 320708.00   192.27     1.50    0.45    0.00    0.45   0.06  20.00</span><br><span class="line"></span><br><span class="line">[root@k28a11352.eu95sqa /var/lib]</span><br><span class="line">#iostat  1</span><br><span class="line">Linux 3.10.0-327.ali2017.alios7.x86_64 (k28a11352.eu95sqa) 	05/13/2021 	_x86_64_	(64 CPU)</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           0.40    0.00    0.20    0.07    0.00   99.33</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda              38.96       334.64      1449.68    1419236    6148304</span><br><span class="line">nvme1n1         324.95         1.43     31201.30       6069  132329072</span><br><span class="line">nvme2n1           0.07         0.90         0.00       3808          0</span><br><span class="line">nvme0n1         256.24         1.60     22918.46       6801   97200388</span><br><span class="line">dm-0            266.98         1.38     22918.46       5849   97200388</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.20    0.00    0.42    0.25    0.00   98.12</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda               0.00         0.00         0.00          0          0</span><br><span class="line">nvme1n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme2n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme0n1        4460.00         0.00    332288.00          0     332288</span><br><span class="line">dm-0           4608.00         0.00    332288.00          0     332288</span><br><span class="line"></span><br><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">           1.35    0.00    0.38    0.22    0.00   98.06</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda              48.00         0.00       200.00          0        200</span><br><span class="line">nvme1n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme2n1           0.00         0.00         0.00          0          0</span><br><span class="line">nvme0n1        4187.00         0.00    332368.00          0     332368</span><br><span class="line">dm-0           4348.00         0.00    332368.00          0     332368</span><br></pre></td></tr></table></figure>

<h2 id="数据总结"><a href="#数据总结" class="headerlink" title="数据总结"></a>数据总结</h2><ul>
<li>性能排序 NVMe SSD &gt; SATA SSD &gt; SAN &gt; ESSD &gt; HDD</li>
<li>本地ssd性能最好、sas机械盘(RAID10)性能最差</li>
<li>san存储走特定的光纤网络，不是走tcp的san（至少从网卡看不到san的流量），性能居中</li>
<li>从rt来看 ssd:san:sas 大概是 1:3:15</li>
<li>san比本地sas机械盘性能要好，这也许取决于san的网络传输性能和san存储中的设备（比如用的ssd而不是机械盘）</li>
<li>NVMe SSD比SATA SSD快很多，latency更稳定</li>
<li>阿里云的云盘ESSD比本地SAS RAID10阵列性能还好</li>
<li>软RAID、LVM等阵列都会导致性能损耗，即使多盘一起读写也不如单盘性能</li>
<li>不同测试场景(4K&#x2F;8K&#x2F; 读写、随机与否)会导致不同品牌性能数据差异较大</li>
</ul>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/d5389994fad1">smartctl</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//raid 阵列查看</span><br><span class="line">smartctl --all /dev/sda -d megaraid,1</span><br></pre></td></tr></table></figure>



<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="http://cizixs.com/2017/01/03/how-slow-is-disk-and-network">http://cizixs.com/2017/01/03/how-slow-is-disk-and-network</a></p>
<p><a target="_blank" rel="noopener" href="https://tobert.github.io/post/2014-04-17-fio-output-explained.html">https://tobert.github.io/post/2014-04-17-fio-output-explained.html</a> </p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40497397">https://zhuanlan.zhihu.com/p/40497397</a></p>
<p><a target="_blank" rel="noopener" href="https://linux.die.net/man/1/fio">https://linux.die.net/man/1/fio</a></p>
<p><a target="_blank" rel="noopener" href="https://www.atatech.org/articles/167736?spm=ata.home.0.0.11fd75362qwsg7&flag_data_from=home_algorithm_article">块存储NVMe云盘原型实践</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247483999&idx=1&sn=238d3d1a8cf24443db0da4aa00c9fb7e&chksm=a6e3036491948a72704e0b114790483f227b7ce82f5eece5dd870ef88a8391a03eca27e8ff61&scene=178&cur_album_id=1371808335259090944#rd">机械硬盘随机IO慢的超乎你的想象</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484023&idx=1&sn=1946b4c286ed72da023b402cc30908b6&chksm=a6e3034c91948a5aa3b0e6beb31c1d3804de9a11c668400d598c2a6b12462e179cf9f1dc33e2&scene=178&cur_album_id=1371808335259090944#rd">搭载固态硬盘的服务器究竟比搭机械硬盘快多少？</a></p>
<p><a target="_blank" rel="noopener" href="http://www.360doc.com/content/15/0318/15/16824943_456186965.shtml">SSD基本工作原理</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/347599423">SSD原理解读</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzAxNDI5NzEzNg==&mid=2651171913&idx=1&sn=68f658c539edc2b5063d6d15d0bfa0cf">Linux 后台开发必知的 I&#x2F;O 优化知识总结</a></p>
<p><a target="_blank" rel="noopener" href="https://www.sohu.com/a/390625596_505795">SSD性能怎么测？看这一篇就够了</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/19/kubernetes%20calico%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/19/kubernetes%20calico%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">kubernetes calico网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-19 11:30:03" itemprop="dateCreated datePublished" datetime="2022-01-19T11:30:03+08:00">2022-01-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-calico网络"><a href="#kubernetes-calico网络" class="headerlink" title="kubernetes calico网络"></a>kubernetes calico网络</h1><h2 id="cni-网络"><a href="#cni-网络" class="headerlink" title="cni 网络"></a>cni 网络</h2><blockquote>
<p> <strong>cni0</strong> is a Linux network bridge device, all <strong>veth</strong> devices will connect to this bridge, so all Pods on the same node can communicate with each other, as explained in <strong>Kubernetes Network Model</strong> and the hotel analogy above.</p>
</blockquote>
<h3 id="cni（Container-Network-Interface）"><a href="#cni（Container-Network-Interface）" class="headerlink" title="cni（Container Network Interface）"></a>cni（Container Network Interface）</h3><p>CNI 全称为 Container Network Interface，是用来定义容器网络的一个 <a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni/blob/master/SPEC.md">规范</a>。<a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni">containernetworking&#x2F;cni</a> 是一个 CNCF 的 CNI 实现项目，包括基本额 bridge,macvlan等基本网络插件。</p>
<p>一般将cni各种网络插件的可执行文件二进制放到 <code>/opt/cni/bin</code> ，在 <code>/etc/cni/net.d/</code> 下创建配置文件，剩下的就交给 K8s 或者 containerd 了，我们不关心也不了解其实现。</p>
<p>比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#ls -lh /opt/cni/bin/</span><br><span class="line">总用量 90M</span><br><span class="line">-rwxr-x--- 1 root root 4.0M 12月 23 09:39 bandwidth</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico-ipam</span><br><span class="line">-rwxr-x--- 1 root root 3.0M 12月 23 09:39 flannel</span><br><span class="line">-rwxr-x--- 1 root root 3.5M 12月 23 09:39 host-local</span><br><span class="line">-rwxr-x--- 1 root root 3.1M 12月 23 09:39 loopback</span><br><span class="line">-rwxr-x--- 1 root root 3.8M 12月 23 09:39 portmap</span><br><span class="line">-rwxr-x--- 1 root root 3.3M 12月 23 09:39 tuning</span><br><span class="line"></span><br><span class="line">[root@hygon3 15:55 /root]</span><br><span class="line">#ls -lh /etc/cni/net.d/</span><br><span class="line">总用量 12K</span><br><span class="line">-rw-r--r-- 1 root root  607 12月 23 09:39 10-calico.conflist</span><br><span class="line">-rw-r----- 1 root root  292 12月 23 09:47 10-flannel.conflist</span><br><span class="line">-rw------- 1 root root 2.6K 12月 23 09:39 calico-kubeconfig</span><br></pre></td></tr></table></figure>

<p>CNI 插件都是直接通过 exec 的方式调用，而不是通过 socket 这样 C&#x2F;S 方式，所有参数都是通过环境变量、标准输入输出来实现的。</p>
<p>Step-by-step communication from <strong>Pod 1</strong> to <strong>Pod 6</strong>:</p>
<ol>
<li><em>Package leaves</em> *<strong>Pod 1 netns*</strong> <em>through the</em> *<strong>eth1*</strong> <em>interface and reaches the</em> <em><strong>root netns*</strong> <em>through the virtual interface</em> <em><strong>veth1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>veth1*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> <em><strong>cni0*</strong> <em>and is redirected to</em> <em><strong>eth0*</strong></em>;</em></li>
<li><em>Package leaves</em> *<strong>eth0*</strong> <em>from</em> <em><strong>Master 1*</strong> <em>and reaches the</em> <em><strong>gateway*</strong></em>;</em></li>
<li><em>Package leaves the</em> *<strong>gateway*</strong> <em>and reaches the</em> *<strong>root netns*</strong> <em>through the</em> <em><strong>eth0*</strong> <em>interface on</em> <em><strong>Worker 1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>eth0*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> *<strong>cni0*</strong> <em>and is redirected to the</em> *<strong>veth6*</strong> <em>virtual interface;</em></li>
<li><em>Package leaves the</em> *<strong>root netns*</strong> <em>through</em> *<strong>veth6*</strong> <em>and reaches the</em> *<strong>Pod 6 netns*</strong> <em>though the</em> *<strong>eth6*</strong> <em>interface;</em></li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115124747936.png" alt="image-20220115124747936"></p>
<h2 id="kubernetes-calico-网络"><a href="#kubernetes-calico-网络" class="headerlink" title="kubernetes calico 网络"></a>kubernetes calico 网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line">#或者老版本的calico</span><br><span class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</span><br></pre></td></tr></table></figure>

<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>跨宿主机的两个容器之间的流量链路是：</p>
<blockquote>
<p>cali-容器eth0-&gt;宿主机cali27dce37c0e8-&gt;tunl0-&gt;内核ipip模块封包-&gt;物理网卡（ipip封包后）—远程–&gt; 物理网卡-&gt;内核ipip模块解包-&gt;tunl0-&gt;cali-容器</p>
</blockquote>
<p><img src="/images/oss/a1767a5f2cbc2c48c1a35da9f3232a2c.png" alt="image.png"></p>
<p>Calico IPIP模式对物理网络无侵入，符合云原生容器网络要求；使用IPIP封包，性能略低于Calico BGP模式；无法使用传统防火墙管理、也无法和存量网络直接打通。Pod在Node做SNAT访问外部，Pod流量不易被监控。</p>
<h2 id="calico-ipip网络不通"><a href="#calico-ipip网络不通" class="headerlink" title="calico ipip网络不通"></a>calico ipip网络不通</h2><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，部分节点之间不通。每台机器部署好calico网络后，会分配一个 &#x2F;26 CIRD 子网（64个ip）。</p>
<h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h3><p>目标机是10.122.127.128（宿主机ip 192.168.3.112），如果从10.122.17.64（宿主机ip 192.168.3.110） ping 10.122.127.128不通，查看10.122.127.128路由表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 10.122.127.128 dev tunl0  //这条路由不通</span><br><span class="line">[root@az3-k8s-13 ~]# ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br><span class="line"></span><br><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink //这样就通了 </span><br></pre></td></tr></table></figure>

<p>在10.122.127.128抓包如下，明显可以看到icmp request到了 tunl0网卡，tunl0网卡也回复了，但是回复包没有经过kernel ipip模块封装后发到eth1上：</p>
<p><img src="/images/oss/d3111417ce646ca1475def5bea01e6b9.png" alt="image.png"></p>
<p>正常机器应该是这样，上图不正常的时候缺少红框中的reply：</p>
<p><img src="/images/oss/9ea9041af1211b2a5b8de4e216044465.png" alt="image.png"></p>
<p>解决：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; </span><br><span class="line">ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br></pre></td></tr></table></figure>

<p>删除错误路由增加新的路由就可以了，新增路由的意思是从tunl0发给10.122.17.64&#x2F;26的包下一跳是 192.168.3.110。</p>
<p> via 192.168.3.110 表示下一跳的ip</p>
<p>onlink参数的作用：<br>使用这个参数将会告诉内核，不必检查网关是否可达。因为在linux内核中，网关与本地的网段不同是被认为不可达的，从而拒绝执行添加路由的操作。</p>
<p>因为tunl0网卡ip的 CIDR 是32，也就是不属于任何子网，那么这个网卡上的路由没有网关，配置路由的话必须是onlink, 内核存也没法根据子网来选择到这块网卡，所以还会加上 dev 指定网卡。</p>
<h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h3><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，只有node2没有192.168.3.111这个ip，结果node2跟其他节点都不通：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| 192.168.0.111 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.112 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.113 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.114 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br></pre></td></tr></table></figure>

<p>从node4 ping node2，然后在node2上抓包，可以看到 icmp request都发到了node2上，但是node2收到后没有发给tunl0：</p>
<p><img src="/images/oss/16fda9322e9a59c37c11629acc611bf3.png" alt="image.png"></p>
<p>所以icmp没有回复，这里的问题在于<strong>kernel收到包后为什么不给tunl0</strong></p>
<p>同样，在node2上ping node4，同时在node2上抓包，可以看到发给node4的request包和reply包：</p>
<p><img src="/images/oss/c6d1706b6f8162cfac528ddf5319c8e2.png" alt="image.png"></p>
<p>从request包可以看到src ip 是0.111， dest ip是 3.113，<strong>因为 node2 没有192.168.3.111这个ip</strong></p>
<p>非常关键的我们看到node4的回复包 src ip 不是3.113，而是0.113（根据node4的路由就应该是0.113）</p>
<p><img src="/images/oss/5c7172e2422579eb99c66e881d47bf99.png" alt="image.png"></p>
<p>这就是问题所在，从node4过来的ipip包src ip都是0.113，实际这里ipip能认识的只是3.113. </p>
<p>如果这个时候在3.113机器上把0.113网卡down掉，那么3.113上的：</p>
<p>10.122.124.128&#x2F;26 via 192.168.0.111 dev tunl0 proto bird onlink 路由被自动删除，3.113将不再回复request。这是因为calico记录的node2的ip是192.168.0.111，所以会自动增加</p>
<p>解决办法，在node4上删除这条路由记录，也就是强制让回复包走3.113网卡，这样收发的ip就能对应上了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ip route del 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.113</span><br><span class="line">//同时将默认路由改到3.113</span><br><span class="line">ip route del default via 192.168.0.253 dev eth0; </span><br><span class="line">ip route add default via 192.168.3.253 dev eth1</span><br></pre></td></tr></table></figure>

<p>最终OK后，node4上的ip route是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-14 ~]# ip route</span><br><span class="line">default via 192.168.3.253 dev eth1 </span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink </span><br><span class="line">10.122.124.128/26 via 192.168.0.111 dev tunl0 proto bird onlink </span><br><span class="line">10.122.127.128/26 via 192.168.3.112 dev tunl0 proto bird onlink </span><br><span class="line">blackhole 10.122.157.128/26 proto bird </span><br><span class="line">10.122.157.129 dev cali19f6ea143e3 scope link </span><br><span class="line">10.122.157.130 dev cali09e016ead53 scope link </span><br><span class="line">10.122.157.131 dev cali0ad3225816d scope link </span><br><span class="line">10.122.157.132 dev cali55a5ff1a4aa scope link </span><br><span class="line">10.122.157.133 dev cali01cf8687c65 scope link </span><br><span class="line">10.122.157.134 dev cali65232d7ada6 scope link </span><br><span class="line">10.122.173.128/26 via 192.168.3.114 dev tunl0 proto bird onlink </span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 </span><br><span class="line">192.168.3.0/24 dev eth1 proto kernel scope link src 192.168.3.113</span><br></pre></td></tr></table></figure>

<p>正常后的抓包, 注意这里drequest的est ip 和reply的 src ip终于一致了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//request</span><br><span class="line">00:16:3e:02:06:1e &gt; ee:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 57971, offset 0, flags [DF], proto IPIP (4), length 104)</span><br><span class="line">    192.168.0.111 &gt; 192.168.3.110: (tos 0x0, ttl 64, id 18953, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.122.124.128 &gt; 10.122.17.64: ICMP echo request, id 22001, seq 4, length 64</span><br><span class="line">    </span><br><span class="line">//reply    </span><br><span class="line">ee:ff:ff:ff:ff:ff &gt; 00:16:3e:02:06:1e, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 2565, offset 0, flags [none], proto IPIP (4), length 104)</span><br><span class="line">    192.168.3.110 &gt; 192.168.0.111: (tos 0x0, ttl 64, id 26374, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.122.17.64 &gt; 10.122.124.128: ICMP echo reply, id 22001, seq 4, length 64</span><br></pre></td></tr></table></figure>

<p>总结下来这两个案例都还是对路由不够了解，特别是案例2，因为有了多个网卡后导致路由更复杂。calico ipip的基本原理就是利用内核进行ipip封包，然后修改路由来保证网络的畅通。</p>
<h2 id="netns-操作"><a href="#netns-操作" class="headerlink" title="netns 操作"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw">netns 操作</a></h2><p>以下case创建一个名为 ren 的netns，然后在里面增加一对虚拟网卡veth1 veth1_p,  veth1放置在ren里面，veth1_p 放在物理机上，给他们配置上ip并up就能通了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"> 1004  [2021-10-27 10:49:08] ip netns add ren</span><br><span class="line"> 1005  [2021-10-27 10:49:12] ip netns show</span><br><span class="line"> 1006  [2021-10-27 10:49:22] ip netns exec ren route   //为空</span><br><span class="line"> 1007  [2021-10-27 10:49:29] ip netns exec ren iptables -L</span><br><span class="line"> 1008  [2021-10-27 10:49:55] ip link add veth1 type veth peer name veth1_p //此时宿主机上能看到这两块网卡</span><br><span class="line"> 1009  [2021-10-27 10:50:07] ip link set veth1 netns ren //将veth1从宿主机默认网络空间挪到ren中，宿主机中看不到veth1了</span><br><span class="line"> 1010  [2021-10-27 10:50:18] ip netns exec ren route  </span><br><span class="line"> 1011  [2021-10-27 10:50:25] ip netns exec ren iptables -L</span><br><span class="line"> 1012  [2021-10-27 10:50:39] ifconfig</span><br><span class="line"> 1013  [2021-10-27 10:50:51] ip link list</span><br><span class="line"> 1014  [2021-10-27 10:51:29] ip netns exec ren ip link list</span><br><span class="line"> 1017  [2021-10-27 10:53:27] ip netns exec ren ip addr add 172.19.0.100/24 dev veth1 </span><br><span class="line"> 1018  [2021-10-27 10:53:31] ip netns exec ren ip link list</span><br><span class="line"> 1019  [2021-10-27 10:53:39] ip netns exec ren ifconfig</span><br><span class="line"> 1020  [2021-10-27 10:53:42] ip netns exec ren ifconfig -a</span><br><span class="line"> 1021  [2021-10-27 10:54:13] ip netns exec ren ip link set dev veth1 up</span><br><span class="line"> 1022  [2021-10-27 10:54:16] ip netns exec ren ifconfig</span><br><span class="line"> 1023  [2021-10-27 10:54:22] ping 172.19.0.100</span><br><span class="line"> 1024  [2021-10-27 10:54:35] ifconfig -a</span><br><span class="line"> 1025  [2021-10-27 10:55:03] ip netns exec ren ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line"> 1026  [2021-10-27 10:55:10] ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line"> 1027  [2021-10-27 10:55:16] ifconfig veth1_p</span><br><span class="line"> 1028  [2021-10-27 10:55:30] ip link set dev veth1_p up</span><br><span class="line"> 1029  [2021-10-27 10:55:32] ifconfig veth1_p</span><br><span class="line"> 1030  [2021-10-27 10:55:38] ping 172.19.0.101</span><br><span class="line"> 1031  [2021-10-27 10:55:43] ping 172.19.0.100</span><br><span class="line"> 1032  [2021-10-27 10:55:53] ip link set dev veth1_p down</span><br><span class="line"> 1033  [2021-10-27 10:55:54] ping 172.19.0.100</span><br><span class="line"> 1034  [2021-10-27 10:55:58] ping 172.19.0.101</span><br><span class="line"> 1035  [2021-10-27 10:56:08] ifconfig veth1_p</span><br><span class="line"> 1036  [2021-10-27 10:56:32] ping 172.19.0.101</span><br><span class="line"> 1037  [2021-10-27 10:57:04] ip netns exec ren route</span><br><span class="line"> 1038  [2021-10-27 10:57:52] ip netns exec ren ping 172.19.0.101</span><br><span class="line"> 1039  [2021-10-27 10:57:58] ip link set dev veth1_p up</span><br><span class="line"> 1040  [2021-10-27 10:57:59] ip netns exec ren ping 172.19.0.101</span><br><span class="line"> 1041  [2021-10-27 10:58:06] ip netns exec ren ping 172.19.0.100</span><br><span class="line"> 1042  [2021-10-27 10:58:14] ip netns exec ren ifconfig</span><br><span class="line"> 1043  [2021-10-27 10:58:19] ip netns exec ren route</span><br><span class="line"> 1044  [2021-10-27 10:58:26] ip netns exec ren ping 172.19.0.100 -I veth1</span><br><span class="line"> 1045  [2021-10-27 10:58:58] ifconfig veth1_p</span><br><span class="line"> 1046  [2021-10-27 10:59:10] ping 172.19.0.100</span><br><span class="line"> 1047  [2021-10-27 10:59:26] ip netns exec ren ping 172.19.0.101 -I veth1</span><br><span class="line"> </span><br><span class="line"> 把网卡加入到docker0的bridge下</span><br><span class="line"> 1160  [2021-10-27 12:17:37] brctl show</span><br><span class="line"> 1161  [2021-10-27 12:18:05] ip link set dev veth3_p master docker0</span><br><span class="line"> 1162  [2021-10-27 12:18:09] ip link set dev veth1_p master docker0</span><br><span class="line"> 1163  [2021-10-27 12:18:13] ip link set dev veth2 master docker0</span><br><span class="line"> 1164  [2021-10-27 12:18:15] brctl show</span><br><span class="line"> </span><br><span class="line">brctl showmacs br0</span><br><span class="line">brctl show cni0</span><br><span class="line">brctl addif cni0 veth1 veth2 veth3  //往cni bridge添加多个容器peer 网卡</span><br></pre></td></tr></table></figure>

<p>Linux 上存在一个默认的网络命名空间，Linux 中的 1 号进程初始使用该默认空间。Linux 上其它所有进程都是由 1 号进程派生出来的，在派生 clone 的时候如果没有额外特别指定，所有的进程都将共享这个默认网络空间。</p>
<p>所有的网络设备刚创建出来都是在宿主机默认网络空间下的。可以通过 <code>ip link set 设备名 netns 网络空间名</code> 将设备移动到另外一个空间里去，socket也是归属在某一个网络命名空间下的，由创建socket进程所在的netns来决定socket所在的netns</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/socket.c</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sock_create</span><span class="params">(<span class="type">int</span> family, <span class="type">int</span> type, <span class="type">int</span> protocol, <span class="keyword">struct</span> socket **res)</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">return</span> __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: include/net/sock.h</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sock_net_set</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="keyword">struct</span> net *net)</span></span><br><span class="line">&#123;</span><br><span class="line"> write_pnet(&amp;sk-&gt;sk_net, net);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>内核提供了三种操作命名空间的方式，分别是 clone、setns 和 unshare。ip netns add 使用的是 unshare，原理和 clone 是类似的。</p>
<p><img src="/images/951413iMgBlog/640-5304524." alt="Image"></p>
<p>每个 net 下都包含了自己的路由表、iptable 以及内核参数配置等等</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://morven.life/notes/networking-3-ipip/">https://morven.life/notes/networking-3-ipip/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bakari/p/10564347.html">https://www.cnblogs.com/bakari/p/10564347.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goldsunshine/p/10701242.html">https://www.cnblogs.com/goldsunshine/p/10701242.html</a></p>
<p><a target="_blank" rel="noopener" href="https://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html">手工拉起flannel网络</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/19/kubernetes_calico%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/19/kubernetes_calico%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">kubernetes calico网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-19 11:30:03" itemprop="dateCreated datePublished" datetime="2022-01-19T11:30:03+08:00">2022-01-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:11:18" itemprop="dateModified" datetime="2025-11-29T15:11:18+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-calico网络"><a href="#kubernetes-calico网络" class="headerlink" title="kubernetes calico网络"></a>kubernetes calico网络</h1><h2 id="cni-网络"><a href="#cni-网络" class="headerlink" title="cni 网络"></a>cni 网络</h2><blockquote>
<p> <strong>cni0</strong> is a Linux network bridge device, all <strong>veth</strong> devices will connect to this bridge, so all Pods on the same node can communicate with each other, as explained in <strong>Kubernetes Network Model</strong> and the hotel analogy above.</p>
</blockquote>
<h3 id="cni（Container-Network-Interface）"><a href="#cni（Container-Network-Interface）" class="headerlink" title="cni（Container Network Interface）"></a>cni（Container Network Interface）</h3><p>CNI 全称为 Container Network Interface，是用来定义容器网络的一个 <a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni/blob/master/SPEC.md">规范</a>。<a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni">containernetworking&#x2F;cni</a> 是一个 CNCF 的 CNI 实现项目，包括基本额 bridge,macvlan等基本网络插件。</p>
<p>一般将cni各种网络插件的可执行文件二进制放到 <code>/opt/cni/bin</code> ，在 <code>/etc/cni/net.d/</code> 下创建配置文件，剩下的就交给 K8s 或者 containerd 了，我们不关心也不了解其实现。</p>
<p>比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#ls -lh /opt/cni/bin/</span><br><span class="line">总用量 90M</span><br><span class="line">-rwxr-x--- 1 root root 4.0M 12月 23 09:39 bandwidth</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico-ipam</span><br><span class="line">-rwxr-x--- 1 root root 3.0M 12月 23 09:39 flannel</span><br><span class="line">-rwxr-x--- 1 root root 3.5M 12月 23 09:39 host-local</span><br><span class="line">-rwxr-x--- 1 root root 3.1M 12月 23 09:39 loopback</span><br><span class="line">-rwxr-x--- 1 root root 3.8M 12月 23 09:39 portmap</span><br><span class="line">-rwxr-x--- 1 root root 3.3M 12月 23 09:39 tuning</span><br><span class="line"></span><br><span class="line">[root@hygon3 15:55 /root]</span><br><span class="line">#ls -lh /etc/cni/net.d/</span><br><span class="line">总用量 12K</span><br><span class="line">-rw-r--r-- 1 root root  607 12月 23 09:39 10-calico.conflist</span><br><span class="line">-rw-r----- 1 root root  292 12月 23 09:47 10-flannel.conflist</span><br><span class="line">-rw------- 1 root root 2.6K 12月 23 09:39 calico-kubeconfig</span><br></pre></td></tr></table></figure>

<p>CNI 插件都是直接通过 exec 的方式调用，而不是通过 socket 这样 C&#x2F;S 方式，所有参数都是通过环境变量、标准输入输出来实现的。</p>
<p>Step-by-step communication from <strong>Pod 1</strong> to <strong>Pod 6</strong>:</p>
<ol>
<li><em>Package leaves</em> *<strong>Pod 1 netns*</strong> <em>through the</em> *<strong>eth1*</strong> <em>interface and reaches the</em> <em><strong>root netns*</strong> <em>through the virtual interface</em> <em><strong>veth1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>veth1*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> <em><strong>cni0*</strong> <em>and is redirected to</em> <em><strong>eth0*</strong></em>;</em></li>
<li><em>Package leaves</em> *<strong>eth0*</strong> <em>from</em> <em><strong>Master 1*</strong> <em>and reaches the</em> <em><strong>gateway*</strong></em>;</em></li>
<li><em>Package leaves the</em> *<strong>gateway*</strong> <em>and reaches the</em> *<strong>root netns*</strong> <em>through the</em> <em><strong>eth0*</strong> <em>interface on</em> <em><strong>Worker 1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>eth0*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> *<strong>cni0*</strong> <em>and is redirected to the</em> *<strong>veth6*</strong> <em>virtual interface;</em></li>
<li><em>Package leaves the</em> *<strong>root netns*</strong> <em>through</em> *<strong>veth6*</strong> <em>and reaches the</em> *<strong>Pod 6 netns*</strong> <em>though the</em> *<strong>eth6*</strong> <em>interface;</em></li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115124747936.png" alt="image-20220115124747936"></p>
<h2 id="kubernetes-calico-网络"><a href="#kubernetes-calico-网络" class="headerlink" title="kubernetes calico 网络"></a>kubernetes calico 网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line">#或者老版本的calico</span><br><span class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</span><br></pre></td></tr></table></figure>

<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>跨宿主机的两个容器之间的流量链路是：</p>
<blockquote>
<p>cali-容器eth0-&gt;宿主机cali27dce37c0e8-&gt;tunl0-&gt;内核ipip模块封包-&gt;物理网卡（ipip封包后）—远程–&gt; 物理网卡-&gt;内核ipip模块解包-&gt;tunl0-&gt;cali-容器</p>
</blockquote>
<p><img src="/images/oss/a1767a5f2cbc2c48c1a35da9f3232a2c.png" alt="image.png"></p>
<p>Calico IPIP模式对物理网络无侵入，符合云原生容器网络要求；使用IPIP封包，性能略低于Calico BGP模式；无法使用传统防火墙管理、也无法和存量网络直接打通。Pod在Node做SNAT访问外部，Pod流量不易被监控。</p>
<p><img src="/images/951413iMgBlog/yyb9c0ee93730542ebb5475a734991c7.jpg" alt="img"></p>
<h2 id="calico-ipip网络不通"><a href="#calico-ipip网络不通" class="headerlink" title="calico ipip网络不通"></a>calico ipip网络不通</h2><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，部分节点之间不通。每台机器部署好calico网络后，会分配一个 &#x2F;26 CIRD 子网（64个ip）。</p>
<h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h3><p>目标机是10.122.127.128（宿主机ip 192.168.3.112），如果从10.122.17.64（宿主机ip 192.168.3.110） ping 10.122.127.128不通，查看10.122.127.128路由表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 10.122.127.128 dev tunl0  //这条路由不通</span><br><span class="line">[root@az3-k8s-13 ~]# ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br><span class="line"></span><br><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink //这样就通了 </span><br></pre></td></tr></table></figure>

<p>在10.122.127.128抓包如下，明显可以看到icmp request到了 tunl0网卡，tunl0网卡也回复了，但是回复包没有经过kernel ipip模块封装后发到eth1上：</p>
<p><img src="/images/oss/d3111417ce646ca1475def5bea01e6b9.png" alt="image.png"></p>
<p>正常机器应该是这样，上图不正常的时候缺少红框中的reply：</p>
<p><img src="/images/oss/9ea9041af1211b2a5b8de4e216044465.png" alt="image.png"></p>
<p>解决：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; </span><br><span class="line">ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br></pre></td></tr></table></figure>

<p>删除错误路由增加新的路由就可以了，新增路由的意思是从tunl0发给10.122.17.64&#x2F;26的包下一跳是 192.168.3.110。</p>
<p> via 192.168.3.110 表示下一跳的ip</p>
<p>onlink参数的作用：<br>使用这个参数将会告诉内核，不必检查网关是否可达。因为在linux内核中，网关与本地的网段不同是被认为不可达的，从而拒绝执行添加路由的操作。</p>
<p>因为tunl0网卡ip的 CIDR 是32，也就是不属于任何子网，那么这个网卡上的路由没有网关，配置路由的话必须是onlink, 内核存也没法根据子网来选择到这块网卡，所以还会加上 dev 指定网卡。</p>
<h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h3><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，只有node2没有192.168.3.111这个ip，结果node2跟其他节点都不通：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| 192.168.0.111 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.112 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.113 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.114 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br></pre></td></tr></table></figure>

<p>从node4 ping node2，然后在node2上抓包，可以看到 icmp request都发到了node2上，但是node2收到后没有发给tunl0：</p>
<p><img src="/images/oss/16fda9322e9a59c37c11629acc611bf3.png" alt="image.png"></p>
<p>所以icmp没有回复，这里的问题在于<strong>kernel收到包后为什么不给tunl0</strong></p>
<p>同样，在node2上ping node4，同时在node2上抓包，可以看到发给node4的request包和reply包：</p>
<p><img src="/images/oss/c6d1706b6f8162cfac528ddf5319c8e2.png" alt="image.png"></p>
<p>从request包可以看到src ip 是0.111， dest ip是 3.113，<strong>因为 node2 没有192.168.3.111这个ip</strong></p>
<p>非常关键的我们看到node4的回复包 src ip 不是3.113，而是0.113（根据node4的路由就应该是0.113）</p>
<p><img src="/images/oss/5c7172e2422579eb99c66e881d47bf99.png" alt="image.png"></p>
<p>这就是问题所在，从node4过来的ipip包src ip都是0.113，实际这里ipip能认识的只是3.113. </p>
<p>如果这个时候在3.113机器上把0.113网卡down掉，那么3.113上的：</p>
<p>10.122.124.128&#x2F;26 via 192.168.0.111 dev tunl0 proto bird onlink 路由被自动删除，3.113将不再回复request。这是因为calico记录的node2的ip是192.168.0.111，所以会自动增加</p>
<p>解决办法，在node4上删除这条路由记录，也就是强制让回复包走3.113网卡，这样收发的ip就能对应上了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ip route del 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.113</span><br><span class="line">//同时将默认路由改到3.113</span><br><span class="line">ip route del default via 192.168.0.253 dev eth0; </span><br><span class="line">ip route add default via 192.168.3.253 dev eth1</span><br></pre></td></tr></table></figure>

<p>最终OK后，node4上的ip route是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-14 ~]# ip route</span><br><span class="line">default via 192.168.3.253 dev eth1 </span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink </span><br><span class="line">10.122.124.128/26 via 192.168.0.111 dev tunl0 proto bird onlink </span><br><span class="line">10.122.127.128/26 via 192.168.3.112 dev tunl0 proto bird onlink </span><br><span class="line">blackhole 10.122.157.128/26 proto bird </span><br><span class="line">10.122.157.129 dev cali19f6ea143e3 scope link </span><br><span class="line">10.122.157.130 dev cali09e016ead53 scope link </span><br><span class="line">10.122.157.131 dev cali0ad3225816d scope link </span><br><span class="line">10.122.157.132 dev cali55a5ff1a4aa scope link </span><br><span class="line">10.122.157.133 dev cali01cf8687c65 scope link </span><br><span class="line">10.122.157.134 dev cali65232d7ada6 scope link </span><br><span class="line">10.122.173.128/26 via 192.168.3.114 dev tunl0 proto bird onlink </span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 </span><br><span class="line">192.168.3.0/24 dev eth1 proto kernel scope link src 192.168.3.113</span><br></pre></td></tr></table></figure>

<p>正常后的抓包, 注意这里reques dest ip 和reply的 src ip终于一致了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//request</span><br><span class="line">00:16:3e:02:06:1e &gt; ee:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 57971, offset 0, flags [DF], proto IPIP (4), length 104)</span><br><span class="line">    192.168.0.111 &gt; 192.168.3.110: (tos 0x0, ttl 64, id 18953, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.122.124.128 &gt; 10.122.17.64: ICMP echo request, id 22001, seq 4, length 64</span><br><span class="line">    </span><br><span class="line">//reply    </span><br><span class="line">ee:ff:ff:ff:ff:ff &gt; 00:16:3e:02:06:1e, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 2565, offset 0, flags [none], proto IPIP (4), length 104)</span><br><span class="line">    192.168.3.110 &gt; 192.168.0.111: (tos 0x0, ttl 64, id 26374, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.122.17.64 &gt; 10.122.124.128: ICMP echo reply, id 22001, seq 4, length 64</span><br></pre></td></tr></table></figure>

<p>总结下来这两个案例都还是对路由不够了解，特别是案例2，因为有了多个网卡后导致路由更复杂。calico ipip的基本原理就是利用内核进行ipip封包，然后修改路由来保证网络的畅通。</p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>如下图，172.16.40.116是宿主机ip，192.168.196.0 是tunl0 ip</p>
<p><img src="/images/951413iMgBlog/image-20230531141428895.png" alt="image-20230531141428895"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://morven.life/notes/networking-3-ipip/">https://morven.life/notes/networking-3-ipip/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bakari/p/10564347.html">https://www.cnblogs.com/bakari/p/10564347.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goldsunshine/p/10701242.html">https://www.cnblogs.com/goldsunshine/p/10701242.html</a></p>
<p><a target="_blank" rel="noopener" href="https://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html">手工拉起flannel网络</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/19/kubernetes_Flannel%E7%BD%91%E7%BB%9C%E5%89%96%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/19/kubernetes_Flannel%E7%BD%91%E7%BB%9C%E5%89%96%E6%9E%90/" class="post-title-link" itemprop="url">kubernetes Flannel网络剖析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-19 11:30:03" itemprop="dateCreated datePublished" datetime="2022-01-19T11:30:03+08:00">2022-01-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-Flannel网络剖析"><a href="#kubernetes-Flannel网络剖析" class="headerlink" title="kubernetes Flannel网络剖析"></a>kubernetes Flannel网络剖析</h1><h2 id="cni（Container-Network-Interface）"><a href="#cni（Container-Network-Interface）" class="headerlink" title="cni（Container Network Interface）"></a>cni（Container Network Interface）</h2><p>CNI 全称为 Container Network Interface，是用来定义容器网络的一个 <a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni/blob/master/SPEC.md">规范</a>。<a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni">containernetworking&#x2F;cni</a> 是一个 CNCF 的 CNI 实现项目，包括基本的 bridge,macvlan等基本网络插件。</p>
<p>一般将cni各种网络插件的可执行文件二进制放到 <code>/usr/libexec/cni/</code> ，在 <code>/etc/cni/net.d/</code> 下创建配置文件，剩下的就交给 K8s 或者 containerd 了，我们不关心也不了解其实现。</p>
<p>比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># ls -lh /usr/libexec/cni/</span><br><span class="line">总用量 133M</span><br><span class="line">-rwxr-xr-x 1 root root 4.4M  8月 18 11:51 bandwidth</span><br><span class="line">-rwxr-xr-x 1 root root 4.3M  3月  6  2021 bridge</span><br><span class="line">-rwxr-x--- 1 root root  31M  8月 18 11:51 calico</span><br><span class="line">-rwxr-x--- 1 root root  30M  8月 18 11:51 calico-ipam</span><br><span class="line">-rwxr-xr-x 1 root root  12M  3月  6  2021 dhcp</span><br><span class="line">-rwxr-xr-x 1 root root 5.6M  3月  6  2021 firewall</span><br><span class="line">-rwxr-xr-x 1 root root 3.1M  8月 18 11:51 flannel</span><br><span class="line">-rwxr-xr-x 1 root root 3.8M  3月  6  2021 host-device</span><br><span class="line">-rwxr-xr-x 1 root root 3.9M  8月 18 11:51 host-local</span><br><span class="line">-rwxr-xr-x 1 root root 4.0M  3月  6  2021 ipvlan</span><br><span class="line">-rwxr-xr-x 1 root root 3.6M  8月 18 11:51 loopback</span><br><span class="line">-rwxr-xr-x 1 root root 4.0M  3月  6  2021 macvlan</span><br><span class="line">-rwxr-xr-x 1 root root 4.2M  8月 18 11:51 portmap</span><br><span class="line">-rwxr-xr-x 1 root root 4.2M  3月  6  2021 ptp</span><br><span class="line">-rwxr-xr-x 1 root root 2.7M  3月  6  2021 sample</span><br><span class="line">-rwxr-xr-x 1 root root 3.2M  3月  6  2021 sbr</span><br><span class="line">-rwxr-xr-x 1 root root 2.8M  3月  6  2021 static</span><br><span class="line">-rwxr-xr-x 1 root root 3.7M  8月 18 11:51 tuning</span><br><span class="line">-rwxr-xr-x 1 root root 4.0M  3月  6  2021 vlan</span><br><span class="line"></span><br><span class="line">#ls -lh /etc/cni/net.d/</span><br><span class="line">总用量 12K</span><br><span class="line">-rw-r--r-- 1 root root  607 12月 23 09:39 10-calico.conflist</span><br><span class="line">-rw-r----- 1 root root  292 12月 23 09:47 10-flannel.conflist</span><br><span class="line">-rw------- 1 root root 2.6K 12月 23 09:39 calico-kubeconfig</span><br></pre></td></tr></table></figure>

<p>CNI 插件都是直接通过 exec 的方式调用，而不是通过 socket 这样 C&#x2F;S 方式，所有参数都是通过环境变量、标准输入输出来实现的。</p>
<h2 id="跨主机通信流程"><a href="#跨主机通信流程" class="headerlink" title="跨主机通信流程"></a>跨主机通信流程</h2><p>Step-by-step communication from <strong>Pod 1</strong> to <strong>Pod 6</strong>:</p>
<ol>
<li><em>Package leaves</em> *<strong>Pod 1 netns*</strong> <em>through the</em> *<strong>eth1*</strong> <em>interface and reaches the</em> <em><strong>root netns*</strong> <em>through the virtual interface</em> <em><strong>veth1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>veth1*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> <em><strong>cni0*</strong> <em>and is redirected to</em> <em><strong>eth0*</strong></em>;</em></li>
<li><em>Package leaves</em> *<strong>eth0*</strong> <em>from</em> <em><strong>Master 1*</strong> <em>and reaches the</em> <em><strong>gateway*</strong></em>;</em></li>
<li><em>Package leaves the</em> *<strong>gateway*</strong> <em>and reaches the</em> *<strong>root netns*</strong> <em>through the</em> <em><strong>eth0*</strong> <em>interface on</em> <em><strong>Worker 1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>eth0*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> *<strong>cni0*</strong> <em>and is redirected to the</em> *<strong>veth6*</strong> <em>virtual interface;</em></li>
<li><em>Package leaves the</em> *<strong>root netns*</strong> <em>through</em> *<strong>veth6*</strong> <em>and reaches the</em> *<strong>Pod 6 netns*</strong> <em>though the</em> *<strong>eth6*</strong> <em>interface;</em></li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115124747936.png" alt="image-20220115124747936"></p>
<blockquote>
<p><strong>cni0</strong> is a Linux network bridge device, all <strong>veth</strong> devices will connect to this bridge, so all Pods on the same node can communicate with each other, as explained in <strong>Kubernetes Network Model</strong> and the hotel analogy above.</p>
</blockquote>
<p>默认cni 网络是没法跨宿主机的，跨宿主机需要走overlay（比如flannel的vxlan）或者仅限宿主机全在一个二层网络可达（比如用flannel的host-gw模式）</p>
<h2 id="flannel-vxlan网络"><a href="#flannel-vxlan网络" class="headerlink" title="flannel vxlan网络"></a><a target="_blank" rel="noopener" href="https://msazure.club/flannel-networking-demystify/">flannel vxlan网络</a></h2><p>什么是 flannel</p>
<blockquote>
<p><em>Flannel is a simple and easy way to configure a layer 3 network fabric designed for Kubernetes.</em></p>
</blockquote>
<p>Flannel 工作原理</p>
<blockquote>
<p><em>Flannel runs a small, single binary agent called</em> <code>flanneld</code> on each host, and is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. Flannel uses either the Kubernetes API or etcd directly to store the network configuration, the allocated subnets, and any auxiliary data (such as the host’s public IP). Packets are forwarded using one of several backend mechanisms including VXLAN and various cloud integrations.</p>
</blockquote>
<p>核心原理就是将pod网络包通过vxlan协议封装成一个udp包，udp包的ip是数据ip，内层是pod原始网络通信包。</p>
<p>假如POD1访问POD4：</p>
<ol>
<li>从POD1中出来的包先到Bridge cni0上（因为POD1对应的veth挂在了cni0上），目标mac地址是cni0的Mac</li>
<li>然后进入到宿主机网络，宿主机有路由 10.244.2.0&#x2F;24 via 10.244.2.0 dev flannel.1 onlink ，也就是目标ip 10.244.2.3的包交由 flannel.1 来处理，目标mac地址是POD4所在机器的flannel.1的Mac</li>
<li>flanneld 进程将包封装成vxlan 丢到eth0从宿主机1离开（封装后的目标ip是192.168.2.91，现在都是由内核来完成flanneld这个封包过程，性能好）</li>
<li>这个封装后的vxlan udp包正确路由到宿主机2</li>
<li>然后经由 flanneld 解包成 10.244.2.3 ，命中宿主机2上的路由：10.244.2.0&#x2F;24 dev cni0 proto kernel scope link src 10.244.2.1 ，交给cni0（<strong>这里会过宿主机iptables</strong>）</li>
<li>cni0将包送给POD4</li>
</ol>
<p><img src="/images/951413iMgBlog/Flannel.jpg" alt="img"></p>
<p>flannel容器启动的时候会给自己所在的node注入一些信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#kubectl describe node hygon4  |grep -i flannel</span><br><span class="line">Annotations:        flannel.alpha.coreos.com/backend-data: &#123;&quot;VNI&quot;:1,&quot;VtepMAC&quot;:&quot;66:c6:ba:a2:8f:a1&quot;&#125;</span><br><span class="line">                    flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">                    flannel.alpha.coreos.com/kube-subnet-manager: true</span><br><span class="line">                    flannel.alpha.coreos.com/public-ip: 10.176.4.245  ---宿主机ip，vxlan封包所用</span><br><span class="line">                    </span><br><span class="line"> &quot;VtepMAC&quot;:&quot;66:c6:ba:a2:8f:a1&quot;----宿主机网卡 flannel.1的mac   </span><br></pre></td></tr></table></figure>

<p>flannel.1 知道如何通过物理网卡打包网络包到目标地址，flanneld 会在每个host 添加 arp，以及将本机的 vxlan fdb 添加到新的 host上</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">//这个 flannel 集群有四个 host，这是其中一个host </span><br><span class="line">//4e:95:a9:e2:ed:28是对方 host 上 flannel.1 的 mac</span><br><span class="line">#ip neigh show dev flannel.1 </span><br><span class="line">172.19.2.0 lladdr 4e:95:a9:e2:ed:28 PERMANENT</span><br><span class="line">172.19.3.0 lladdr 2e:8b:65:d7:54:3e PERMANENT</span><br><span class="line">172.19.1.0 lladdr 6a:78:f3:db:b1:9e PERMANENT</span><br><span class="line"></span><br><span class="line">#bridge fdb show flannel.1</span><br><span class="line">01:00:5e:00:00:01 dev enp125s0f0 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev enp125s0f1 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev enp125s0f2 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev enp125s0f3 self permanent</span><br><span class="line">33:33:00:00:00:01 dev enp125s0f3 self permanent</span><br><span class="line">33:33:ff:8e:d6:ac dev enp125s0f3 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev enp2s0f0 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev enp2s0f1 self permanent</span><br><span class="line">33:33:00:00:00:01 dev cni0 self permanent</span><br><span class="line">01:00:5e:00:00:01 dev cni0 self permanent</span><br><span class="line">f2:64:e3:49:4c:c8 dev cni0 vlan 1 master cni0 permanent</span><br><span class="line">f2:64:e3:49:4c:c8 dev cni0 master cni0 permanent</span><br><span class="line">72:d6:f3:54:7d:d6 dev vethe54b12b5 master cni0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># ip neigh show dev flannel.1 //另一个host</span><br><span class="line">172.19.2.0 lladdr 4e:95:a9:e2:ed:28 PERMANENT</span><br><span class="line">172.19.3.0 lladdr 2e:8b:65:d7:54:3e PERMANENT</span><br><span class="line">172.19.0.0 lladdr 92:5c:b2:af:37:62 PERMANENT</span><br></pre></td></tr></table></figure>

<p>包流程：</p>
<p><img src="/images/951413iMgBlog/image-20220915113511706.png" alt="image-20220915113511706"></p>
<p><a target="_blank" rel="noopener" href="https://blog.michaelfmcnamara.com/2008/02/what-are-the-arp-and-fdb-tables/">ARP 和 FDB:</a></p>
<p>ARP (<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Address_Resolution_Protocol">Address Resolution Protocol</a>) table is used by a <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Layer_3">Layer 3</a> device (router, switch, server, desktop) to store the IP address to MAC address entries for a specific network device. </p>
<p>The FDB (<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Forwarding_table">forwarding database</a>) table is used by a Layer 2 device (switch&#x2F;bridge) to store the MAC addresses that have been learned and which ports that MAC address was learned on. The MAC addresses are learned through <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Transparent_bridge">transparent bridging</a> on switches and dedicated bridges.</p>
<h3 id="抓包演示packet流转以及封包解包"><a href="#抓包演示packet流转以及封包解包" class="headerlink" title="抓包演示packet流转以及封包解包"></a>抓包演示packet流转以及封包解包</h3><p>一次完整的抓包过程演示包的流转，从hygon3上的pod 192.168.0.4（22:d8:63:6c:e8:96） 访问 hygon4上的pod 192.168.2.56（52:e6:8e:02:80:35）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">//hygon3上的pod 192.168.0.4（22:d8:63:6c:e8:96） 访问 hygon4上的pod 192.168.2.56（52:e6:8e:02:80:35），在cni0（a2:99:4f:dc:9d:5c）上抓包，跨机不走peer veth</span><br><span class="line">[root@hygon3 11:08 /root]</span><br><span class="line">#tcpdump -i cni0 host 192.168.2.56 -nnetvv</span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: listening on cni0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">22:d8:63:6c:e8:96 &gt; a2:99:4f:dc:9d:5c, ethertype IPv4 (0x0800), length 614: (tos 0x0, ttl 64, id 53303, offset 0, flags [DF], proto TCP (6), length 600)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x85d7 (incorrect -&gt; 0x801a), seq 150533649:150534197, ack 3441674662, win 507, options [nop,nop,TS val 1239838869 ecr 2297983667], length 548</span><br><span class="line"></span><br><span class="line">//hygon3上的pod 192.168.0.4 访问 hygon4上的pod 192.168.2.56，在本机flannel.1（a2:06:5e:83:44:78）上抓包</span><br><span class="line">[root@hygon3 10:53 /root]</span><br><span class="line">#tcpdump -i flannel.1 host 192.168.0.4 -nnetvv </span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">a2:06:5e:83:44:78 &gt; 66:c6:ba:a2:8f:a1, ethertype IPv4 (0x0800), length 729: (tos 0x0, ttl 63, id 52997, offset 0, flags [DF], proto TCP (6), length 715)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x864a (incorrect -&gt; 0x02ae), seq 150429115:150429778, ack 3441664870, win 507, options [nop,nop,TS val 1239381169 ecr 2297525566], length 663</span><br><span class="line">       </span><br><span class="line"> [root@hygon3 11:13 /root] //通过arp 可以看到对端 flannel.1 的mac地址被缓存到了本地</span><br><span class="line">#arp -n |grep 66:c6:ba:a2:8f:a1</span><br><span class="line">192.168.2.0              ether   66:c6:ba:a2:8f:a1   CM                    flannel.1</span><br><span class="line">#ip route</span><br><span class="line">default via 10.176.3.247 dev p1p1</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1</span><br><span class="line">192.168.0.0/24 dev cni0 proto kernel scope link src 192.168.0.1</span><br><span class="line">192.168.1.0/24 via 192.168.1.0 dev flannel.1 onlink</span><br><span class="line">192.168.2.0/24 via 192.168.2.0 dev flannel.1 onlink</span><br><span class="line">192.168.3.0/24 via 192.168.3.0 dev flannel.1 onlink</span><br><span class="line">#ip a</span><br><span class="line">18: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether a2:06:5e:83:44:78 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.0/32 brd 192.168.0.0 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">19: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether a2:99:4f:dc:9d:5c brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.1/24 brd 192.168.0.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">//宿主机物理网卡抓包，被封成了udp的vxlan包    </span><br><span class="line">[root@hygon3 11:12 /root]</span><br><span class="line">#tcpdump -i p1p1 udp and port 8472 -nnetvv</span><br><span class="line">0c:42:a1:db:b1:a8 &gt; 88:66:39:89:9b:cc, ethertype IPv4 (0x0800), length 967: (tos 0x0, ttl 64, id 33722, offset 0, flags [none], proto UDP (17), length 953)</span><br><span class="line">    10.176.3.245.45173 &gt; 10.176.4.245.8472: [bad udp cksum 0x88c6 -&gt; 0xe4db!] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">a2:06:5e:83:44:78 &gt; 66:c6:ba:a2:8f:a1, ethertype IPv4 (0x0800), length 917: (tos 0x0, ttl 63, id 53539, offset 0, flags [DF], proto TCP (6), length 903)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x8706 (incorrect -&gt; 0xe31b), seq 150613328:150614179, ack 3441682214, win 507, options [nop,nop,TS val 1240166469 ecr 2298311268], length 851</span><br><span class="line"></span><br><span class="line">---------跨机分割线--------</span><br><span class="line"></span><br><span class="line">[root@hygon4 11:15 /root] //udp ttl为61，经过了3跳(icmp ttl为63)，不过这些都和vxlan内容无关了</span><br><span class="line">#tcpdump -i p1p1 udp and port 8472 -nnetvv</span><br><span class="line">88:66:39:2b:3f:ec &gt; 0c:42:a1:e9:77:2c, ethertype IPv4 (0x0800), length 736: (tos 0x0, ttl 61, id 49748, offset 0, flags [none], proto UDP (17), length 722)</span><br><span class="line">    10.176.3.245.45173 &gt; 10.176.4.245.8472: [udp sum ok] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">a2:06:5e:83:44:78 &gt; 66:c6:ba:a2:8f:a1, ethertype IPv4 (0x0800), length 686: (tos 0x0, ttl 63, id 53631, offset 0, flags [DF], proto TCP (6), length 672)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x7f0c (correct), seq 150646020:150646640, ack 3441685158, win 507, options [nop,nop,TS val 1240301769 ecr 2298444568], length 620</span><br><span class="line">0c:42:a1:e9:77:2c &gt; 88:66:39:2b:3f:ec, ethertype IPv4 (0x0800), length 180: (tos 0x0, ttl 64, id 57062, offset 0, flags [none], proto UDP (17), length 166)</span><br><span class="line">    10.176.4.245.41515 &gt; 10.176.3.245.8472: [bad udp cksum 0x9a23 -&gt; 0x8e11!] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">66:c6:ba:a2:8f:a1 &gt; a2:06:5e:83:44:78, ethertype IPv4 (0x0800), length 130: (tos 0x0, ttl 63, id 12391, offset 0, flags [DF], proto TCP (6), length 116)</span><br><span class="line">    192.168.2.56.3100 &gt; 192.168.0.4.40712: Flags [P.], cksum 0x83f3 (incorrect -&gt; 0x77e1), seq 1:65, ack 620, win 501, options [nop,nop,TS val 2298447868 ecr 1240301769], length 64</span><br><span class="line">    </span><br><span class="line">//到对端hygon4上抓包, 因为途中都是vxlan，所以ttl、mac地址都不变</span><br><span class="line">[root@hygon4 10:55 /root]</span><br><span class="line">#tcpdump -i flannel.1 host 192.168.2.56 -nnetvv</span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: listening on flannel.1, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">a2:06:5e:83:44:78 &gt; 66:c6:ba:a2:8f:a1, ethertype IPv4 (0x0800), length 933: (tos 0x0, ttl 63, id 52807, offset 0, flags [DF], proto TCP (6), length 919)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x8d0d (correct), seq 150361706:150362573, ack 3441658790, win 507, options [nop,nop,TS val 1239073069 ecr 2297216169], length 867</span><br><span class="line">    </span><br><span class="line">#ip a //only for flannel.1 and cni0</span><br><span class="line">10: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether 66:c6:ba:a2:8f:a1 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.2.0/32 brd 192.168.2.0 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">11: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 16:97:3a:7b:53:00 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.2.1/24 brd 192.168.2.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever       </span><br><span class="line"></span><br><span class="line">[root@hygon4 11:24 /root]</span><br><span class="line">#arp -n | grep 44:78</span><br><span class="line">192.168.0.0              ether   a2:06:5e:83:44:78   CM                    flannel.1   </span><br><span class="line"> </span><br><span class="line"> //mac地址替换，ttl减1</span><br><span class="line"> [root@hygon4 10:55 /root]</span><br><span class="line">#tcpdump -i cni0 host 192.168.2.56 -nnetvv</span><br><span class="line">dropped privs to tcpdump</span><br><span class="line">tcpdump: listening on cni0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">16:97:3a:7b:53:00 &gt; 52:e6:8e:02:80:35, ethertype IPv4 (0x0800), length 935: (tos 0x0, ttl 62, id 52829, offset 0, flags [DF], proto TCP (6), length 921)</span><br><span class="line">    192.168.0.4.40712 &gt; 192.168.2.56.3100: Flags [P.], cksum 0x7aa8 (correct), seq 150369440:150370309, ack 3441659494, win 507, options [nop,nop,TS val 1239115869 ecr 2297259166], length 869   </span><br></pre></td></tr></table></figure>

<p>这个流转流程如下图：</p>
<p><img src="/images/951413iMgBlog/flannel-network-flow.jpg" alt="flannel-network-flow"></p>
<p>对应宿主机查询到的ip、路由信息（和上图不是对应的）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#ip -d -4 addr show cni0</span><br><span class="line">475: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 8e:34:ba:e2:a4:c6 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q bridge_id 8000.8e:34:ba:e2:a4:c6 designated_root 8000.8e:34:ba:e2:a4:c6 root_port 0 root_path_cost 0 topology_change 0 topology_change_detected 0 hello_timer    0.00 tcn_timer    0.00 topology_change_timer    0.00 gc_timer  161.46 vlan_default_pvid 1 vlan_stats_enabled 0 group_fwd_mask 0 group_address 01:80:c2:00:00:00 mcast_snooping 1 mcast_router 1 mcast_query_use_ifaddr 0 mcast_querier 0 mcast_hash_elasticity 4 mcast_hash_max 512 mcast_last_member_count 2 mcast_startup_query_count 2 mcast_last_member_interval 100 mcast_membership_interval 26000 mcast_querier_interval 25500 mcast_query_interval 12500 mcast_query_response_interval 1000 mcast_startup_query_interval 3124 mcast_stats_enabled 0 mcast_igmp_version 2 mcast_mld_version 1 nf_call_iptables 0 nf_call_ip6tables 0 nf_call_arptables 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.1/24 brd 192.168.3.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">#ip -d -4 addr show flannel.1 //vxlan id 1 local 10.133.2.252 dev bond0 --指定了物理网卡</span><br><span class="line">474: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether fe:49:64:ae:36:af brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 10.133.2.252 dev bond0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.0/32 brd 192.168.3.0 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever    </span><br></pre></td></tr></table></figure>

<p>包流转<a target="_blank" rel="noopener" href="https://blog.laputa.io/kubernetes-flannel-networking-6a1cb1f8ec7c">示意图</a></p>
<p><img src="/images/951413iMgBlog/image-20220119114929034.png" alt="image-20220119114929034"></p>
<h2 id="flannel网络不通排查案例"><a href="#flannel网络不通排查案例" class="headerlink" title="flannel网络不通排查案例"></a>flannel网络不通排查案例</h2><p>当网络不通时，可以根据以上演示的包流转路径在不同的网络设备上抓包来定位哪个环节不通</p>
<h3 id="firewalld"><a href="#firewalld" class="headerlink" title="firewalld"></a>firewalld</h3><p>在麒麟系统的物理机上通过kubeadm setup集群，发现有的环境flannel网络不通，在宿主机上ping 其它物理机flannel.0网卡的ip，通过在对端宿主机抓包发现icmp收到后被防火墙扔掉了，抓包中可以看到错误信息：icmp unreachable - admin prohibited</p>
<p>下图中正常的icmp是直接ping 物理机ip</p>
<p><img src="/images/951413iMgBlog/image-20211228203650921.png" alt="image-20211228203650921"></p>
<blockquote>
<p>The “admin prohibited filter” seen in the tcpdump output means there is a firewall blocking a connection. It does it by sending back an ICMP packet meaning precisely that: the admin of that firewall doesn’t want those packets to get through. It could be a firewall at the destination site. It could be a firewall in between. It could be iptables on the Linux system.</p>
</blockquote>
<p>发现有问题的环境中宿主机的防火墙设置报错了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &#x27;/usr/sbin/iptables -w10 -t filter -X DOCKER-ISOLATION-STAGE-1&#x27; failed: iptables: No chain/target/match by that name.</span><br><span class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &#x27;/usr/sbin/iptables -w10 -t filter -F DOCKER-ISOLATION-STAGE-2&#x27; failed: iptables: No chain/target/match by that name.</span><br></pre></td></tr></table></figure>

<p>应该是因为启动docker的时候 firewalld 是运行着的</p>
<blockquote>
<p>Do you have firewalld enabled, and was it (re)started after docker was started? If so, then it’s likely that firewalld wiped docker’s IPTables rules. Restarting the docker daemon should re-create those rules.</p>
</blockquote>
<p><strong>停掉 firewalld 服务可以解决这个问题</strong>，k8s集群</p>
<h3 id="flannel网络不通"><a href="#flannel网络不通" class="headerlink" title="flannel网络不通"></a><a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/issues/799">flannel网络不通</a></h3><blockquote>
<p>Starting from Docker 1.13 default iptables policy for FORWARDING is DROP</p>
</blockquote>
<p>flannel能收到包，但是cni0收不到包，说明包进到了目标宿主机，但是从flannel解开udp转送到cni的时候出了问题，大概率是iptables 拦截了包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">It seems docker version &gt;=1.13 will add iptables rule like below,and it make this issue happen:</span><br><span class="line">iptables -P FORWARD DROP </span><br><span class="line"></span><br><span class="line">All you need to do is add a rule below:</span><br><span class="line">iptables -P FORWARD ACCEPT //将FORWARD 默认规则(没有匹配到其它规则的话）改成ACCEPT</span><br><span class="line"></span><br><span class="line">//flannel 会检查 forward chain并将之改成 accept？以下是flannel 容器日志</span><br><span class="line">I0913 07:52:30.965060       1 main.go:698] Using interface with name enp2s0f0 and address 192.168.0.1</span><br><span class="line">I0913 07:52:30.965128       1 main.go:720] Defaulting external address to interface address (192.168.0.1)</span><br><span class="line">I0913 07:52:30.965134       1 main.go:733] Defaulting external v6 address to interface address (&lt;nil&gt;)</span><br><span class="line">I0913 07:52:30.965243       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</span><br><span class="line">I0913 07:52:30.966878       1 kube.go:339] Setting NodeNetworkUnavailable</span><br><span class="line">I0913 07:52:30.977942       1 main.go:340] Setting up masking rules</span><br><span class="line">I0913 07:52:31.332105       1 main.go:361] Changing default FORWARD chain policy to ACCEPT</span><br></pre></td></tr></table></figure>

<h2 id="宿主机多-ip-下-flannel-网络不通"><a href="#宿主机多-ip-下-flannel-网络不通" class="headerlink" title="宿主机多 ip 下 flannel 网络不通"></a>宿主机多 ip 下 flannel 网络不通</h2><p>宿主机有两个ip，flannel组网ip是192.168，但是默认路由在1.1.网络下，此时能 ping 通，但是curl不通端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#tcpdump -i enp2s0f0 -nettvv host 192.168.0.3 and udp</span><br><span class="line">tcpdump: listening on enp2s0f0, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line"></span><br><span class="line">//握手请求syn包，udp src ip:192.168.0.1</span><br><span class="line">1660897108.334556 0c:42:a1:4f:d1:e2 &gt; 0c:42:a1:4f:d1:ee, ethertype IPv4 (0x0800), length 124: (tos 0x0, ttl 64, id 32118, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    192.168.0.1.56773 &gt; 192.168.0.3.otv: [bad udp cksum 0x81c0 -&gt; 0x459f!] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">56:fa:69:e3:dc:6b &gt; 4e:95:a9:e2:ed:28, ethertype IPv4 (0x0800), length 74: (tos 0x0, ttl 63, id 41108, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.19.0.6.35118 &gt; 172.19.2.39.http: Flags [S], cksum 0x10c8 (correct), seq 582983385, win 64860, options [mss 1410,sackOK,TS val 2648241865 ecr 0,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">//对端回复syn包, 注意udp的目标ip:1.1.1.198,应该是 192.168.0.1 才对，mac是192.168.0.1 的，mac和ip不匹配，所以被内核扔掉（但是icmp不会被扔，原因未知）</span><br><span class="line">1660897108.334738 0c:42:a1:4f:d1:ee &gt; 0c:42:a1:4f:d1:e2, ethertype IPv4 (0x0800), length 124: (tos 0x0, ttl 64, id 41433, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    192.168.0.3.38086 &gt; 1.1.1.198.otv: [bad udp cksum 0x5aff -&gt; 0x1769!] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">4e:95:a9:e2:ed:28 &gt; 56:fa:69:e3:dc:6b, ethertype IPv4 (0x0800), length 74: (tos 0x0, ttl 63, id 0, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.19.2.39.http &gt; 172.19.0.6.35118: Flags [S.], cksum 0x8027 (correct), seq 3633913151, ack 582983386, win 64308, options [mss 1410,sackOK,TS val 3514485603 ecr 2648241865,nop,wscale 7], length 0</span><br><span class="line"></span><br><span class="line">//没有回复第三次握手，继续发syn，因为收到syn+ack后被扔掉了</span><br><span class="line">1660897109.363382 0c:42:a1:4f:d1:e2 &gt; 0c:42:a1:4f:d1:ee, ethertype IPv4 (0x0800), length 124: (tos 0x0, ttl 64, id 32123, offset 0, flags [none], proto UDP (17), length 110)</span><br><span class="line">    192.168.0.1.60933 &gt; 192.168.0.3.otv: [bad udp cksum 0x81c0 -&gt; 0x355f!] OTV, flags [I] (0x08), overlay 0, instance 1</span><br><span class="line">56:fa:69:e3:dc:6b &gt; 4e:95:a9:e2:ed:28, ethertype IPv4 (0x0800), length 74: (tos 0x0, ttl 63, id 41109, offset 0, flags [DF], proto TCP (6), length 60)</span><br><span class="line">    172.19.0.6.35118 &gt; 172.19.2.39.http: Flags [S], cksum 0x0cc3 (correct), seq 582983385, win 64860, options [mss 1410,sackOK,TS val 2648242894 ecr 0,nop,wscale 7], length 0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>多ip宿主机的网卡及路由</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">5: enp125s0f3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">    link/ether 64:2c:ac:e9:78:3d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 1.1.1.198/25 brd 1.1.1.255 scope global dynamic noprefixroute enp125s0f3</span><br><span class="line">       valid_lft 12463sec preferred_lft 12463sec</span><br><span class="line">    inet6 fe80::859a:7861:378e:d6ac/64 scope link noprefixroute</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">6: enp2s0f0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000</span><br><span class="line">    link/ether 0c:42:a1:4f:d1:e2 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.0.1/24 brd 192.168.0.255 scope global noprefixroute enp2s0f0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">       </span><br><span class="line">#ip route</span><br><span class="line">default via 1.1.1.254 dev enp125s0f3 proto dhcp metric 101</span><br><span class="line">1.1.1.128/25 dev enp125s0f3 proto kernel scope link src 1.1.1.198 metric 101</span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 linkdown</span><br><span class="line">172.19.0.0/24 dev cni0 proto kernel scope link src 172.19.0.1</span><br><span class="line">172.19.2.0/24 via 172.19.2.0 dev flannel.1 onlink</span><br><span class="line">172.19.3.0/24 via 172.19.3.0 dev flannel.1 onlink</span><br><span class="line">192.168.0.0/24 dev enp2s0f0 proto kernel scope link src 192.168.0.1 metric 100       </span><br></pre></td></tr></table></figure>

<p>解决办法：真正生效的是 flannel.1 中的地址</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//比如 flannel 选用了以下公网ip（默认路由上的ip）导致flannel网络不通，应该选内网ip</span><br><span class="line">#ip -details link show flannel.1</span><br><span class="line">29: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default</span><br><span class="line">    link/ether 96:ad:e2:29:29:09 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 30.1.1.1 dev eno1 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br></pre></td></tr></table></figure>

<p>解决办法得先删掉 flannel 网络，然后在 flannel.yaml 中指定内网网卡：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kube-flannel</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">registry:5000/quay.io/coreos/flannel:v0.14.0</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/opt/bin/flanneld</span></span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--ip-masq</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">--kube-subnet-mgr</span></span><br><span class="line">        <span class="comment">#指定网卡, enp33s0f0 为内网网卡，不是默认路由</span></span><br><span class="line">        <span class="comment">#- --iface=enp33s0f0</span></span><br><span class="line">        <span class="comment">#— --iface-regex=[enp0s8|enp0s9]</span></span><br><span class="line"></span><br><span class="line"><span class="string">//然后会看到</span> <span class="string">flannel.1</span> <span class="string">的地址用的是</span> <span class="string">enp33s0f0（192.168.0.1）</span></span><br><span class="line"><span class="comment">#ip -details link show flannel.1</span></span><br><span class="line"><span class="attr">40: flannel.1:</span> <span class="string">&lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt;</span> <span class="string">mtu</span> <span class="number">1450 </span><span class="string">qdisc</span> <span class="string">noqueue</span> <span class="string">state</span> <span class="string">UNKNOWN</span> <span class="string">mode</span> <span class="string">DEFAULT</span> <span class="string">group</span> <span class="string">default</span></span><br><span class="line">    <span class="string">link/ether</span> <span class="number">92</span><span class="string">:5c:b2:af:37:62</span> <span class="string">brd</span> <span class="string">ff:ff:ff:ff:ff:ff</span> <span class="string">promiscuity</span> <span class="number">0</span> <span class="string">minmtu</span> <span class="number">68</span> <span class="string">maxmtu</span> <span class="number">65535</span></span><br><span class="line">    <span class="string">vxlan</span> <span class="string">id</span> <span class="number">1</span> <span class="string">local</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span> <span class="string">dev</span> <span class="string">enp2s0f0</span> <span class="string">srcport</span> <span class="number">0</span> <span class="number">0</span> <span class="string">dstport</span> <span class="number">8472 </span><span class="string">nolearning</span> <span class="string">ttl</span> <span class="string">auto</span> <span class="string">ageing</span> <span class="number">300</span> <span class="string">udpcsum</span> <span class="string">noudp6zerocsumtx</span> <span class="string">noudp6zerocsumrx</span> <span class="string">addrgenmode</span> <span class="string">eui64</span> <span class="string">numtxqueues</span> <span class="number">1</span> <span class="string">numrxqueues</span> <span class="number">1</span> <span class="string">gso_max_size</span> <span class="number">65536</span> <span class="string">gso_max_segs</span> <span class="number">65535</span>        </span><br></pre></td></tr></table></figure>

<blockquote>
<p> If you happen to have different interfaces to be matched, you can match it on a regex pattern. Let’s say the worker nodes could’ve enp0s8 or enp0s9 configured, then the flannel args would be <code>— --iface-regex=[enp0s8|enp0s9]</code></p>
</blockquote>
<p>修改node的annotation中flannel的 public-ip。如果因为 public-ip 不对导致网络不通，在annotation中修改public-ip没用，这个值是 flannel 读取underlay 网络配置后写进来的，同时也写到了 flannel.1 的 config 中</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl annotate node ky1 flannel.alpha.coreos.com/public-ip-</span><br><span class="line">kubectl annotate node ky1 flannel.alpha.coreos.com/public-ip=192.168.0.1</span><br></pre></td></tr></table></figure>

<h2 id="容器调试"><a href="#容器调试" class="headerlink" title="容器调试"></a>容器调试</h2><p>可以起一个容器，里面带有各种工具，然后attach 到目标容器 ：<a target="_blank" rel="noopener" href="https://github.com/zeromake/docker-debug/blob/master/README-zh-Hans.md">https://github.com/zeromake/docker-debug/blob/master/README-zh-Hans.md</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./docker-debug-linux-amd64 --image=CentOS8 nginx top -Hp 12 //可以先把工具安装在CentOS8，然后attach 到被调试的 nginx容器</span><br></pre></td></tr></table></figure>



<h2 id="抓包和调试-–-nsenter"><a href="#抓包和调试-–-nsenter" class="headerlink" title="抓包和调试 – nsenter"></a>抓包和调试 – nsenter</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">获取pid：docker inspect -f &#123;&#123;.State.Pid&#125;&#125; c8f874efea06</span><br><span class="line"></span><br><span class="line">进入namespace：nsenter --net --pid --target 17277</span><br><span class="line">nsenter --net --pid --target `docker inspect -f &#123;&#123;.State.Pid&#125;&#125; c8f874efea06`</span><br><span class="line"></span><br><span class="line">//只进入network namespace，这样看到的文件还是宿主机的，能直接用tcpdump，但是看到的网卡是容器的</span><br><span class="line">nsenter --target 17277 --net </span><br><span class="line"></span><br><span class="line">// ip netns 获取容器网络信息</span><br><span class="line"> 1022  [2021-04-14 15:53:06] docker inspect -f &#x27;&#123;&#123;.State.Pid&#125;&#125;&#x27; ab4e471edf50   //获取容器进程id</span><br><span class="line"> 1023  [2021-04-14 15:53:30] ls /proc/79828/ns/net</span><br><span class="line"> 1024  [2021-04-14 15:53:57] ln -sfT /proc/79828/ns/net /var/run/netns/ab4e471edf50 //link 以便ip netns List能访问</span><br><span class="line"> </span><br><span class="line">// 宿主机上查看容器ip</span><br><span class="line"> 1026  [2021-04-14 15:54:11] ip netns list</span><br><span class="line"> 1028  [2021-04-14 15:55:19] ip netns exec ab4e471edf50 ifconfig</span><br><span class="line"> </span><br><span class="line"> //nsenter 调试网络</span><br><span class="line"> Get the pause container&#x27;s sandboxkey: </span><br><span class="line">root@worker01:~# docker inspect k8s_POD_ubuntu-5846f86795-bcbqv_default_ea44489d-3dd4-11e8-bb37-02ecc586c8d5_0 | grep SandboxKey</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/82ec9e32d486&quot;,</span><br><span class="line">root@worker01:~#</span><br><span class="line">Now, using nsenter you can see the container&#x27;s information.</span><br><span class="line">root@worker01:~# nsenter --net=/var/run/docker/netns/82ec9e32d486 ip addr show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default</span><br><span class="line">   link/ether 0a:58:0a:f4:01:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">   inet 10.244.1.2/24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">Identify the peer_ifindex, and finally you can see the veth pair endpoint in root namespace.</span><br><span class="line">root@worker01:~# nsenter --net=/var/run/docker/netns/82ec9e32d486 ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">     peer_ifindex: 7</span><br><span class="line">root@worker01:~#</span><br><span class="line">root@worker01:~# ip -d link show | grep &#x27;7: veth&#x27;</span><br><span class="line">7: veth5e43ca47@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default</span><br><span class="line">root@worker01:~#</span><br></pre></td></tr></table></figure>

<p>nsenter相当于在setns的示例程序之上做了一层封装，使我们无需指定命名空间的文件描述符，而是指定进程号即可，<a target="_blank" rel="noopener" href="https://medium.com/@anilkreddyr/kubernetes-with-flannel-understanding-the-networking-part-2-78b53e5364c7">详细case</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#docker inspect cb7b05d82153 | grep -i SandboxKey   //根据 pause 容器id找network namespace</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/d6b2ef3cf886&quot;,</span><br><span class="line"></span><br><span class="line">[root@hygon252 19:00 /root]</span><br><span class="line">#nsenter --net=/var/run/docker/netns/d6b2ef3cf886 ip addr show</span><br><span class="line">3: eth0@if496: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default  //496对应宿主机上的veth编号</span><br><span class="line">    link/ether 1e:95:dd:d9:88:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 192.168.3.22/24 brd 192.168.3.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">#nsenter --net=/var/run/docker/netns/d6b2ef3cf886 ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">     peer_ifindex: 496</span><br><span class="line">     </span><br><span class="line">#ip -d -4 addr show cni0</span><br><span class="line">475: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 8e:34:ba:e2:a4:c6 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q bridge_id 8000.8e:34:ba:e2:a4:c6 designated_root 8000.8e:34:ba:e2:a4:c6 root_port 0 root_path_cost 0 topology_change 0 topology_change_detected 0 hello_timer    0.00 tcn_timer    0.00 topology_change_timer    0.00 gc_timer   43.31 vlan_default_pvid 1 vlan_stats_enabled 0 group_fwd_mask 0 group_address 01:80:c2:00:00:00 mcast_snooping 1 mcast_router 1 mcast_query_use_ifaddr 0 mcast_querier 0 mcast_hash_elasticity 4 mcast_hash_max 512 mcast_last_member_count 2 mcast_startup_query_count 2 mcast_last_member_interval 100 mcast_membership_interval 26000 mcast_querier_interval 25500 mcast_query_interval 12500 mcast_query_response_interval 1000 mcast_startup_query_interval 3124 mcast_stats_enabled 0 mcast_igmp_version 2 mcast_mld_version 1 nf_call_iptables 0 nf_call_ip6tables 0 nf_call_arptables 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.1/24 brd 192.168.3.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever     </span><br></pre></td></tr></table></figure>



<h2 id="清理"><a href="#清理" class="headerlink" title="清理"></a><a target="_blank" rel="noopener" href="https://serverfault.com/questions/247767/cannot-delete-gre-tunnel">清理</a></h2><p>cni信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/etc/cni/net.d/*</span><br><span class="line">/var/lib/cni/ 下存放有ip分配信息</span><br><span class="line"></span><br><span class="line">#cat /run/flannel/subnet.env</span><br><span class="line">FLANNEL_NETWORK=192.168.0.0/16</span><br><span class="line">FLANNEL_SUBNET=192.168.0.1/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br></pre></td></tr></table></figure>

<p>calico创建的tunl0网卡是个tunnel，可以通过 ip tunnel show来查看，<a target="_blank" rel="noopener" href="https://askubuntu.com/questions/1190684/how-can-i-permanently-delete-tun-interfaces#:~:text=doing%20sudo%20ip%20link%20delete,which%20removes%20all%20tun%20devices">清理不掉</a>（重启可以清理掉tunl0）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ip link set dev tunl0 name tunl0_fallback</span><br><span class="line">或者</span><br><span class="line">/sbin/ip link set eth1 down</span><br><span class="line">/sbin/ip link set eth1 name eth123</span><br><span class="line">/sbin/ip link set eth123 up</span><br></pre></td></tr></table></figure>

<h3 id="清理和创建flannel网络"><a href="#清理和创建flannel网络" class="headerlink" title="清理和创建flannel网络"></a>清理和创建flannel网络</h3><p>查看容器网卡和宿主机上的虚拟网卡veth pair:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link //宿主机上执行</span><br><span class="line">cat /sys/class/net/eth0/iflink //容器中执行</span><br></pre></td></tr></table></figure>

<p>清理</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link delete cni0</span><br><span class="line">ip link delete flannel.1</span><br></pre></td></tr></table></figure>

<p>创建</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ip link add cni0 type bridge</span><br><span class="line">ip addr add dev cni0 172.30.0.0/24</span><br><span class="line"></span><br><span class="line">查看A simpler solution:</span><br><span class="line">ip -details link show</span><br><span class="line">ls -l /sys/class/net/ - virtual ones will show all in virtual and lan is on the PCI bus.</span><br><span class="line"></span><br><span class="line">brctl show cni0</span><br><span class="line">brctl addif cni0 veth1 veth2 veth3  //往cni bridge添加多个容器peer 网卡</span><br></pre></td></tr></table></figure>

<p>完全可以手工创建cni0、flannel.1等网络设备，然后将 veth添加到cni0网桥上，再在宿主机配置ip route，基本一个纯手工版本打造的flannel vxlan网络就实现了，深入理解到此任何flannel网络问题都可以解决了。</p>
<h3 id="flannel-ip在多个node之间分配错乱"><a href="#flannel-ip在多个node之间分配错乱" class="headerlink" title="flannel ip在多个node之间分配错乱"></a>flannel ip在多个node之间分配错乱</h3><p>当铲掉重新部署的时候可能cni等网络有残留，导致下一次部署会报ip已存在的错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(combined from similar events): Failed create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;f7aa44bf81b27bf0ff6c02339df2d2743cf952c1519fead4c563892d2d41a979&quot; network for pod &quot;nginx-deployment-6c8c86b759-f8fb7&quot;: NetworkPlugin cni failed to set up pod &quot;nginx-deployment-6c8c86b759-f8fb7_default&quot; network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from 172.19.2.1/24</span><br></pre></td></tr></table></figure>

<p> 可以铲掉网卡重新分配，或者给cni重新分配错误信息提示的ip</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig cni0 172.19.2.1/24</span><br></pre></td></tr></table></figure>

<p>or</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ip link set cni0 down &amp;&amp; ip link set flannel.1 down </span><br><span class="line">ip link delete cni0 &amp;&amp; ip link delete flannel.1</span><br><span class="line">systemctl restart containerd &amp;&amp; systemctl restart kubelet</span><br></pre></td></tr></table></figure>

<h2 id="host-gw"><a href="#host-gw" class="headerlink" title="host-gw"></a><a target="_blank" rel="noopener" href="https://msazure.club/flannel-networking-demystify/">host-gw</a></h2><p>实现超级简单，就是在宿主机上配置路由规则，把其它宿主机ip当成其上所有pod的下一跳，不用封包解包，所以性能奇好，但是要求所有宿主机在一个2层网络，因为ip路由规则要求是直达其它宿主机。</p>
<p>手工配置实现就是vxlan的超级精简版，略！</p>
<h2 id="netns-操作"><a href="#netns-操作" class="headerlink" title="netns 操作"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw">netns 操作</a></h2><p>以下case创建一个名为 ren 的netns，然后在里面增加一对虚拟网卡veth1 veth1_p,  veth1放置在ren里面，veth1_p 放在物理机上，给他们配置上ip并up就能通了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"> 1004  [2021-10-27 10:49:08] ip netns add ren</span><br><span class="line"> 1005  [2021-10-27 10:49:12] ip netns show</span><br><span class="line"> 1006  [2021-10-27 10:49:22] ip netns exec ren route   //为空</span><br><span class="line"> 1007  [2021-10-27 10:49:29] ip netns exec ren iptables -L</span><br><span class="line"> 1008  [2021-10-27 10:49:55] ip link add veth1 type veth peer name veth1_p //此时宿主机上能看到这两块网卡</span><br><span class="line"> 1009  [2021-10-27 10:50:07] ip link set veth1 netns ren //将veth1从宿主机默认网络空间挪到ren中，宿主机中看不到veth1了</span><br><span class="line"> 1010  [2021-10-27 10:50:18] ip netns exec ren route  </span><br><span class="line"> 1011  [2021-10-27 10:50:25] ip netns exec ren iptables -L</span><br><span class="line"> 1012  [2021-10-27 10:50:39] ifconfig</span><br><span class="line"> 1013  [2021-10-27 10:50:51] ip link list</span><br><span class="line"> 1014  [2021-10-27 10:51:29] ip netns exec ren ip link list</span><br><span class="line"> 1017  [2021-10-27 10:53:27] ip netns exec ren ip addr add 172.19.0.100/24 dev veth1 </span><br><span class="line"> 1018  [2021-10-27 10:53:31] ip netns exec ren ip link list</span><br><span class="line"> 1019  [2021-10-27 10:53:39] ip netns exec ren ifconfig</span><br><span class="line"> 1020  [2021-10-27 10:53:42] ip netns exec ren ifconfig -a</span><br><span class="line"> 1021  [2021-10-27 10:54:13] ip netns exec ren ip link set dev veth1 up</span><br><span class="line"> 1022  [2021-10-27 10:54:16] ip netns exec ren ifconfig</span><br><span class="line"> 1023  [2021-10-27 10:54:22] ping 172.19.0.100</span><br><span class="line"> 1024  [2021-10-27 10:54:35] ifconfig -a</span><br><span class="line"> 1025  [2021-10-27 10:55:03] ip netns exec ren ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line"> 1026  [2021-10-27 10:55:10] ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line"> 1027  [2021-10-27 10:55:16] ifconfig veth1_p</span><br><span class="line"> 1028  [2021-10-27 10:55:30] ip link set dev veth1_p up</span><br><span class="line"> 1029  [2021-10-27 10:55:32] ifconfig veth1_p</span><br><span class="line"> 1030  [2021-10-27 10:55:38] ping 172.19.0.101</span><br><span class="line"> 1031  [2021-10-27 10:55:43] ping 172.19.0.100</span><br><span class="line"> 1032  [2021-10-27 10:55:53] ip link set dev veth1_p down</span><br><span class="line"> 1033  [2021-10-27 10:55:54] ping 172.19.0.100</span><br><span class="line"> 1034  [2021-10-27 10:55:58] ping 172.19.0.101</span><br><span class="line"> 1035  [2021-10-27 10:56:08] ifconfig veth1_p</span><br><span class="line"> 1036  [2021-10-27 10:56:32] ping 172.19.0.101</span><br><span class="line"> 1037  [2021-10-27 10:57:04] ip netns exec ren route</span><br><span class="line"> 1038  [2021-10-27 10:57:52] ip netns exec ren ping 172.19.0.101</span><br><span class="line"> 1039  [2021-10-27 10:57:58] ip link set dev veth1_p up</span><br><span class="line"> 1040  [2021-10-27 10:57:59] ip netns exec ren ping 172.19.0.101</span><br><span class="line"> 1041  [2021-10-27 10:58:06] ip netns exec ren ping 172.19.0.100</span><br><span class="line"> 1042  [2021-10-27 10:58:14] ip netns exec ren ifconfig</span><br><span class="line"> 1043  [2021-10-27 10:58:19] ip netns exec ren route</span><br><span class="line"> 1044  [2021-10-27 10:58:26] ip netns exec ren ping 172.19.0.100 -I veth1</span><br><span class="line"> 1045  [2021-10-27 10:58:58] ifconfig veth1_p</span><br><span class="line"> 1046  [2021-10-27 10:59:10] ping 172.19.0.100</span><br><span class="line"> 1047  [2021-10-27 10:59:26] ip netns exec ren ping 172.19.0.101 -I veth1</span><br><span class="line"> </span><br><span class="line"> 把网卡加入到docker0的bridge下</span><br><span class="line"> 1160  [2021-10-27 12:17:37] brctl show</span><br><span class="line"> 1161  [2021-10-27 12:18:05] ip link set dev veth3_p master docker0</span><br><span class="line"> 1162  [2021-10-27 12:18:09] ip link set dev veth1_p master docker0</span><br><span class="line"> 1163  [2021-10-27 12:18:13] ip link set dev veth2 master docker0</span><br><span class="line"> 1164  [2021-10-27 12:18:15] brctl show</span><br><span class="line"> </span><br><span class="line">brctl showmacs br0</span><br><span class="line">brctl show cni0</span><br><span class="line">brctl addif cni0 veth1 veth2 veth3  //往cni bridge添加多个容器peer 网卡</span><br></pre></td></tr></table></figure>

<p>Linux 上存在一个默认的网络命名空间，Linux 中的 1 号进程初始使用该默认空间。Linux 上其它所有进程都是由 1 号进程派生出来的，在派生 clone 的时候如果没有额外特别指定，所有的进程都将共享这个默认网络空间。</p>
<p>所有的网络设备刚创建出来都是在宿主机默认网络空间下的。可以通过 <code>ip link set 设备名 netns 网络空间名</code> 将设备移动到另外一个空间里去，socket也是归属在某一个网络命名空间下的，由创建socket进程所在的netns来决定socket所在的netns</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/socket.c</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sock_create</span><span class="params">(<span class="type">int</span> family, <span class="type">int</span> type, <span class="type">int</span> protocol, <span class="keyword">struct</span> socket **res)</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">return</span> __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: include/net/sock.h</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sock_net_set</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="keyword">struct</span> net *net)</span></span><br><span class="line">&#123;</span><br><span class="line"> write_pnet(&amp;sk-&gt;sk_net, net);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>内核提供了三种操作命名空间的方式，分别是 clone、setns 和 unshare。ip netns add 使用的是 unshare，原理和 clone 是类似的。</p>
<p><img src="/images/951413iMgBlog/640-5304524." alt="Image"></p>
<p>每个 net 下都包含了自己的路由表、iptable 以及内核参数配置等等</p>
<h2 id="etcd-中存储的-flannel-配置"><a href="#etcd-中存储的-flannel-配置" class="headerlink" title="etcd 中存储的 flannel 配置"></a>etcd 中存储的 flannel 配置</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">kubectl exec -it etcd-uos21 -n=kube-system -- /bin/sh</span><br><span class="line"></span><br><span class="line">然后：</span><br><span class="line">ETCDCTL_API=3 etcdctl --key /etc/kubernetes/pki/etcd/peer.key --cert /etc/kubernetes/pki/etcd/peer.crt --cacert /etc/kubernetes/pki/etcd/ca.crt --endpoints=https://localhost:2379 get /registry/configmaps/kube-system/kube-flannel-cfg</span><br><span class="line"></span><br><span class="line">cni-conf.json�&#123;</span><br><span class="line">  &quot;name&quot;: &quot;cbr0&quot;,</span><br><span class="line">  &quot;cniVersion&quot;: &quot;0.3.1&quot;,</span><br><span class="line">  &quot;plugins&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;type&quot;: &quot;flannel&quot;,</span><br><span class="line">      &quot;delegate&quot;: &#123;</span><br><span class="line">        &quot;hairpinMode&quot;: true,</span><br><span class="line">        &quot;isDefaultGateway&quot;: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;type&quot;: &quot;portmap&quot;,</span><br><span class="line">      &quot;capabilities&quot;: &#123;</span><br><span class="line">        &quot;portMappings&quot;: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">Z</span><br><span class="line">net-conf.jsonI&#123;</span><br><span class="line">  &quot;Network&quot;: &quot;172.19.0.0/18&quot;,</span><br><span class="line">  &quot;Backend&quot;: &#123;</span><br><span class="line">    &quot;Type&quot;: &quot;vxlan&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">&quot;</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过无论是对flannel还是calico的学习，不管是使用vxlan还是host-gw发现这些所谓的overlay网络不过是披着一层udp的皮而已，只要我们对ip route&#x2F;mac地址足够了解，这些新技术剖析下来仍然逃不过 <a target="_blank" rel="noopener" href="https://datatracker.ietf.org/doc/html/rfc1180">RFC1180</a> 描述的几个最基础的知识点（基础知识的力量）的使用而已，这一切硬核的基础知识无比简单，只要你多看看我这篇旧文<a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">《就是要你懂网络–一个网络包的旅程》</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://morven.life/notes/networking-3-ipip/">https://morven.life/notes/networking-3-ipip/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bakari/p/10564347.html">https://www.cnblogs.com/bakari/p/10564347.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goldsunshine/p/10701242.html">https://www.cnblogs.com/goldsunshine/p/10701242.html</a></p>
<p><a target="_blank" rel="noopener" href="https://docker-k8s-lab.readthedocs.io/en/latest/docker/docker-flannel.html">手工拉起flannel网络</a></p>
<p><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">《就是要你懂网络–一个网络包的旅程》</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/13/%E4%B8%8D%E5%90%8CCPU%E6%80%A7%E8%83%BD%E5%A4%A7PK/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/13/%E4%B8%8D%E5%90%8CCPU%E6%80%A7%E8%83%BD%E5%A4%A7PK/" class="post-title-link" itemprop="url">不同CPU性能大PK</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-13 17:30:03" itemprop="dateCreated datePublished" datetime="2022-01-13T17:30:03+08:00">2022-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="不同CPU性能大PK"><a href="#不同CPU性能大PK" class="headerlink" title="不同CPU性能大PK"></a>不同CPU性能大PK</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>比较Hygon7280、Intel、AMD、鲲鹏920、飞腾2500的性能情况</p>
<table>
<thead>
<tr>
<th>CPU型号</th>
<th>Hygon 7280</th>
<th>AMD  7H12</th>
<th>AMD  7T83</th>
<th>Intel 8163</th>
<th>鲲鹏920</th>
<th>飞腾2500</th>
<th>倚天710</th>
</tr>
</thead>
<tbody><tr>
<td>物理核数</td>
<td>32</td>
<td>32</td>
<td>64</td>
<td>24</td>
<td>48</td>
<td>64</td>
<td>128core</td>
</tr>
<tr>
<td>超线程</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>路</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>1</td>
</tr>
<tr>
<td>NUMA Node</td>
<td>8</td>
<td>2</td>
<td>4</td>
<td>2</td>
<td>4</td>
<td>16</td>
<td>2</td>
</tr>
<tr>
<td>L1d</td>
<td>32K</td>
<td>32K</td>
<td>32K</td>
<td>32K</td>
<td>64K</td>
<td>32K</td>
<td>64K</td>
</tr>
<tr>
<td>L2</td>
<td>512K</td>
<td>512K</td>
<td>512K</td>
<td>1024K</td>
<td>512K</td>
<td>2048K</td>
<td>1024K</td>
</tr>
</tbody></table>
<p>AMD 7T83 有8个Die, 每个Die L3大小 32M，L2 大小4MiB, 每个Die上  L1I&#x2F;L1D 各256KiB，每个Die有8core，2、3代都是带有独立 IO Die<br>倚天710是一路服务器，单芯片2块对称的 Die</p>
<p><img src="/images/951413iMgBlog/image-20220528105526139.png" alt="image-20220528105526139"></p>
<h2 id="参与比较的几款CPU参数"><a href="#参与比较的几款CPU参数" class="headerlink" title="参与比较的几款CPU参数"></a>参与比较的几款CPU参数</h2><p>IPC的说明：</p>
<blockquote>
<p>IPC: insns per cycle  insn&#x2F;cycles  也就是每个时钟周期能执行的指令数量，越大程序跑的越快</p>
<p>程序的执行时间 &#x3D; 指令数&#x2F;(主频*IPC) &#x2F;&#x2F;单核下，多核的话再除以核数</p>
</blockquote>
<h3 id="Hygon-7280"><a href="#Hygon-7280" class="headerlink" title="Hygon 7280"></a>Hygon 7280</h3><p>Hygon 7280 就是AMD Zen架构，最大IPC能到5. </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">架构：                           x86_64</span><br><span class="line">CPU 运行模式：                   32-bit, 64-bit</span><br><span class="line">字节序：                         Little Endian</span><br><span class="line">Address sizes:                   43 bits physical, 48 bits virtual</span><br><span class="line">CPU:                             128</span><br><span class="line">在线 CPU 列表：                  0-127</span><br><span class="line">每个核的线程数：                 2</span><br><span class="line">每个座的核数：                   32</span><br><span class="line">座：                             2</span><br><span class="line">NUMA 节点：                      8</span><br><span class="line">厂商 ID：                        HygonGenuine</span><br><span class="line">CPU 系列：                       24</span><br><span class="line">型号：                           1</span><br><span class="line">型号名称：                       Hygon C86 7280 32-core Processor</span><br><span class="line">步进：                           1</span><br><span class="line">CPU MHz：                        2194.586</span><br><span class="line">BogoMIPS：                       3999.63</span><br><span class="line">虚拟化：                         AMD-V</span><br><span class="line">L1d 缓存：                       2 MiB</span><br><span class="line">L1i 缓存：                       4 MiB</span><br><span class="line">L2 缓存：                        32 MiB</span><br><span class="line">L3 缓存：                        128 MiB</span><br><span class="line">NUMA 节点0 CPU：                 0-7,64-71</span><br><span class="line">NUMA 节点1 CPU：                 8-15,72-79</span><br><span class="line">NUMA 节点2 CPU：                 16-23,80-87</span><br><span class="line">NUMA 节点3 CPU：                 24-31,88-95</span><br><span class="line">NUMA 节点4 CPU：                 32-39,96-103</span><br><span class="line">NUMA 节点5 CPU：                 40-47,104-111</span><br><span class="line">NUMA 节点6 CPU：                 48-55,112-119</span><br><span class="line">NUMA 节点7 CPU：                 56-63,120-127</span><br></pre></td></tr></table></figure>

<p><strong>架构说明：</strong></p>
<p> 每个CPU有4个Die，每个Die有两个CCX（2 core-Complexes），每个CCX最多有4core（例如7280&#x2F;7285）共享一个L3 cache；每个Die有两个Memory Channel，每个CPU带有8个Memory Channel，并且每个Memory Channel最多支持2根Memory；</p>
<p>海光7系列架构图：</p>
<p><img src="data:image/png;base64,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" alt="img"></p>
<p>曙光H620-G30A 机型硬件结构，CPU是hygon 7280（截图只截取了Socket0）</p>
<p><img src="/images/951413iMgBlog/image-20211231202402561.png" alt="image-20211231202402561"></p>
<h3 id="AMD-EPYC-7T83-NC"><a href="#AMD-EPYC-7T83-NC" class="headerlink" title="AMD EPYC 7T83(NC)"></a>AMD EPYC 7T83(NC)</h3><p>两路服务器，4 numa node，<a target="_blank" rel="noopener" href="https://en.wikichip.org/wiki/amd/microarchitectures/zen_3">Z3架构</a></p>
<p><img src="/images/951413iMgBlog/AMD-EPYC-Milan-Zen-3-Server-CPU.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/image-20220902113036283.png" alt="image-20220902113036283"></p>
<p><img src="/images/951413iMgBlog/image-20220902113336705.png" alt="image-20220902113336705"></p>
<p>详细信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                256</span><br><span class="line">On-line CPU(s) list:   0-255</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    64</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          4</span><br><span class="line">Vendor ID:             AuthenticAMD</span><br><span class="line">CPU family:            25</span><br><span class="line">Model:                 1</span><br><span class="line">Model name:            AMD EPYC 7T83 64-Core Processor</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               2154.005</span><br><span class="line">CPU max MHz:           2550.0000</span><br><span class="line">CPU min MHz:           1500.0000</span><br><span class="line">BogoMIPS:              5090.93</span><br><span class="line">Virtualization:        AMD-V</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              512K</span><br><span class="line">L3 cache:              32768K</span><br><span class="line">NUMA node0 CPU(s):     0-31,128-159</span><br><span class="line">NUMA node1 CPU(s):     32-63,160-191</span><br><span class="line">NUMA node2 CPU(s):     64-95,192-223</span><br><span class="line">NUMA node3 CPU(s):     96-127,224-255</span><br><span class="line"></span><br><span class="line">#cat /sys/devices/system/cpu/cpu&#123;0,1,8,16,30,31,32,128&#125;/cache/index3/shared_cpu_map</span><br><span class="line">00000000,00000000,00000000,000000ff,00000000,00000000,00000000,000000ff</span><br><span class="line">00000000,00000000,00000000,000000ff,00000000,00000000,00000000,000000ff</span><br><span class="line">00000000,00000000,00000000,0000ff00,00000000,00000000,00000000,0000ff00</span><br><span class="line">00000000,00000000,00000000,00ff0000,00000000,00000000,00000000,00ff0000</span><br><span class="line">00000000,00000000,00000000,ff000000,00000000,00000000,00000000,ff000000</span><br><span class="line">00000000,00000000,00000000,ff000000,00000000,00000000,00000000,ff000000</span><br><span class="line">00000000,00000000,000000ff,00000000,00000000,00000000,000000ff,00000000</span><br><span class="line">00000000,00000000,00000000,000000ff,00000000,00000000,00000000,000000ff</span><br><span class="line"></span><br><span class="line">#cat /sys/devices/system/cpu/cpu0/cache/index2/shared_cpu_map</span><br><span class="line">00000000,00000000,00000000,00000001,00000000,00000000,00000000,00000001</span><br></pre></td></tr></table></figure>

<p>L3是8个物理核，16个超线程共享，相当于单核2MB，一块CPU有8个L3，总共是256MB</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#cat cpu0/cache/index3/shared_cpu_list</span><br><span class="line">0-7,128-135</span><br><span class="line">#cat cpu0/cache/index3/size</span><br><span class="line">32768K</span><br><span class="line">#cat cpu0/cache/index2/shared_cpu_list</span><br><span class="line">0,128</span><br><span class="line"></span><br><span class="line">#cat /sys/devices/system/cpu/cpu&#123;0,1,8,16,30,31,32,128&#125;/cache/index3/shared_cpu_list</span><br><span class="line">0-7,128-135</span><br><span class="line">0-7,128-135</span><br><span class="line">8-15,136-143</span><br><span class="line">16-23,144-151</span><br><span class="line">24-31,152-159</span><br><span class="line">24-31,152-159</span><br><span class="line">32-39,160-167</span><br><span class="line">0-7,128-135</span><br></pre></td></tr></table></figure>

<p>L1D、L1I各为 2MiB，单物理核为32KB</p>
<p>空跑nop的IPC为6（有点吓人）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#perf stat ./cpu/test</span><br><span class="line"> Performance counter stats for process id &#x27;449650&#x27;:</span><br><span class="line"></span><br><span class="line">          2,574.29 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">                 0      context-switches          #    0.000 K/sec</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      page-faults               #    0.000 K/sec</span><br><span class="line">     8,985,622,182      cycles                    #    3.491 GHz                      (83.33%)</span><br><span class="line">         4,390,929      stalled-cycles-frontend   #    0.05% frontend cycles idle     (83.34%)</span><br><span class="line">     4,387,560,442      stalled-cycles-backend    #   48.83% backend cycles idle      (83.34%)</span><br><span class="line">    53,711,907,863      instructions              #    5.98  insn per cycle</span><br><span class="line">                                                  #    0.08  stalled cycles per insn  (83.34%)</span><br><span class="line">       418,902,363      branches                  #  162.725 M/sec                    (83.34%)</span><br><span class="line">            15,036      branch-misses             #    0.00% of all branches          (83.32%)</span><br><span class="line"></span><br><span class="line">       2.574347594 seconds time elapsed</span><br></pre></td></tr></table></figure>

<p>sysbench 测试7T83 比7H12 略好，可能是ECS、OS等带来的差异。</p>
<p>测试环境：4.19.91-011.ali4000.alios7.x86_64，5.7.34-log MySQL Community Server (GPL)</p>
<table>
<thead>
<tr>
<th>测试核数</th>
<th>AMD EPYC 7H12 2.5G（QPS、IPC）</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>单核</td>
<td>24363 0.58</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>一对HT</td>
<td>33519  0.40</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>2物理核(0-1)</td>
<td>48423 0.57</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>2物理核(0,32) 跨node</td>
<td>46232 0.55</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>2物理核(0,64) 跨socket</td>
<td>45072 0.52</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>4物理核(0-3)</td>
<td>97759 0.58</td>
<td>CPU跑满</td>
</tr>
<tr>
<td>16物理核(0-15)</td>
<td>367992 0.55</td>
<td>CPU跑满，sys占比20%，si 10%</td>
</tr>
<tr>
<td>32物理核(0-31)</td>
<td>686998 0.51</td>
<td>CPU跑满，sys占比20%, si 12%</td>
</tr>
<tr>
<td>64物理核(0-63)</td>
<td>1161079 0.50</td>
<td>CPU跑到95%以上，sys占比20%, si 12%</td>
</tr>
<tr>
<td>64物理核(0-31,64-95)</td>
<td>964441 0.49</td>
<td>socket2上的32核一直比较闲，数据无参考意义</td>
</tr>
<tr>
<td>64物理核(0-31,64-95)</td>
<td>1147846 0.48</td>
<td>重启mysqld，立即绑核，sysbench 在32-63上，导致0-31的CPU只能跑到89%</td>
</tr>
</tbody></table>
<p>说明，压测过程动态通过taskset绑核，所以会有数据残留其它核的cache问题</p>
<p>跨socket taskset绑核的时候要压很久任务才会跨socket迁移过去，也就是刚taskset后CPU是跑不满的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">#numastat -p 437803</span><br><span class="line"></span><br><span class="line">Per-node process memory usage (in MBs) for PID 437803 (mysqld)</span><br><span class="line">                           Node 0          Node 1          Node 2</span><br><span class="line">                  --------------- --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00            0.00</span><br><span class="line">Heap                         1.15            0.00         5403.27</span><br><span class="line">Stack                        0.00            0.00            0.09</span><br><span class="line">Private                   1921.60           16.22        10647.66</span><br><span class="line">----------------  --------------- --------------- ---------------</span><br><span class="line">Total                     1922.75           16.22        16051.02</span><br><span class="line"></span><br><span class="line">                           Node 3           Total</span><br><span class="line">                  --------------- ---------------</span><br><span class="line">Huge                         0.00            0.00</span><br><span class="line">Heap                         0.03         5404.45</span><br><span class="line">Stack                        0.00            0.09</span><br><span class="line">Private                     16.20        12601.68</span><br><span class="line">----------------  --------------- ---------------</span><br><span class="line">Total                       16.23        18006.22</span><br></pre></td></tr></table></figure>

<h3 id="AMD-EPYC-7H12-ECS"><a href="#AMD-EPYC-7H12-ECS" class="headerlink" title="AMD EPYC 7H12(ECS)"></a>AMD EPYC 7H12(ECS)</h3><p>AMD EPYC 7H12 64-Core（ECS，非物理机），最大IPC能到5. </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">lscpu</span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                64</span><br><span class="line">On-line CPU(s) list:   0-63</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    16</span><br><span class="line">座：                 2</span><br><span class="line">NUMA 节点：         2</span><br><span class="line">厂商 ID：           AuthenticAMD</span><br><span class="line">CPU 系列：          23</span><br><span class="line">型号：              49</span><br><span class="line">型号名称：        AMD EPYC 7H12 64-Core Processor</span><br><span class="line">步进：              0</span><br><span class="line">CPU MHz：             2595.124</span><br><span class="line">BogoMIPS：            5190.24</span><br><span class="line">虚拟化：           AMD-V</span><br><span class="line">超管理器厂商：  KVM</span><br><span class="line">虚拟化类型：     完全</span><br><span class="line">L1d 缓存：          32K</span><br><span class="line">L1i 缓存：          32K</span><br><span class="line">L2 缓存：           512K</span><br><span class="line">L3 缓存：           16384K</span><br><span class="line">NUMA 节点0 CPU：    0-31</span><br><span class="line">NUMA 节点1 CPU：    32-63</span><br></pre></td></tr></table></figure>

<p>AMD EPYC 7T83 ECS </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@bugu88 cpu0]# cd /sys/devices/system/cpu/cpu0</span><br><span class="line">[root@bugu88 cpu0]# cat cache/index0/size</span><br><span class="line">32K</span><br><span class="line">[root@bugu88 cpu0]# cat cache/index1/size</span><br><span class="line">32K</span><br><span class="line">[root@bugu88 cpu0]# cat cache/index2/size</span><br><span class="line">512K</span><br><span class="line">[root@bugu88 cpu0]# cat cache/index3/size</span><br><span class="line">32768K</span><br><span class="line">[root@bugu88 cpu0]# lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                16</span><br><span class="line">On-line CPU(s) list:   0-15</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    8</span><br><span class="line">座：                 1</span><br><span class="line">NUMA 节点：         1</span><br><span class="line">厂商 ID：           AuthenticAMD</span><br><span class="line">CPU 系列：          25</span><br><span class="line">型号：              1</span><br><span class="line">型号名称：        AMD EPYC 7T83 64-Core Processor</span><br><span class="line">步进：              1</span><br><span class="line">CPU MHz：             2545.218</span><br><span class="line">BogoMIPS：            5090.43</span><br><span class="line">超管理器厂商：  KVM</span><br><span class="line">虚拟化类型：     完全</span><br><span class="line">L1d 缓存：          32K</span><br><span class="line">L1i 缓存：          32K</span><br><span class="line">L2 缓存：           512K</span><br><span class="line">L3 缓存：           32768K</span><br><span class="line">NUMA 节点0 CPU：    0-15</span><br></pre></td></tr></table></figure>

<p>stream：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@bugu88 lmbench-master]# for i in $(seq 0 15); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">0</span><br><span class="line">STREAM copy latency: 0.68 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 23509.84 MB/sec</span><br><span class="line">STREAM scale latency: 0.69 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 23285.51 MB/sec</span><br><span class="line">STREAM add latency: 0.96 nanoseconds</span><br><span class="line">STREAM add bandwidth: 25043.73 MB/sec</span><br><span class="line">STREAM triad latency: 1.40 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 17121.79 MB/sec</span><br><span class="line">1</span><br><span class="line">STREAM copy latency: 0.68 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 23513.96 MB/sec</span><br><span class="line">STREAM scale latency: 0.68 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 23580.06 MB/sec</span><br><span class="line">STREAM add latency: 0.96 nanoseconds</span><br><span class="line">STREAM add bandwidth: 25049.96 MB/sec</span><br><span class="line">STREAM triad latency: 1.35 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 17741.93 MB/sec</span><br></pre></td></tr></table></figure>

<h3 id="Intel-8163"><a href="#Intel-8163" class="headerlink" title="Intel 8163"></a>Intel 8163</h3><p>这次对比测试的Intel 8163 CPU信息如下，最大IPC 是4：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">lscpu</span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    24</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          1</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 85</span><br><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2499.121</span><br><span class="line">CPU max MHz:           3100.0000</span><br><span class="line">CPU min MHz:           1000.0000</span><br><span class="line">BogoMIPS:              4998.90</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              33792K</span><br><span class="line">NUMA node0 CPU(s):     0-95</span><br><span class="line"></span><br><span class="line">-----8269CY</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">lscpu</span></span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                104</span><br><span class="line">On-line CPU(s) list:   0-103</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    26</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 85</span><br><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</span><br><span class="line">Stepping:              7</span><br><span class="line">CPU MHz:               3200.000</span><br><span class="line">CPU max MHz:           3800.0000</span><br><span class="line">CPU min MHz:           1200.0000</span><br><span class="line">BogoMIPS:              4998.89</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              36608K</span><br><span class="line">NUMA node0 CPU(s):     0-25,52-77</span><br><span class="line">NUMA node1 CPU(s):     26-51,78-103</span><br></pre></td></tr></table></figure>

<h4 id="不同-intel-型号的差异"><a href="#不同-intel-型号的差异" class="headerlink" title="不同 intel 型号的差异"></a>不同 intel 型号的差异</h4><p>如下图是8269CY和E5-2682上跑的MySQL在相同业务、相同流量下的差异：</p>
<p><img src="/images/951413iMgBlog/image-20221121105650582.png" alt="image-20221121105650582"></p>
<p>CPU使用率差异(下图8051C是E5-2682，其它是 8269CY，主频也有30%的差异)</p>
<p><img src="/images/951413iMgBlog/image-20221121110004127.png" alt="image-20221121110004127"></p>
<h3 id="鲲鹏920"><a href="#鲲鹏920" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">[root@ARM 19:15 /root/lmbench3]</span><br><span class="line">#numactl -H</span><br><span class="line">available: 4 nodes (0-3)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23</span><br><span class="line">node 0 size: 192832 MB</span><br><span class="line">node 0 free: 146830 MB</span><br><span class="line">node 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47</span><br><span class="line">node 1 size: 193533 MB</span><br><span class="line">node 1 free: 175354 MB</span><br><span class="line">node 2 cpus: 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71</span><br><span class="line">node 2 size: 193533 MB</span><br><span class="line">node 2 free: 175718 MB</span><br><span class="line">node 3 cpus: 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span><br><span class="line">node 3 size: 193532 MB</span><br><span class="line">node 3 free: 183643 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1   2   3</span><br><span class="line">  0:  10  12  20  22</span><br><span class="line">  1:  12  10  22  24</span><br><span class="line">  2:  20  22  10  12</span><br><span class="line">  3:  22  24  12  10</span><br><span class="line">  </span><br><span class="line">  #lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    48</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          4</span><br><span class="line">Model:                 0</span><br><span class="line">CPU max MHz:           2600.0000</span><br><span class="line">CPU min MHz:           200.0000</span><br><span class="line">BogoMIPS:              200.00</span><br><span class="line">L1d cache:             64K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              512K</span><br><span class="line">L3 cache:              24576K</span><br><span class="line">NUMA node0 CPU(s):     0-23</span><br><span class="line">NUMA node1 CPU(s):     24-47</span><br><span class="line">NUMA node2 CPU(s):     48-71</span><br><span class="line">NUMA node3 CPU(s):     72-95</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure>



<h3 id="飞腾2500"><a href="#飞腾2500" class="headerlink" title="飞腾2500"></a>飞腾2500</h3><p>飞腾2500用nop去跑IPC的话，只能到1，但是跑其它代码能到2.33</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">lscpu</span></span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    64</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          16</span><br><span class="line">Model:                 3</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              2048K</span><br><span class="line">L3 cache:              65536K</span><br><span class="line">NUMA node0 CPU(s):     0-7</span><br><span class="line">NUMA node1 CPU(s):     8-15</span><br><span class="line">NUMA node2 CPU(s):     16-23</span><br><span class="line">NUMA node3 CPU(s):     24-31</span><br><span class="line">NUMA node4 CPU(s):     32-39</span><br><span class="line">NUMA node5 CPU(s):     40-47</span><br><span class="line">NUMA node6 CPU(s):     48-55</span><br><span class="line">NUMA node7 CPU(s):     56-63</span><br><span class="line">NUMA node8 CPU(s):     64-71</span><br><span class="line">NUMA node9 CPU(s):     72-79</span><br><span class="line">NUMA node10 CPU(s):    80-87</span><br><span class="line">NUMA node11 CPU(s):    88-95</span><br><span class="line">NUMA node12 CPU(s):    96-103</span><br><span class="line">NUMA node13 CPU(s):    104-111</span><br><span class="line">NUMA node14 CPU(s):    112-119</span><br><span class="line">NUMA node15 CPU(s):    120-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">perf <span class="built_in">stat</span> ./nop</span></span><br><span class="line">failed to read counter stalled-cycles-frontend</span><br><span class="line">failed to read counter stalled-cycles-backend</span><br><span class="line">failed to read counter branches</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &#x27;./nop&#x27;:</span><br><span class="line"></span><br><span class="line">      78638.700540      task-clock (msec)         #    0.999 CPUs utilized</span><br><span class="line">              1479      context-switches          #    0.019 K/sec</span><br><span class="line">                55      cpu-migrations            #    0.001 K/sec</span><br><span class="line">                37      page-faults               #    0.000 K/sec</span><br><span class="line">      165127619524      cycles                    #    2.100 GHz</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">      165269372437      instructions              #    1.00  insns per cycle</span><br><span class="line">   &lt;not supported&gt;      branches</span><br><span class="line">           3057191      branch-misses             #    0.00% of all branches</span><br><span class="line"></span><br><span class="line">      78.692839007 seconds time elapsed</span><br><span class="line">      </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">dmidecode -t processor</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">dmidecode 3.0</span></span><br><span class="line">Getting SMBIOS data from sysfs.</span><br><span class="line">SMBIOS 3.2.0 present.</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">SMBIOS implementations newer than version 3.0 are not</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">fully supported by this version of dmidecode.</span></span><br><span class="line"></span><br><span class="line">Handle 0x0004, DMI type 4, 48 bytes</span><br><span class="line">Processor Information</span><br><span class="line">	Socket Designation: BGA3576</span><br><span class="line">	Type: Central Processor</span><br><span class="line">	Family: &lt;OUT OF SPEC&gt;</span><br><span class="line">	Manufacturer: PHYTIUM</span><br><span class="line">	ID: 00 00 00 00 70 1F 66 22</span><br><span class="line">	Version: S2500</span><br><span class="line">	Voltage: 0.8 V</span><br><span class="line">	External Clock: 50 MHz</span><br><span class="line">	Max Speed: 2100 MHz</span><br><span class="line">	Current Speed: 2100 MHz</span><br><span class="line">	Status: Populated, Enabled</span><br><span class="line">	Upgrade: Other</span><br><span class="line">	L1 Cache Handle: 0x0005</span><br><span class="line">	L2 Cache Handle: 0x0007</span><br><span class="line">	L3 Cache Handle: 0x0008</span><br><span class="line">	Serial Number: N/A</span><br><span class="line">	Asset Tag: No Asset Tag</span><br><span class="line">	Part Number: NULL</span><br><span class="line">	Core Count: 64</span><br><span class="line">	Core Enabled: 64</span><br><span class="line">	Thread Count: 64</span><br><span class="line">	Characteristics:</span><br><span class="line">		64-bit capable</span><br><span class="line">		Multi-Core</span><br><span class="line">		Hardware Thread</span><br><span class="line">		Execute Protection</span><br><span class="line">		Enhanced Virtualization</span><br><span class="line">		Power/Performance Control</span><br></pre></td></tr></table></figure>

<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>2Die，2node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    128</span><br><span class="line">Socket(s):             1</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Model:                 0</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             64K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              65536K //64core share</span><br><span class="line">NUMA node0 CPU(s):     0-63</span><br><span class="line">NUMA node1 CPU(s):     64-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh</span><br><span class="line"></span><br><span class="line">#cat cpu&#123;0,1,8,16,30,31,32,127&#125;/cache/index3/shared_cpu_list</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">0-63</span><br><span class="line">64-127</span><br><span class="line"></span><br><span class="line">#grep -E &quot;core|64.000&quot; lat.log</span><br><span class="line">core:0</span><br><span class="line">64.00000 59.653</span><br><span class="line">core:8</span><br><span class="line">64.00000 62.265</span><br><span class="line">core:16</span><br><span class="line">64.00000 59.411</span><br><span class="line">core:24</span><br><span class="line">64.00000 55.836</span><br><span class="line">core:32</span><br><span class="line">64.00000 55.909</span><br><span class="line">core:40</span><br><span class="line">64.00000 56.176</span><br><span class="line">core:48</span><br><span class="line">64.00000 57.240</span><br><span class="line">core:56</span><br><span class="line">64.00000 59.485</span><br><span class="line">core:64</span><br><span class="line">64.00000 131.818</span><br><span class="line">core:72</span><br><span class="line">64.00000 127.182</span><br><span class="line">core:80</span><br><span class="line">64.00000 122.452</span><br><span class="line">core:88</span><br><span class="line">64.00000 121.673</span><br><span class="line">core:96</span><br><span class="line">64.00000 126.533</span><br><span class="line">core:104</span><br><span class="line">64.00000 125.673</span><br><span class="line">core:112</span><br><span class="line">64.00000 124.188</span><br><span class="line">core:120</span><br><span class="line">64.00000 130.202</span><br><span class="line"></span><br><span class="line">#numactl -H</span><br><span class="line">available: 2 nodes (0-1)</span><br><span class="line">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63</span><br><span class="line">node 0 size: 515652 MB</span><br><span class="line">node 0 free: 514913 MB</span><br><span class="line">node 1 cpus: 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127</span><br><span class="line">node 1 size: 516086 MB</span><br><span class="line">node 1 free: 514815 MB</span><br><span class="line">node distances:</span><br><span class="line">node   0   1</span><br><span class="line">  0:  10  15</span><br><span class="line">  1:  15  10</span><br></pre></td></tr></table></figure>

<h2 id="单核以及HT计算Prime性能比较"><a href="#单核以及HT计算Prime性能比较" class="headerlink" title="单核以及HT计算Prime性能比较"></a>单核以及HT计算Prime性能比较</h2><p>以上两款CPU但从物理上的指标来看似乎AMD要好很多，从工艺上AMD也要领先一代(2年），从单核参数上来说是2.0 VS 2.5GHz，但是IPC 是5 VS 4，算下来理想的单核性能刚好一致（2*5&#x3D;2.5 *4）。</p>
<p>从外面的一些跑分结果显示也是AMD 要好，但是实际性能怎么样呢？</p>
<p>测试命令，这个测试命令无论在哪个CPU下，用2个物理核用时都是一个物理核的一半，所以这个计算是可以完全并行的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskset -c 1 /usr/bin/sysbench --num-threads=1 --test=cpu --cpu-max-prime=50000 run //单核用一个threads，绑核; HT用2个threads，绑一对HT</span><br></pre></td></tr></table></figure>

<p>测试结果为耗时，单位秒</p>
<table>
<thead>
<tr>
<th align="left">测试项</th>
<th>AMD EPYC 7H12 2.5G CentOS 7.9</th>
<th>Hygon 7280 2.1GHz CentOS</th>
<th align="left">Hygon 7280 2.1GHz 麒麟</th>
<th>Intel 8269 2.50G</th>
<th align="left">Intel 8163 CPU @ 2.50GHz</th>
<th align="left">Intel E5-2682 v4 @ 2.50GHz</th>
</tr>
</thead>
<tbody><tr>
<td align="left">单核  prime 50000 耗时</td>
<td>59秒  IPC 0.56</td>
<td>77秒 IPC 0.55</td>
<td align="left">89秒  IPC 0.56;</td>
<td>83 0.41</td>
<td align="left">105秒  IPC 0.41</td>
<td align="left">109秒  IPC 0.39</td>
</tr>
<tr>
<td align="left">HT  prime 50000 耗时</td>
<td>57秒  IPC 0.31</td>
<td>74秒 IPC 0.29</td>
<td align="left">87秒  IPC 0.29</td>
<td>48 0.35</td>
<td align="left">60秒   IPC 0.36</td>
<td align="left">74秒    IPC 0.29</td>
</tr>
</tbody></table>
<p>相同CPU下的 指令数 基本&#x3D; 耗时 * IPC * 核数</p>
<p>以上测试结果显示Hygon 7280单核计算能力是要强过Intel 8163的，但是超线程在这个场景下太不给力，相当于没有。</p>
<p>当然上面的计算Prime太单纯了，代表不了复杂的业务场景，所以接下来用MySQL的查询场景来看看。</p>
<p>如果是arm芯片在计算prime上明显要好过x86，猜测是除法取余指令上有优化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#taskset -c 11 sysbench cpu --threads=1 --events=50000  run</span><br><span class="line">sysbench 1.0.20 (using bundled LuaJIT 2.1.0-beta2)</span><br></pre></td></tr></table></figure>

<p>测试结果为10秒钟的event</p>
<table>
<thead>
<tr>
<th align="left">测试项</th>
<th>FT2500 2.1G</th>
<th>鲲鹏920-4826 2.6GHz</th>
<th align="left">Intel 8163 CPU @ 2.50GHz</th>
<th>Hygon C86 7280 2.1GHz</th>
<th>AMD 7T83</th>
</tr>
</thead>
<tbody><tr>
<td align="left">单核  prime 10秒 events</td>
<td>21626  IPC 0.89</td>
<td>30299 IPC 1.01</td>
<td align="left">8435  IPC 0.41</td>
<td>10349  IPC 0.63</td>
<td>40112  IPC 1.38</td>
</tr>
</tbody></table>
<h2 id="对比MySQL-sysbench和tpcc性能"><a href="#对比MySQL-sysbench和tpcc性能" class="headerlink" title="对比MySQL sysbench和tpcc性能"></a>对比MySQL sysbench和tpcc性能</h2><p>分别将MySQL 5.7.34社区版部署到intel+AliOS以及hygon 7280+CentOS上，将mysqld绑定到单核，一样的压力配置均将CPU跑到100%，然后用sysbench测试点查， HT表示将mysqld绑定到一对HT核。</p>
<h3 id="sysbench点查"><a href="#sysbench点查" class="headerlink" title="sysbench点查"></a>sysbench点查</h3><p> 测试命令类似如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysbench --test=&#x27;/usr/share/doc/sysbench/tests/db/select.lua&#x27; --oltp_tables_count=1 --report-interval=1 --oltp-table-size=10000000  --mysql-port=3307 --mysql-db=sysbench_single --mysql-user=root --mysql-password=&#x27;Bj6f9g96!@#&#x27;  --max-requests=0   --oltp_skip_trx=on --oltp_auto_inc=on  --oltp_range_size=5  --mysql-table-engine=innodb --rand-init=on   --max-time=300 --mysql-host=x86.51 --num-threads=4 run</span><br></pre></td></tr></table></figure>

<p>测试结果(测试中的差异AMD、Hygon CPU跑在CentOS7.9， intel CPU、Kunpeng 920 跑在AliOS上, xdb表示用集团的xdb替换社区的MySQL Server， 麒麟是国产OS)：</p>
<table>
<thead>
<tr>
<th align="left">测试核数</th>
<th>AMD EPYC 7H12 2.5G</th>
<th align="left">Hygon 7280 2.1G</th>
<th>Hygon 7280 2.1GHz 麒麟</th>
<th>Intel 8269 2.50G</th>
<th align="left">Intel 8163 2.50G</th>
<th align="left">Intel 8163 2.50G XDB5.7</th>
<th>鲲鹏 920-4826 2.6G</th>
<th>鲲鹏 920-4826 2.6G XDB8.0</th>
<th>FT2500 alisql 8.0 本地–socket</th>
</tr>
</thead>
<tbody><tr>
<td align="left">单核</td>
<td>24674  0.54</td>
<td align="left">13441  0.46</td>
<td>10236  0.39</td>
<td>28208 0.75</td>
<td align="left">25474   0.84</td>
<td align="left">29376    0.89</td>
<td>9694  0.49</td>
<td>8301  0.46</td>
<td>3602 0.53</td>
</tr>
<tr>
<td align="left">一对HT</td>
<td>36157 0.42</td>
<td align="left">21747  0.38</td>
<td>19417  0.37</td>
<td>36754 0.49</td>
<td align="left">35894  0.6</td>
<td align="left">40601  0.65</td>
<td>无HT</td>
<td>无HT</td>
<td>无HT</td>
</tr>
<tr>
<td align="left">4物理核</td>
<td>94132 0.52</td>
<td align="left">49822 0.46</td>
<td>38033  0.37</td>
<td>90434 0.69 350%</td>
<td align="left">87254  0.73</td>
<td align="left">106472  0.83</td>
<td>34686  0.42</td>
<td>28407  0.39</td>
<td>14232 0.53</td>
</tr>
<tr>
<td align="left">16物理核</td>
<td>325409 0.48</td>
<td align="left">171630 0.38</td>
<td>134980  0.34</td>
<td>371718 0.69 1500%</td>
<td align="left">332967  0.72</td>
<td align="left">446290  0.85 &#x2F;&#x2F;16核比4核好！</td>
<td>116122  0.35</td>
<td>94697  0.33</td>
<td>59199  0.6  8core:31210 0.59</td>
</tr>
<tr>
<td align="left">32物理核</td>
<td>542192 0.43</td>
<td align="left">298716 0.37</td>
<td>255586  0.33</td>
<td>642548 0.64 2700%</td>
<td align="left">588318  0.67</td>
<td align="left">598637  0.81 CPU 2400%</td>
<td>228601  0.36</td>
<td>177424  0.32</td>
<td>114020 0.65</td>
</tr>
</tbody></table>
<ul>
<li>麒麟OS下CPU很难跑满，大致能跑到90%-95%左右，麒麟上装的社区版MySQL-5.7.29；飞腾要特别注意mysqld所在socket，同时以上飞腾数据都是走–sock压测所得，32core走网络压测QPS为：99496（15%的网络损耗）[^说明]</li>
</ul>
<h3 id="Mysqld-二进制代码所在-page-cache带来的性能影响"><a href="#Mysqld-二进制代码所在-page-cache带来的性能影响" class="headerlink" title="Mysqld 二进制代码所在 page cache带来的性能影响"></a>Mysqld 二进制代码所在 page cache带来的性能影响</h3><p>如果是飞腾跨socket影响很大，mysqld二进制跨socket性能会下降30%以上</p>
<p>对于鲲鹏920，双路服务器上测试，mysqld绑在node0, 但是分别将mysqld二进制load进不同的node上的page cache，然后执行点查</p>
<table>
<thead>
<tr>
<th>mysqld</th>
<th>node0</th>
<th>node1</th>
<th>node2</th>
<th>node3</th>
</tr>
</thead>
<tbody><tr>
<td>QPS</td>
<td>190120 IPC 0.40</td>
<td>182518 IPC 0.39</td>
<td>189046 IPC 0.40</td>
<td>186533 IPC 0.40</td>
</tr>
</tbody></table>
<p>以上数据可以看出这里node0到node1还是很慢的，居然比跨socket还慢，反过来说鲲鹏跨socket性能很好</p>
<p>绑定mysqld到不同node的page cache操作</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#systemctl stop mysql-server</span><br><span class="line"></span><br><span class="line">[root@poc65 /root/vmtouch]</span><br><span class="line">#vmtouch -e /usr/local/mysql/bin/mysqld</span><br><span class="line">           Files: 1</span><br><span class="line">     Directories: 0</span><br><span class="line">   Evicted Pages: 5916 (23M)</span><br><span class="line">         Elapsed: 0.00322 seconds</span><br><span class="line"></span><br><span class="line">#vmtouch -v /usr/local/mysql/bin/mysqld</span><br><span class="line">/usr/local/mysql/bin/mysqld</span><br><span class="line">[                                                            ] 0/5916</span><br><span class="line"></span><br><span class="line">           Files: 1</span><br><span class="line">     Directories: 0</span><br><span class="line">  Resident Pages: 0/5916  0/23M  0%</span><br><span class="line">         Elapsed: 0.000204 seconds</span><br><span class="line"></span><br><span class="line">#taskset -c 24 md5sum /usr/local/mysql/bin/mysqld</span><br><span class="line"></span><br><span class="line">#grep mysqld /proc/`pidof mysqld`/numa_maps  //检查mysqld具体绑定在哪个node上</span><br><span class="line">00400000 default file=/usr/local/mysql/bin/mysqld mapped=3392 active=1 N0=3392 kernelpagesize_kB=4</span><br><span class="line">0199b000 default file=/usr/local/mysql/bin/mysqld anon=10 dirty=10 mapped=134 active=10 N0=134 kernelpagesize_kB=4</span><br><span class="line">01a70000 default file=/usr/local/mysql/bin/mysqld anon=43 dirty=43 mapped=120 active=43 N0=120 kernelpagesize_kB=4</span><br></pre></td></tr></table></figure>

<h3 id="网卡以及node距离带来的性能差异"><a href="#网卡以及node距离带来的性能差异" class="headerlink" title="网卡以及node距离带来的性能差异"></a>网卡以及node距离带来的性能差异</h3><p>在鲲鹏920+mysql5.7+alios，将内存分配锁在node0上，然后分别绑核在1、24、48、72core，进行sysbench点查对比</p>
<table>
<thead>
<tr>
<th></th>
<th>Core1</th>
<th>Core24</th>
<th>Core48</th>
<th>Core72</th>
</tr>
</thead>
<tbody><tr>
<td>QPS</td>
<td>10800</td>
<td>10400</td>
<td>7700</td>
<td>7700</td>
</tr>
</tbody></table>
<p>以上测试的时候业务进程分配的内存全限制在node0上（下面的网卡中断测试也是同样内存结构）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#/root/numa-maps-summary.pl &lt;/proc/123853/numa_maps</span><br><span class="line">N0        :      5085548 ( 19.40 GB)</span><br><span class="line">N1        :         4479 (  0.02 GB)</span><br><span class="line">N2        :            1 (  0.00 GB)</span><br><span class="line">active    :            0 (  0.00 GB)</span><br><span class="line">anon      :      5085455 ( 19.40 GB)</span><br><span class="line">dirty     :      5085455 ( 19.40 GB)</span><br><span class="line">kernelpagesize_kB:         2176 (  0.01 GB)</span><br><span class="line">mapmax    :          348 (  0.00 GB)</span><br><span class="line">mapped    :         4626 (  0.02 GB)</span><br></pre></td></tr></table></figure>

<p>对比测试，将内存锁在node3上，重复进行以上测试结果如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>Core1</th>
<th>Core24</th>
<th>Core48</th>
<th>Core72</th>
</tr>
</thead>
<tbody><tr>
<td>QPS</td>
<td>10500</td>
<td>10000</td>
<td>8100</td>
<td>8000</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#/root/numa-maps-summary.pl &lt;/proc/54478/numa_maps</span><br><span class="line">N0        :           16 (  0.00 GB)</span><br><span class="line">N1        :         4401 (  0.02 GB)</span><br><span class="line">N2        :            1 (  0.00 GB)</span><br><span class="line">N3        :      1779989 (  6.79 GB)</span><br><span class="line">active    :            0 (  0.00 GB)</span><br><span class="line">anon      :      1779912 (  6.79 GB)</span><br><span class="line">dirty     :      1779912 (  6.79 GB)</span><br><span class="line">kernelpagesize_kB:         1108 (  0.00 GB)</span><br><span class="line">mapmax    :          334 (  0.00 GB)</span><br><span class="line">mapped    :         4548 (  0.02 GB)</span><br></pre></td></tr></table></figure>

<p>机器上网卡eth1插在node0上，由以上两组对比测试发现网卡影响比内存跨node影响更大，网卡影响有20%。而内存的影响基本看不到（就近好那么一点点，但是不明显，只能解释为cache命中率很高了）</p>
<p>此时软中断都在node0上，如果将软中断绑定到node3上，第72core的QPS能提升到8500，并且非常稳定。同时core0的QPS下降到10000附近。</p>
<h4 id="网卡软中断以及网卡远近的测试结论"><a href="#网卡软中断以及网卡远近的测试结论" class="headerlink" title="网卡软中断以及网卡远近的测试结论"></a>网卡软中断以及网卡远近的测试结论</h4><p>测试机器只是用了一块网卡，网卡插在node0上。</p>
<p>一般网卡中断会占用一些CPU，如果把网卡中断挪到其它node的core上，在鲲鹏920上测试，业务跑在node3（使用全部24core），网卡中断分别在node0和node3，QPS分别是：179000 VS 175000 （此时把中断放到node0或者是和node3最近的node2上差别不大）</p>
<p>如果将业务跑在node0上（全部24core），网卡中断分别在node0和node1上得到的QPS分别是：204000 VS 212000 </p>
<h3 id="tpcc-1000仓"><a href="#tpcc-1000仓" class="headerlink" title="tpcc 1000仓"></a>tpcc 1000仓</h3><p>测试结果(测试中Hygon 7280分别跑在CentOS7.9和麒麟上， 鲲鹏&#x2F;intel CPU 跑在AliOS、麒麟是国产OS)：</p>
<p>tpcc测试数据，结果为1000仓，tpmC (NewOrders) ，未标注CPU 则为跑满了</p>
<table>
<thead>
<tr>
<th>测试核数</th>
<th>Intel 8269 2.50G</th>
<th>Intel 8163 2.50G</th>
<th>Hygon 7280 2.1GHz 麒麟</th>
<th>Hygon 7280 2.1G CentOS 7.9</th>
<th>鲲鹏 920-4826 2.6G</th>
<th>鲲鹏 920-4826 2.6G XDB8.0</th>
</tr>
</thead>
<tbody><tr>
<td>1物理核</td>
<td>12392</td>
<td>9902</td>
<td>4706</td>
<td>7011</td>
<td>6619</td>
<td>4653</td>
</tr>
<tr>
<td>一对HT</td>
<td>17892</td>
<td>15324</td>
<td>8950</td>
<td>11778</td>
<td>无HT</td>
<td>无HT</td>
</tr>
<tr>
<td>4物理核</td>
<td>51525</td>
<td>40877</td>
<td>19387 380%</td>
<td>30046</td>
<td>23959</td>
<td>20101</td>
</tr>
<tr>
<td>8物理核</td>
<td>100792</td>
<td>81799</td>
<td>39664 750%</td>
<td>60086</td>
<td>42368</td>
<td>40572</td>
</tr>
<tr>
<td>16物理核</td>
<td>160798 抖动</td>
<td>140488 CPU抖动</td>
<td>75013 1400%</td>
<td>106419 1300-1550%</td>
<td>70581  1200%</td>
<td>79844</td>
</tr>
<tr>
<td>24物理核</td>
<td>188051</td>
<td>164757 1600-2100%</td>
<td>100841 1800-2000%</td>
<td>130815 1600-2100%</td>
<td>88204  1600%</td>
<td>115355</td>
</tr>
<tr>
<td>32物理核</td>
<td>195292</td>
<td>185171 2000-2500%</td>
<td>116071 1900-2400%</td>
<td>142746 1800-2400%</td>
<td>102089  1900%</td>
<td>143567</td>
</tr>
<tr>
<td>48物理核</td>
<td>19969l</td>
<td>195730 2100-2600%</td>
<td>128188  2100-2800%</td>
<td>149782 2000-2700%</td>
<td>116374  2500%</td>
<td>206055  4500%</td>
</tr>
</tbody></table>
<p>tpcc并发到一定程度后主要是锁导致性能上不去，所以超多核意义不大。</p>
<p>如果在Hygon 7280 2.1GHz 麒麟上起两个MySQLD实例，每个实例各绑定32物理core，性能刚好翻倍：<img src="/images/951413iMgBlog/image-20210823082702539.png" alt="image-20210823082702539"></p>
<p>测试过程CPU均跑满（未跑满的话会标注出来），IPC跑不起来性能就必然低，超线程虽然总性能好了但是会导致IPC降低(参考前面的公式)。可以看到对本来IPC比较低的场景，启用超线程后一般对性能会提升更大一些。</p>
<p>CPU核数增加到32核后，MySQL社区版性能追平xdb， 此时sysbench使用120线程压性能较好（AMD得240线程压）</p>
<p>32核的时候对比下MySQL 社区版在Hygon7280和Intel 8163下的表现：</p>
<p><img src="/images/951413iMgBlog/image-20210817181752243.png" alt="image-20210817181752243"></p>
<h2 id="三款CPU的性能指标"><a href="#三款CPU的性能指标" class="headerlink" title="三款CPU的性能指标"></a>三款CPU的性能指标</h2><table>
<thead>
<tr>
<th align="left">测试项</th>
<th>AMD EPYC 7H12 2.5G</th>
<th align="left">Hygon 7280 2.1GHz</th>
<th align="left">Intel 8163 CPU @ 2.50GHz</th>
</tr>
</thead>
<tbody><tr>
<td align="left">内存带宽(MiB&#x2F;s)</td>
<td>12190.50</td>
<td align="left">6206.06</td>
<td align="left">7474.45</td>
</tr>
<tr>
<td align="left">内存延时(遍历很大一个数组)</td>
<td>0.334ms</td>
<td align="left">0.336ms</td>
<td align="left">0.429ms</td>
</tr>
</tbody></table>
<h2 id="在lmbench上的测试数据"><a href="#在lmbench上的测试数据" class="headerlink" title="在lmbench上的测试数据"></a>在lmbench上的测试数据</h2><p>stream主要用于测试带宽，对应的时延是在带宽跑满情况下的带宽。</p>
<p>lat_mem_rd用来测试操作不同数据大小的时延。总的来说带宽看stream、时延看lat_mem_rd</p>
<h3 id="飞腾2500-1"><a href="#飞腾2500-1" class="headerlink" title="飞腾2500"></a>飞腾2500</h3><p>用stream测试带宽和latency，可以看到带宽随着numa距离不断减少、对应的latency不断增加，到最近的numa node有10%的损耗，这个损耗和numactl给出的距离完全一致。跨socket访问内存latency是node内的3倍，带宽是三分之一，但是socket1性能和socket0性能完全一致</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line">time for i in $(seq 7 8 128); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 0 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 2.84 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 5638.21 MB/sec</span><br><span class="line">STREAM scale latency: 2.72 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 5885.97 MB/sec</span><br><span class="line">STREAM add latency: 2.26 nanoseconds</span><br><span class="line">STREAM add bandwidth: 10615.13 MB/sec</span><br><span class="line">STREAM triad latency: 4.53 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 5297.93 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 1 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.16 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 5058.71 MB/sec</span><br><span class="line">STREAM scale latency: 3.15 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 5074.78 MB/sec</span><br><span class="line">STREAM add latency: 2.35 nanoseconds</span><br><span class="line">STREAM add bandwidth: 10197.36 MB/sec</span><br><span class="line">STREAM triad latency: 5.12 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 4686.37 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 2 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.85 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 4150.98 MB/sec</span><br><span class="line">STREAM scale latency: 3.95 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 4054.30 MB/sec</span><br><span class="line">STREAM add latency: 2.64 nanoseconds</span><br><span class="line">STREAM add bandwidth: 9100.12 MB/sec</span><br><span class="line">STREAM triad latency: 6.39 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 3757.70 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.69 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 4340.24 MB/sec</span><br><span class="line">STREAM scale latency: 3.62 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 4422.18 MB/sec</span><br><span class="line">STREAM add latency: 2.47 nanoseconds</span><br><span class="line">STREAM add bandwidth: 9704.82 MB/sec</span><br><span class="line">STREAM triad latency: 5.74 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 4177.85 MB/sec</span><br><span class="line"></span><br><span class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</span><br><span class="line">#numactl -C 7 -m 7 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.95 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 4051.51 MB/sec</span><br><span class="line">STREAM scale latency: 3.94 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 4060.63 MB/sec</span><br><span class="line">STREAM add latency: 2.54 nanoseconds</span><br><span class="line">STREAM add bandwidth: 9434.51 MB/sec</span><br><span class="line">STREAM triad latency: 6.13 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 3913.36 MB/sec</span><br><span class="line"></span><br><span class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</span><br><span class="line">#numactl -C 7 -m 10 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 8.80 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 1817.78 MB/sec</span><br><span class="line">STREAM scale latency: 8.59 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 1861.65 MB/sec</span><br><span class="line">STREAM add latency: 5.55 nanoseconds</span><br><span class="line">STREAM add bandwidth: 4320.68 MB/sec</span><br><span class="line">STREAM triad latency: 13.94 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 1721.76 MB/sec</span><br><span class="line"></span><br><span class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</span><br><span class="line">#numactl -C 7 -m 11 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 9.27 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 1726.52 MB/sec</span><br><span class="line">STREAM scale latency: 9.31 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 1718.10 MB/sec</span><br><span class="line">STREAM add latency: 5.65 nanoseconds</span><br><span class="line">STREAM add bandwidth: 4250.89 MB/sec</span><br><span class="line">STREAM triad latency: 14.09 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 1703.66 MB/sec</span><br><span class="line"></span><br><span class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</span><br><span class="line">#numactl -C 88 -m 11 ./bin/stream  -W 5 -N 5 -M 64M //在另外一个socket上测试本numa，和node0性能完全一致</span><br><span class="line">STREAM copy latency: 2.93 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 5454.67 MB/sec</span><br><span class="line">STREAM scale latency: 2.96 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 5400.03 MB/sec</span><br><span class="line">STREAM add latency: 2.28 nanoseconds</span><br><span class="line">STREAM add bandwidth: 10543.42 MB/sec</span><br><span class="line">STREAM triad latency: 4.52 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 5308.40 MB/sec</span><br><span class="line"></span><br><span class="line">[root@101a05001.cloud.a05.am11 /root/lmbench3]</span><br><span class="line">#numactl -C 7 -m 15 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 8.73 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 1831.77 MB/sec</span><br><span class="line">STREAM scale latency: 8.81 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 1815.13 MB/sec</span><br><span class="line">STREAM add latency: 5.63 nanoseconds</span><br><span class="line">STREAM add bandwidth: 4265.21 MB/sec</span><br><span class="line">STREAM triad latency: 13.09 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 1833.68 MB/sec</span><br></pre></td></tr></table></figure>

<p>Lat_mem_rd 用cpu7访问node0和node15对比结果，随着数据的加大，延时在加大，64M时能有3倍差距，和上面测试一致</p>
<p>下图 第一列 表示读写数据的大小（单位M），第二列表示访问延时（单位纳秒），一般可以看到在L1&#x2F;L2&#x2F;L3 cache大小的地方延时会有跳跃，远超过L3大小后，延时就是内存延时了</p>
<p><img src="/images/951413iMgBlog/image-20210924185044090.png" alt="image-20210924185044090"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numactl -C 7 -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M  //-C 7 cpu 7, -m 0 node0, -W 热身 -t stride</span><br></pre></td></tr></table></figure>

<p>同样的机型，开关numa的测试结果，关numa 时延、带宽都差了几倍</p>
<p><img src="/images/951413iMgBlog/image-20220323153507557.png" alt="image-20220323153507557"></p>
<p>关闭numa的机器上测试结果随机性很强，这应该是和内存分配在那里有关系，不过如果机器一直保持这个状态反复测试的话，快的core一直快，慢的core一直慢，这是因为物理地址分配有一定的规律，在物理内存没怎么变化的情况下，快的core恰好分到的内存比较近。</p>
<p>同时不同机器状态（内存使用率）测试结果也不一样</p>
<h3 id="鲲鹏920-1"><a href="#鲲鹏920-1" class="headerlink" title="鲲鹏920"></a>鲲鹏920</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">#for i in $(seq 0 15); do echo core:$i; numactl -N $i -m 7 ./bin/stream  -W 5 -N 5 -M 64M; done</span><br><span class="line">STREAM copy latency: 1.84 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 8700.75 MB/sec</span><br><span class="line">STREAM scale latency: 1.86 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 8623.60 MB/sec</span><br><span class="line">STREAM add latency: 2.18 nanoseconds</span><br><span class="line">STREAM add bandwidth: 10987.04 MB/sec</span><br><span class="line">STREAM triad latency: 3.03 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 7926.87 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 1 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 2.05 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 7802.45 MB/sec</span><br><span class="line">STREAM scale latency: 2.08 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 7681.87 MB/sec</span><br><span class="line">STREAM add latency: 2.19 nanoseconds</span><br><span class="line">STREAM add bandwidth: 10954.76 MB/sec</span><br><span class="line">STREAM triad latency: 3.17 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 7559.86 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 2 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.51 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 4556.86 MB/sec</span><br><span class="line">STREAM scale latency: 3.58 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 4463.66 MB/sec</span><br><span class="line">STREAM add latency: 2.71 nanoseconds</span><br><span class="line">STREAM add bandwidth: 8869.79 MB/sec</span><br><span class="line">STREAM triad latency: 5.92 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 4057.12 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 7 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 3.94 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 4064.25 MB/sec</span><br><span class="line">STREAM scale latency: 3.82 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 4188.67 MB/sec</span><br><span class="line">STREAM add latency: 2.86 nanoseconds</span><br><span class="line">STREAM add bandwidth: 8390.70 MB/sec</span><br><span class="line">STREAM triad latency: 4.78 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 5024.25 MB/sec</span><br><span class="line"></span><br><span class="line">#numactl -C 24 -m 3 ./bin/stream  -W 5 -N 5 -M 64M</span><br><span class="line">STREAM copy latency: 4.10 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 3904.63 MB/sec</span><br><span class="line">STREAM scale latency: 4.03 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 3969.41 MB/sec</span><br><span class="line">STREAM add latency: 3.07 nanoseconds</span><br><span class="line">STREAM add bandwidth: 7816.08 MB/sec</span><br><span class="line">STREAM triad latency: 5.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 4738.66 MB/sec</span><br></pre></td></tr></table></figure>

<h3 id="海光7280"><a href="#海光7280" class="headerlink" title="海光7280"></a>海光7280</h3><p>可以看到跨numa（一个numa也就是一个socket，等同于跨socket）RT从1.5上升到2.5，这个数据比鲲鹏920要好很多</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">[root@hygon8 14:32 /root/lmbench-master]</span><br><span class="line">#lscpu</span><br><span class="line">架构：                           x86_64</span><br><span class="line">CPU 运行模式：                   32-bit, 64-bit</span><br><span class="line">字节序：                         Little Endian</span><br><span class="line">Address sizes:                   43 bits physical, 48 bits virtual</span><br><span class="line">CPU:                             128</span><br><span class="line">在线 CPU 列表：                  0-127</span><br><span class="line">每个核的线程数：                 2</span><br><span class="line">每个座的核数：                   32</span><br><span class="line">座：                             2</span><br><span class="line">NUMA 节点：                      8</span><br><span class="line">厂商 ID：                        HygonGenuine</span><br><span class="line">CPU 系列：                       24</span><br><span class="line">型号：                           1</span><br><span class="line">型号名称：                       Hygon C86 7280 32-core Processor</span><br><span class="line">步进：                           1</span><br><span class="line">CPU MHz：                        2194.586</span><br><span class="line">BogoMIPS：                       3999.63</span><br><span class="line">虚拟化：                         AMD-V</span><br><span class="line">L1d 缓存：                       2 MiB</span><br><span class="line">L1i 缓存：                       4 MiB</span><br><span class="line">L2 缓存：                        32 MiB</span><br><span class="line">L3 缓存：                        128 MiB</span><br><span class="line">NUMA 节点0 CPU：                 0-7,64-71</span><br><span class="line">NUMA 节点1 CPU：                 8-15,72-79</span><br><span class="line">NUMA 节点2 CPU：                 16-23,80-87</span><br><span class="line">NUMA 节点3 CPU：                 24-31,88-95</span><br><span class="line">NUMA 节点4 CPU：                 32-39,96-103</span><br><span class="line">NUMA 节点5 CPU：                 40-47,104-111</span><br><span class="line">NUMA 节点6 CPU：                 48-55,112-119</span><br><span class="line">NUMA 节点7 CPU：                 56-63,120-127</span><br><span class="line"></span><br><span class="line">//可以看到7号core比15、23、31号core明显要快，就近访问node 0的内存，跨numa node（跨Die）没有内存交织分配</span><br><span class="line">[root@hygon8 14:32 /root/lmbench-master]</span><br><span class="line">#time for i in $(seq 7 8 64); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">7</span><br><span class="line">STREAM copy latency: 1.38 nanoseconds    </span><br><span class="line">STREAM copy bandwidth: 11559.53 MB/sec</span><br><span class="line">STREAM scale latency: 1.16 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 13815.87 MB/sec</span><br><span class="line">STREAM add latency: 1.40 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17145.85 MB/sec</span><br><span class="line">STREAM triad latency: 1.44 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 16637.18 MB/sec</span><br><span class="line">15</span><br><span class="line">STREAM copy latency: 1.67 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 9591.77 MB/sec</span><br><span class="line">STREAM scale latency: 1.56 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10242.50 MB/sec</span><br><span class="line">STREAM add latency: 1.45 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16581.00 MB/sec</span><br><span class="line">STREAM triad latency: 2.00 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12028.83 MB/sec</span><br><span class="line">23</span><br><span class="line">STREAM copy latency: 1.65 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 9701.49 MB/sec</span><br><span class="line">STREAM scale latency: 1.53 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10427.98 MB/sec</span><br><span class="line">STREAM add latency: 1.42 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16846.10 MB/sec</span><br><span class="line">STREAM triad latency: 1.97 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12189.72 MB/sec</span><br><span class="line">31</span><br><span class="line">STREAM copy latency: 1.64 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 9742.86 MB/sec</span><br><span class="line">STREAM scale latency: 1.52 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10510.80 MB/sec</span><br><span class="line">STREAM add latency: 1.45 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16559.86 MB/sec</span><br><span class="line">STREAM triad latency: 1.92 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12490.01 MB/sec</span><br><span class="line">39</span><br><span class="line">STREAM copy latency: 2.55 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 6286.25 MB/sec</span><br><span class="line">STREAM scale latency: 2.51 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 6383.11 MB/sec</span><br><span class="line">STREAM add latency: 1.76 nanoseconds</span><br><span class="line">STREAM add bandwidth: 13660.83 MB/sec</span><br><span class="line">STREAM triad latency: 3.68 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 6523.02 MB/sec</span><br></pre></td></tr></table></figure>

<p>如果这种芯片在bios里设置Die interleaving，4块die当成一个numa node吐出来给OS</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">架构：                           x86_64</span><br><span class="line">CPU 运行模式：                   32-bit, 64-bit</span><br><span class="line">字节序：                         Little Endian</span><br><span class="line">Address sizes:                   43 bits physical, 48 bits virtual</span><br><span class="line">CPU:                             128</span><br><span class="line">在线 CPU 列表：                  0-127</span><br><span class="line">每个核的线程数：                 2</span><br><span class="line">每个座的核数：                   32</span><br><span class="line">座：                             2</span><br><span class="line">NUMA 节点：                      2</span><br><span class="line">厂商 ID：                        HygonGenuine</span><br><span class="line">CPU 系列：                       24</span><br><span class="line">型号：                           1</span><br><span class="line">型号名称：                       Hygon C86 7280 32-core Processor</span><br><span class="line">步进：                           1</span><br><span class="line">CPU MHz：                        2108.234</span><br><span class="line">BogoMIPS：                       3999.45</span><br><span class="line">虚拟化：                         AMD-V</span><br><span class="line">L1d 缓存：                       2 MiB</span><br><span class="line">L1i 缓存：                       4 MiB</span><br><span class="line">L2 缓存：                        32 MiB</span><br><span class="line">L3 缓存：                        128 MiB</span><br><span class="line">//注意这里和真实物理架构不一致，bios配置了Die Interleaving Enable</span><br><span class="line">//表示每路内多个Die内存交织分配，这样整个一路就是一个大Die</span><br><span class="line">NUMA 节点0 CPU：                 0-31,64-95  </span><br><span class="line">NUMA 节点1 CPU：                 32-63,96-127</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//enable die interleaving 后继续streaming测试</span><br><span class="line">//最终测试结果表现就是7/15/23/31 core性能一致，因为默认一个numa内内存交织分配</span><br><span class="line">//可以看到同一路下的四个die内存交织访问，所以4个node内存延时一样了（被平均），都不如就近快</span><br><span class="line">[root@hygon3 16:09 /root/lmbench-master]</span><br><span class="line">#time for i in $(seq 7 8 64); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">7</span><br><span class="line">STREAM copy latency: 1.48 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10782.58 MB/sec</span><br><span class="line">STREAM scale latency: 1.20 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 13364.38 MB/sec</span><br><span class="line">STREAM add latency: 1.46 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16408.32 MB/sec</span><br><span class="line">STREAM triad latency: 1.53 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 15696.00 MB/sec</span><br><span class="line">15</span><br><span class="line">STREAM copy latency: 1.51 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10601.25 MB/sec</span><br><span class="line">STREAM scale latency: 1.24 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 12855.87 MB/sec</span><br><span class="line">STREAM add latency: 1.46 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16382.42 MB/sec</span><br><span class="line">STREAM triad latency: 1.53 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 15691.48 MB/sec</span><br><span class="line">23</span><br><span class="line">STREAM copy latency: 1.50 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10700.61 MB/sec</span><br><span class="line">STREAM scale latency: 1.27 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 12634.63 MB/sec</span><br><span class="line">STREAM add latency: 1.47 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16370.67 MB/sec</span><br><span class="line">STREAM triad latency: 1.55 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 15455.75 MB/sec</span><br><span class="line">31</span><br><span class="line">STREAM copy latency: 1.50 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10637.39 MB/sec</span><br><span class="line">STREAM scale latency: 1.25 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 12778.99 MB/sec</span><br><span class="line">STREAM add latency: 1.46 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16420.65 MB/sec</span><br><span class="line">STREAM triad latency: 1.61 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 14946.80 MB/sec</span><br><span class="line">39</span><br><span class="line">STREAM copy latency: 2.35 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 6807.09 MB/sec</span><br><span class="line">STREAM scale latency: 2.32 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 6906.93 MB/sec</span><br><span class="line">STREAM add latency: 1.63 nanoseconds</span><br><span class="line">STREAM add bandwidth: 14729.23 MB/sec</span><br><span class="line">STREAM triad latency: 3.36 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 7151.67 MB/sec</span><br><span class="line">47</span><br><span class="line">STREAM copy latency: 2.31 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 6938.47 MB/sec</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://support.huawei.com/enterprise/zh/doc/EDOC1100088653/32aa8773">以华为泰山服务器(鲲鹏920芯片)配置为例</a>：<img src="/images/951413iMgBlog/image-20211228165542167.png" alt="image-20211228165542167"></p>
<blockquote>
<p>Die Interleaving 控制是否使能DIE交织。使能DIE交织能充分利用系统的DDR带宽，并尽量保证各DDR通道的带宽均衡，提升DDR的利用率</p>
</blockquote>
<h3 id="hygon5280测试数据"><a href="#hygon5280测试数据" class="headerlink" title="hygon5280测试数据"></a>hygon5280测试数据</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">-----hygon5280测试数据</span><br><span class="line">[root@localhost lmbench-master]# for i in $(seq 0 8 24); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">0</span><br><span class="line">STREAM copy latency: 1.22 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 13166.34 MB/sec</span><br><span class="line">STREAM scale latency: 1.13 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 14166.95 MB/sec</span><br><span class="line">STREAM add latency: 1.15 nanoseconds</span><br><span class="line">STREAM add bandwidth: 20818.63 MB/sec</span><br><span class="line">STREAM triad latency: 1.39 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 17211.81 MB/sec</span><br><span class="line">8</span><br><span class="line">STREAM copy latency: 1.56 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10273.07 MB/sec</span><br><span class="line">STREAM scale latency: 1.50 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10701.89 MB/sec</span><br><span class="line">STREAM add latency: 1.20 nanoseconds</span><br><span class="line">STREAM add bandwidth: 19996.68 MB/sec</span><br><span class="line">STREAM triad latency: 1.93 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12443.70 MB/sec</span><br><span class="line">16</span><br><span class="line">STREAM copy latency: 2.52 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 6357.71 MB/sec</span><br><span class="line">STREAM scale latency: 2.48 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 6454.95 MB/sec</span><br><span class="line">STREAM add latency: 1.67 nanoseconds</span><br><span class="line">STREAM add bandwidth: 14362.51 MB/sec</span><br><span class="line">STREAM triad latency: 3.65 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 6572.85 MB/sec</span><br><span class="line">24</span><br><span class="line">STREAM copy latency: 2.44 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 6554.24 MB/sec</span><br><span class="line">STREAM scale latency: 2.41 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 6642.80 MB/sec</span><br><span class="line">STREAM add latency: 1.44 nanoseconds</span><br><span class="line">STREAM add bandwidth: 16695.82 MB/sec</span><br><span class="line">STREAM triad latency: 3.61 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 6639.18 MB/sec</span><br><span class="line">[root@localhost lmbench-master]# lscpu</span><br><span class="line">架构：                           x86_64</span><br><span class="line">CPU 运行模式：                   32-bit, 64-bit</span><br><span class="line">字节序：                         Little Endian</span><br><span class="line">Address sizes:                   43 bits physical, 48 bits virtual</span><br><span class="line">CPU:                             64</span><br><span class="line">在线 CPU 列表：                  0-63</span><br><span class="line">每个核的线程数：                 2</span><br><span class="line">每个座的核数：                   16</span><br><span class="line">座：                             2</span><br><span class="line">NUMA 节点：                      4</span><br><span class="line">厂商 ID：                        HygonGenuine</span><br><span class="line">CPU 系列：                       24</span><br><span class="line">型号：                           1</span><br><span class="line">型号名称：                       Hygon C86 5280 16-core Processor</span><br><span class="line">步进：                           1</span><br><span class="line">Frequency boost:                 enabled</span><br><span class="line">CPU MHz：                        2799.311</span><br><span class="line">CPU 最大 MHz：                   2500.0000</span><br><span class="line">CPU 最小 MHz：                   1600.0000</span><br><span class="line">BogoMIPS：                       4999.36</span><br><span class="line">虚拟化：                         AMD-V</span><br><span class="line">L1d 缓存：                       1 MiB</span><br><span class="line">L1i 缓存：                       2 MiB</span><br><span class="line">L2 缓存：                        16 MiB</span><br><span class="line">L3 缓存：                        64 MiB</span><br><span class="line">NUMA 节点0 CPU：                 0-7,32-39</span><br><span class="line">NUMA 节点1 CPU：                 8-15,40-47</span><br><span class="line">NUMA 节点2 CPU：                 16-23,48-55</span><br><span class="line">NUMA 节点3 CPU：                 24-31,56-63</span><br><span class="line">Vulnerability Itlb multihit:     Not affected</span><br><span class="line">Vulnerability L1tf:              Not affected</span><br><span class="line">Vulnerability Mds:               Not affected</span><br><span class="line">Vulnerability Meltdown:          Not affected</span><br><span class="line">Vulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp</span><br><span class="line">Vulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization</span><br><span class="line">Vulnerability Spectre v2:        Mitigation; Full AMD retpoline, IBPB conditional, STIBP disabled, RSB</span><br><span class="line">                                 filling</span><br><span class="line">Vulnerability Srbds:             Not affected</span><br><span class="line">Vulnerability Tsx async abort:   Not affected</span><br><span class="line">标记：                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3</span><br><span class="line">                                 6 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdts</span><br><span class="line">                                 cp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid amd_dcm</span><br><span class="line">                                  aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe p</span><br><span class="line">                                 opcnt xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy</span><br><span class="line">                                 abm sse4a misalignsse 3dnowprefetch osvw skinit wdt tce topoext perfct</span><br><span class="line">                                 r_core perfctr_nb bpext perfctr_llc mwaitx cpb hw_pstate sme ssbd sev</span><br><span class="line">                                 ibpb vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt s</span><br><span class="line">                                 ha_ni xsaveopt xsavec xgetbv1 xsaves clzero irperf xsaveerptr arat npt</span><br><span class="line">                                  lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassist</span><br><span class="line">                                 s pausefilter pfthreshold avic v_vmsave_vmload vgif overflow_recov suc</span><br><span class="line">                                 cor smca</span><br></pre></td></tr></table></figure>

<h3 id="intel-8269CY"><a href="#intel-8269CY" class="headerlink" title="intel 8269CY"></a>intel 8269CY</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                104</span><br><span class="line">On-line CPU(s) list:   0-103</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    26</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 85</span><br><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz</span><br><span class="line">Stepping:              7</span><br><span class="line">CPU MHz:               3200.000</span><br><span class="line">CPU max MHz:           3800.0000</span><br><span class="line">CPU min MHz:           1200.0000</span><br><span class="line">BogoMIPS:              4998.89</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              36608K</span><br><span class="line">NUMA node0 CPU(s):     0-25,52-77</span><br><span class="line">NUMA node1 CPU(s):     26-51,78-103</span><br><span class="line"></span><br><span class="line">[root@numaopen.cloud.et93 /home/ren/lmbench3]</span><br><span class="line">#time for i in $(seq 0 8 51); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">0</span><br><span class="line">STREAM copy latency: 1.15 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 13941.80 MB/sec</span><br><span class="line">STREAM scale latency: 1.16 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 13799.89 MB/sec</span><br><span class="line">STREAM add latency: 1.31 nanoseconds</span><br><span class="line">STREAM add bandwidth: 18318.23 MB/sec</span><br><span class="line">STREAM triad latency: 1.56 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 15356.72 MB/sec</span><br><span class="line">16</span><br><span class="line">STREAM copy latency: 1.12 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 14293.68 MB/sec</span><br><span class="line">STREAM scale latency: 1.13 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 14162.47 MB/sec</span><br><span class="line">STREAM add latency: 1.31 nanoseconds</span><br><span class="line">STREAM add bandwidth: 18293.27 MB/sec</span><br><span class="line">STREAM triad latency: 1.53 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 15692.47 MB/sec</span><br><span class="line">32</span><br><span class="line">STREAM copy latency: 1.52 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10551.71 MB/sec</span><br><span class="line">STREAM scale latency: 1.52 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10508.33 MB/sec</span><br><span class="line">STREAM add latency: 1.38 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17363.22 MB/sec</span><br><span class="line">STREAM triad latency: 2.00 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12024.52 MB/sec</span><br><span class="line">40</span><br><span class="line">STREAM copy latency: 1.49 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10758.50 MB/sec</span><br><span class="line">STREAM scale latency: 1.50 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10680.17 MB/sec</span><br><span class="line">STREAM add latency: 1.34 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17948.34 MB/sec</span><br><span class="line">STREAM triad latency: 1.98 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12133.22 MB/sec</span><br><span class="line">48</span><br><span class="line">STREAM copy latency: 1.49 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10736.56 MB/sec</span><br><span class="line">STREAM scale latency: 1.50 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10692.93 MB/sec</span><br><span class="line">STREAM add latency: 1.34 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17902.85 MB/sec</span><br><span class="line">STREAM triad latency: 1.96 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 12239.44 MB/sec</span><br></pre></td></tr></table></figure>

<h3 id="Intel-R-Xeon-R-CPU-E5-2682-v4"><a href="#Intel-R-Xeon-R-CPU-E5-2682-v4" class="headerlink" title="Intel(R) Xeon(R) CPU E5-2682 v4"></a>Intel(R) Xeon(R) CPU E5-2682 v4</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">#time for i in $(seq 0 8 51); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">0</span><br><span class="line">STREAM copy latency: 1.59 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10092.31 MB/sec</span><br><span class="line">STREAM scale latency: 1.57 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10169.16 MB/sec</span><br><span class="line">STREAM add latency: 1.31 nanoseconds</span><br><span class="line">STREAM add bandwidth: 18360.83 MB/sec</span><br><span class="line">STREAM triad latency: 2.28 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 10503.81 MB/sec</span><br><span class="line">8</span><br><span class="line">STREAM copy latency: 1.55 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 10312.14 MB/sec</span><br><span class="line">STREAM scale latency: 1.56 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 10283.70 MB/sec</span><br><span class="line">STREAM add latency: 1.30 nanoseconds</span><br><span class="line">STREAM add bandwidth: 18416.26 MB/sec</span><br><span class="line">STREAM triad latency: 2.23 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 10777.08 MB/sec</span><br><span class="line">16</span><br><span class="line">STREAM copy latency: 2.02 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 7914.25 MB/sec</span><br><span class="line">STREAM scale latency: 2.02 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 7919.85 MB/sec</span><br><span class="line">STREAM add latency: 1.39 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17276.06 MB/sec</span><br><span class="line">STREAM triad latency: 2.92 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 8231.18 MB/sec</span><br><span class="line">24</span><br><span class="line">STREAM copy latency: 1.99 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 8032.18 MB/sec</span><br><span class="line">STREAM scale latency: 1.98 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 8061.12 MB/sec</span><br><span class="line">STREAM add latency: 1.39 nanoseconds</span><br><span class="line">STREAM add bandwidth: 17313.94 MB/sec</span><br><span class="line">STREAM triad latency: 2.88 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 8318.93 MB/sec</span><br><span class="line"></span><br><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                64</span><br><span class="line">On-line CPU(s) list:   0-63</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    16</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 79</span><br><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</span><br><span class="line">Stepping:              1</span><br><span class="line">CPU MHz:               2500.000</span><br><span class="line">CPU max MHz:           3000.0000</span><br><span class="line">CPU min MHz:           1200.0000</span><br><span class="line">BogoMIPS:              5000.06</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              256K</span><br><span class="line">L3 cache:              40960K</span><br><span class="line">NUMA node0 CPU(s):     0-15,32-47</span><br><span class="line">NUMA node1 CPU(s):     16-31,48-63</span><br></pre></td></tr></table></figure>

<h3 id="AMD-EPYC-7T83"><a href="#AMD-EPYC-7T83" class="headerlink" title="AMD EPYC 7T83"></a>AMD EPYC 7T83</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br></pre></td><td class="code"><pre><span class="line">#time for i in $(seq 0 8 255); do echo $i; numactl -C $i -m 0 ./bin/stream -W 5 -N 5 -M 64M; done</span><br><span class="line">0</span><br><span class="line">STREAM copy latency: 0.49 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 32561.30 MB/sec</span><br><span class="line">STREAM scale latency: 0.49 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 32620.66 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27575.20 MB/sec</span><br><span class="line">STREAM triad latency: 0.70 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 34397.15 MB/sec</span><br><span class="line">8</span><br><span class="line">STREAM copy latency: 0.52 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 30764.47 MB/sec</span><br><span class="line">STREAM scale latency: 0.53 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 30056.59 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27575.20 MB/sec</span><br><span class="line">STREAM triad latency: 0.69 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 34789.45 MB/sec</span><br><span class="line">16</span><br><span class="line">STREAM copy latency: 0.53 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 30173.15 MB/sec</span><br><span class="line">STREAM scale latency: 0.54 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 29895.91 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27496.11 MB/sec</span><br><span class="line">STREAM triad latency: 0.70 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 34128.93 MB/sec</span><br><span class="line">24</span><br><span class="line">STREAM copy latency: 0.78 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 20417.69 MB/sec</span><br><span class="line">STREAM scale latency: 0.51 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 31354.70 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27548.79 MB/sec</span><br><span class="line">STREAM triad latency: 0.69 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 34589.22 MB/sec</span><br><span class="line">32</span><br><span class="line">STREAM copy latency: 0.60 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 26862.34 MB/sec</span><br><span class="line">STREAM scale latency: 0.58 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27376.00 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27518.66 MB/sec</span><br><span class="line">STREAM triad latency: 0.78 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 30779.17 MB/sec</span><br><span class="line">40</span><br><span class="line">STREAM copy latency: 0.59 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 27230.21 MB/sec</span><br><span class="line">STREAM scale latency: 0.59 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27284.18 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27503.63 MB/sec</span><br><span class="line">STREAM triad latency: 0.77 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31242.48 MB/sec</span><br><span class="line">48</span><br><span class="line">STREAM copy latency: 0.59 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 27102.37 MB/sec</span><br><span class="line">STREAM scale latency: 0.59 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27164.08 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27503.63 MB/sec</span><br><span class="line">STREAM triad latency: 0.76 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31422.90 MB/sec</span><br><span class="line">56</span><br><span class="line">STREAM copy latency: 0.92 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 17453.54 MB/sec</span><br><span class="line">STREAM scale latency: 0.59 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27267.55 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27488.61 MB/sec</span><br><span class="line">STREAM triad latency: 0.77 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31169.92 MB/sec</span><br><span class="line">64</span><br><span class="line">STREAM copy latency: 0.88 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18231.15 MB/sec</span><br><span class="line">STREAM scale latency: 0.84 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18976.06 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26413.87 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22310.12 MB/sec</span><br><span class="line">72</span><br><span class="line">STREAM copy latency: 0.86 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18552.45 MB/sec</span><br><span class="line">STREAM scale latency: 0.84 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 19113.88 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26375.81 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22151.79 MB/sec</span><br><span class="line">80</span><br><span class="line">STREAM copy latency: 0.89 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18037.59 MB/sec</span><br><span class="line">STREAM scale latency: 0.87 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18398.59 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26142.91 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22133.53 MB/sec</span><br><span class="line">88</span><br><span class="line">STREAM copy latency: 0.93 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 17119.60 MB/sec</span><br><span class="line">STREAM scale latency: 0.94 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 17030.54 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26146.30 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22159.10 MB/sec</span><br><span class="line">96</span><br><span class="line">STREAM copy latency: 1.39 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 11512.93 MB/sec</span><br><span class="line">STREAM scale latency: 0.87 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18406.16 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 25991.03 MB/sec</span><br><span class="line">STREAM triad latency: 1.09 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22078.91 MB/sec</span><br><span class="line">104</span><br><span class="line">STREAM copy latency: 0.86 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18546.04 MB/sec</span><br><span class="line">STREAM scale latency: 1.39 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 11518.85 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26300.01 MB/sec</span><br><span class="line">STREAM triad latency: 1.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22599.38 MB/sec</span><br><span class="line">112</span><br><span class="line">STREAM copy latency: 0.88 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18253.46 MB/sec</span><br><span class="line">STREAM scale latency: 0.85 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18758.59 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26413.87 MB/sec</span><br><span class="line">STREAM triad latency: 1.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22648.95 MB/sec</span><br><span class="line">120</span><br><span class="line">STREAM copy latency: 0.86 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18607.75 MB/sec</span><br><span class="line">STREAM scale latency: 0.84 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18957.30 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26427.74 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22313.83 MB/sec</span><br><span class="line">128</span><br><span class="line">STREAM copy latency: 0.82 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 19432.13 MB/sec</span><br><span class="line">STREAM scale latency: 0.87 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18421.31 MB/sec</span><br><span class="line">STREAM add latency: 0.98 nanoseconds</span><br><span class="line">STREAM add bandwidth: 24546.03 MB/sec</span><br><span class="line">STREAM triad latency: 1.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22702.59 MB/sec</span><br><span class="line">136</span><br><span class="line">STREAM copy latency: 0.74 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 21568.01 MB/sec</span><br><span class="line">STREAM scale latency: 0.74 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 21668.99 MB/sec</span><br><span class="line">STREAM add latency: 0.90 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26697.59 MB/sec</span><br><span class="line">STREAM triad latency: 0.91 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 26320.64 MB/sec</span><br><span class="line">144</span><br><span class="line">STREAM copy latency: 0.79 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 20268.45 MB/sec</span><br><span class="line">STREAM scale latency: 0.66 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 24279.61 MB/sec</span><br><span class="line">STREAM add latency: 0.89 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26822.08 MB/sec</span><br><span class="line">STREAM triad latency: 0.84 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 28540.76 MB/sec</span><br><span class="line">152</span><br><span class="line">STREAM copy latency: 0.85 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18903.90 MB/sec</span><br><span class="line">STREAM scale latency: 0.56 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 28734.25 MB/sec</span><br><span class="line">STREAM add latency: 0.88 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27335.58 MB/sec</span><br><span class="line">STREAM triad latency: 0.75 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31911.01 MB/sec</span><br><span class="line">160</span><br><span class="line">STREAM copy latency: 0.64 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 25068.68 MB/sec</span><br><span class="line">STREAM scale latency: 0.63 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 25550.68 MB/sec</span><br><span class="line">STREAM add latency: 0.88 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27313.33 MB/sec</span><br><span class="line">STREAM triad latency: 0.82 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 29416.50 MB/sec</span><br><span class="line">168</span><br><span class="line">STREAM copy latency: 0.61 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 26232.33 MB/sec</span><br><span class="line">STREAM scale latency: 0.60 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 26717.96 MB/sec</span><br><span class="line">STREAM add latency: 0.88 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27398.82 MB/sec</span><br><span class="line">STREAM triad latency: 0.79 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 30411.86 MB/sec</span><br><span class="line">176</span><br><span class="line">STREAM copy latency: 0.58 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 27380.19 MB/sec</span><br><span class="line">STREAM scale latency: 0.58 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27740.96 MB/sec</span><br><span class="line">STREAM add latency: 0.94 nanoseconds</span><br><span class="line">STREAM add bandwidth: 25666.31 MB/sec</span><br><span class="line">STREAM triad latency: 0.77 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31150.63 MB/sec</span><br><span class="line">184</span><br><span class="line">STREAM copy latency: 0.90 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 17730.21 MB/sec</span><br><span class="line">STREAM scale latency: 0.57 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 27918.40 MB/sec</span><br><span class="line">STREAM add latency: 0.87 nanoseconds</span><br><span class="line">STREAM add bandwidth: 27458.61 MB/sec</span><br><span class="line">STREAM triad latency: 0.76 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 31457.27 MB/sec</span><br><span class="line">192</span><br><span class="line">STREAM copy latency: 0.91 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 17558.57 MB/sec</span><br><span class="line">STREAM scale latency: 0.88 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18115.49 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26031.36 MB/sec</span><br><span class="line">STREAM triad latency: 1.12 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 21443.95 MB/sec</span><br><span class="line">200</span><br><span class="line">STREAM copy latency: 1.34 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 11911.40 MB/sec</span><br><span class="line">STREAM scale latency: 0.85 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18893.26 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26306.88 MB/sec</span><br><span class="line">STREAM triad latency: 1.09 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22013.73 MB/sec</span><br><span class="line">208</span><br><span class="line">STREAM copy latency: 1.36 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 11724.12 MB/sec</span><br><span class="line">STREAM scale latency: 0.86 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18631.00 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26166.69 MB/sec</span><br><span class="line">STREAM triad latency: 1.10 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 21763.86 MB/sec</span><br><span class="line">216</span><br><span class="line">STREAM copy latency: 0.88 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18270.85 MB/sec</span><br><span class="line">STREAM scale latency: 0.85 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18848.15 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26176.90 MB/sec</span><br><span class="line">STREAM triad latency: 1.10 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 21799.20 MB/sec</span><br><span class="line">224</span><br><span class="line">STREAM copy latency: 0.89 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18047.29 MB/sec</span><br><span class="line">STREAM scale latency: 0.86 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 18677.66 MB/sec</span><br><span class="line">STREAM add latency: 0.92 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26112.39 MB/sec</span><br><span class="line">STREAM triad latency: 1.09 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 21966.89 MB/sec</span><br><span class="line">232</span><br><span class="line">STREAM copy latency: 1.35 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 11818.58 MB/sec</span><br><span class="line">STREAM scale latency: 0.82 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 19568.11 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26469.44 MB/sec</span><br><span class="line">STREAM triad latency: 1.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22702.59 MB/sec</span><br><span class="line">240</span><br><span class="line">STREAM copy latency: 0.87 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18325.74 MB/sec</span><br><span class="line">STREAM scale latency: 0.83 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 19331.37 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26455.52 MB/sec</span><br><span class="line">STREAM triad latency: 1.06 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22580.37 MB/sec</span><br><span class="line">248</span><br><span class="line">STREAM copy latency: 0.87 nanoseconds</span><br><span class="line">STREAM copy bandwidth: 18418.79 MB/sec</span><br><span class="line">STREAM scale latency: 0.84 nanoseconds</span><br><span class="line">STREAM scale bandwidth: 19019.09 MB/sec</span><br><span class="line">STREAM add latency: 0.91 nanoseconds</span><br><span class="line">STREAM add bandwidth: 26483.37 MB/sec</span><br><span class="line">STREAM triad latency: 1.08 nanoseconds</span><br><span class="line">STREAM triad bandwidth: 22148.13 MB/sec</span><br></pre></td></tr></table></figure>

<h3 id="stream对比数据"><a href="#stream对比数据" class="headerlink" title="stream对比数据"></a>stream对比数据</h3><p>总结下几个CPU用stream测试访问内存的RT以及抖动和带宽对比数据，重点关注带宽，这个测试中时延不重要</p>
<table>
<thead>
<tr>
<th></th>
<th>最小RT</th>
<th>最大RT</th>
<th>最大copy bandwidth</th>
<th>最小copy bandwidth</th>
</tr>
</thead>
<tbody><tr>
<td>申威3231(2numa node)</td>
<td>7.09</td>
<td>8.75</td>
<td>2256.59 MB&#x2F;sec</td>
<td>1827.88 MB&#x2F;sec</td>
</tr>
<tr>
<td>飞腾2500(16 numa node)</td>
<td>2.84</td>
<td>10.34</td>
<td>5638.21 MB&#x2F;sec</td>
<td>1546.68 MB&#x2F;sec</td>
</tr>
<tr>
<td>鲲鹏920(4 numa node)</td>
<td>1.84</td>
<td>3.87</td>
<td>8700.75 MB&#x2F;sec</td>
<td>4131.81 MB&#x2F;sec</td>
</tr>
<tr>
<td>海光7280(8 numa node)</td>
<td>1.38</td>
<td>2.58</td>
<td>11591.48 MB&#x2F;sec</td>
<td>6206.99 MB&#x2F;sec</td>
</tr>
<tr>
<td>海光5280(4 numa node)</td>
<td>1.22</td>
<td>2.52</td>
<td>13166.34 MB&#x2F;sec</td>
<td>6357.71 MB&#x2F;sec</td>
</tr>
<tr>
<td>Intel8269CY(2 numa node)</td>
<td>1.12</td>
<td>1.52</td>
<td>14293.68 MB&#x2F;sec</td>
<td>10551.71 MB&#x2F;sec</td>
</tr>
<tr>
<td>Intel E5-2682(2 numa node)</td>
<td>1.58</td>
<td>2.02</td>
<td>10092.31 MB&#x2F;sec</td>
<td>7914.25 MB&#x2F;sec</td>
</tr>
<tr>
<td>AMD EPYC 7T83(4 numa node)</td>
<td>0.49</td>
<td>1.39</td>
<td>32561.30 MB&#x2F;sec</td>
<td>11512.93 MB&#x2F;sec</td>
</tr>
<tr>
<td>Y</td>
<td>1.83</td>
<td>3.48</td>
<td>8764.72 MB&#x2F;sec</td>
<td>4593.25 MB&#x2F;sec</td>
</tr>
</tbody></table>
<p>从以上数据可以看出这5款CPU性能一款比一款好，飞腾2500慢的core上延时快到intel 8269的10倍了，平均延时5倍以上了。延时数据基本和单核上测试sysbench TPS一致。性能差不多就是：常数*主频&#x2F;RT</p>
<h3 id="lat-mem-rd对比数据"><a href="#lat-mem-rd对比数据" class="headerlink" title="lat_mem_rd对比数据"></a>lat_mem_rd对比数据</h3><p>用不同的node上的core 跑lat_mem_rd测试访问node0内存的RT，只取最大64M的时延，时延和node距离完全一致</p>
<table>
<thead>
<tr>
<th></th>
<th>RT变化</th>
</tr>
</thead>
<tbody><tr>
<td>飞腾2500(16 numa node)</td>
<td>core:0	  149.976<br/>core:8	  168.805<br/>core:16	 191.415<br/>core:24	 178.283<br/>core:32	 170.814<br/>core:40	 185.699<br/>core:48	 212.281<br/>core:56	 202.479<br/>core:64	 426.176<br/>core:72	 444.367<br/>core:80	 465.894<br/>core:88	 452.245<br/>core:96	 448.352<br/>core:104   460.603<br/>core:112   485.989<br/>core:120	490.402</td>
</tr>
<tr>
<td>鲲鹏920(4 numa node)</td>
<td>core:0 117.323<br/>core:24 135.337<br/>core:48 197.782<br/>core:72 219.416</td>
</tr>
<tr>
<td>海光7280(8 numa node)</td>
<td>numa0    106.839<br/>numa1    168.583<br/>numa2    163.925<br/>numa3    163.690<br/>numa4    289.628<br/>numa5    288.632<br/>numa6    236.615<br/>numa7    291.880<br/>分割行<br/>enabled die interleaving <br/>core:0 153.005<br/>core:16 152.458<br/>core:32 272.057<br/>core:48 269.441</td>
</tr>
<tr>
<td>海光5280(4 numa node)</td>
<td>core:0   102.574<br/>core:8   160.989<br/>core:16  286.850<br/>core:24  231.197</td>
</tr>
<tr>
<td>海光7260(1 numa node)</td>
<td>core:0  265</td>
</tr>
<tr>
<td>Intel 8269CY(2 numa node)</td>
<td>core:0        69.792<br/>core:26      93.107</td>
</tr>
<tr>
<td>Intel 8163(2 NUMA node)</td>
<td>core:0        68.785<br/>core:24      100.314</td>
</tr>
<tr>
<td>Intel 8163(1 NUMA node)</td>
<td>core:0         100.652<br/>core:24       67.925 &#x2F;&#x2F;内存没有做交织</td>
</tr>
<tr>
<td>申威3231(2numa node)</td>
<td>core:0     215.146<br/>core:32   282.443</td>
</tr>
<tr>
<td>AMD EPYC 7T83(4 numa node)</td>
<td>core:0 71.656<br/>core:32 80.129<br/>core:64 131.334<br/>core:96 129.563</td>
</tr>
<tr>
<td>Y7（2Die，2node，1socket）</td>
<td>core:8   42.395<br/>core:40   36.434<br/>core:104  105.745<br/>core:88  124.384<br/><br/>core:24   62.979<br/>core:8      69.324<br/>core:64  137.233<br/>core:88  127.250<br/><br/>133ns 205ns （待测）</td>
</tr>
</tbody></table>
<p>测试命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in $(seq 0 8 127); do echo core:$i; numactl -C $i -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M; done &gt;lat.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>

<p>测试结果和numactl -H 看到的node distance完全一致，芯片厂家应该就是这样测试然后把这个延迟当做距离写进去了</p>
<p>AMD EPYC 7T83(4 numa node)的时延相对抖动有点大，这和架构多个小Die合并成一块CPU有关</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#grep -E &quot;core|64.00000&quot; lat.log</span><br><span class="line">core:0</span><br><span class="line">64.00000 71.656</span><br><span class="line">core:32</span><br><span class="line">64.00000 80.129</span><br><span class="line">core:64</span><br><span class="line">64.00000 131.334</span><br><span class="line">core:88</span><br><span class="line">64.00000 136.774</span><br><span class="line">core:96</span><br><span class="line">64.00000 129.563</span><br><span class="line">core:120</span><br><span class="line">64.00000 140.151</span><br></pre></td></tr></table></figure>

<p>AMD EPYC 7T83(4 numa node)比Intel 8269时延要大，但是带宽也高很多</p>
<h4 id="bios-numa-on-off"><a href="#bios-numa-on-off" class="headerlink" title="bios numa on&#x2F;off"></a>bios numa on&#x2F;off</h4><p>NUMA 参数：</p>
<table>
<thead>
<tr>
<th></th>
<th>BIOS ON</th>
<th>BIOS OFF</th>
</tr>
</thead>
<tbody><tr>
<td>cmdline numa&#x3D;on（默认值）</td>
<td>NUMA 开启，内存在Node内做交织，就近有快慢之分</td>
<td>bios 关闭后numa后，OS层面完全不知道下层的结构，默认全局内存做交织，时延是个平均值</td>
</tr>
<tr>
<td>cmdline numa&#x3D;off</td>
<td>交织关闭，效果同上</td>
<td>同上</td>
</tr>
</tbody></table>
<p>测试在bios中开关numa，以及在OS 启动参数里设置 numa&#x3D;on&#x2F;off 这四种组合来对比内存时延的差异</p>
<p>测试CPU型号如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    24</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              33792K</span><br><span class="line">NUMA node0 CPU(s):     0-23,48-71 //bios on + cmdline on</span><br><span class="line">NUMA node1 CPU(s):     24-47,72-95</span><br><span class="line"></span><br><span class="line">#cat /proc/cmdline</span><br><span class="line">BOOT_IMAGE=/vmlinuz-3.10.0-327.x86_64  ro crashkernel=auto vconsole.font=latarcyrheb-sun16 vconsole.keymap=us biosdevname=0 console=tty0 console=ttyS0,115200 scsi_mod.scan=sync intel_idle.max_cstate=0 pci=pcie_bus_perf ipv6.disable=1 rd.driver.pre=ahci numa=on nosmt=force</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/intel/lmbench">测试命令</a>以及测试结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">for i in $(seq 0 24 95); do echo core:$i; numactl -C $i -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M; done &gt;lat.log 2&gt;&amp;1</span><br><span class="line"></span><br><span class="line">//从下面两种测试来看，bios层面 on后，不管OS 层面是否on，都不会跨node 做交织，抖动存在</span><br><span class="line">//bios on 即使在OS层面关闭numa也不跨node做内存交织，抖动存在</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.biosON.cmdlineOff </span><br><span class="line">core:0 //第0号核</span><br><span class="line">64.00000 100.717 //64.0000为64MB， 100.717 是平均时延100.717ns, 即0号核访问node0 下的内存64MB的平均延时是100纳秒</span><br><span class="line">core:24</span><br><span class="line">64.00000 68.484</span><br><span class="line">core:48</span><br><span class="line">64.00000 101.070</span><br><span class="line">core:72</span><br><span class="line">64.00000 68.483</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.biosON.cmdlineON</span><br><span class="line">core:0</span><br><span class="line">64.00000 67.094</span><br><span class="line">core:24</span><br><span class="line">64.00000 100.237</span><br><span class="line">core:48</span><br><span class="line">64.00000 67.614</span><br><span class="line">core:72</span><br><span class="line">64.00000 101.096</span><br><span class="line"></span><br><span class="line">//从下面两种测试来看只要bios off了内存就会跨node交织，大规模测试下latency是个平均值</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.biosOff.cmdlineOff //bios off 做内存交织，latency就是平均值</span><br><span class="line">core:0</span><br><span class="line">64.00000 85.657</span><br><span class="line">core:24</span><br><span class="line">64.00000 85.741</span><br><span class="line">core:48</span><br><span class="line">64.00000 85.977</span><br><span class="line">core:72</span><br><span class="line">64.00000 86.671</span><br><span class="line"></span><br><span class="line">//bios 关闭后numa后，OS层面完全不知道下层的结构，默认一定是做交织</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.biosOff.cmdlineON</span><br><span class="line">core:0</span><br><span class="line">64.00000 89.123</span><br><span class="line">core:24</span><br><span class="line">64.00000 87.137</span><br><span class="line">core:48</span><br><span class="line">64.00000 87.239</span><br><span class="line">core:72</span><br><span class="line">64.00000 87.323</span><br></pre></td></tr></table></figure>

<p>结论：在OS 启动引导参数里设置 numa&#x3D;off 完全没有必要、也不能起作用，反而设置了 numa&#x3D;off 只能是掩耳盗铃，让用户看不到numa结构</p>
<p>为什么是平均值，而不是短板效应的最慢值？</p>
<p>测试软件只能通过大规模数据的读写来测试获取一个平均值，所以当一大块内存读取时，虽然通过交织大块内存被切分到了快慢物理内存上，但是因为规模大慢的被平均掉了。</p>
<h4 id="bios-on-同时-cmdline-off时"><a href="#bios-on-同时-cmdline-off时" class="headerlink" title="bios&#x3D;on 同时 cmdline off时"></a>bios&#x3D;on 同时 cmdline off时</h4><p>再用<a target="_blank" rel="noopener" href="https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html">Intel 的 mlc 验证下</a>，这个结果有点意思，latency稳定在 145 而不是81 和 145两个值随机出现，应该是mlc默认选到了0核，对应这个测试数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//从下面两种测试来看，bios层面 on后，不管OS 层面是否on，都不会跨node 做交织，抖动存在</span><br><span class="line">//bios on 即使在OS层面关闭numa也不跨node做内存交织，抖动存在</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.biosON.cmdlineOff  </span><br><span class="line">core:0</span><br><span class="line">64.00000 100.717</span><br><span class="line">core:24</span><br><span class="line">64.00000 68.484</span><br><span class="line">core:48</span><br><span class="line">64.00000 101.070</span><br><span class="line">core:72</span><br><span class="line">64.00000 68.483</span><br></pre></td></tr></table></figure>

<p>对应的mlc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">#./mlc</span><br><span class="line">Intel(R) Memory Latency Checker - v3.9</span><br><span class="line">Measuring idle latencies (in ns)...</span><br><span class="line">		Numa node</span><br><span class="line">Numa node	     0</span><br><span class="line">       0	 145.8</span><br><span class="line"></span><br><span class="line">Measuring Peak Injection Memory Bandwidths for the system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using traffic with the following read-write ratios</span><br><span class="line">ALL Reads        :	110598.7</span><br><span class="line">3:1 Reads-Writes :	93408.5</span><br><span class="line">2:1 Reads-Writes :	89249.5</span><br><span class="line">1:1 Reads-Writes :	64137.3</span><br><span class="line">Stream-triad like:	77310.4</span><br><span class="line"></span><br><span class="line">Measuring Memory Bandwidths between nodes within system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using Read-only traffic type</span><br><span class="line">		Numa node</span><br><span class="line">Numa node	     0</span><br><span class="line">       0	110598.4</span><br><span class="line"></span><br><span class="line">Measuring Loaded Latencies for the system</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using Read-only traffic type</span><br><span class="line">Inject	Latency	Bandwidth</span><br><span class="line">Delay	(ns)	MB/sec</span><br><span class="line">==========================</span><br><span class="line"> 00000	506.00	 111483.5</span><br><span class="line"> 00002	505.74	 112576.9</span><br><span class="line"> 00008	505.87	 112644.3</span><br><span class="line"> 00015	508.96	 112643.6</span><br><span class="line"> 00050	574.36	 112701.5</span><br><span class="line"> 00100	501.32	 112775.9</span><br><span class="line"> 00200	475.47	 112839.3</span><br><span class="line"> 00300	224.52	  91560.4</span><br><span class="line"> 00400	194.54	  70515.6</span><br><span class="line"> 00500	185.13	  57233.2</span><br><span class="line"> 00700	178.71	  41591.6</span><br><span class="line"> 01000	170.46	  29524.1</span><br><span class="line"> 01300	165.43	  22933.2</span><br><span class="line"> 01700	164.33	  17702.9</span><br><span class="line"> 02500	164.14	  12206.9</span><br></pre></td></tr></table></figure>

<p>两个值都为on 时的mlc 测试结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#./mlc</span><br><span class="line">Intel(R) Memory Latency Checker - v3.9</span><br><span class="line">Measuring idle latencies (in ns)...</span><br><span class="line">		Numa node</span><br><span class="line">Numa node	     0	     1</span><br><span class="line">       0	  81.6	 145.9</span><br><span class="line">       1	 144.9	  81.2</span><br><span class="line"></span><br><span class="line">Measuring Peak Injection Memory Bandwidths for the system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using traffic with the following read-write ratios</span><br><span class="line">ALL Reads        :	227204.2</span><br><span class="line">3:1 Reads-Writes :	212432.5</span><br><span class="line">2:1 Reads-Writes :	210423.3</span><br><span class="line">1:1 Reads-Writes :	196677.2</span><br><span class="line">Stream-triad like:	189691.4</span><br></pre></td></tr></table></figure>

<p>说明：mlc和 lmbench 测试结果不一样，mlc 时81和145，lmbench测试是68和100，这是两种测试方法的差异而已，但是快慢差距基本是一致的</p>
<h3 id="龙芯测试数据"><a href="#龙芯测试数据" class="headerlink" title="龙芯测试数据"></a>龙芯测试数据</h3><p>3A5000为龙芯，执行的命令为.&#x2F;lat_mem_rd 128M 4096，其中 4096 参数为跳步大小。其基本原理是，通过按 给定间隔去循环读一定大小的内存区域，测量每个读平均的时间。如果区域大小小于 L1 Cache 大 小，时间应该接近 L1 的访问延迟;如果大于 L1 小于 L2，则接近 L2 访问延迟;依此类推。图中横坐 标为访问的字节数，纵坐标为访存的拍数(cycles)。</p>
<p><img src="/images/951413iMgBlog/image-20220221113929547.png" alt="image-20220221113929547"></p>
<p>基于跳步访问的 3A5000 和 Zen1、Skylake 各级延迟的比较(cycles)</p>
<p><img src="/images/951413iMgBlog/image-20220221112527936.png" alt="image-20220221112527936"></p>
<p>下图给出了 LMbench 测试得到的访存操作的并发性，执行的命令为.&#x2F;par_mem。访存操作的并 发性是各级 Cache 和内存所支持并发访问的能力。在 LMbench 中，访存操作并发性的测试是设计一 个链表，不断地遍历访问下一个链表中的元素，链表所跳的距离和需要测量的 Cache 容量相关，在 一段时间能并发的发起对链表的追逐操作，也就是同时很多链表在遍历，如果发现这一段时间内 能同时完成 N 个链表的追逐操作，就认为访存的并发操作是 N。</p>
<p><img src="/images/951413iMgBlog/image-20220221112727377.png" alt="image-20220221112727377"></p>
<p>下图列出了三款处理器的功能部件操作延迟数据，使用的命令是.&#x2F;lat_ops。</p>
<p><img src="/images/951413iMgBlog/image-20220221112853404.png" alt="image-20220221112853404"></p>
<h4 id="龙芯stream数据"><a href="#龙芯stream数据" class="headerlink" title="龙芯stream数据"></a>龙芯stream数据</h4><p>LMbench 包含了 STREAM 带宽测试工具，可以用来测试可持续的内存访问带宽情况。图表12.25列 出了三款处理器的 STREAM 带宽数据，其中 STREAM 数组大小设置为 1 亿个元素，采用 OpenMP 版本 同时运行四个线程来测试满载带宽;相应测试平台均为 CPU 的两个内存控制器各接一根内存条， 3A5000 和 Zen1 用 DDR4 3200 内存条，Skylake 用 DDR4 2400 内存条(它最高只支持这个规格)。</p>
<p><img src="/images/951413iMgBlog/image-20220221113037332.png" alt="image-20220221113037332"></p>
<p>从数据可以看到，虽然硬件上 3A5000 和 Zen1 都实现了 DDR4 3200，但 3A5000 的实测可持续带宽 还是有一定差距。用户程序看到的内存带宽不仅仅和内存的物理频率有关系，也和处理器内部的 各种访存队列、内存控制器的调度策略、预取器和内存时序参数设置等相关，需要进行更多分析 来定位具体的瓶颈点。像 STREAM 这样的软件测试工具，能够更好地反映某个子系统的综合能力， 因而被广泛采用。</p>
<h2 id="对比结论"><a href="#对比结论" class="headerlink" title="对比结论"></a>对比结论</h2><ul>
<li>AMD单核跑分数据比较好</li>
<li>MySQL 查询场景下Intel的性能好很多</li>
<li>xdb比社区版性能要好</li>
<li>MySQL8.0比5.7在多核锁竞争场景下性能要好</li>
<li>intel最好，AMD接近Intel，海光差的比较远但是又比鲲鹏好很多，飞腾最差，尤其是跨socket简直是灾难</li>
<li>麒麟OS性能也比CentOS略差一些</li>
<li>从perf指标来看 鲲鹏920的L1d命中率高于8163是因为鲲鹏L1 size大；L2命中率低于8163，同样是因为鲲鹏 L2 size小；同样L1i 鲲鹏也大于8163，但是实际跑起来L1i Miss Rate更高，这说明 ARM对 L1d 使用效率低</li>
</ul>
<p>整体来说AMD用领先了一代的工艺（7nm VS 14nm)，在MySQL查询场景中终于可以接近Intel了，但是海光、鲲鹏、飞腾还是不给力。</p>
<h2 id="附表"><a href="#附表" class="headerlink" title="附表"></a>附表</h2><p>鲲鹏920 和 8163 在 MySQL 场景下的 perf 指标对比</p>
<table>
<thead>
<tr>
<th>整体对比</th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td>指标</td>
<td>X86</td>
<td>ARM</td>
<td>增加幅度</td>
</tr>
<tr>
<td>IPC</td>
<td>0.4979</td>
<td>0.495</td>
<td>-0.6%</td>
</tr>
<tr>
<td>Branchs</td>
<td>237606414772</td>
<td>415979894985</td>
<td>75.1%</td>
</tr>
<tr>
<td>Branch-misses</td>
<td>8104247620</td>
<td>28983836845</td>
<td>257.6%</td>
</tr>
<tr>
<td>Branch-missed rate</td>
<td>0.034</td>
<td>0.070</td>
<td>104.3%</td>
</tr>
<tr>
<td>内存读带宽（GB&#x2F;S)</td>
<td>25.0</td>
<td>25.0</td>
<td>-0.2%</td>
</tr>
<tr>
<td>内存写带宽（GB&#x2F;S)</td>
<td>24.6</td>
<td>67.8</td>
<td>175.5%</td>
</tr>
<tr>
<td>内存读写带宽（GB&#x2F;S)</td>
<td>49.7</td>
<td>92.8</td>
<td>86.8%</td>
</tr>
<tr>
<td>UNALIGNED_ACCESS</td>
<td>1329146645</td>
<td>13686011901</td>
<td>929.7%</td>
</tr>
<tr>
<td>L1d_MISS_RATIO</td>
<td>0.06055</td>
<td>0.04281</td>
<td>-29.3%</td>
</tr>
<tr>
<td>L1d_MISS_RATE</td>
<td>0.01645</td>
<td>0.01711</td>
<td>4.0%</td>
</tr>
<tr>
<td>L2_MISS_RATIO</td>
<td>0.34824</td>
<td>0.47162</td>
<td>35.4%</td>
</tr>
<tr>
<td>L2_MISS_RATE</td>
<td>0.00577</td>
<td>0.03493</td>
<td>504.8%</td>
</tr>
<tr>
<td>L1_ITLB_MISS_RATE</td>
<td>0.0028</td>
<td>0.005</td>
<td>78.6%</td>
</tr>
<tr>
<td>L1_DTLB_MISS_RATE</td>
<td>0.0025</td>
<td>0.0102</td>
<td>308.0%</td>
</tr>
<tr>
<td>context-switchs</td>
<td>8407195</td>
<td>11614981</td>
<td>38.2%</td>
</tr>
<tr>
<td>Pagefault</td>
<td>228371</td>
<td>741189</td>
<td>224.6%</td>
</tr>
</tbody></table>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="/2021/06/01/CPU%E7%9A%84%E5%88%B6%E9%80%A0%E5%92%8C%E6%A6%82%E5%BF%B5/">CPU的制造和概念</a></p>
<p>[CPU 性能和Cache Line](&#x2F;2021&#x2F;05&#x2F;16&#x2F;CPU Cache Line 和性能&#x2F;)</p>
<p>[Perf IPC以及CPU性能](&#x2F;2021&#x2F;05&#x2F;16&#x2F;Perf IPC以及CPU利用率&#x2F;)</p>
<p><a href="/2021/06/18/%E5%87%A0%E6%AC%BECPU%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></p>
<p><a href="/2021/05/15/%E9%A3%9E%E8%85%BEARM%E8%8A%AF%E7%89%87(FT2500)%E7%9A%84%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/">飞腾ARM芯片(FT2500)的性能测试</a></p>
<p><a href="/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">十年后数据库还是不敢拥抱NUMA？</a></p>
<p><a href="/2021/03/07/%E4%B8%80%E6%AC%A1%E6%B5%B7%E5%85%89%E7%89%A9%E7%90%86%E6%9C%BA%E8%B5%84%E6%BA%90%E7%AB%9E%E4%BA%89%E5%8E%8B%E6%B5%8B%E7%9A%84%E8%AE%B0%E5%BD%95/">一次海光物理机资源竞争压测的记录</a></p>
<p>[Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的](&#x2F;2019&#x2F;12&#x2F;16&#x2F;Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的&#x2F;)</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xuanjian_bjtu/article/details/107178226">lmbench测试要考虑cache等</a> </p>
<p>comment：</p>
<p>Intel 8163 IPC是0.67，和在PostgreSQL下测得数据基本一致。Oracle可以达到更高的IPC。从8163的perf结果中，看不出来访存在总周期中的占比。可以添加几个cycle_activity.cycles_l1d_miss、cycle_activity.stalls_mem_any，看看访存耗用的周期占比。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/01/01/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2022/01/01/%E7%BD%91%E7%BB%9C%E6%8A%93%E5%8C%85%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="post-title-link" itemprop="url">网络抓包常用命令</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-01-01 14:30:03" itemprop="dateCreated datePublished" datetime="2022-01-01T14:30:03+08:00">2022-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-12-02 10:33:24" itemprop="dateModified" datetime="2025-12-02T10:33:24+08:00">2025-12-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tcpdump/" itemprop="url" rel="index"><span itemprop="name">tcpdump</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="网络抓包常用命令"><a href="#网络抓包常用命令" class="headerlink" title="网络抓包常用命令"></a>网络抓包常用命令</h1><p>详细解析和Demo版本：<a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--WireShark%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88tshark/">就是要你懂抓包–WireShark之命令行版tshark</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br></pre></td><td class="code"><pre><span class="line">//抓取一个子网范围</span><br><span class="line">tcpdump -i bond0 port 3001 and net 1.2.3.0/24 and host not 1.2.3.211 -nn -X</span><br><span class="line"></span><br><span class="line">//抓取 DNAT 包，tcp options 里面的 246 代表 DNAT</span><br><span class="line">tcpdump -nn –vvv -i eth0 tcp dst port 3306 and &#x27;(tcp[tcpflags] &amp; (tcp-syn) != 0) and (tcp[20] =246) &#x27;</span><br><span class="line"></span><br><span class="line">//在上面的基础上，抓取指定 vip：10.142.*.*</span><br><span class="line">tcpdump -nn –vvv -i eth0 tcp dst port 3306 and &#x27;(tcp[tcpflags] &amp; (tcp-syn) != 0) and tcp[20]=246 and tcp[24]=10 and tcp[25]=142&#x27;</span><br><span class="line"></span><br><span class="line">//抓取 DNAT 包，tcp options 里面的 252 代表 DNAT</span><br><span class="line">tcpdump -nn –vvv -i eth0 tcp dst port 3306 and &#x27;(tcp[tcpflags] &amp; (tcp-ack) != 0) and (tcp[20] =252) &#x27;</span><br><span class="line"></span><br><span class="line">//根据指定的VPC IP抓包，例如172.16.x.x</span><br><span class="line">tcpdump -nn –vvv -i eth0 tcp dst port 3306 and &#x27;(tcp[tcpflags] &amp; (tcp-ack) != 0) and (tcp[32] =172) and (tcp[33] =16)&#x27;</span><br><span class="line"></span><br><span class="line">//根据客户端IP抓包FNAT的包，例如172.16.x.x</span><br><span class="line">tcpdump -nn –vvv -i eth0 tcp dst port 3306 and &#x27;(tcp[tcpflags] &amp; (tcp-ack) != 0) and(tcp[20]=252) and (tcp[24]=172) and (tcp[25]=16)&#x27;</span><br><span class="line"></span><br><span class="line">用tcpdump抓取并保存包：</span><br><span class="line">sudo tcpdump -i eth0 port 3306 -w plantegg.cap</span><br><span class="line"></span><br><span class="line">抓到的包存储在plantegg.cap中，可以用作wireshark、tshark详细分析</span><br><span class="line">如果明确知道目的ip、端口等可以通过指定条件来明确只抓取某个连接的包</span><br><span class="line"></span><br><span class="line">只抓本机的8080端口：</span><br><span class="line">tcpdump -i eth0 &#x27;(src port 8001 and src host 11.59.10.106) or (dst port 8001 and dst host 11.59.10.106)&#x27; -nn -X</span><br><span class="line"></span><br><span class="line">//http 流量</span><br><span class="line">// -f 抓取过滤条件 tcp port 80 and host 11.59.10.106</span><br><span class="line">//-Y 展示过滤条件</span><br><span class="line">tshark -i eth0 -f &#x27;(tcp src port 8080 and src host 11.59.10.106) or (tcp dst port 8080 and dst host 11.59.10.106)&#x27; -t a  -Y &quot; (http.request or http.response)&quot; -T fields -e frame.number -e frame.time  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e http.request.full_uri -e http.response.code -e http.response.phrase</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">抓取详细SQL语句：</span><br><span class="line">sudo tshark -i eth0 -Y &quot;mysql.command==3&quot; -T fields -e mysql.query</span><br><span class="line">sudo tshark -i eth0 -R mysql.query        -T fields -e mysql.query</span><br><span class="line"></span><br><span class="line">sudo tshark -i any -f &#x27;port 8527&#x27; -s 0 -l -w - |strings</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">parse 8507/4444 as mysql protocol, default only parse 3306 as mysql.</span></span><br><span class="line">sudo tshark -i eth0 -d tcp.port==8507,mysql -T fields -e mysql.query &#x27;port 8507&#x27;</span><br><span class="line">sudo tshark -i any -c 50 -d tcp.port==4444,mysql -Y &quot; ((tcp.port eq 4444 )  )&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e mysql.query</span><br><span class="line"></span><br><span class="line">sudo tshark -i eth0 -R &quot;ip.addr==10.18.106.95&quot; -d tcp.port==3306,mysql -T fields -e mysql.query &#x27;port 3306&#x27;</span><br><span class="line">sudo tshark -i eth0 -R &quot;tcp.srcport==62877&quot; -d tcp.port==3001,mysql -T fields -e tcp.srcport -e mysql.query &#x27;port 3001&#x27;</span><br><span class="line"></span><br><span class="line">sudo tshark -i br1.10 -Y tcp.port==4000,mysql -T fields -e tcp.srcport -e mysql.query &#x27;port 4000&#x27;</span><br><span class="line">tshark -i br1.10 -d tcp.port==4000,mysql -T fields -e tcp.srcport -e _ws.col.Info -e mysql.query</span><br><span class="line"></span><br><span class="line">tshark -i eth0 -d tcp.port==4000,mysql -T fields -e tcp.srcport -e _ws.col.Info -e mysql.query</span><br><span class="line"></span><br><span class="line">//将3307端口解析成MySQL 协议分析</span><br><span class="line">tshark -i lo -d tcp.port==3307,mysql -T fields -e frame.number -e frame.time -e frame.time_delta -e tcp.srcport -e tcp.dstport -e tcp.len -e _ws.col.Info -e mysql.query</span><br><span class="line"></span><br><span class="line">如果MySQL开启了SSL，那么抓包后的内容tshark/wireshark分析不到MySQL的具体内容，可以强制关闭：connectionProperties里加上useSSL=false</span><br><span class="line"></span><br><span class="line">查看SQL具体内容</span><br><span class="line">sudo tshark -r gege_plantegg.cap -Y &quot;mysql.query or (  tcp.stream==1)&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e frame.time_delta_displayed  -e tcp.stream -e tcp.len -e mysql.query </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">按mysql查询分析响应时间</span><br><span class="line">对于rt分析，要注意一个query多个response情况（response结果多，分包了），分析这种rt的时候只看query之后的第一个response，其它连续response需要忽略掉。</span><br><span class="line"></span><br><span class="line">以上抓包结果文件可以用tshark进行详细分析</span><br><span class="line"></span><br><span class="line">对抓包按 stream 进行切分：</span><br><span class="line">for i in &#123;0..314&#125;;do tshark -r 11216253112_3055.pcap -Y &quot;tcp.stream eq $i&quot; -w $i.pcap; done</span><br><span class="line">tshark -r 0.pcap &quot;ip.src eq 11.216.253.112&quot; -T fields -e frame.number  -e frame.time_delta -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e tcp.analysis.ack_rtt</span><br><span class="line"></span><br><span class="line">分析MySQL rt，倒数第四列基本就是rt</span><br><span class="line">tshark -r gege_plantegg.pcap -Y &quot; ((tcp.srcport eq 3306 ) and tcp.len&gt;0 )&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e tcp.analysis.ack_rtt   </span><br><span class="line"></span><br><span class="line">或者排序一下</span><br><span class="line">tshark -r 213_php.cap -Y &quot;mysql.query or (  tcp.srcport==3306)&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e mysql.query |sort -nk9 -nk1</span><br><span class="line"></span><br><span class="line">MySQL响应时间直方图【第八列的含义-- Time since previous frame in this TCP stream: seconds】：</span><br><span class="line">tshark -r gege_plantegg.pcap -Y &quot;mysql.query or (tcp.srcport3306 and tcp.len&gt;60)&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len | awk &#x27;BEGIN &#123;sum0=0;sum3=0;sum10=0;sum30=0;sum50=0;sum100=0;sum300=0;sum500=0;sum1000=0;sumo=0;count=0;sum=0&#125; &#123;rt=$8; if(rt&gt;=0.000) sum=sum+rt; count=count+1; if(rt&lt;=0.000) sum0=sum0+1; else if(rt&lt;0.003) sum3=sum3+1 ; else if(rt&lt;0.01) sum10=sum10+1; else if(rt&lt;0.03) sum30=sum30+1; else if(rt&lt;0.05) sum50=sum50+1; else if(rt &lt; 0.1) sum100=sum100+1; else if(rt &lt; 0.3) sum300=sum300+1; else if(rt &lt; 0.5) sum500=sum500+1; else if(rt &lt; 1) sum1000=sum1000+1; else sum=sum+1 ;&#125; END&#123;printf &quot;-------------\n3ms:\t%s \n10ms:\t%s \n30ms:\t%s \n50ms:\t%s \n100ms:\t%s \n300ms:\t%s \n500ms:\t%s \n1000ms:\t%s \n&gt;1s:\t %s\n-------------\navg: %.6f \n&quot; , sum3,sum10,sum30,sum50,sum100,sum300,sum500,sum1000,sumo,sum/count;&#125;&#x27;</span><br><span class="line"></span><br><span class="line">按http response分析响应时间</span><br><span class="line">tshark -nr 213_php.cap -o tcp.calculate_timestamps:true  -Y &quot;http.request or http.response&quot; -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e ip.dst -e tcp.stream  -e http.request.full_uri -e http.response.code -e http.response.phrase | sort -nk6 -nk1</span><br><span class="line"></span><br><span class="line">分析rtt、丢包、deplicate等等，可以得到整体网络状态</span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">tshark -r retrans.cap -q -z io,<span class="built_in">stat</span>,1,<span class="string">&quot;AVG(tcp.analysis.ack_rtt)tcp.analysis.ack_rtt&quot;</span>,<span class="string">&quot;COUNT(tcp.analysis.retransmission)  tcp.analysis.retransmission&quot;</span>,<span class="string">&quot;COUNT(tcp.analysis.fast_retransmission) tcp.analysis.fast_retransmission&quot;</span>,<span class="string">&quot;COUNT(tcp.analysis.duplicate_ack) tcp.analysis.duplicate_ack&quot;</span>,<span class="string">&quot;COUNT(tcp.analysis.lost_segment) tcp.analysis.lost_segment&quot;</span>,<span class="string">&quot;MIN(tcp.window_size)tcp.window_size&quot;</span></span></span><br><span class="line"></span><br><span class="line">===================================================================================</span><br><span class="line">| IO Statistics                                                                   |</span><br><span class="line">|                                                                                 |</span><br><span class="line">| Duration: 89.892365 secs                                                        |</span><br><span class="line">| Interval:  2 secs                                                               |</span><br><span class="line">|                                                                                 |</span><br><span class="line">| Col 1: AVG(tcp.analysis.ack_rtt)tcp.analysis.ack_rtt                            |</span><br><span class="line">|     2: COUNT(tcp.analysis.retransmission)  tcp.analysis.retransmission          |</span><br><span class="line">|     3: COUNT(tcp.analysis.fast_retransmission) tcp.analysis.fast_retransmission |</span><br><span class="line">|     4: COUNT(tcp.analysis.duplicate_ack) tcp.analysis.duplicate_ack             |</span><br><span class="line">|     5: COUNT(tcp.analysis.lost_segment) tcp.analysis.lost_segment               |</span><br><span class="line">|     6: AVG(tcp.window_size)tcp.window_size                                      |</span><br><span class="line">|---------------------------------------------------------------------------------|</span><br><span class="line">|          |1         |2      |3      |4      |5      |6      |                   |</span><br><span class="line">| Interval |    AVG   | COUNT | COUNT | COUNT | COUNT |  AVG  |                   |</span><br><span class="line">|-------------------------------------------------------------|                   |</span><br><span class="line">|  0 &lt;&gt;  2 | 0.001152 |     0 |     0 |     0 |     0 |  4206 |                   |</span><br><span class="line">|  2 &lt;&gt;  4 | 0.002088 |     0 |     0 |     0 |     1 |  6931 |                   |</span><br><span class="line">|  4 &lt;&gt;  6 | 0.001512 |     0 |     0 |     0 |     0 |  7099 |                   |</span><br><span class="line">|  6 &lt;&gt;  8 | 0.002859 |     0 |     0 |     0 |     0 |  7171 |                   |</span><br><span class="line">|  8 &lt;&gt; 10 | 0.001716 |     0 |     0 |     0 |     0 |  6472 |                   |</span><br><span class="line">| 10 &lt;&gt; 12 | 0.000319 |     0 |     0 |     0 |     2 |  5575 |                   |</span><br><span class="line">| 12 &lt;&gt; 14 | 0.002030 |     0 |     0 |     0 |     0 |  6922 |                   |</span><br><span class="line">| 14 &lt;&gt; 16 | 0.003371 |     0 |     0 |     0 |     2 |  5884 |                   |</span><br><span class="line">| 16 &lt;&gt; 18 | 0.000138 |     0 |     0 |     0 |     1 |  3480 |                   |</span><br><span class="line">| 18 &lt;&gt; 20 | 0.000999 |     0 |     0 |     0 |     4 |  6665 |                   |</span><br><span class="line">| 20 &lt;&gt; 22 | 0.000682 |     0 |     0 |    41 |     2 |  5484 |                   |</span><br><span class="line">| 22 &lt;&gt; 24 | 0.002302 |     2 |     0 |    19 |     0 |  7127 |                   |</span><br><span class="line">| 24 &lt;&gt; 26 | 0.000156 |     1 |     0 |    22 |     0 |  3042 |                   |</span><br><span class="line">| 26 &lt;&gt; 28 | 0.000000 |     1 |     0 |    19 |     1 |   152 |                   |</span><br><span class="line">| 28 &lt;&gt; 30 | 0.001498 |     1 |     0 |    24 |     0 |  5615 |                   |</span><br><span class="line">| 30 &lt;&gt; 32 | 0.000235 |     0 |     0 |    44 |     0 |  1880 |                   |</span><br><span class="line">1</span><br><span class="line">===================================================================================</span><br><span class="line">2</span><br><span class="line">| IO Statistics                                                                   |</span><br><span class="line">3</span><br><span class="line">|                                                                                 |</span><br><span class="line">4</span><br><span class="line">| Duration: 89.892365 secs                                                        |</span><br><span class="line">5</span><br><span class="line">| Interval:  2 secs                                                               |</span><br><span class="line">6</span><br><span class="line">|                                                                                 |</span><br><span class="line">7</span><br><span class="line">| Col 1: AVG(tcp.analysis.ack_rtt)tcp.analysis.ack_rtt                            |</span><br><span class="line">8</span><br><span class="line">|     2: COUNT(tcp.analysis.retransmission)  tcp.analysis.retransmission          |</span><br><span class="line">9</span><br><span class="line">|     3: COUNT(tcp.analysis.fast_retransmission) tcp.analysis.fast_retransmission |</span><br><span class="line">10</span><br><span class="line">|     4: COUNT(tcp.analysis.duplicate_ack) tcp.analysis.duplicate_ack             |</span><br><span class="line">11</span><br><span class="line">|     5: COUNT(tcp.analysis.lost_segment) tcp.analysis.lost_segment               |</span><br><span class="line">12</span><br><span class="line">|     6: AVG(tcp.window_size)tcp.window_size                                      |</span><br><span class="line">13</span><br><span class="line">|---------------------------------------------------------------------------------|</span><br><span class="line">14</span><br><span class="line">|          |1         |2      |3      |4      |5      |6      |                   |</span><br><span class="line">15</span><br><span class="line">| Interval |    AVG   | COUNT | COUNT | COUNT | COUNT |  AVG  |                   |</span><br><span class="line">16</span><br><span class="line">|-------------------------------------------------------------|                   |</span><br><span class="line">17</span><br><span class="line">|  0 &lt;&gt;  2 | 0.001152 |     0 |     0 |     0 |     0 |  4206 |                   |</span><br><span class="line">18</span><br><span class="line">|  2 &lt;&gt;  4 | 0.002088 |     0 |     0 |     0 |     1 |  6931 |                   |</span><br><span class="line">19</span><br><span class="line">|  4 &lt;&gt;  6 | 0.001512 |     0 |     0 |     0 |     0 |  7099 |                   |</span><br><span class="line">20</span><br><span class="line">|  6 &lt;&gt;  8 | 0.002859 |     0 |     0 |     0 |     0 |  7171 |                   |</span><br><span class="line">21</span><br><span class="line">|  8 &lt;&gt; 10 | 0.001716 |     0 |     0 |     0 |     0 |  6472 |                   |</span><br><span class="line">22</span><br><span class="line">| 10 &lt;&gt; 12 | 0.000319 |     0 |     0 |     0 |     2 |  5575 |                   |</span><br><span class="line">23</span><br><span class="line">| 12 &lt;&gt; 14 | 0.002030 |     0 |     0 |     0 |     0 |  6922 |                   |</span><br><span class="line">24</span><br><span class="line">| 14 &lt;&gt; 16 | 0.003371 |     0 |     0 |     0 |     2 |  5884 |                   |</span><br><span class="line">25</span><br><span class="line">| 16 &lt;&gt; 18 | 0.000138 |     0 |     0 |     0 |     1 |  3480 |                   |</span><br><span class="line">26</span><br><span class="line">| 18 &lt;&gt; 20 | 0.000999 |     0 |     0 |     0 |     4 |  6665 |                   |</span><br><span class="line">27</span><br><span class="line">| 20 &lt;&gt; 22 | 0.000682 |     0 |     0 |    41 |     2 |  5484 |                   |</span><br><span class="line">28</span><br><span class="line">| 22 &lt;&gt; 24 | 0.002302 |     2 |     0 |    19 |     0 |  7127 |                   |</span><br><span class="line">29</span><br><span class="line">| 24 &lt;&gt; 26 | 0.000156 |     1 |     0 |    22 |     0 |  3042 |                   |</span><br><span class="line">30</span><br><span class="line">| 26 &lt;&gt; 28 | 0.000000 |     1 |     0 |    19 |     1 |   152 |                   |</span><br><span class="line">31</span><br><span class="line">| 28 &lt;&gt; 30 | 0.001498 |     1 |     0 |    24 |     0 |  5615 |                   |</span><br><span class="line">32</span><br><span class="line">| 30 &lt;&gt; 32 | 0.000235 |     0 |     0 |    44 |     0 |  1880 |                   |</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">tshark</span></span><br><span class="line">tshark -r ./mysql-compress.cap -o tcp.calculate_timestamps:true -T fields -e mysql.caps.cp -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e frame.time_delta_displayed  -e tcp.stream -e tcp.len -e mysql.query </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">用tcpdump抓取并保存包：</span></span><br><span class="line">sudo tcpdump -i eth0 port 3306 -w plantegg.cap</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">每隔3秒钟生成一个新文件，总共生成5个文件后（15秒后）终止抓包，然后包名也按时间规范好了</span></span><br><span class="line">sudo  tcpdump -t -s 0 tcp port 6379  -w &#x27;dump_%Y-%m-%d_%H:%M:%S.pcap&#x27;   -G 3 -W 5 -Z root</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">每隔30分钟生成一个包并压缩，保留48个抓包，也就是24小的内的包</span></span><br><span class="line">nohup sudo tcpdump -i eth0 -t -s 0 tcp and port 6379 -w &#x27;dump_%Y-%m-%d_%H:%M:%S.pcap&#x27; -G 1800 -W 48 -Z root -z gzip &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">file size 512M  按文件大小不支持时间戳</span></span><br><span class="line">nohup sudo tcpdump -i eth0 -t -s 0 tcp and port 3306 -w &quot;dump_size.pcap&quot;  -C 1 -W 2 -Z root -z gzip &amp;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">port range</span></span><br><span class="line">sudo  tcpdump -i eth0 -t -s 0 portrange 3000-3100  -w &#x27;dump_%Y-%m-%d_%H:%M:%S.pcap&#x27;   -G 60 -W 100 -Z root</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">subnet</span></span><br><span class="line">sudo  tcpdump -i enp44s0f0 -t -s 0 net 192.168.0.1/28 -w &#x27;dump_%Y-%m-%d_%H:%M:%S.pcap&#x27;   -G 60 -W 100 -Z root</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">抓取详细SQL语句, 快速确认client发过来的具体SQL内容：</span></span><br><span class="line">sudo tshark -i any -f &#x27;port 8527&#x27; -s 0 -l -w - |strings</span><br><span class="line">sudo tshark -i eth0 -d tcp.port==3306,mysql -T fields -e mysql.query &#x27;port 3306&#x27;</span><br><span class="line">sudo tshark -i eth0 -R &quot;ip.addr==11.163.182.137&quot; -d tcp.port==3306,mysql -T fields -e mysql.query &#x27;port 3306&#x27;</span><br><span class="line">sudo tshark -i eth0 -R &quot;tcp.srcport==62877&quot; -d tcp.port==3001,mysql -T fields -e tcp.srcport -e mysql.query &#x27;port 3001&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">query <span class="keyword">time</span></span></span><br><span class="line">sudo tshark -i eth0 -Y &quot; ((tcp.port eq 3306 ) and tcp.len&gt;0 )&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e mysql.query</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">如果MySQL开启了SSL，那么抓包后的内容tshark/wireshark分析不到MySQL的具体内容，可以强制关闭：connectionProperties里加上useSSL=<span class="literal">false</span></span></span><br><span class="line"></span><br><span class="line">tshark -r ./manager.cap -o tcp.calculate_timestamps:true -Y &quot; tcp.analysis.retransmission &quot;  -T fields -e tcp.stream -e frame.number -e frame.time -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst | sort</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">MySQL响应时间直方图【第八列的含义-- Time since previous frame <span class="keyword">in</span> this TCP stream: seconds】：</span></span><br><span class="line">tshark -r gege_plantegg.pcap -Y &quot;mysql.query or (tcp.srcport3306 and tcp.len&gt;60)&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len | awk &#x27;BEGIN &#123;sum0=0;sum3=0;sum10=0;sum30=0;sum50=0;sum100=0;sum300=0;sum500=0;sum1000=0;sumo=0;count=0;sum=0&#125; &#123;rt=$8; if(rt&gt;=0.000) sum=sum+rt; count=count+1; if(rt&lt;=0.000) sum0=sum0+1; else if(rt&lt;0.003) sum3=sum3+1 ; else if(rt&lt;0.01) sum10=sum10+1; else if(rt&lt;0.03) sum30=sum30+1; else if(rt&lt;0.05) sum50=sum50+1; else if(rt &lt; 0.1) sum100=sum100+1; else if(rt &lt; 0.3) sum300=sum300+1; else if(rt &lt; 0.5) sum500=sum500+1; else if(rt &lt; 1) sum1000=sum1000+1; else sum=sum+1 ;&#125; END&#123;printf &quot;-------------\n3ms:\t%s \n10ms:\t%s \n30ms:\t%s \n50ms:\t%s \n100ms:\t%s \n300ms:\t%s \n500ms:\t%s \n1000ms:\t%s \n&gt;1s:\t %s\n-------------\navg: %.6f \n&quot; , sum3,sum10,sum30,sum50,sum100,sum300,sum500,sum1000,sumo,sum/count;&#125;&#x27;</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">分析MySQL rt，倒数第四列基本就是rt</span></span><br><span class="line">tshark -r gege_plantegg.pcap -Y &quot; ((tcp.srcport eq 3306 ) and tcp.len&gt;0 )&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e tcp.analysis.ack_rtt   </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">或者排序一下</span></span><br><span class="line">tshark -r 213_php.cap -Y &quot;mysql.query or (  tcp.srcport==3306)&quot; -o tcp.calculate_timestamps:true -T fields -e frame.number -e frame.time_epoch  -e frame.time_delta_displayed  -e ip.src -e tcp.srcport -e tcp.dstport -e ip.dst -e tcp.time_delta -e tcp.stream -e tcp.len -e mysql.query |sort -nk9 -nk1</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将 tls key和抓包文件合并</span></span><br><span class="line">editcap --inject-secrets tls,key.log in.pcap out.pcap</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">把包长截掉，只保留前面54，可以脱敏包内容</span></span><br><span class="line">editcap -s 54 old.pcap new.pcap</span><br></pre></td></tr></table></figure>



<p>DNAT:</p>
<p><img src="/images/951413iMgBlog/format,webp-4378791." alt="img"></p>
<p>FNAT:</p>
<p><img src="/images/951413iMgBlog/format,webp-20240823100700538" alt="img"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
