<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/11/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="技术,编程,博客">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://plantegg.github.io/page/11/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/11/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">plantegg</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">185</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">274</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/21/kubernetes%20%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/21/kubernetes%20%E5%A4%9A%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">kubernetes 多集群管理</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-21 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-21T17:30:03+08:00">2020-01-21</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-多集群管理"><a href="#kubernetes-多集群管理" class="headerlink" title="kubernetes 多集群管理"></a>kubernetes 多集群管理</h1><h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>指定config配置文件的方式访问不同的集群</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes</span><br></pre></td></tr></table></figure>

<p>一个kubectl可以管理多个集群，主要是 ~&#x2F;.kube&#x2F;config 里面的配置，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: /root/k8s-cluster.ca</span><br><span class="line">    server: https://192.168.0.80:6443</span><br><span class="line">  name: context-az1</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</span><br><span class="line">    server: https://192.168.0.97:6443</span><br><span class="line">  name: context-az3</span><br><span class="line"></span><br><span class="line">- context:</span><br><span class="line">    cluster: context-az1</span><br><span class="line">    namespace: default</span><br><span class="line">    user: az1-admin</span><br><span class="line">  name: az1</span><br><span class="line">- context:</span><br><span class="line">    cluster: context-az3</span><br><span class="line">    namespace: default</span><br><span class="line">    user: az3-read</span><br><span class="line">  name: az3</span><br><span class="line">current-context: az3  //当前使用的集群</span><br><span class="line"></span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: az1-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</span><br><span class="line">    client-key: /root/k8s.key</span><br><span class="line">- name: az3-read</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</span><br><span class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</span><br></pre></td></tr></table></figure>

<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube&#x2F;config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</span><br><span class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </span><br><span class="line">kubectl config view --flatten</span><br><span class="line"></span><br><span class="line">#激活这个上下文</span><br><span class="line">kubectl config use-context az1 </span><br><span class="line"></span><br><span class="line">#查看所有context</span><br><span class="line">kubectl config get-contexts </span><br><span class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</span><br><span class="line">          az1    context-az1   az1-admin          default</span><br><span class="line">*         az2    kubernetes    kubernetes-admin   </span><br><span class="line">          az3    context-az3   az3-read           default</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>背后的原理类似于这个流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </span><br><span class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</span><br><span class="line"></span><br><span class="line"># 添加用户 需要指定crt，key文件，上一步有获取</span><br><span class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</span><br><span class="line"></span><br><span class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</span><br><span class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="http://coreos.com/blog/kubectl-tips-and-tricks">http://coreos.com/blog/kubectl-tips-and-tricks</a></p>
<p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config">https://stackoverflow.com/questions/46184125/how-to-merge-kubectl-config-file-with-kube-config</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/15/Linux%20%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/15/Linux%20%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB/" class="post-title-link" itemprop="url">Linux 内存问题汇总</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-15 16:30:03" itemprop="dateCreated datePublished" datetime="2020-01-15T16:30:03+08:00">2020-01-15</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Memory/" itemprop="url" rel="index"><span itemprop="name">Memory</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Linux-内存问题汇总"><a href="#Linux-内存问题汇总" class="headerlink" title="Linux 内存问题汇总"></a>Linux 内存问题汇总</h1><p>本系列有如下几篇</p>
<p>[Linux 内存问题汇总](&#x2F;2020&#x2F;01&#x2F;15&#x2F;Linux 内存问题汇总&#x2F;)</p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--pagecache/">Linux内存–PageCache</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E7%AE%A1%E7%90%86%E5%92%8C%E7%A2%8E%E7%89%87/">Linux内存–管理和碎片</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--HugePage/">Linux内存–HugePage</a></p>
<p><a href="/2020/11/15/Linux%E5%86%85%E5%AD%98--%E9%9B%B6%E6%8B%B7%E8%B4%9D/">Linux内存–零拷贝</a></p>
<h2 id="内存使用观察"><a href="#内存使用观察" class="headerlink" title="内存使用观察"></a>内存使用观察</h2><pre><code># free -m
         total       used       free     shared    buffers     cached
Mem:          7515       1115       6400          0        189        492
-/+ buffers/cache:        432       7082
Swap:            0          0          0
</code></pre>
<p>其中，<a target="_blank" rel="noopener" href="https://spongecaptain.cool/SimpleClearFileIO/1.%20page%20cache.html">cached 列表示当前的页缓存（Page Cache）占用量</a>，buffers 列表示当前的块缓存（buffer cache）占用量。用一句话来解释：**Page Cache 用于缓存文件的页数据，buffer cache 用于缓存块设备（如磁盘）的块数据。**页是逻辑上的概念，因此 Page Cache 是与文件系统同级的；块是物理上的概念，因此 buffer cache 是与块设备驱动程序同级的。</p>
<img src="/images/oss/f8d944e2c7a8611384acb820c4471007.png" alt="image.png" style="zoom:80%;" />

<p><strong>上图中-&#x2F;+ buffers&#x2F;cache: -是指userd去掉buffers&#x2F;cached后真正使用掉的内存; +是指free加上buffers和cached后真正free的内存大小。</strong></p>
<h2 id="free"><a href="#free" class="headerlink" title="free"></a><a target="_blank" rel="noopener" href="https://aleiwu.com/post/linux-memory-monitring/">free</a></h2><p>free是从 &#x2F;proc&#x2F;meminfo 读取数据然后展示：</p>
<blockquote>
<p>buff&#x2F;cache &#x3D; Buffers + Cached + SReclaimable</p>
<p>Buffers + Cached + SwapCached &#x3D; Active(file) + Inactive(file) + Shmem + SwapCached</p>
</blockquote>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@az1-drds-79 ~]# cat /proc/meminfo |egrep -i &quot;buff|cach|SReclai&quot;</span><br><span class="line">Buffers:          817764 kB</span><br><span class="line">Cached:         76629252 kB</span><br><span class="line">SwapCached:            0 kB</span><br><span class="line">SReclaimable:    7202264 kB</span><br><span class="line">[root@az1-drds-79 ~]# free -k</span><br><span class="line">             total       used       free     shared    buffers     cached</span><br><span class="line">Mem:      97267672   95522336    1745336          0     817764   76629352</span><br><span class="line">-/+ buffers/cache:   18075220   79192452</span><br><span class="line">Swap:            0          0          0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在内核启动时，物理页面将加入到伙伴系统 （Buddy System）中，用户申请内存时分配，释放时回收。为了照顾慢速设备及兼顾多种 workload，Linux 将页面类型分为匿名页（Anon Page）和文件页 （Page Cache），及 swapness，使用 Page Cache 缓存文件 （慢速设备），通过 swap cache 和 swapness 交由用户根据负载特征决定内存不足时回收二者的比例。</p>
<h2 id="cached过高回收"><a href="#cached过高回收" class="headerlink" title="cached过高回收"></a>cached过高回收</h2><p>系统内存大体可分为三块，应用程序使用内存、系统Cache 使用内存（包括page cache、buffer，内核slab 等）和Free 内存。</p>
<ul>
<li><p>应用程序使用内存：应用使用都是虚拟内存，应用申请内存时只是分配了地址空间，并未真正分配出物理内存，等到应用真正访问内存时会触发内核的缺页中断，这时候才真正的分配出物理内存，映射到用户的地址空间，因此应用使用内存是不需要连续的，内核有机制将非连续的物理映射到连续的进程地址空间中（mmu），缺页中断申请的物理内存，内核优先给低阶碎内存。</p>
</li>
<li><p>系统Cache 使用内存：使用的也是虚拟内存，申请机制与应用程序相同。</p>
</li>
<li><p>Free 内存，未被使用的物理内存，这部分内存以4k 页的形式被管理在内核伙伴算法结构中，相邻的2^n 个物理页会被伙伴算法组织到一起，形成一块连续物理内存，所谓的阶内存就是这里的n (0&lt;&#x3D; n &lt;&#x3D;10)，高阶内存指的就是一块连续的物理内存，在OSS 的场景中，如果3阶内存个数比较小的情况下，如果系统有吞吐burst 就会触发Drop cache 情况。</p>
</li>
</ul>
<p>cache回收：	<br>	echo 1&#x2F;2&#x2F;3 &gt;&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</p>
<p>查看回收后：</p>
<pre><code>cat /proc/meminfo
</code></pre>
<p>手动回收系统Cache、Buffer，这个文件可以设置的值分别为1、2、3。它们所表示的含义为：</p>
<p><strong>echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</strong>:表示清除pagecache。</p>
<p><strong>echo 2 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</strong>:表示清除回收slab分配器中的对象（包括目录项缓存和inode缓存）。slab分配器是内核中管理内存的一种机制，其中很多缓存数据实现都是用的pagecache。</p>
<p><strong>echo 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</strong>:表示清除pagecache和slab分配器中的缓存对象。</p>
<h2 id="cached无法回收"><a href="#cached无法回收" class="headerlink" title="cached无法回收"></a>cached无法回收</h2><p>可能是正打开的文件占用了cached，比如 vim 打开了一个巨大的文件；比如 mount的 tmpfs； 比如 journald 日志等等</p>
<h3 id="通过vmtouch-查看"><a href="#通过vmtouch-查看" class="headerlink" title="通过vmtouch 查看"></a>通过<a target="_blank" rel="noopener" href="https://hoytech.com/vmtouch/">vmtouch</a> 查看</h3><pre><code># vmtouch -v test.x86_64.rpm 
test.x86_64.rpm
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 10988/10988

           Files: 1
     Directories: 0
  Resident Pages: 10988/10988  42M/42M  100%
         Elapsed: 0.000594 seconds

# ls -lh test.x86_64.rpm
-rw-r--r-- 1 root root 43M 10月  8 14:11 test.x86_64.rpm
</code></pre>
<p>如上，表示整个文件 test.x86_64.rpm 都被cached了，回收的话执行：</p>
<pre><code>vmtouch -e test.x86_64.rpm // 或者： echo 3 &gt;/proc/sys/vm/drop_cached
</code></pre>
<h3 id="遍历某个目录下的所有文件被cached了多少"><a href="#遍历某个目录下的所有文件被cached了多少" class="headerlink" title="遍历某个目录下的所有文件被cached了多少"></a>遍历某个目录下的所有文件被cached了多少</h3><pre><code># vmtouch -vt /var/log/journal/
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000011ba49-00059979e0926f43.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000152f41-00059b2c88eb4344.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-00000000000f2181-000598335fcd492f.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 4096/4096
/var/log/journal/20190829214900434421844640356160/system@782ec314565e436b900454c59655247c-0000000000129aea-000599e83996db80.journal
[OOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO] 14336/14336
/var/log/journal/20190829214900434421844640356160/user-1000@ad408d9cb9d94f9f93f2c2396c26b542-000000000009f171-000595a722ead670.journal
…………
           Files: 48
 Directories: 2
 Touched Pages: 468992 (1G)
 Elapsed: 13.274 seconds
</code></pre>
<h3 id="vmtouch-清理目录"><a href="#vmtouch-清理目录" class="headerlink" title="vmtouch 清理目录"></a>vmtouch 清理目录</h3><p>如下脚本传入一个指定目录(业务方来确认哪些目录占用 pagecache 较大, 且可以清理)，然后用vmtouch 遍历排序最大的几个清理掉，可能会造成业务的卡度</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">#</span><br><span class="line">#echo &quot;*/2 * * * * root bash /root/cron/os_pagecache_clean.sh -n 5 -e &gt; /root/cron/os_pagecache_clean.out 2&gt;&amp;1&quot; &gt; /etc/cron.d/os_pagecache_clean</span><br><span class="line"></span><br><span class="line">function usage()&#123;</span><br><span class="line">cat &lt;&lt; EOF</span><br><span class="line">usage:</span><br><span class="line">    $0 -n topN [-l|-e]</span><br><span class="line">option:</span><br><span class="line">    -l list top n redis_dir</span><br><span class="line">    -e list and evict top n redis_dir</span><br><span class="line">    -n top n</span><br><span class="line">EOF</span><br><span class="line">exit 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">while getopts &quot;n:leh&quot; opt; do</span><br><span class="line">  case $opt in</span><br><span class="line">    l) list=1 ;;</span><br><span class="line">    e) list=1 &amp;&amp; evict=1 ;;</span><br><span class="line">    n) n=$&#123;OPTARG&#125; ;;</span><br><span class="line">    h) usage ;;</span><br><span class="line">  esac</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">[[ -z $n ]] &amp;&amp; usage</span><br><span class="line">[[ -z $list &amp;&amp; -z $evict ]] &amp;&amp; usage</span><br><span class="line"></span><br><span class="line"># list must = 1</span><br><span class="line">cd /root &amp;&amp; ls | while read dirname ; do</span><br><span class="line">    page=$(vmtouch $dirname |  grep &quot;Resident Pages&quot;)</span><br><span class="line">    echo -e &quot;$dirname\t$page&quot;</span><br><span class="line">done | tr &quot;/&quot; &quot; &quot; |   sort -nr -k4 | head -n $n | awk &#x27;&#123;print $1,$6&#125;&#x27; | while read dirname cache_size; do</span><br><span class="line">    echo -e &quot;$dirname\t$cache_size&quot;</span><br><span class="line">    [[ $evict == 1 ]] &amp;&amp; vmtouch -e $dirname</span><br><span class="line">done</span><br></pre></td></tr></table></figure>



<h2 id="消失的内存"><a href="#消失的内存" class="headerlink" title="消失的内存"></a>消失的内存</h2><p>OS刚启动后就报内存不够了，什么都没跑就500G没了，cached和buffer基本没用，纯粹就是used占用高，top按内存排序没有超过0.5%的进程</p>
<p>参考： <a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1087455">https://cloud.tencent.com/developer/article/1087455</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[aliyun@uos15 18:40 /u02/backup_15/leo/benchmark/run]</span><br><span class="line">$free -g</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            503         501           1           0           0           1</span><br><span class="line">Swap:            15          12           3</span><br><span class="line"></span><br><span class="line">$cat /proc/meminfo </span><br><span class="line">MemTotal:       528031512 kB</span><br><span class="line">MemFree:         1469632 kB</span><br><span class="line">MemAvailable:          0 kB</span><br><span class="line">VmallocTotal:   135290290112 kB</span><br><span class="line">VmallocUsed:           0 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:            81920 kB</span><br><span class="line">AnonHugePages:    950272 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">HugePages_Total:   252557   ----- 预分配太多，一个2M，加起来刚好500G了</span><br><span class="line">HugePages_Free:    252557</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:        517236736 kB</span><br><span class="line"></span><br><span class="line">以下是一台正常的机器对比：</span><br><span class="line">Percpu:            41856 kB</span><br><span class="line">AnonHugePages:  11442176 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">HugePages_Total:       0            ----没有做预分配</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:               0 kB</span><br><span class="line"></span><br><span class="line">[aliyun@uos16 18:43 /home/aliyun]</span><br><span class="line">$free -g</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            503          20         481           0           1         480</span><br><span class="line">Swap:            15           0          15</span><br><span class="line"></span><br><span class="line">对有问题的机器执行：</span><br><span class="line"># echo 1024 &gt; /proc/sys/vm/nr_hugepages</span><br><span class="line">可以看到内存恢复正常了 </span><br><span class="line">root@uos15:/u02/backup_15/leo/benchmark/run# free -g</span><br><span class="line">              total        used        free      shared  buff/cache   available</span><br><span class="line">Mem:            503          10         492           0           0         490</span><br><span class="line">Swap:            15          12           3</span><br><span class="line">root@uos15:/u02/backup_15/leo/benchmark/run# cat /proc/meminfo </span><br><span class="line">MemTotal:       528031512 kB</span><br><span class="line">MemFree:        516106832 kB</span><br><span class="line">MemAvailable:   514454408 kB</span><br><span class="line">VmallocTotal:   135290290112 kB</span><br><span class="line">VmallocUsed:           0 kB</span><br><span class="line">VmallocChunk:          0 kB</span><br><span class="line">Percpu:            81920 kB</span><br><span class="line">AnonHugePages:    313344 kB</span><br><span class="line">ShmemHugePages:        0 kB</span><br><span class="line">ShmemPmdMapped:        0 kB</span><br><span class="line">HugePages_Total:    1024</span><br><span class="line">HugePages_Free:     1024</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">Hugetlb:         2097152 kB</span><br></pre></td></tr></table></figure>

<h2 id="定制内存"><a href="#定制内存" class="headerlink" title="定制内存"></a>定制内存</h2><p>物理内存700多G，要求OS只能用512G：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">24条32G的内存条，总内存768G</span><br><span class="line"># dmidecode -t memory |grep &quot;Size: 32 GB&quot;</span><br><span class="line">  Size: 32 GB</span><br><span class="line">…………</span><br><span class="line">  Size: 32 GB</span><br><span class="line">  Size: 32 GB</span><br><span class="line">root@uos15:/etc# dmidecode -t memory |grep &quot;Size: 32 GB&quot; | wc -l</span><br><span class="line">24</span><br><span class="line"></span><br><span class="line"># cat /boot/grub/grub.cfg  |grep 512</span><br><span class="line">  linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</span><br><span class="line">    linux /vmlinuz-4.19.0-arm64-server root=UUID=dbc68010-8c36-40bf-b794-271e59ff5727 ro  splash quiet console=tty video=VGA-1:1280x1024@60 mem=512G DEEPIN_GFXMODE=$DEEPIN_GFXMODE</span><br></pre></td></tr></table></figure>

<p>​	</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://www.atatech.org/articles/66885">https://www.atatech.org/articles/66885</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1087455">https://cloud.tencent.com/developer/article/1087455</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiaolincoding/p/13719610.html">https://www.cnblogs.com/xiaolincoding/p/13719610.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/13/kubernetes%20%E5%8D%B7%E5%92%8Cvolume/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/13/kubernetes%20%E5%8D%B7%E5%92%8Cvolume/" class="post-title-link" itemprop="url">kubernetes volume and storage</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-13 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-13T17:30:03+08:00">2020-01-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-volume-and-storage"><a href="#kubernetes-volume-and-storage" class="headerlink" title="kubernetes volume and storage"></a>kubernetes volume and storage</h1><p>通常部署应用需要一些永久存储，kubernetes提供了PersistentVolume （PV，实际存储）、PersistentVolumeClaim （PVC，Pod访问PV的接口）、StorageClass来支持。</p>
<p>它为 PersistentVolume 定义了 <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#class">StorageClass 名称</a> 为 <code>manual</code>，StorageClass 名称用来将 PersistentVolumeClaim 请求绑定到该 PersistentVolume。</p>
<p>PVC是用来描述希望使用什么样的或者说是满足什么条件的存储，它的全称是Persistent Volume Claim，也就是持久化存储声明。开发人员使用这个来描述该容器需要一个什么存储。</p>
<p>PVC就相当于是容器和PV之间的一个接口，使用人员只需要和PVC打交道即可。另外你可能也会想到如果当前环境中没有合适的PV和我的PVC绑定，那么我创建的POD不就失败了么？的确是这样的，不过如果发现这个问题，那么就赶快创建一个合适的PV，那么这时候持久化存储循环控制器会不断的检查PVC和PV，当发现有合适的可以绑定之后它会自动给你绑定上然后被挂起的POD就会自动启动，而不需要你重建POD。</p>
<p>创建 PersistentVolumeClaim 之后，Kubernetes 控制平面将查找满足申领要求的 PersistentVolume。 如果控制平面找到具有相同 StorageClass 的适当的 PersistentVolume，则将 PersistentVolumeClaim 绑定到该 PersistentVolume 上。<strong>PVC的大小可以小于PV的大小</strong>。</p>
<p>一旦 PV 和 PVC 绑定后，<code>PersistentVolumeClaim</code> 绑定是排他性的，不管它们是如何绑定的。 PVC 跟 PV 绑定是一对一的映射。</p>
<p><strong>注意</strong>：PV必须先于POD创建，而且只能是网络存储不能属于任何Node，虽然它支持HostPath类型但由于你不知道POD会被调度到哪个Node上，所以你要定义HostPath类型的PV就要保证所有节点都要有HostPath中指定的路径。</p>
<h2 id="PV-和PVC的关系"><a href="#PV-和PVC的关系" class="headerlink" title="PV 和PVC的关系"></a>PV 和PVC的关系</h2><p>PVC就会和PV进行绑定，绑定的一些原则：</p>
<ol>
<li>PV和PVC中的spec关键字段要匹配，比如存储（storage）大小。</li>
<li>PV和PVC中的storageClassName字段必须一致，这个后面再说。</li>
<li>上面的labels中的标签只是增加一些描述，对于PVC和PV的绑定没有关系</li>
</ol>
<p>PV的accessModes：支持三种类型</p>
<ul>
<li>ReadWriteMany 多路读写，卷能被集群多个节点挂载并读写</li>
<li>ReadWriteOnce 单路读写，卷只能被单一集群节点挂载读写</li>
<li>ReadOnlyMany 多路只读，卷能被多个集群节点挂载且只能读</li>
</ul>
<p>PV状态：</p>
<ul>
<li>Available – 资源尚未被claim使用</li>
<li>Bound – 卷已经被绑定到claim了</li>
<li>Released – claim被删除，卷处于释放状态，但未被集群回收。</li>
<li>Failed – 卷自动回收失败</li>
</ul>
<p> PV<strong>回收Recycling</strong>—pv可以设置三种回收策略：保留（Retain），回收（Recycle）和删除（Delete）。</p>
<ul>
<li>保留（Retain）： 当删除与之绑定的PVC时候，这个PV被标记为released（PVC与PV解绑但还没有执行回收策略）且之前的数据依然保存在该PV上，但是该PV不可用，需要手动来处理这些数据并删除该PV。</li>
<li>删除（Delete）：当删除与之绑定的PVC时候</li>
<li>回收（Recycle）：这个在1.14版本中以及被废弃，取而代之的是推荐使用动态存储供给策略，它的功能是当删除与该PV关联的PVC时，自动删除该PV中的所有数据</li>
</ul>
<h3 id="更改-PersistentVolume-的回收策略"><a href="#更改-PersistentVolume-的回收策略" class="headerlink" title="更改 PersistentVolume 的回收策略"></a>更改 PersistentVolume 的回收策略</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#kubectl patch pv wordpress-data -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;persistentVolumeReclaimPolicy&quot;:&quot;Delete&quot;&#125;&#125;&#x27;</span><br><span class="line">persistentvolume/wordpress-data patched</span><br></pre></td></tr></table></figure>

<p>本地卷（hostPath）也就是LPV不支持动态供给的方式，延迟绑定，就是为了综合考虑所有因素再进行POD调度。其根本原因是动态供给是先调度POD到节点，然后动态创建PV以及绑定PVC最后运行POD；而LPV是先创建与某一节点关联的PV，然后在调度的时候综合考虑各种因素而且要包括PV在哪个节点，然后再进行调度，到达该节点后在进行PVC的绑定。也就说动态供给不考虑节点，LPV必须考虑节点。所以这两种机制有冲突导致无法在动态供给策略下使用LPV。换句话说动态供给是PV跟着POD走，而LPV是POD跟着PV走。</p>
<h2 id="PV-和-PVC"><a href="#PV-和-PVC" class="headerlink" title="PV 和 PVC"></a>PV 和 PVC</h2><p>创建 pv controller 和pvc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#cat mysql-pv.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: simple-pv-volume</span><br><span class="line">  labels:</span><br><span class="line">    type: local</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 20Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  hostPath:</span><br><span class="line">    path: &quot;/mnt/simple&quot;</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">metadata:</span><br><span class="line">  name: pv-claim</span><br><span class="line">spec:</span><br><span class="line">  storageClassName: manual</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 20Gi</span><br></pre></td></tr></table></figure>

<h3 id="StorageClass"><a href="#StorageClass" class="headerlink" title="StorageClass"></a>StorageClass</h3><p>PV是运维人员来创建的，开发操作PVC，可是大规模集群中可能会有很多PV，如果这些PV都需要运维手动来处理这也是一件很繁琐的事情，所以就有了动态供给概念，也就是Dynamic Provisioning。而我们上面的创建的PV都是静态供给方式，也就是Static Provisioning。而动态供给的关键就是StorageClass，它的作用就是创建PV模板。</p>
<p>创建StorageClass里面需要定义PV属性比如存储类型、大小等；另外创建这种PV需要用到存储插件。最终效果是，用户提交PVC，里面指定存储类型，如果符合我们定义的StorageClass，则会为其自动创建PV并进行绑定。</p>
<p><strong>简单可以把storageClass理解为名字，只是这个名字可以重复，然后pvc和pv之间通过storageClass来绑定。</strong></p>
<p>如下case中两个pv和两个pvc的绑定就是通过storageClass(一致)来实现的（当然pvc要求的大小也必须和pv一致）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#kubectl get pv</span><br><span class="line">NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                              STORAGECLASS   REASON   AGE</span><br><span class="line">mariadb-pv       8Gi        RWO            Retain           Bound    default/data-wordpress-mariadb-0   db                      3m54s</span><br><span class="line">wordpress-data   10Gi       RWO            Retain           Bound    default/wordpress                  wordpress               3m54s</span><br><span class="line"></span><br><span class="line">[root@az3-k8s-11 15:35 /root/charts/bitnami/wordpress]</span><br><span class="line">#kubectl get pvc</span><br><span class="line">NAME                       STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">data-wordpress-mariadb-0   Bound    mariadb-pv       8Gi        RWO            db             4m21s</span><br><span class="line">wordpress                  Bound    wordpress-data   10Gi       RWO            wordpress      4m21s</span><br><span class="line"></span><br><span class="line">#cat create-pv.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: mariadb-pv</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 8Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  storageClassName: db</span><br><span class="line">  hostPath:</span><br><span class="line">    path: /mnt/mariadb-pv</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: PersistentVolume</span><br><span class="line">metadata:</span><br><span class="line">  name: wordpress-data</span><br><span class="line">spec:</span><br><span class="line">  capacity:</span><br><span class="line">    storage: 10Gi</span><br><span class="line">  accessModes:</span><br><span class="line">    - ReadWriteOnce</span><br><span class="line">  persistentVolumeReclaimPolicy: Retain</span><br><span class="line">  storageClassName: wordpress</span><br><span class="line">  hostPath:</span><br><span class="line">    path: /mnt/wordpress-pv</span><br><span class="line"></span><br><span class="line">----对应 pvc的定义参数：</span><br><span class="line">persistence:</span><br><span class="line">  enabled: true</span><br><span class="line">  storageClass: &quot;wordpress&quot;</span><br><span class="line">  accessMode: ReadWriteOnce</span><br><span class="line">  size: 10Gi</span><br><span class="line">  </span><br><span class="line">  persistence:</span><br><span class="line">    enabled: true</span><br><span class="line">    mountPath: /bitnami/mariadb</span><br><span class="line">    storageClass: &quot;db&quot;</span><br><span class="line">    annotations: &#123;&#125;</span><br><span class="line">    accessModes:</span><br><span class="line">      - ReadWriteOnce</span><br><span class="line">    size: 8Gi</span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<h4 id="定义StorageClass"><a href="#定义StorageClass" class="headerlink" title="定义StorageClass"></a>定义StorageClass</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">kind: StorageClass</span><br><span class="line">apiVersion: storage.k8s.io/v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-storage</span><br><span class="line">provisioner: kubernetes.io/no-provisioner</span><br><span class="line">volumeBindingMode: WaitForFirstConsumer</span><br></pre></td></tr></table></figure>

<h4 id="定义PVC"><a href="#定义PVC" class="headerlink" title="定义PVC"></a>定义PVC</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">kind: PersistentVolumeClaim</span><br><span class="line">apiVersion: v1</span><br><span class="line">metadata:</span><br><span class="line">  name: local-claim</span><br><span class="line">spec:</span><br><span class="line">  accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">  resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br><span class="line">  storageClassName: local-storage</span><br></pre></td></tr></table></figure>

<h2 id="delete-pv-卡住"><a href="#delete-pv-卡住" class="headerlink" title="delete pv 卡住"></a>delete pv 卡住</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#kubectl describe pv wordpress-pv</span><br><span class="line">Name:            wordpress-pv</span><br><span class="line">Labels:          &lt;none&gt;</span><br><span class="line">Annotations:     pv.kubernetes.io/bound-by-controller: yes</span><br><span class="line">Finalizers:      [kubernetes.io/pv-protection]  --- 问题在finalizers</span><br><span class="line">StorageClass:    </span><br><span class="line">Status:          Terminating (lasts 18h)</span><br><span class="line">Claim:           default/wordpress</span><br><span class="line">Reclaim Policy:  Retain</span><br><span class="line">Access Modes:    RWO</span><br><span class="line">VolumeMode:      Filesystem</span><br><span class="line">Capacity:        10Gi</span><br><span class="line">Node Affinity:   &lt;none&gt;</span><br><span class="line">Message:         </span><br><span class="line">Source:</span><br><span class="line">    Type:      NFS (an NFS mount that lasts the lifetime of a pod)</span><br><span class="line">    Server:    192.168.0.111</span><br><span class="line">    Path:      /mnt/wordpress-pv</span><br><span class="line">    ReadOnly:  false</span><br><span class="line">Events:        &lt;none&gt;</span><br><span class="line"></span><br><span class="line">先执行后就能自动删除了：</span><br><span class="line">kubectl patch pv wordpress-pv -p &#x27;&#123;&quot;metadata&quot;:&#123;&quot;finalizers&quot;: []&#125;&#125;&#x27; --type=merge</span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/12/kubernetes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/12/kubernetes/" class="post-title-link" itemprop="url">kubernetes 集群部署</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-12 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-12T17:30:03+08:00">2020-01-12</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-集群部署"><a href="#kubernetes-集群部署" class="headerlink" title="kubernetes 集群部署"></a>kubernetes 集群部署</h1><h2 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h2><p>系统参数修改</p>
<p>docker部署</p>
<p>kubeadm install</p>
<p><a target="_blank" rel="noopener" href="https://www.kubernetes.org.cn/4256.html">https://www.kubernetes.org.cn/4256.html</a> </p>
<p><a target="_blank" rel="noopener" href="https://github.com/opsnull/follow-me-install-kubernetes-cluster">https://github.com/opsnull/follow-me-install-kubernetes-cluster</a></p>
<p>镜像源被墙，可以用阿里云镜像源</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 配置源</span><br><span class="line">cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name=Kubernetes</span><br><span class="line">baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">repo_gpgcheck=1</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"># 安装</span><br><span class="line">yum install -y kubelet kubeadm kubectl ipvsadm</span><br></pre></td></tr></table></figure>

<h2 id="初始化集群"><a href="#初始化集群" class="headerlink" title="初始化集群"></a>初始化集群</h2><p>多网卡情况下有必要指定网卡：–apiserver-advertise-address&#x3D;192.168.0.80</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">使用本地 image repository</span></span><br><span class="line">kubeadm init --kubernetes-version=1.18.0  --apiserver-advertise-address=192.168.0.110   --image-repository registry:5000/registry.aliyuncs.com/google_containers  --service-cidr=10.10.0.0/16 --pod-network-cidr=10.122.0.0/16 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">给api-server 指定外网地址，在服务器有内网、外网多个ip的时候适用</span></span><br><span class="line">kubeadm init --control-plane-endpoint 外网-ip:6443 --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.21.0  --pod-network-cidr=172.16.0.0/16</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">--apiserver-advertise-address=30.1.1.1，设置 apiserver 的 IP 地址，对于多网卡服务器来说很重要（比如 VirtualBox 虚拟机就用了两块网卡），可以指定 apiserver 在哪个网卡上对外提供服务。</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">node <span class="built_in">join</span> <span class="built_in">command</span></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">kubeadm token create --print-join-command</span></span><br><span class="line">kubeadm join 192.168.0.110:6443 --token 1042rl.b4qn9iuz6xv1ri7b     --discovery-token-ca-cert-hash sha256:341a4bcfde9668077ef29211c2a151fe6e9334eea8955f645698706b3bf47a49 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="comment"># 查看集群配置</span></span></span><br><span class="line">kubectl get configmap -n kube-system kubeadm-config -o yaml</span><br></pre></td></tr></table></figure>

<p>将一个node设置为不可调度，隔离出来，比如master 默认是不可调度的</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl cordon &lt;node-name&gt;</span><br><span class="line">kubectl uncordon &lt;node-name&gt;</span><br></pre></td></tr></table></figure>

<h2 id="kubectl-管理多集群"><a href="#kubectl-管理多集群" class="headerlink" title="kubectl 管理多集群"></a>kubectl 管理多集群</h2><p>一个kubectl可以管理多个集群，主要是 ~&#x2F;.kube&#x2F;config 里面的配置，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority: /root/k8s-cluster.ca</span><br><span class="line">    server: https://192.168.0.80:6443</span><br><span class="line">  name: context-az1</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCQl0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K</span><br><span class="line">    server: https://192.168.0.97:6443</span><br><span class="line">  name: context-az3</span><br><span class="line"></span><br><span class="line">- context:</span><br><span class="line">    cluster: context-az1</span><br><span class="line">    namespace: default</span><br><span class="line">    user: az1-admin</span><br><span class="line">  name: az1</span><br><span class="line">- context:</span><br><span class="line">    cluster: context-az3</span><br><span class="line">    namespace: default</span><br><span class="line">    user: az3-read</span><br><span class="line">  name: az3</span><br><span class="line">current-context: az3  //当前使用的集群</span><br><span class="line"></span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: az1-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate: /root/k8s.crt  //key放在配置文件中</span><br><span class="line">    client-key: /root/k8s.key</span><br><span class="line">- name: az3-read</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUQ0FURS0tLS0tCg==</span><br><span class="line">    client-key-data: LS0tLS1CRUdJThuL2VPM0YxSWpEcXBQdmRNbUdiU2c9PQotLS0tLUVORCBSU0EgUFJJVkFURSBLRVktLS0tLQo=</span><br></pre></td></tr></table></figure>

<p>多个集群中切换的话 ： kubectl config use-context az3</p>
<h3 id="快速合并两个cluster"><a href="#快速合并两个cluster" class="headerlink" title="快速合并两个cluster"></a>快速合并两个cluster</h3><p>简单来讲就是把两个集群的 .kube&#x2F;config 文件合并，注意context、cluster name别重复了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 必须提前保证两个config文件中的cluster、context名字不能重复</span><br><span class="line">export KUBECONFIG=~/.kube/config:~/someotherconfig </span><br><span class="line">kubectl config view --flatten</span><br><span class="line"></span><br><span class="line">#激活这个上下文</span><br><span class="line">kubectl config use-context az1 </span><br><span class="line"></span><br><span class="line">#查看所有context</span><br><span class="line">kubectl config get-contexts </span><br><span class="line">CURRENT   NAME   CLUSTER       AUTHINFO           NAMESPACE</span><br><span class="line">          az1    context-az1   az1-admin          default</span><br><span class="line">*         az2    kubernetes    kubernetes-admin   </span><br><span class="line">          az3    context-az3   az3-read           default</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>背后的原理类似于这个流程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 添加集群 集群地址上一步有获取 ，需要指定ca文件，上一步有获取 </span><br><span class="line">kubectl config set-cluster cluster-az1 --server https://192.168.146.150:6444  --certificate-authority=/usr/program/k8s-certs/k8s-cluster.ca</span><br><span class="line"></span><br><span class="line"># 添加用户 需要指定crt，key文件，上一步有获取</span><br><span class="line">kubectl config set-credentials az1-admin --client-certificate=/usr/program/k8s-certs/k8s.crt --client-key=/usr/program/k8s-certs/k8s.key</span><br><span class="line"></span><br><span class="line"># 指定一个上下文的名字，我这里叫做 az1，随便你叫啥 关联刚才的用户</span><br><span class="line">kubectl config set-context az1 --cluster=context-az1  --namespace=default --user=az1-admin </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="apiserver高可用"><a href="#apiserver高可用" class="headerlink" title="apiserver高可用"></a>apiserver高可用</h2><p>默认只有一个apiserver，可以考虑用haproxy和keepalive来做一组apiserver的负载均衡：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name kube-haproxy \</span><br><span class="line">-v /etc/haproxy:/usr/local/etc/haproxy:ro \</span><br><span class="line">-p 8443:8443 \</span><br><span class="line">-p 1080:1080 \</span><br><span class="line">--restart always \</span><br><span class="line">haproxy:1.7.8-alpine</span><br></pre></td></tr></table></figure>

<p>haproxy配置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">#cat /etc/haproxy/haproxy.cfg </span><br><span class="line">global</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  uid 99</span><br><span class="line">  gid 99</span><br><span class="line">  #daemon</span><br><span class="line">  nbproc 1</span><br><span class="line">  pidfile haproxy.pid</span><br><span class="line"></span><br><span class="line">defaults</span><br><span class="line">  mode http</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  maxconn 50000</span><br><span class="line">  retries 3</span><br><span class="line">  timeout connect 5s</span><br><span class="line">  timeout client 30s</span><br><span class="line">  timeout server 30s</span><br><span class="line">  timeout check 2s</span><br><span class="line"></span><br><span class="line">listen admin_stats</span><br><span class="line">  mode http</span><br><span class="line">  bind 0.0.0.0:1080</span><br><span class="line">  log 127.0.0.1 local0 err</span><br><span class="line">  stats refresh 30s</span><br><span class="line">  stats uri     /haproxy-status</span><br><span class="line">  stats realm   Haproxy\ Statistics</span><br><span class="line">  stats auth    will:will</span><br><span class="line">  stats hide-version</span><br><span class="line">  stats admin if TRUE</span><br><span class="line"></span><br><span class="line">frontend k8s-https</span><br><span class="line">  bind 0.0.0.0:8443</span><br><span class="line">  mode tcp</span><br><span class="line">  #maxconn 50000</span><br><span class="line">  default_backend k8s-https</span><br><span class="line"></span><br><span class="line">backend k8s-https</span><br><span class="line">  mode tcp</span><br><span class="line">  balance roundrobin</span><br><span class="line">  server lab1 192.168.1.81:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server lab2 192.168.1.82:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br><span class="line">  server lab3 192.168.1.83:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3</span><br></pre></td></tr></table></figure>



<h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line">#或者老版本的calico</span><br><span class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</span><br></pre></td></tr></table></figure>

<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>在所有node节点都在一个二层网络时候，flannel提供hostgw实现，避免vxlan实现的udp封装开销，估计是目前最高效的；calico也针对L3 Fabric，推出了IPinIP的选项，利用了GRE隧道封装；因此这些插件都能适合很多实际应用场景。</p>
<p>Service cluster IP尽可在集群内部访问，外部请求需要通过NodePort、LoadBalance或者Ingress来访问</p>
<p>网络插件由 containernetworking-plugins rpm包来提供，一般里面会有flannel、vlan等，安装在 &#x2F;usr&#x2F;libexec&#x2F;cni&#x2F; 下（老版本没有带calico）</p>
<p>kubelet启动参数会配置 KUBELET_NETWORK_ARGS&#x3D;–network-plugin&#x3D;cni –cni-conf-dir&#x3D;&#x2F;etc&#x2F;cni&#x2F;net.d –cni-bin-dir&#x3D;&#x2F;usr&#x2F;libexec&#x2F;cni </p>
<h2 id="kubectl-启动容器"><a href="#kubectl-启动容器" class="headerlink" title="kubectl 启动容器"></a>kubectl 启动容器</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl run -i --tty busybox --image=registry:5000/busybox -- sh</span><br><span class="line">kubectl attach busybox -c busybox -i -t</span><br></pre></td></tr></table></figure>

<h2 id="dashboard"><a href="#dashboard" class="headerlink" title="dashboard"></a>dashboard</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc7/aio/deploy/recommented.yaml</span><br><span class="line"></span><br><span class="line">#暴露 dashboard 服务端口 (recommended中如果已经定义了 30000这个nodeport，所以这个命令不需要了)</span><br><span class="line">kubectl port-forward -n kubernetes-dashboard  svc/kubernetes-dashboard 30000:443 --address 0.0.0.0</span><br></pre></td></tr></table></figure>

<p>dashboard login token：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#kubectl describe secrets -n kubernetes-dashboard   | grep token | awk &#x27;NR==3&#123;print $2&#125;&#x27;</span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IndRc0hiMkdpWHRwN1FObTcyeUdhOHI0eUxYLTlvODd2U0NBcU1GY0t1Sk0ifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkZWZhdWx0LXRva2VuLXRia3o5Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImRlZmF1bHQiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiIwYzM2MzBhOS0xMjBjLTRhNmYtYjM0ZS0zM2JhMTE1OWU1OWMiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6ZGVmYXVsdCJ9.SP4JEw0kGDmyxrtcUC3HALq99Xr99E-tie5fk4R8odLJBAYN6HxEx80RbTSnkeSMJNApbtwXBLrp4I_w48kTkr93HJFM-oxie3RVLK_mEpZBF2JcfMk6qhfz4RjPiqmG6mGyW47mmY4kQ4fgpYSmZYR4LPJmVMw5W2zo5CGhZT8rKtgmi5_ROmYpWcd2ZUORaexePgesjjKwY19bLEXFOwdsqekwEvj1_zaJhKAehF_dBdgW9foFXkbXOX0xAC0QNnKUwKPanuFOVZDg1fhyV-eyi6c9-KoTYqZMJTqZyIzscIwruIRw0oauJypcdgi7ykxAubMQ4sWEyyFafSEYWg</span><br></pre></td></tr></table></figure>

<p>dashboard 显示为空的话(留意报错信息，一般是用户权限，重新授权即可)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete clusterrolebinding kubernetes-dashboard</span><br><span class="line">kubectl create clusterrolebinding kubernetes-dashboard --clusterrole=cluster-admin --serviceaccount=kube-system:kubernetes-dashboard --user=&quot;system:serviceaccount:kubernetes-dashboard:default&quot;</span><br></pre></td></tr></table></figure>

<p>其中：system:serviceaccount:kubernetes-dashboard:default 来自于报错信息中的用户名</p>
<p>默认dashboard login很快expired，可以设置不过期：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl -n kubernetes-dashboard edit deployments kubernetes-dashboard</span><br><span class="line">...</span><br><span class="line">spec:</span><br><span class="line">      containers:</span><br><span class="line">      - args:</span><br><span class="line">        - --auto-generate-certificates</span><br><span class="line">        - --token-ttl=0                //增加这行表示不expire</span><br><span class="line">        </span><br><span class="line">        --enable-skip-login            //增加这行表示不需要token 就能login，不推荐</span><br></pre></td></tr></table></figure>

<p>kubectl proxy –address 0.0.0.0 –accept-hosts ‘.*’</p>
<h2 id="node管理调度"><a href="#node管理调度" class="headerlink" title="node管理调度"></a>node管理调度</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">//如何优雅删除node</span><br><span class="line">kubectl drain my-node        # 对 my-node 节点进行清空操作，为节点维护做准备</span><br><span class="line">kubectl drain ky4 --ignore-daemonsets --delete-local-data # 驱逐pod</span><br><span class="line">kubectl delete node ky4			 # 删除node</span><br><span class="line"></span><br><span class="line">kubectl cordon my-node       # 标记 my-node 节点为不可调度</span><br><span class="line">kubectl uncordon my-node     # 标记 my-node 节点为可以调度</span><br><span class="line">kubectl top node my-node     # 显示给定节点的度量值</span><br><span class="line">kubectl cluster-info         # 显示主控节点和服务的地址</span><br><span class="line">kubectl cluster-info dump    # 将当前集群状态转储到标准输出</span><br><span class="line">kubectl cluster-info dump --output-directory=/path/to/cluster-state   # 将当前集群状态输出到 /path/to/cluster-state</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果已存在具有指定键和效果的污点，则替换其值为指定值</span></span><br><span class="line">kubectl taint nodes foo dedicated=special-user:NoSchedule</span><br><span class="line">kubectl taint nodes poc65 node-role.kubernetes.io/master:NoSchedule-</span><br></pre></td></tr></table></figure>

<h3 id="地址"><a href="#地址" class="headerlink" title="地址 "></a>地址<a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/concepts/architecture/nodes/#addresses"> </a></h3><p>这些字段的用法取决于你的云服务商或者物理机配置。</p>
<ul>
<li>HostName：由节点的内核设置。可以通过 kubelet 的 <code>--hostname-override</code> 参数覆盖。</li>
<li>ExternalIP：通常是节点的可外部路由（从集群外可访问）的 IP 地址。</li>
<li>InternalIP：通常是节点的仅可在集群内部路由的 IP 地址。</li>
</ul>
<h3 id="状况"><a href="#状况" class="headerlink" title="状况"></a>状况</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME             STATUS                     ROLES    AGE    VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME</span><br><span class="line">172.26.137.114   Ready                      master   6d1h   v1.19.0   172.26.137.114   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</span><br><span class="line">172.26.137.115   Ready                      node     6d1h   v1.19.0   172.26.137.115   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</span><br><span class="line">172.26.137.116   Ready,SchedulingDisabled   node     6d1h   v1.19.0   172.26.137.116   &lt;none&gt;        CentOS Linux 7 (Core)   3.10.0-957.21.3.el7.x86_64   docker://19.3.8</span><br></pre></td></tr></table></figure>

<p>如果 Ready 条件处于 <code>Unknown</code> 或者 <code>False</code> 状态的时间超过了 <code>pod-eviction-timeout</code> 值， （一个传递给 <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/generated/kube-controller-manager/">kube-controller-manager</a> 的参数）， 节点上的所有 Pod 都会被节点控制器计划删除。默认的逐出超时时长为 <strong>5 分钟</strong>。 某些情况下，当节点不可达时，API 服务器不能和其上的 kubelet 通信。 删除 Pod 的决定不能传达给 kubelet，直到它重新建立和 API 服务器的连接为止。 与此同时，被计划删除的 Pod 可能会继续在游离的节点上运行。</p>
<h2 id="node-cidr-缺失"><a href="#node-cidr-缺失" class="headerlink" title="node cidr 缺失"></a>node cidr 缺失</h2><p>flannel pod 运行正常，pod无法创建，检查flannel日志发现该node cidr缺失</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">I0818 08:06:38.951132       1 main.go:733] Defaulting external v6 address to interface address (&lt;nil&gt;)</span><br><span class="line">I0818 08:06:38.951231       1 vxlan.go:137] VXLAN config: VNI=1 Port=0 GBP=false Learning=false DirectRouting=false</span><br><span class="line">E0818 08:06:38.951550       1 main.go:325] Error registering network: failed to acquire lease: node &quot;ky3&quot; pod cidr not assigned</span><br><span class="line">I0818 08:06:38.951604       1 main.go:439] Stopping shutdownHandler...</span><br></pre></td></tr></table></figure>

<p>正常来说describe node会看到如下的cidr信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> Kube-Proxy Version:         v1.15.8-beta.0</span><br><span class="line">PodCIDR:                     172.19.1.0/24</span><br><span class="line">Non-terminated Pods:         (3 in total)</span><br></pre></td></tr></table></figure>

<p>可以手工给node添加cidr</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl patch node ky3 -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;podCIDR&quot;:&quot;172.19.3.0/24&quot;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>

<h2 id="prometheus"><a href="#prometheus" class="headerlink" title="prometheus"></a>prometheus</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/coreos/kube-prometheus.git</span><br><span class="line">kubectl apply -f manifests/setup</span><br><span class="line">kubectl apply -f manifests/</span><br></pre></td></tr></table></figure>

<p>暴露grafana端口：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward --address 0.0.0.0 svc/grafana -n monitoring 3000:3000 </span><br></pre></td></tr></table></figure>



<h2 id="部署应用"><a href="#部署应用" class="headerlink" title="部署应用"></a>部署应用</h2><h3 id="DRDS-deployment"><a href="#DRDS-deployment" class="headerlink" title="DRDS deployment"></a>DRDS deployment</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Namespace</span><br><span class="line">metadata:</span><br><span class="line">  name: drds</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">apiVersion: apps/v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: drds-deployment</span><br><span class="line">  namespace: drds</span><br><span class="line">  labels:</span><br><span class="line">    app: drds-server</span><br><span class="line">spec:</span><br><span class="line">  # 创建2个nginx容器</span><br><span class="line">  replicas: 3</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: drds-server</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: drds-server</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: drds-server</span><br><span class="line">        image: registry:5000/drds-image:v5_wisp_5.4.5-15940932</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 8507</span><br><span class="line">        - containerPort: 8607</span><br><span class="line">        env:</span><br><span class="line">        - name: diamond_server_port</span><br><span class="line">          value: &quot;8100&quot;</span><br><span class="line">        - name: diamond_server_list</span><br><span class="line">          value: &quot;192.168.0.79,192.168.0.82&quot;</span><br><span class="line">        - name: drds_server_id</span><br><span class="line">          value: &quot;1&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="DRDS-Service"><a href="#DRDS-Service" class="headerlink" title="DRDS Service"></a>DRDS Service</h3><p>每个 drds 容器会通过8507提供服务，service通过3306来为一组8507做负载均衡，这个service的3306是在cluster-ip上，外部无法访问</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: drds-service</span><br><span class="line">  namespace: drds</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    app: drds-server</span><br><span class="line">  ports:</span><br><span class="line">    - protocol: TCP</span><br><span class="line">      port: 3306</span><br><span class="line">      targetPort: 8507</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>通过node port来访问 drds service（同时会有负载均衡）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl port-forward --address 0.0.0.0 svc/drds-service -n drds 3306:3306</span><br></pre></td></tr></table></figure>

<h3 id="部署mysql-statefulset应用"><a href="#部署mysql-statefulset应用" class="headerlink" title="部署mysql statefulset应用"></a>部署mysql statefulset应用</h3><p>drds-pv-mysql-0 后面的mysql 会用来做存储，下面用到了三个mysql(需要三个pvc)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#cat mysql-deployment.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - port: 3306</span><br><span class="line">  selector:</span><br><span class="line">    app: mysql</span><br><span class="line">  clusterIP: None</span><br><span class="line">---</span><br><span class="line">apiVersion: apps/v1 </span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: mysql</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: mysql</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - image: mysql:5.7</span><br><span class="line">        name: mysql</span><br><span class="line">        env:</span><br><span class="line">          # Use secret in real usage</span><br><span class="line">        - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">          value: &quot;123456&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 3306</span><br><span class="line">          name: mysql</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: mysql-persistent-storage</span><br><span class="line">          mountPath: /var/lib/mysql</span><br><span class="line">      volumes:</span><br><span class="line">      - name: mysql-persistent-storage</span><br><span class="line">        persistentVolumeClaim:</span><br><span class="line">          claimName: pv-claim</span><br></pre></td></tr></table></figure>

<p>清理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete deployment,svc mysql</span><br><span class="line">kubectl delete pvc mysql-pv-claim</span><br><span class="line">kubectl delete pv mysql-pv-volume</span><br></pre></td></tr></table></figure>

<p>查看所有pod ip以及node ip：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl get pods -o wide</span><br></pre></td></tr></table></figure>

<h2 id="配置-Pod-使用-ConfigMap"><a href="#配置-Pod-使用-ConfigMap" class="headerlink" title="配置 Pod 使用 ConfigMap"></a>配置 Pod 使用 ConfigMap</h2><p>ConfigMap 允许你将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># cat mysql-configmap.yaml  //mysql配置文件放入： configmap</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">data:</span><br><span class="line">  master.cnf: |</span><br><span class="line">    # Apply this config only on the master.</span><br><span class="line">    [mysqld]</span><br><span class="line">    log-bin</span><br><span class="line"></span><br><span class="line">  mysqld.cnf: |</span><br><span class="line">    [mysqld]</span><br><span class="line">    pid-file        = /var/run/mysqld/mysqld.pid</span><br><span class="line">    socket          = /var/run/mysqld/mysqld.sock</span><br><span class="line">    datadir         = /var/lib/mysql</span><br><span class="line">    #log-error      = /var/log/mysql/error.log</span><br><span class="line">    # By default we only accept connections from localhost</span><br><span class="line">    #bind-address   = 127.0.0.1</span><br><span class="line">    # Disabling symbolic-links is recommended to prevent assorted security risks</span><br><span class="line">    symbolic-links=0</span><br><span class="line">   sql_mode=&#x27;STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION&#x27;</span><br><span class="line">    # 慢查询阈值，查询时间超过阈值时写入到慢日志中</span><br><span class="line">    long_query_time = 2</span><br><span class="line">    innodb_buffer_pool_size = 257M</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  slave.cnf: |</span><br><span class="line">    # Apply this config only on slaves.</span><br><span class="line">    [mysqld]</span><br><span class="line">    super-read-only</span><br><span class="line"></span><br><span class="line">  786  26/08/20 15:27:00 kubectl create configmap game-config-env-file --from-env-file=configure-pod-container/configmap/game-env-file.properties</span><br><span class="line">  787  26/08/20 15:28:10 kubectl get configmap -n kube-system kubeadm-config -o yaml</span><br><span class="line">  788  26/08/20 15:28:11 kubectl get configmap game-config-env-file -o yaml</span><br></pre></td></tr></table></figure>

<p>将mysql root密码放入secret并查看 secret密码：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">cat</span> mysql-secret.yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-root-password</span><br><span class="line">type: Opaque</span><br><span class="line">data:</span><br><span class="line">  password: MTIz</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash"><span class="built_in">echo</span> -n <span class="string">&#x27;123&#x27;</span> | <span class="built_in">base64</span>  //生成密码编码</span>  </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">kubectl get secret mysql-root-password -o jsonpath=<span class="string">&#x27;&#123;.data.password&#125;&#x27;</span> | <span class="built_in">base64</span> --decode -</span></span><br><span class="line"></span><br><span class="line">或者创建一个新的 secret：</span><br><span class="line">kubectl create secret generic my-secret --from-literal=password=&quot;Password&quot;</span><br></pre></td></tr></table></figure>

<p>在mysql容器中使用以上configmap中的参数： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">spec:</span><br><span class="line">  volumes:</span><br><span class="line">  - name: conf</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">  - name: myconf</span><br><span class="line">    emptyDir: &#123;&#125;</span><br><span class="line">  - name: config-map</span><br><span class="line">    configMap:</span><br><span class="line">      name: mysql</span><br><span class="line">  initContainers:</span><br><span class="line">  - name: init-mysql</span><br><span class="line">    image: mysql:5.7</span><br><span class="line">    command:</span><br><span class="line">    - bash</span><br><span class="line">    - &quot;-c&quot;</span><br><span class="line">    - |</span><br><span class="line">      set -ex</span><br><span class="line">      # Generate mysql server-id from pod ordinal index.</span><br><span class="line">      [[ `hostname` =~ -([0-9]+)$ ]] || exit 1</span><br><span class="line">      ordinal=$&#123;BASH_REMATCH[1]&#125;</span><br><span class="line">      echo [mysqld] &gt; /mnt/conf.d/server-id.cnf</span><br><span class="line">      # Add an offset to avoid reserved server-id=0 value.</span><br><span class="line">      echo server-id=$((100 + $ordinal)) &gt;&gt; /mnt/conf.d/server-id.cnf</span><br><span class="line">      #echo &quot;innodb_buffer_pool_size=512m&quot; &gt; /mnt/rds.cnf</span><br><span class="line">      # Copy appropriate conf.d files from config-map to emptyDir.</span><br><span class="line">      #if [[ $ordinal -eq 0 ]]; then</span><br><span class="line">      cp /mnt/config-map/master.cnf /mnt/conf.d/</span><br><span class="line">      cp /mnt/config-map/mysqld.cnf /mnt/mysql.conf.d/</span><br><span class="line">      #else</span><br><span class="line">      #  cp /mnt/config-map/slave.cnf /mnt/conf.d/</span><br><span class="line">      #fi</span><br><span class="line">    volumeMounts:</span><br><span class="line">    - name: conf</span><br><span class="line">      mountPath: /mnt/conf.d</span><br><span class="line">    - name: myconf</span><br><span class="line">      mountPath: /mnt/mysql.conf.d</span><br><span class="line">    - name: config-map</span><br><span class="line">      mountPath: /mnt/config-map</span><br><span class="line">  containers:</span><br><span class="line">  - name: mysql</span><br><span class="line">    image: mysql:5.7</span><br><span class="line">    env:</span><br><span class="line">    #- name: MYSQL_ALLOW_EMPTY_PASSWORD</span><br><span class="line">    #  value: &quot;1&quot;</span><br><span class="line">    - name: MYSQL_ROOT_PASSWORD</span><br><span class="line">      valueFrom:</span><br><span class="line">        secretKeyRef:</span><br><span class="line">          name: mysql-root-password</span><br><span class="line">          key: password</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>通过挂载方式进入到容器里的 Secret，一旦其对应的 Etcd 里的数据被更新，这些 Volume 里的文件内容，同样也会被更新。其实，这是 kubelet 组件在定时维护这些 Volume。</strong></p>
<p>集群会自动创建一个 default-token-**** 的secret，然后所有pod都会自动将这个 secret通过 Porjected Volume挂载到容器，也叫 ServiceAccountToken，是一种特殊的Secret</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    Environment:    &lt;none&gt;</span><br><span class="line">    Mounts:</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-ncgdl (ro)</span><br><span class="line">Conditions:</span><br><span class="line">  Type              Status</span><br><span class="line">  Initialized       True </span><br><span class="line">  Ready             True </span><br><span class="line">  ContainersReady   True </span><br><span class="line">  PodScheduled      True </span><br><span class="line">Volumes:</span><br><span class="line">  default-token-ncgdl:</span><br><span class="line">    Type:        Secret (a volume populated by a Secret)</span><br><span class="line">    SecretName:  default-token-ncgdl</span><br><span class="line">    Optional:    false</span><br><span class="line">QoS Class:       BestEffort</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="apply-create操作"><a href="#apply-create操作" class="headerlink" title="apply create操作"></a>apply create操作</h2><p>先 kubectl create，再 replace 的操作，我们称为命令式配置文件操作</p>
<p>kubectl apply 命令才是“声明式 API”</p>
<blockquote>
<p>kubectl replace 的执行过程，是使用新的 YAML 文件中的 API 对象，替换原有的 API 对象；</p>
<p>而 kubectl apply，则是执行了一个对原有 API 对象的 PATCH 操作。</p>
<p>kubectl set image 和 kubectl edit 也是对已有 API 对象的修改</p>
</blockquote>
<p> kube-apiserver 在响应命令式请求（比如，kubectl replace）的时候，一次只能处理一个写请求，否则会有产生冲突的可能。而对于声明式请求（比如，kubectl apply），一次能处理多个写操作，并且具备 Merge 能力</p>
<p>声明式 API，相当于对外界所有操作（并发接收）串行merge，才是 Kubernetes 项目编排能力“赖以生存”的核心所在</p>
<blockquote>
<p>如何使用控制器模式，同 Kubernetes 里 API 对象的“增、删、改、查”进行协作，进而完成用户业务逻辑的编写过程。</p>
</blockquote>
<h2 id="label"><a href="#label" class="headerlink" title="label"></a>label</h2><p>给多个节点加标签</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl label  --overwrite=true nodes 10.0.0.172 10.0.1.192 10.0.2.48 topology.kubernetes.io/region=cn-hangzhou</span><br><span class="line"></span><br><span class="line">//查看</span><br><span class="line">kubectl get nodes --show-labels</span><br></pre></td></tr></table></figure>



<h2 id="helm"><a href="#helm" class="headerlink" title="helm"></a>helm</h2><p>Helm 是 Kubernetes 的包管理器。包管理器类似于我们在 Ubuntu 中使用的apt、Centos中使用的yum 或者Python中的 pip 一样，能快速查找、下载和安装软件包。Helm 由客户端组件 helm 和服务端组件 Tiller 组成, 能够将一组K8S资源打包统一管理, 是查找、共享和使用为Kubernetes构建的软件的最佳方式。</p>
<p>建立local repo index：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">helm repo index [DIR] [flags]</span><br></pre></td></tr></table></figure>

<p>仓库只能index 到 helm package 发布后的tgz包，意义不大。每次index后需要 helm repo update</p>
<p>然后可以启动一个http服务：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -m SimpleHTTPServer 8089 &amp;</span><br></pre></td></tr></table></figure>

<p>将local repo加入到仓库：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> helm repo add local http://127.0.0.1:8089</span><br><span class="line"> </span><br><span class="line"> # helm repo list</span><br><span class="line">NAME 	URL                  </span><br><span class="line">local	http://127.0.0.1:8089</span><br></pre></td></tr></table></figure>

<p>install chart：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//helm3 默认不自动创建namespace，不带参数就报没有 ame 的namespace错误</span><br><span class="line">helm install -name wordpress -n test --create-namespace .</span><br><span class="line"></span><br><span class="line">helm list -n test</span><br><span class="line"></span><br><span class="line">&#123;&#123; .Release.Name &#125;&#125; 这种是helm内部自带的值，都是一些内建的变量，所有人都可以访问</span><br><span class="line"></span><br><span class="line">image: &quot;&#123;&#123; .Values.image.repository &#125;&#125;:&#123;&#123; .Values.image.tag | default .Chart.AppVersion &#125;&#125;&quot;  这种是我们从values.yaml文件中获取或者从命令行中获取的值。</span><br></pre></td></tr></table></figure>

<p>quote是一个模板方法，可以将输入的参数添加双引号</p>
<h3 id="模板片段"><a href="#模板片段" class="headerlink" title="模板片段"></a>模板片段</h3><p>之前我们看到有个文件叫做_helpers.tpl，我们介绍是说存储模板片段的地方。</p>
<p>模板片段其实也可以在文件中定义，但是为了更好管理，可以在_helpers.tpl中定义，使用时直接调用即可。</p>
<h2 id="自动补全"><a href="#自动补全" class="headerlink" title="自动补全"></a>自动补全</h2><p>kubernetes自动补全：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source &lt;(kubectl completion bash) </span><br><span class="line"></span><br><span class="line">echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>helm自动补全：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~</span><br><span class="line">helm completion bash &gt; .helmrc &amp;&amp; echo &quot;source .helmrc&quot; &gt;&gt; .bashrc &amp;&amp; source .bashrc</span><br></pre></td></tr></table></figure>

<p>两者都需要依赖 auto-completion，所以得先：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># yum install -y bash-completion</span><br><span class="line"># source /usr/share/bash-completion/bash_completion</span><br></pre></td></tr></table></figure>

<p>kubectl -s polarx-test-ackk8s-atp-3826.adbgw.alibabacloud.test exec -it bushu016polarx282bc7216f-5161 bash</p>
<h2 id="启动时间排序"><a href="#启动时间排序" class="headerlink" title="启动时间排序"></a>启动时间排序</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">532  [2021-08-24 18:37:19] kubectl get po --sort-by=.status.startTime -ndrds</span><br><span class="line">533  [2021-08-24 18:37:41] kubectl get pods --sort-by=.metadata.creationTimestamp -ndrds</span><br></pre></td></tr></table></figure>

<h2 id="kubeadm"><a href="#kubeadm" class="headerlink" title="kubeadm"></a>kubeadm</h2><p>初始化集群的时候第一看kubelet能否起来（cgroup配置），第二就是看kubelet静态起pod，kubelet参数指定yaml目录，然后kubelet拉起这个目录下的所有yaml。</p>
<p>kubeadm启动集群就是如此。kubeadm生成证书、etcd.yaml等yaml、然后拉起kubelet，kubelet拉起etcd、apiserver等pod，kubeadm init 的时候主要是在轮询等待apiserver的起来。</p>
<p>可以通过kubelet –v 256来看详细日志，kubeadm本身所做的事情并不多，所以日志没有太多的信息，主要是等待轮询apiserver的拉起。</p>
<h3 id="Kubeadm-config"><a href="#Kubeadm-config" class="headerlink" title="Kubeadm config"></a>Kubeadm config</h3><p>Init 可以指定仓库以及版本</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --image-repository=registry:5000/registry.aliyuncs.com/google_containers --kubernetes-version=v1.14.6  --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure>

<p>查看并修改配置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo kubeadm config view &gt; kubeadm-config.yaml</span><br><span class="line">edit kubeadm-config.yaml and replace k8s.gcr.io with your repo</span><br><span class="line">sudo kubeadm upgrade apply --config kubeadm-config.yaml</span><br><span class="line"></span><br><span class="line">kubeadm config images pull --config=&quot;/root/kubeadm-config.yaml&quot;</span><br><span class="line"></span><br><span class="line">kubectl get cm -n kube-system kubeadm-config -o yaml</span><br></pre></td></tr></table></figure>

<p>pod镜像拉取不到的话可以在kebelet启动参数中写死pod镜像（pod_infra_container_image）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash"><span class="built_in">cat</span> /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf</span></span><br><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS --pod_infra_container_image=registry:5000/registry.aliyuncs.com/google_containers/pause:3.1</span><br></pre></td></tr></table></figure>

<h3 id="构建离线镜像库"><a href="#构建离线镜像库" class="headerlink" title="构建离线镜像库"></a>构建离线镜像库</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubeadm config images list &gt;1.24.list</span><br><span class="line"></span><br><span class="line">cat 1.24.list | awk -F / &#x27;&#123; print $0 &quot;    &quot; $3&#125;&#x27; &gt; 1.24.aarch.list</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="cni-报x509-certificate-signed-by-unknown-authority"><a href="#cni-报x509-certificate-signed-by-unknown-authority" class="headerlink" title="cni 报x509: certificate signed by unknown authority"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/huiyichanmian/p/15760579.html">cni 报x509: certificate signed by unknown authority</a></h3><p>一个集群下反复部署calico&#x2F;flannel插件后，在 &#x2F;etc&#x2F;cni&#x2F;net.d&#x2F; 下会有cni 网络配置文件残留，导致 flannel 创建容器网络的时候报证书错误。其实这不只是证书错误，还可能报其它cni配置错误，总之这是因为 10-calico.conflist 不符合 flannel要求所导致的。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># find /etc/cni/net.d/</span><br><span class="line">/etc/cni/net.d/</span><br><span class="line">/etc/cni/net.d/calico-kubeconfig</span><br><span class="line">/etc/cni/net.d/10-calico.conflist   //默认读取了这个配置文件，不符合flannel</span><br><span class="line">/etc/cni/net.d/10-flannel.conflist</span><br></pre></td></tr></table></figure>

<p>因为calico 排在 flannel前面，所以即使用flannel配置文件也是用的 10-calico.conflist。每次 kubeadm reset 的时候是不会去做 cni 的reset 的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]</span><br><span class="line">[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]</span><br><span class="line"></span><br><span class="line">The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d</span><br></pre></td></tr></table></figure>

<h2 id="kubernetes-API-案例"><a href="#kubernetes-API-案例" class="headerlink" title="kubernetes API 案例"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/1ouLZbw-Z7G-fKz53uJZag">kubernetes API 案例</a></h2><p>用kubeadm部署kubernetes集群，会生成如下证书：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#ls /etc/kubernetes/pki/</span><br><span class="line">apiserver-etcd-client.crt  apiserver-kubelet-client.crt  apiserver.crt  ca.crt  etcd  front-proxy-ca.key      front-proxy-client.key  sa.pub</span><br><span class="line">apiserver-etcd-client.key  apiserver-kubelet-client.key  apiserver.key  ca.key  front-proxy-ca.crt  front-proxy-client.crt  sa.key</span><br></pre></td></tr></table></figure>

<p>curl访问api必须提供证书</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://ip:6443/apis/apps/v1/deployments</span><br></pre></td></tr></table></figure>

<p>&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;ca.crt —- CA机构</p>
<p>由CA机构签发：&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;apiserver-kubelet-client.crt </p>
<p><img src="/images/951413iMgBlog/640-5609125.jpeg" alt="Image"></p>
<p><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/using-api/api-concepts/">获取default namespace下的deployment</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># JWT_TOKEN_DEFAULT_DEFAULT=$(kubectl get secrets \</span><br><span class="line">    $(kubectl get serviceaccounts/default -o jsonpath=&#x27;&#123;.secrets[0].name&#125;&#x27;) \</span><br><span class="line">    -o jsonpath=&#x27;&#123;.data.token&#125;&#x27; | base64 --decode)</span><br><span class="line"></span><br><span class="line">#curl --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;DeploymentList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;apps/v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;resourceVersion&quot;: &quot;1233307&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;items&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;metadata&quot;: &#123;</span><br><span class="line">        &quot;name&quot;: &quot;nginx-deployment&quot;,</span><br><span class="line"> </span><br><span class="line">//列出default namespace下所有的pod </span><br><span class="line">#curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/api/v1/namespaces/default/pods --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;       </span><br><span class="line"></span><br><span class="line">//对应的kubectl生成的curl命令</span><br><span class="line">curl  --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -v -XGET  -H &quot;Accept: application/json;as=Table;v=v1;g=meta.k8s.io,application/json;as=Table;v=v1beta1;g=meta.k8s.io,application/json&quot; -H &quot;User-Agent: kubectl/v1.23.3 (linux/arm64) kubernetes/816c97a&quot; &#x27;https://11.158.239.200:6443/api/v1/namespaces/default/pods?limit=500&#x27;</span><br></pre></td></tr></table></figure>

<p>对应地可以通过 kubectl -v 256 get pods 来看kubectl的处理过程，以及具体访问的api、参数、返回结果等。实际kubectl最终也是通过libcurl来访问的这些api。这样也不用对api-server抓包分析了。</p>
<p>或者将kube api-server 代理成普通http服务</p>
<blockquote>
<p><em># Make Kubernetes API available on localhost:8080</em><br><em># to bypass the auth step in subsequent queries:</em><br>$ kubectl proxy –port&#x3D;8080 </p>
<p>然后</p>
<p>curl <a target="_blank" rel="noopener" href="http://localhost:8080/api/v1/namespaces">http://localhost:8080/api/v1/namespaces</a></p>
</blockquote>
<p><img src="/images/951413iMgBlog/640-5609622.png" alt="Image"></p>
<h2 id="抓包"><a href="#抓包" class="headerlink" title="抓包"></a>抓包</h2><p>用curl调用kubernetes api-server来调试，需要抓包，先在执行curl的服务器上配置环境变量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export SSLKEYLOGFILE=/root/ssllog/apiserver-ssl.log</span><br></pre></td></tr></table></figure>

<p>然后执行tcpdump对api-server的6443端口抓包，然后将&#x2F;root&#x2F;ssllog&#x2F;apiserver-ssl.log和抓包文件下载到本地，wireshark打开抓包文件，同时配置tls。</p>
<p>以下是个完整case（技巧指定curl的本地端口为12345，然后tcpdump只抓12345，所得的请求、response结果都会解密–如果抓api-server的6443则只能看到请求被解密）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl --local-port 12345 --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key https://11.158.239.200:6443/apis/apps/v1/namespaces/default/deployments --header &quot;Authorization: Bearer $JWT_TOKEN_DEFAULT_DEFAULT&quot;</span><br><span class="line"></span><br><span class="line">#cat $JWT_TOKEN_DEFAULT_DEFAULT eyJhbGciOiJSUzI1NiIsImtpZCI6ImlNVVFVNmxUM2t4c3Y2Q3IyT1BzV2hDZGRVSmVxTHc5RV8wUXZ4RVM5REEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZWZhdWx0Iiwia3ViZXJ: File name too long</span><br></pre></td></tr></table></figure>

<p><img src="/images/951413iMgBlog/image-20220223170008311.png" alt="image-20220223170008311"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/">https://kubernetes.io/zh/docs/reference/kubectl/cheatsheet/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/04/Java%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/04/Java%E6%8A%80%E5%B7%A7%E5%90%88%E9%9B%86/" class="post-title-link" itemprop="url">Java 技巧合集</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-04 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-04T17:30:03+08:00">2020-01-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Java-技巧合集"><a href="#Java-技巧合集" class="headerlink" title="Java 技巧合集"></a>Java 技巧合集</h1><h2 id="获取一直FullGC下的java进程HeapDump的小技巧"><a href="#获取一直FullGC下的java进程HeapDump的小技巧" class="headerlink" title="获取一直FullGC下的java进程HeapDump的小技巧"></a>获取一直FullGC下的java进程HeapDump的小技巧</h2><p>就是小技巧，操作步骤需要查询，随手记录</p>
<ul>
<li>找到java进程，gdb attach上去， 例如 <code>gdb -p 12345</code></li>
<li>找到这个<code>HeapDumpBeforeFullGC</code>的地址（这个flag如果为true，会在FullGC之前做HeapDump，默认是false）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) p &amp;HeapDumpBeforeFullGC</span><br><span class="line">$2 = (&lt;data variable, no debug info&gt; *) 0x7f7d50fc660f &lt;HeapDumpBeforeFullGC&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>Copy 地址：0x7f7d50fc660f</li>
<li>然后把他设置为true，这样下次FGC之前就会生成一份dump文件</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) set *0x7f7d50fc660f = 1</span><br><span class="line">(gdb) quit</span><br></pre></td></tr></table></figure>

<ul>
<li>最后，等一会，等下次FullGC触发，你就有HeapDump了！<br>(如果没有指定heapdump的名字，默认是 java_pidxxx.hprof)</li>
</ul>
<p>(PS. <code>jstat -gcutil pid</code> 可以查看gc的概况)</p>
<p>(操作完成后记得gdb上去再设置回去，不然可能一直fullgc，导致把磁盘打满).</p>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>在jvm还有响应的时候可以： jinfo -flag +HeapDumpBeforeFullGC pid 设置HeapDumpBeforeFullGC 为true（- 为false，+-都不要为只打印值）</p>
<p>kill -3 产生coredump  存放在 kernel.core_pattern&#x3D;&#x2F;root&#x2F;core （&#x2F;etc&#x2F;sysctl.conf , 先 ulimit -c unlimited；或者 gcore id 获取coredump)</p>
<p>得到core文件后，采用 gdb -c 执行文件 core文件 进入调试模式，对于java，有以下2个技巧：</p>
<p>进入gdb调试模式后，输入如下命令： info threads，观察异常的线程，定位到异常的线程后，则可以输入如下命令：thread 线程编号，则会打印出当前java代码的工作流程。</p>
<p> 而对于这个core，亦可以用jstack jmap打印出堆信息，线程信息，具体命令：</p>
<p>  jmap -heap 执行文件 core文件   jstack -F -l 执行文件 core文件</p>
<p><strong>容器中的进程的话需要到宿主机操作，并且将容器中的 jdk文件夹复制到宿主机对应的位置。</strong></p>
<p>  <strong>ps auxff |grep 容器id -A10 找到JVM在宿主机上的进程id</strong></p>
<h2 id="coredump"><a href="#coredump" class="headerlink" title="coredump"></a>coredump</h2><blockquote>
<p>Coredump叫做核心转储，它是进程运行时在突然崩溃的那一刻的一个内存快照。操作系统在程序发生异常而异常在进程内部又没有被捕获的情况下，会把进程此刻内存、寄存器状态、运行堆栈等信息转储保存在一个文件里。</p>
</blockquote>
<p>kill -3 产生coredump  存放在 kernel.core_pattern&#x3D;&#x2F;root&#x2F;core （&#x2F;etc&#x2F;sysctl.conf , 先 ulimit -c unlimited；）</p>
<p>或者 gcore id 获取coredump</p>
<p><a target="_blank" rel="noopener" href="https://www.baeldung.com/linux/managing-core-dumps">coredump 所在位置</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/kernel/core_pattern</span><br><span class="line">/home/admin/</span><br></pre></td></tr></table></figure>

<h3 id="coredump-分析"><a href="#coredump-分析" class="headerlink" title="coredump 分析"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46605905">coredump 分析</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">//打开 coredump</span><br><span class="line">$gdb /opt/taobao/java/bin/java core.24086</span><br><span class="line">[New LWP 27184]</span><br><span class="line">[New LWP 27186]</span><br><span class="line">[New LWP 24086]</span><br><span class="line">[Thread debugging using libthread_db enabled]</span><br><span class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</span><br><span class="line">Core was generated by `/opt/tt/java_coroutine/bin/java&#x27;.</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">Missing separate debuginfos, use: debuginfo-install jdk-8.9.14-20200203164153.alios7.x86_64</span><br><span class="line">(gdb) info threads  //查看所有thread</span><br><span class="line">  Id   Target Id         Frame</span><br><span class="line">  583  Thread 0x7f2fa56177c0 (LWP 24086) 0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">  582  Thread 0x7f2f695f3700 (LWP 27186) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  581  Thread 0x7f2f6cbfb700 (LWP 27184) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  580  Thread 0x7f2f691ef700 (LWP 27176) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  579  Thread 0x7f2f698f6700 (LWP 27174) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">(gdb) thread apply all bt  //查看所有线程堆栈</span><br><span class="line">Thread 583 (Thread 0x7f2fa56177c0 (LWP 24086)):</span><br><span class="line">#0  0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa4b85085 in ContinueInNewThread0 (continuation=continuation@entry=0x7f2fa4b7fd70 &lt;JavaMain&gt;, stack_size=1048576, args=args@entry=0x7ffe529432d0)</span><br><span class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1044</span><br><span class="line">#2  0x00007f2fa4b81877 in ContinueInNewThread (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=0x7f2fa3c163a8, mode=mode@entry=1,</span><br><span class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:2033</span><br><span class="line">#3  0x00007f2fa4b8513b in JVMInit (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, mode=mode@entry=1,</span><br><span class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=ret@entry=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1091</span><br><span class="line">#4  0x00007f2fa4b8254d in JLI_Launch (argc=0, argv=0x7f2fa3c163a8, jargc=&lt;optimized out&gt;, jargv=&lt;optimized out&gt;, appclassc=1, appclassv=0x0, fullversion=0x400885 &quot;1.8.0_232-b604&quot;,</span><br><span class="line">    dotversion=0x400881 &quot;1.8&quot;, pname=0x40087c &quot;java&quot;, lname=0x40087c &quot;java&quot;, javaargs=0 &#x27;\000&#x27;, cpwildcard=1 &#x27;\001&#x27;, javaw=0 &#x27;\000&#x27;, ergo=0)</span><br><span class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:304</span><br><span class="line">#5  0x0000000000400635 in main ()</span><br><span class="line"></span><br><span class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#3  0x00007f2f9343b44a in ?? ()</span><br><span class="line">#4  0x000000008082e778 in ?? ()</span><br><span class="line">#5  0x0000000000000003 in ?? ()</span><br><span class="line">#6  0x00007f2f88e32758 in ?? ()</span><br><span class="line">#7  0x00007f2f6f532800 in ?? ()</span><br><span class="line"></span><br><span class="line">(gdb) thread apply 582 bt //查看582这个线程堆栈，LWP 27186(0x6a32)对应jstack 线程10进程id</span><br><span class="line"></span><br><span class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#3  0x00007f2f9343b44a in ?? ()</span><br><span class="line">#35 0x00000000f26fc738 in ?? ()</span><br><span class="line">#36 0x00007f2fa51cec5b in arena_run_split_remove (arena=0x7f2f6ab09c34, chunk=0x80, run_ind=0, flag_dirty=0, flag_decommitted=&lt;optimized out&gt;, need_pages=0) at src/arena.c:398</span><br><span class="line">#37 0x00007f2f695f2980 in ?? ()</span><br><span class="line">#38 0x0000000000000001 in ?? ()</span><br><span class="line">#39 0x00007f2f88e32758 in ?? ()</span><br><span class="line">#40 0x00007f2f695f2920 in ?? ()</span><br><span class="line">#41 0x00007f2fa32f46b8 in CallInfo::set_common(KlassHandle, KlassHandle, methodHandle, methodHandle, CallInfo::CallKind, int, Thread*) ()</span><br><span class="line">   from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#42 0x00007f2f7d800000 in ?? ()</span><br></pre></td></tr></table></figure>

<p>以上堆栈涉及到Java代码部分都是看不到函数，需要进一步把Java 符号替换进去</p>
<h3 id="coredump-转-jmap-hprof"><a href="#coredump-转-jmap-hprof" class="headerlink" title="coredump 转 jmap hprof"></a>coredump 转 jmap hprof</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format=b,file=24086.hprof /opt/taobao/java/bin/java core.24086</span><br></pre></td></tr></table></figure>

<p>以上命令输入是 core.24086 这个 coredump，输出是一个 jmap 的dump 24086.hprof</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$jmap -J-d64 /opt/taobao/java/bin/java core.24086</span><br><span class="line"></span><br><span class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 25.232-b604</span><br><span class="line">0x0000000000400000      8K      /opt/taobao/java/bin/java</span><br><span class="line">0x00007f2fa51be000      6679K   /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/libjemalloc.so.2</span><br><span class="line">0x00007f2fa4fa2000      138K    /lib64/libpthread.so.0</span><br><span class="line">0x00007f2fa4d8c000      88K     /lib64/libz.so.1</span><br><span class="line">0x00007f2fa4b7d000      280K    /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/jli/libjli.so</span><br><span class="line">0x00007f2fa4979000      18K     /lib64/libdl.so.2</span><br><span class="line">0x00007f2fa45ab000      2105K   /lib64/libc.so.6</span><br><span class="line">0x00007f2fa43a3000      42K     /lib64/librt.so.1</span><br><span class="line">0x00007f2fa40a1000      1110K   /lib64/libm.so.6</span><br><span class="line">0x00007f2fa5406000      159K    /lib64/ld-linux-x86-64.so.2</span><br><span class="line">0x00007f2fa2af1000      17898K  /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">0x00007f2fa25f1000      64K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libverify.so</span><br><span class="line">0x00007f2fa23c2000      228K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libjava.so</span><br><span class="line">0x00007f2fa21af000      60K     /lib64/libnss_files.so.2</span><br><span class="line">0x00007f2fa1fa5000      47K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libzip.so</span><br><span class="line">0x00007f2f80ded000      96K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnio.so</span><br><span class="line">0x00007f2f80bd4000      119K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnet.so</span><br><span class="line">0x00007f2f7e1f6000      50K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libmanagement.so</span><br><span class="line">0x00007f2f75dc8000      209K    /home/admin/drds-server/lib/native/libsigar-amd64-linux.so</span><br><span class="line">0x00007f2f6d8ad000      293K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libsunec.so</span><br><span class="line">0x00007f2f6d697000      86K     /lib64/libgcc_s.so.1</span><br><span class="line">0x00007f2f6bdf9000      30K     /lib64/libnss_dns.so.2</span><br><span class="line">0x00007f2f6bbdf000      107K    /lib64/libresolv.so.2</span><br></pre></td></tr></table></figure>

<h2 id="coredump-生成-java-stack"><a href="#coredump-生成-java-stack" class="headerlink" title="coredump 生成 java stack"></a><a target="_blank" rel="noopener" href="https://www.javacodegeeks.com/2013/02/analysing-a-java-core-dump.html">coredump 生成 java stack</a></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">jstack -J-d64 /opt/taobao/java/bin/java core.24086 </span><br><span class="line"></span><br><span class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 25.232-b604</span><br><span class="line">Deadlock Detection:</span><br><span class="line"></span><br><span class="line">No deadlocks found.</span><br><span class="line"></span><br><span class="line">Thread 27186: (state = BLOCKED)</span><br><span class="line"> - sun.misc.Unsafe.park0(boolean, long) @bci=0 (Compiled frame; information may be imprecise)</span><br><span class="line"> - sun.misc.Unsafe.park(boolean, long) @bci=63, line=1038 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=176 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2047 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=446 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Compiled frame)</span><br><span class="line"> - java.lang.Thread.run() @bci=11, line=858 (Compiled frame)</span><br></pre></td></tr></table></figure>

<h2 id="gdb-coredump-with-java-symbol"><a href="#gdb-coredump-with-java-symbol" class="headerlink" title="gdb coredump with java symbol"></a><a target="_blank" rel="noopener" href="https://mail.openjdk.org/pipermail/hotspot-dev/2016-May/023255.html">gdb coredump with java symbol</a></h2><p>需要安装JVM debug info包，同时要求gdb版本在7.10以上</p>
<p>设置：</p>
<p>home 目录下创建 .gdbinit 然后放入如下内容，libjvm.so-gdb.py 就是 dbg.py 脚本，gdb启动的时候会自动加载这个脚本 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$cat ~/.gdbinit</span><br><span class="line">add-auto-load-safe-path /opt/install/jdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so-gdb.py</span><br></pre></td></tr></table></figure>

<p>使用：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gdb -iex &quot;set auto-load safe-path /&quot; /opt/install/java/bin/java ./core.24086</span><br></pre></td></tr></table></figure>

<h2 id="G1-GC为什么快"><a href="#G1-GC为什么快" class="headerlink" title="G1 GC为什么快"></a>G1 GC为什么快</h2><p><a target="_blank" rel="noopener" href="https://ata.alibaba-inc.com/articles/199497">https://ata.alibaba-inc.com/articles/199497</a></p>
<p>G1比CMS GC效率更高，暂停短、更稳定，但是最终吞吐大概率是CMS要好，这是因为G1编译后代码更大</p>
<p><code>-XX:InlineSmallCode=3000</code>告诉编译器, 汇编3000字节以内的函数需要被inline, 这个值默认是2000</p>
<p>另外CMS用的是Dirty Card，而G1 为了降低GC时间在Remeber Set（类似Dirty Card）的维护上花了更多的代价</p>
<p>Dirty Card维护代价：</p>
<ul>
<li>会影响code size<br>Code size影响了inline机会<br>Code size增大则instruction cache miss几率变大 (几十倍的执行时间差距)</li>
<li>本身执行mark dirty动作耗时, 这是一个写内存+GC&#x2F;mutator线程同步的操作, 可以很复杂, 也可以很简单</li>
</ul>
<p>比如G1为了降低暂停时间，就要尽量控制Remeber Set的更新，所以还需要判断write动作是否真的有必要更新Remeber Set(类似<code>old.ref = null</code>这种写操作是不需要更新Remeber Set的)</p>
<p>简单说CMS的每次 Dirty Card维护只需要3条汇编，而G1的Remember Set维护需要十多条、几十条汇编</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/04/%E8%8E%B7%E5%8F%96%E4%B8%80%E7%9B%B4FullGC%E4%B8%8B%E7%9A%84java%E8%BF%9B%E7%A8%8BHeapDump%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/04/%E8%8E%B7%E5%8F%96%E4%B8%80%E7%9B%B4FullGC%E4%B8%8B%E7%9A%84java%E8%BF%9B%E7%A8%8BHeapDump%E7%9A%84%E5%B0%8F%E6%8A%80%E5%B7%A7/" class="post-title-link" itemprop="url">获取一直FullGC下的java进程HeapDump的小技巧</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-04 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-04T17:30:03+08:00">2020-01-04</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Java/" itemprop="url" rel="index"><span itemprop="name">Java</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="获取一直FullGC下的java进程HeapDump的小技巧"><a href="#获取一直FullGC下的java进程HeapDump的小技巧" class="headerlink" title="获取一直FullGC下的java进程HeapDump的小技巧"></a>获取一直FullGC下的java进程HeapDump的小技巧</h1><p>就是小技巧，操作步骤需要查询，随手记录</p>
<ul>
<li>找到java进程，gdb attach上去， 例如 <code>gdb -p 12345</code></li>
<li>找到这个<code>HeapDumpBeforeFullGC</code>的地址（这个flag如果为true，会在FullGC之前做HeapDump，默认是false）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) p &amp;HeapDumpBeforeFullGC</span><br><span class="line">$2 = (&lt;data variable, no debug info&gt; *) 0x7f7d50fc660f &lt;HeapDumpBeforeFullGC&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>Copy 地址：0x7f7d50fc660f</li>
<li>然后把他设置为true，这样下次FGC之前就会生成一份dump文件</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(gdb) set *0x7f7d50fc660f = 1</span><br><span class="line">(gdb) quit</span><br></pre></td></tr></table></figure>

<ul>
<li>最后，等一会，等下次FullGC触发，你就有HeapDump了！<br>(如果没有指定heapdump的名字，默认是 java_pidxxx.hprof)</li>
</ul>
<p>(PS. <code>jstat -gcutil pid</code> 可以查看gc的概况)</p>
<p>(操作完成后记得gdb上去再设置回去，不然可能一直fullgc，导致把磁盘打满).</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>在jvm还有响应的时候可以： jinfo -flag +HeapDumpBeforeFullGC pid 设置HeapDumpBeforeFullGC 为true（- 为false，+-都不要为只打印值）</p>
<p>kill -3 产生coredump  存放在 kernel.core_pattern&#x3D;&#x2F;root&#x2F;core （&#x2F;etc&#x2F;sysctl.conf , 先 ulimit -c unlimited；或者 gcore id 获取coredump)</p>
<p>得到core文件后，采用 gdb -c 执行文件 core文件 进入调试模式，对于java，有以下2个技巧：</p>
<p>进入gdb调试模式后，输入如下命令： info threads，观察异常的线程，定位到异常的线程后，则可以输入如下命令：thread 线程编号，则会打印出当前java代码的工作流程。</p>
<p> 而对于这个core，亦可以用jstack jmap打印出堆信息，线程信息，具体命令：</p>
<p>  jmap -heap 执行文件 core文件   jstack -F -l 执行文件 core文件</p>
<p><strong>容器中的进程的话需要到宿主机操作，并且将容器中的 jdk文件夹复制到宿主机对应的位置。</strong></p>
<p>  <strong>ps auxff |grep 容器id -A10 找到JVM在宿主机上的进程id</strong></p>
<h2 id="coredump"><a href="#coredump" class="headerlink" title="coredump"></a>coredump</h2><p>kill -3 产生coredump  存放在 kernel.core_pattern&#x3D;&#x2F;root&#x2F;core （&#x2F;etc&#x2F;sysctl.conf , 先 ulimit -c unlimited；）</p>
<p>或者 gcore id 获取coredump</p>
<p><a target="_blank" rel="noopener" href="https://www.baeldung.com/linux/managing-core-dumps">coredump 所在位置</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$cat /proc/sys/kernel/core_pattern</span><br><span class="line">/home/admin/</span><br></pre></td></tr></table></figure>

<h3 id="coredump-分析"><a href="#coredump-分析" class="headerlink" title="coredump 分析"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/46605905">coredump 分析</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">//打开 coredump</span><br><span class="line">$gdb /opt/taobao/java/bin/java core.24086</span><br><span class="line">[New LWP 27184]</span><br><span class="line">[New LWP 27186]</span><br><span class="line">[New LWP 24086]</span><br><span class="line">[Thread debugging using libthread_db enabled]</span><br><span class="line">Using host libthread_db library &quot;/lib64/libthread_db.so.1&quot;.</span><br><span class="line">Core was generated by `/opt/tt/java_coroutine/bin/java&#x27;.</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">Missing separate debuginfos, use: debuginfo-install jdk-8.9.14-20200203164153.alios7.x86_64</span><br><span class="line">(gdb) info threads  //查看所有thread</span><br><span class="line">  Id   Target Id         Frame</span><br><span class="line">  583  Thread 0x7f2fa56177c0 (LWP 24086) 0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">  582  Thread 0x7f2f695f3700 (LWP 27186) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  581  Thread 0x7f2f6cbfb700 (LWP 27184) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  580  Thread 0x7f2f691ef700 (LWP 27176) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  579  Thread 0x7f2f698f6700 (LWP 27174) 0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">(gdb) thread apply all bt  //查看所有线程堆栈</span><br><span class="line">Thread 583 (Thread 0x7f2fa56177c0 (LWP 24086)):</span><br><span class="line">#0  0x00007f2fa4fab017 in pthread_join () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa4b85085 in ContinueInNewThread0 (continuation=continuation@entry=0x7f2fa4b7fd70 &lt;JavaMain&gt;, stack_size=1048576, args=args@entry=0x7ffe529432d0)</span><br><span class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1044</span><br><span class="line">#2  0x00007f2fa4b81877 in ContinueInNewThread (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=0x7f2fa3c163a8, mode=mode@entry=1,</span><br><span class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:2033</span><br><span class="line">#3  0x00007f2fa4b8513b in JVMInit (ifn=ifn@entry=0x7ffe529433d0, threadStackSize=&lt;optimized out&gt;, argc=&lt;optimized out&gt;, argv=&lt;optimized out&gt;, mode=mode@entry=1,</span><br><span class="line">    what=what@entry=0x7ffe5294be17 &quot;com.taobao.tddl.server.TddlLauncher&quot;, ret=ret@entry=0) at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/solaris/bin/java_md_solinux.c:1091</span><br><span class="line">#4  0x00007f2fa4b8254d in JLI_Launch (argc=0, argv=0x7f2fa3c163a8, jargc=&lt;optimized out&gt;, jargv=&lt;optimized out&gt;, appclassc=1, appclassv=0x0, fullversion=0x400885 &quot;1.8.0_232-b604&quot;,</span><br><span class="line">    dotversion=0x400881 &quot;1.8&quot;, pname=0x40087c &quot;java&quot;, lname=0x40087c &quot;java&quot;, javaargs=0 &#x27;\000&#x27;, cpwildcard=1 &#x27;\001&#x27;, javaw=0 &#x27;\000&#x27;, ergo=0)</span><br><span class="line">    at /ssd1/jenkins_home/workspace/ajdk.8.build.master/jdk/src/share/bin/java.c:304</span><br><span class="line">#5  0x0000000000400635 in main ()</span><br><span class="line"></span><br><span class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#3  0x00007f2f9343b44a in ?? ()</span><br><span class="line">#4  0x000000008082e778 in ?? ()</span><br><span class="line">#5  0x0000000000000003 in ?? ()</span><br><span class="line">#6  0x00007f2f88e32758 in ?? ()</span><br><span class="line">#7  0x00007f2f6f532800 in ?? ()</span><br><span class="line"></span><br><span class="line">(gdb) thread apply 582 bt //查看582这个线程堆栈，LWP 27186(0x6a32)对应jstack 线程10进程id</span><br><span class="line"></span><br><span class="line">Thread 582 (Thread 0x7f2f695f3700 (LWP 27186)):</span><br><span class="line">#0  0x00007f2fa4fada35 in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0</span><br><span class="line">#1  0x00007f2fa342d863 in Parker::park(bool, long) () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#2  0x00007f2fa35ba3c3 in Unsafe_Park () from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#3  0x00007f2f9343b44a in ?? ()</span><br><span class="line">#35 0x00000000f26fc738 in ?? ()</span><br><span class="line">#36 0x00007f2fa51cec5b in arena_run_split_remove (arena=0x7f2f6ab09c34, chunk=0x80, run_ind=0, flag_dirty=0, flag_decommitted=&lt;optimized out&gt;, need_pages=0) at src/arena.c:398</span><br><span class="line">#37 0x00007f2f695f2980 in ?? ()</span><br><span class="line">#38 0x0000000000000001 in ?? ()</span><br><span class="line">#39 0x00007f2f88e32758 in ?? ()</span><br><span class="line">#40 0x00007f2f695f2920 in ?? ()</span><br><span class="line">#41 0x00007f2fa32f46b8 in CallInfo::set_common(KlassHandle, KlassHandle, methodHandle, methodHandle, CallInfo::CallKind, int, Thread*) ()</span><br><span class="line">   from /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">#42 0x00007f2f7d800000 in ?? ()</span><br></pre></td></tr></table></figure>



<h3 id="coredump-转-jmap-hprof"><a href="#coredump-转-jmap-hprof" class="headerlink" title="coredump 转 jmap hprof"></a>coredump 转 jmap hprof</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jmap -dump:format=b,file=24086.hprof /opt/taobao/java/bin/java core.24086</span><br></pre></td></tr></table></figure>

<p>以上命令输入是 core.24086 这个 coredump，输出是一个 jmap 的dump 24086.hprof</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$jmap -J-d64 /opt/taobao/java/bin/java core.24086</span><br><span class="line"></span><br><span class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 25.232-b604</span><br><span class="line">0x0000000000400000      8K      /opt/taobao/java/bin/java</span><br><span class="line">0x00007f2fa51be000      6679K   /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/libjemalloc.so.2</span><br><span class="line">0x00007f2fa4fa2000      138K    /lib64/libpthread.so.0</span><br><span class="line">0x00007f2fa4d8c000      88K     /lib64/libz.so.1</span><br><span class="line">0x00007f2fa4b7d000      280K    /opt/taobao/install/ajdk-8_9_14-b604/bin/../lib/amd64/jli/libjli.so</span><br><span class="line">0x00007f2fa4979000      18K     /lib64/libdl.so.2</span><br><span class="line">0x00007f2fa45ab000      2105K   /lib64/libc.so.6</span><br><span class="line">0x00007f2fa43a3000      42K     /lib64/librt.so.1</span><br><span class="line">0x00007f2fa40a1000      1110K   /lib64/libm.so.6</span><br><span class="line">0x00007f2fa5406000      159K    /lib64/ld-linux-x86-64.so.2</span><br><span class="line">0x00007f2fa2af1000      17898K  /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/server/libjvm.so</span><br><span class="line">0x00007f2fa25f1000      64K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libverify.so</span><br><span class="line">0x00007f2fa23c2000      228K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libjava.so</span><br><span class="line">0x00007f2fa21af000      60K     /lib64/libnss_files.so.2</span><br><span class="line">0x00007f2fa1fa5000      47K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libzip.so</span><br><span class="line">0x00007f2f80ded000      96K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnio.so</span><br><span class="line">0x00007f2f80bd4000      119K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libnet.so</span><br><span class="line">0x00007f2f7e1f6000      50K     /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libmanagement.so</span><br><span class="line">0x00007f2f75dc8000      209K    /home/admin/drds-server/lib/native/libsigar-amd64-linux.so</span><br><span class="line">0x00007f2f6d8ad000      293K    /opt/taobao/install/ajdk-8_9_14-b604/jre/lib/amd64/libsunec.so</span><br><span class="line">0x00007f2f6d697000      86K     /lib64/libgcc_s.so.1</span><br><span class="line">0x00007f2f6bdf9000      30K     /lib64/libnss_dns.so.2</span><br><span class="line">0x00007f2f6bbdf000      107K    /lib64/libresolv.so.2</span><br></pre></td></tr></table></figure>

<h3 id="coredump-生成-java-stack"><a href="#coredump-生成-java-stack" class="headerlink" title="coredump 生成 java stack"></a><a target="_blank" rel="noopener" href="https://www.javacodegeeks.com/2013/02/analysing-a-java-core-dump.html">coredump 生成 java stack</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">jstack -J-d64 /opt/taobao/java/bin/java core.24086 </span><br><span class="line"></span><br><span class="line">Attaching to core core.24086 from executable /opt/taobao/java/bin/java, please wait...</span><br><span class="line">Debugger attached successfully.</span><br><span class="line">Server compiler detected.</span><br><span class="line">JVM version is 25.232-b604</span><br><span class="line">Deadlock Detection:</span><br><span class="line"></span><br><span class="line">No deadlocks found.</span><br><span class="line"></span><br><span class="line">Thread 27186: (state = BLOCKED)</span><br><span class="line"> - sun.misc.Unsafe.park0(boolean, long) @bci=0 (Compiled frame; information may be imprecise)</span><br><span class="line"> - sun.misc.Unsafe.park(boolean, long) @bci=63, line=1038 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=176 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await() @bci=42, line=2047 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.LinkedBlockingQueue.take() @bci=29, line=446 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor.getTask() @bci=149, line=1074 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=26, line=1134 (Compiled frame)</span><br><span class="line"> - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Compiled frame)</span><br><span class="line"> - java.lang.Thread.run() @bci=11, line=858 (Compiled frame)</span><br></pre></td></tr></table></figure>

<h3 id="gdb-coredump-with-java-symbol"><a href="#gdb-coredump-with-java-symbol" class="headerlink" title="gdb coredump with java symbol"></a><a target="_blank" rel="noopener" href="https://mail.openjdk.org/pipermail/hotspot-dev/2016-May/023255.html">gdb coredump with java symbol</a></h3>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/02/Linux%20%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/02/Linux%20%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/" class="post-title-link" itemprop="url">Linux 问题总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-02 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-02T17:30:03+08:00">2020-01-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Linux-问题总结"><a href="#Linux-问题总结" class="headerlink" title="Linux 问题总结"></a>Linux 问题总结</h1><h2 id="crond文件权限的坑"><a href="#crond文件权限的坑" class="headerlink" title="crond文件权限的坑"></a>crond文件权限的坑</h2><p>crond第一次加载的时候（刚启动）会去检查文件属性，不是644的话以后都不会执行了，即使后面chmod改成了644. </p>
<p>手工随便修改一下该文件的内容就能触发自动执行了，或者重启crond, 或者 sudo service crond reload， 或者 &#x2F;etc&#x2F;cron.d&#x2F;下有任何修改都会触发crond reload配置(包含 touch )。</p>
<p>总之 crond会每分钟去检查job有没有change，有的话才触发reload，这个change看的时候change time有没有变化，不看权限的变化，仅仅是权限的变化不会触发crond reload。</p>
<p> crond会每分钟去检查一下job有没有修改，有修改的话会reload，但是这个<strong>修改不包含权限的修改</strong>。可以简单地理解这个修改是指文件的change time。</p>
<h2 id="cgroup目录报No-space-left-on-device"><a href="#cgroup目录报No-space-left-on-device" class="headerlink" title="cgroup目录报No space left on device"></a><a target="_blank" rel="noopener" href="https://rotadev.com/cgroup-no-space-left-on-device-server-fault/">cgroup目录报No space left on device</a></h2><p>可能是因为某个规则下的 cpuset.cpus 文件是空导致的</p>
<h2 id="容器中root用户执行-su-admin-切换失败"><a href="#容器中root用户执行-su-admin-切换失败" class="headerlink" title="容器中root用户执行 su - admin 切换失败"></a>容器中root用户执行 su - admin 切换失败</h2><p>问题原因：<a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/30316">https://access.redhat.com/solutions/30316</a></p>
<p><img src="/images/oss/63a4ac6669f820156bff035e7dc49ac2.png" alt="image.png"></p>
<p>如上图去掉 admin nproc限制就可以了</p>
<p>这是因为root用户的nproc是unlimited，但是admin的是65535，所以切不过去</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@i22h08323 /home/admin]</span><br><span class="line">#ulimit -u</span><br><span class="line">unlimited</span><br></pre></td></tr></table></figure>

<h2 id="容器中ulimit限制了sudo的执行"><a href="#容器中ulimit限制了sudo的执行" class="headerlink" title="容器中ulimit限制了sudo的执行"></a>容器中ulimit限制了sudo的执行</h2><p>容器启动的时候默认nofile为65535（可以通过 docker run –ulimit nofile&#x3D;655360 来设置），如果容器中的 &#x2F;etc&#x2F;security&#x2F;limits.conf 中设置的nofile大于 65535就会报错，因为容器的1号进程就是65535了，比如在容器中用root用户执行sudo ls报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#sudo ls</span><br><span class="line">sudo: pam_open_session: Permission denied</span><br><span class="line">sudo: policy plugin failed session initialization</span><br></pre></td></tr></table></figure>

<p>可以修改容器中的 ulimit 不要超过默认的65535或者修改容器的启动参数来解决。</p>
<p>子进程都会继承父进程的一些环境变量，比如 limits.conf, sudo&#x2F;su&#x2F;crond&#x2F;passwd等都会触发重新加载limits, </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -rin pam_limit /etc/pam.d //可以看到触发重新加载的场景</span><br></pre></td></tr></table></figure>

<h2 id="systemd-limits"><a href="#systemd-limits" class="headerlink" title="systemd limits"></a>systemd limits</h2><p>&#x2F;etc&#x2F;security&#x2F;limits.conf 的配置，只适用于通过PAM 认证登录用户的资源限制，它对systemd 的service 的资源限制不生效。</p>
<p>因此登录用户的限制，通过&#x2F;etc&#x2F;security&#x2F;limits.conf 与&#x2F;etc&#x2F;security&#x2F;limits.d 下的文件设置即可。</p>
<p>对于systemd service 的资源设置，则需修改全局配置，全局配置文件放在&#x2F;etc&#x2F;systemd&#x2F;system.conf 和&#x2F;etc&#x2F;systemd&#x2F;user.conf，同时也会加载两个对应目录中的所有.conf 文件&#x2F;etc&#x2F;systemd&#x2F;system.conf.d&#x2F;.conf 和&#x2F;etc&#x2F;systemd&#x2F;user.conf.d&#x2F;.conf。</p>
<h2 id="open-files-限制在1024"><a href="#open-files-限制在1024" class="headerlink" title="open files 限制在1024"></a>open files 限制在1024</h2><p>docker 容器内 nofile只有1024，检查：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/sysconfig/docker</span><br><span class="line">或者</span><br><span class="line">cat /usr/lib/systemd/system/docker.service</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNPROC=1048576</span><br></pre></td></tr></table></figure>

<h3 id="关于ulimit的一些知识点"><a href="#关于ulimit的一些知识点" class="headerlink" title="关于ulimit的一些知识点"></a>关于ulimit的一些知识点</h3><p>参考 <a target="_blank" rel="noopener" href="https://feichashao.com/ulimit_demo/">Ulimit</a> <a target="_blank" rel="noopener" href="http://blog.yufeng.info/archives/2568">http://blog.yufeng.info/archives/2568</a></p>
<ul>
<li>limit的设定值是 per-process 的</li>
<li>在 Linux 中，每个普通进程可以调用 getrlimit() 来查看自己的 limits，也可以调用 setrlimit() 来改变自身的 soft limits</li>
<li>要改变 hard limit, 则需要进程有 CAP_SYS_RESOURCE 权限</li>
<li>进程 fork() 出来的子进程，会继承父进程的 limits 设定</li>
<li><code>ulimit</code> 是 shell 的内置命令。在执行<code>ulimit</code>命令时，其实是 shell 自身调用 getrlimit()&#x2F;setrlimit() 来获取&#x2F;改变自身的 limits. 当我们在 shell 中执行应用程序时，相应的进程就会继承当前 shell 的 limits 设定</li>
<li>shell 的初始 limits 通常是 pam_limits 设定的。顾名思义，pam_limits 是一个 PAM 模块，用户登录后，pam_limits 会给用户的 shell 设定在 limits.conf 定义的值</li>
</ul>
<p>ulimit, limits.conf 和 pam_limits模块 的关系，大致是这样的：</p>
<ol>
<li>用户进行登录，触发 pam_limits;</li>
<li>pam_limits 读取 limits.conf，相应地设定用户所获得的 shell 的 limits；</li>
<li>用户在 shell 中，可以通过 ulimit 命令，查看或者修改当前 shell 的 limits;</li>
<li>当用户在 shell 中执行程序时，该程序进程会继承 shell 的 limits 值。于是，limits 在进程中生效了</li>
</ol>
<p>判断要分配的句柄号是不是超过了 limits.conf 中 nofile 的限制。fd 是当前进程相关的，是一个从 0 开始的整数<br>结论1：soft nofile 和 fs.nr_open的作用一样，它两都是限制的单个进程的最大文件数量。区别是 soft nofile 可以按用户来配置，而 fs.nr_open 所有用户只能配一个。注意 hard nofile 一定要比 fs.nr_open 要小，否则可能导致用户无法登陆。<br>结论2：fs.file-max: 整个系统上可打开的最大文件数，但不限制 root 用户</p>
<h2 id="pam-权限报错"><a href="#pam-权限报错" class="headerlink" title="pam 权限报错"></a>pam 权限报错</h2><p><img src="/images/oss/b646979272e71e015de4a47c62b89747.png" alt="image.png"></p>
<p>从debug信息看如果是pam权限报错的话，需要将 required 改成 sufficient</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$cat /etc/pam.d/crond </span><br><span class="line">#</span><br><span class="line"># The PAM configuration file for the cron daemon</span><br><span class="line">#</span><br><span class="line">#</span><br><span class="line"># No PAM authentication called, auth modules not needed</span><br><span class="line">account    required   pam_access.so</span><br><span class="line">account    include    system-auth</span><br><span class="line">session    required   pam_loginuid.so //required 改成 sufficient</span><br><span class="line">session    include    system-auth</span><br><span class="line">auth       include    system-auth</span><br></pre></td></tr></table></figure>

<p>PAM 提供四个安全领域的特性，但是应用程序不太可能同时需要所有这些方面。例如，<code>passwd</code> 命令只需要下面列表中的第三组：</p>
<ul>
<li><code>account</code> 处理账户限制。对于有效的用户，允许他做什么？</li>
<li><code>auth</code> 处理用户识别 — 例如，通过输入用户名和密码。</li>
<li><code>password</code> 只处理与密码相关的问题，比如设置新密码。</li>
<li><code>session</code> 处理连接管理，包括日志记录。</li>
</ul>
<p>在 &#x2F;etc&#x2F;pam.d 目录中为将使用 PAM 的每个应用程序创建一个配置文件，文件名与应用程序名相同。例如，<code>login</code> 命令的配置文件是 &#x2F;etc&#x2F;pam.d&#x2F;login。</p>
<p>必须定义将应用哪些模块，创建一个动作 “堆”。PAM 运行堆中的所有模块，根据它们的结果允许或拒绝用户的请求。还必须定义检查是否是必需的。最后，<em>other</em> 文件为没有特殊规则的所有应用程序提供默认规则。</p>
<ul>
<li><code>optional</code> 模块可以成功，也可以失败；PAM 根据模块是否最终成功返回 <code>success</code> 或 <code>failure</code>。</li>
<li><code>required</code> 模块必须成功。如果失败，PAM 返回 <code>failure</code>，但是会在运行堆中的其他模块之后返回。</li>
<li><code>requisite</code> 模块也必须成功。但是，如果失败，PAM 立即返回 <code>failure</code>，不再运行其他模块。</li>
<li><code>sufficient</code> 模块在成功时导致 PAM 立即返回 <code>success</code>，不再运行其他模块。</li>
</ul>
<p>当pam安装之后有两大部分：在&#x2F;lib64&#x2F;security目录下的各种pam模块以及&#x2F;etc&#x2F;pam.d和&#x2F;etc&#x2F;pam.d目录下的针对各种服务和应用已经定义好的pam配置文件。当某一个有认证需求的应用程序需要验证的时候，一般在应用程序中就会定义负责对其认证的PAM配置文件。以vsftpd为例，在它的配置文件&#x2F;etc&#x2F;vsftpd&#x2F;vsftpd.conf中就有这样一行定义：</p>
<blockquote>
<p>pam_service_name&#x3D;vsftpd</p>
</blockquote>
<p>表示登录FTP服务器的时候进行认证是根据&#x2F;etc&#x2F;pam.d&#x2F;vsftpd文件定义的内容进行。</p>
<h3 id="PAM-认证过程"><a href="#PAM-认证过程" class="headerlink" title="PAM 认证过程"></a>PAM 认证过程</h3><p>当程序需要认证的时候已经找到相关的pam配置文件，认证过程是如何进行的？下面我们将通过解读&#x2F;etc&#x2F;pam.d&#x2F;system-auth文件予以说明。</p>
<p>首先要声明一点的是：system-auth是一个非常重要的pam配置文件，主要负责用户登录系统的认证工作。而且该文件不仅仅只是负责用户登录系统认证，其它的程序和服务通过include接口也可以调用到它，从而节省了很多重新自定义配置的工作。所以应该说该文件是系统安全的总开关和核心的pam配置文件。</p>
<p>下面是&#x2F;etc&#x2F;pam.d&#x2F;system-auth文件的全部内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$cat /etc/pam.d/system-auth</span><br><span class="line">#%PAM-1.0</span><br><span class="line"># This file is auto-generated.</span><br><span class="line"># User changes will be destroyed the next time authconfig is run.</span><br><span class="line">auth        required      pam_env.so</span><br><span class="line">auth        required      pam_faildelay.so delay=2000000</span><br><span class="line">auth        sufficient    pam_unix.so nullok try_first_pass</span><br><span class="line">auth        requisite     pam_succeed_if.so uid &gt;= 1000 quiet_success</span><br><span class="line">auth        required      pam_deny.so</span><br><span class="line"></span><br><span class="line">account     required      pam_unix.so</span><br><span class="line">account     sufficient    pam_localuser.so</span><br><span class="line">account     sufficient    pam_succeed_if.so uid &lt; 1000 quiet</span><br><span class="line">account     required      pam_permit.so</span><br><span class="line"></span><br><span class="line">password    requisite     pam_pwquality.so try_first_pass local_users_only retry=3 authtok_type=</span><br><span class="line">password    sufficient    pam_unix.so sha512 shadow nullok try_first_pass use_authtok</span><br><span class="line">password    required      pam_deny.so</span><br><span class="line"></span><br><span class="line">session     optional      pam_keyinit.so revoke</span><br><span class="line">session     required      pam_limits.so</span><br><span class="line">-session     optional      pam_systemd.so</span><br><span class="line">session     [success=1 default=ignore] pam_succeed_if.so service in crond quiet use_uid</span><br><span class="line">session     required      pam_unix.so</span><br></pre></td></tr></table></figure>

<h4 id="第一部分"><a href="#第一部分" class="headerlink" title="第一部分"></a>第一部分</h4><p>当用户登录的时候，首先会通过auth类接口对用户身份进行识别和密码认证。所以在该过程中验证会经过几个带auth的配置项。</p>
<p>其中的第一步是通过pam_env.so模块来定义用户登录之后的环境变量， pam_env.so允许设置和更改用户登录时候的环境变量，默认情况下，若没有特别指定配置文件，将依据&#x2F;etc&#x2F;security&#x2F;pam_env.conf进行用户登录之后环境变量的设置。</p>
<p>然后通过pam_unix.so模块来提示用户输入密码，并将用户密码与&#x2F;etc&#x2F;shadow中记录的密码信息进行对比，如果密码比对结果正确则允许用户登录，而且<strong>该配置项的使用的是“sufficient”控制位，即表示只要该配置项的验证通过，用户即可完全通过认证而不用再去走下面的认证项</strong>。不过在特殊情况下，用户允许使用空密码登录系统，例如当将某个用户在&#x2F;etc&#x2F;shadow中的密码字段删除之后，该用户可以只输入用户名直接登录系统。</p>
<p>下面的配置项中，通过pam_succeed_if.so对用户的登录条件做一些限制，表示允许uid大于500的用户在通过密码验证的情况下登录，在Linux系统中，一般系统用户的uid都在500之内，所以该项即表示允许使用useradd命令以及默认选项建立的普通用户直接由本地控制台登录系统。</p>
<p>最后通过pam_deny.so模块对所有不满足上述任意条件的登录请求直接拒绝，pam_deny.so是一个特殊的模块，该模块返回值永远为否，类似于大多数安全机制的配置准则，在所有认证规则走完之后，对不匹配任何规则的请求直接拒绝。</p>
<h4 id="第二部分"><a href="#第二部分" class="headerlink" title="第二部分"></a>第二部分</h4><p>三个配置项主要表示通过account账户类接口来识别账户的合法性以及登录权限。</p>
<p>第一行仍然使用pam_unix.so模块来声明用户需要通过密码认证。第二行承认了系统中uid小于500的系统用户的合法性。之后对所有类型的用户登录请求都开放控制台。</p>
<h4 id="第三部分"><a href="#第三部分" class="headerlink" title="第三部分"></a>第三部分</h4><p>会通过password口令类接口来确认用户使用的密码或者口令的合法性。第一行配置项表示需要的情况下将调用pam_cracklib来验证用户密码复杂度。如果用户输入密码不满足复杂度要求或者密码错，最多将在三次这种错误之后直接返回密码错误的提示，否则期间任何一次正确的密码验证都允许登录。需要指出的是，pam_cracklib.so是一个常用的控制密码复杂度的pam模块，关于其用法举例我们会在之后详细介绍。之后带pam_unix.so和pam_deny.so的两行配置项的意思与之前类似。都表示需要通过密码认证并对不符合上述任何配置项要求的登录请求直接予以拒绝。不过用户如果执行的操作是单纯的登录，则这部分配置是不起作用的。</p>
<h4 id="第四部分"><a href="#第四部分" class="headerlink" title="第四部分"></a>第四部分</h4><p>主要将通过session会话类接口为用户初始化会话连接。其中几个比较重要的地方包括，使用pam_keyinit.so表示当用户登录的时候为其建立相应的密钥环，并在用户登出的时候予以撤销。不过该行配置的控制位使用的是optional，表示这并非必要条件。之后通过pam_limits.so限制用户登录时的会话连接资源，相关pam_limit.so配置文件是&#x2F;etc&#x2F;security&#x2F;limits.conf，默认情况下对每个登录用户都没有限制。关于该模块的配置方法在后面也会详细介绍。</p>
<h3 id="常用的PAM模块介绍"><a href="#常用的PAM模块介绍" class="headerlink" title="常用的PAM模块介绍"></a>常用的PAM模块介绍</h3><table>
<thead>
<tr>
<th>PAM模块</th>
<th>结合管理类型</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>pam_unix.so</td>
<td>auth</td>
<td>提示用户输入密码,并与&#x2F;etc&#x2F;shadow文件相比对.匹配返回0</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>account</td>
<td>检查用户的账号信息(包括是否过期等).帐号可用时,返回0.</td>
</tr>
<tr>
<td>pam_unix.so</td>
<td>password</td>
<td>修改用户的密码. 将用户输入的密码,作为用户的新密码更新shadow文件</td>
</tr>
<tr>
<td>pam_shells.so</td>
<td>auth、account</td>
<td>如果用户想登录系统，那么它的shell必须是在&#x2F;etc&#x2F;shells文件中之一的shell</td>
</tr>
<tr>
<td>pam_deny.so</td>
<td>account、auth、password、session</td>
<td>该模块可用于拒绝访问</td>
</tr>
<tr>
<td>pam_permit.so</td>
<td>account、auth、password、session</td>
<td>模块任何时候都返回成功.</td>
</tr>
<tr>
<td>pam_securetty.so</td>
<td>auth</td>
<td>如果用户要以root登录时,则登录的tty必须在&#x2F;etc&#x2F;securetty之中.</td>
</tr>
<tr>
<td>pam_listfile.so</td>
<td>account、auth、password、session</td>
<td>访问应用程的控制开关</td>
</tr>
<tr>
<td>pam_cracklib.so</td>
<td>password</td>
<td>这个模块可以插入到一个程序的密码栈中,用于检查密码的强度.</td>
</tr>
<tr>
<td>pam_limits.so</td>
<td>session</td>
<td>定义使用系统资源的上限，root用户也会受此限制，可以通过&#x2F;etc&#x2F;security&#x2F;limits.conf或&#x2F;etc&#x2F;security&#x2F;limits.d&#x2F;*.conf来设定</td>
</tr>
</tbody></table>
<h2 id="debug-crond"><a href="#debug-crond" class="headerlink" title="debug crond"></a>debug crond</h2><p>先停掉 crond service，然后开启debug参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop crond</span><br><span class="line">crond -x proc //不想真正执行的话：test</span><br></pre></td></tr></table></figure>

<p>或者增加更多的debug信息， debug sudo&#x2F;sudoers , 在 &#x2F;etc&#x2F;sudo.conf 中增加了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Debug sudo /var/log/sudo_debug all@warn</span><br><span class="line">Debug sudoers.so /var/log/sudoers_debug all@debug</span><br></pre></td></tr></table></figure>

<h2 id="crond-ERROR-getpwnam-failed"><a href="#crond-ERROR-getpwnam-failed" class="headerlink" title="crond ERROR (getpwnam() failed)"></a>crond ERROR (getpwnam() failed)</h2><p><a target="_blank" rel="noopener" href="https://www.ibm.com/support/pages/cron-job-fails-error-message-getpwnam-failed-no-such-file-or-directory">报错信息</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">crond[246590]: (/usr/bin/ssh) ERROR (getpwnam() failed)</span><br></pre></td></tr></table></figure>

<p>要特别注意crond格式是 时间  <strong>用户</strong>  命令</p>
<p>有时候我们可以省略用户，但是在 <strong>&#x2F;etc&#x2F;cron.d&#x2F;</strong> 中省略用户后报错如上</p>
<h2 id="进程和线程"><a href="#进程和线程" class="headerlink" title="进程和线程"></a>进程和线程</h2><p>把进程看做是资源分配的单位，把线程才看成一个具体的执行实体。</p>
<h2 id="deleted-文件"><a href="#deleted-文件" class="headerlink" title="deleted 文件"></a>deleted 文件</h2><p><code>lsof +L1</code> 或者<code> lsof | grep delete</code> 发现有被删除的文件，且占用大量磁盘空间</p>
<p>更多 lsof 用法：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzAwNTM5Njk3Mw==&mid=2247518966&idx=1&sn=6ebf794b9743abb04c9ed20d30c90746">https://mp.weixin.qq.com/s?__biz=MzAwNTM5Njk3Mw==&amp;mid=2247518966&amp;idx=1&amp;sn=6ebf794b9743abb04c9ed20d30c90746</a></p>
<p>lsof &#x2F;path&#x2F;file 列出打开文件的进程，也可以是路径，还可以通过参数 “+D” 来递归路径 </p>
<p>清理：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">先通过 lsof +L1 找到 deleted 文件以及 pid</span><br><span class="line">cd /proc/&#123;上一步的 pid&#125;/fd</span><br><span class="line">ll # 在列出的文件名中找到 lsof +L1 看到的文件名，记录对应的 fd 值</span><br><span class="line">cat /dev/null &gt; &#123;上一步找到的 fd &#125;</span><br></pre></td></tr></table></figure>



<h2 id="No-route-to-host"><a href="#No-route-to-host" class="headerlink" title="No route to host"></a>No route to host</h2><p>如果ping ip能通,但是curl&#x2F;telnet 访问 ip+port 报not route to host 错误,这肯定不是route问题(因为ping能通), 一般都是目标机器防火墙的问题</p>
<p>可以停掉防火墙验证,或者添加端口到防火墙:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#firewall-cmd --permanent --add-port=8090/tcp</span><br><span class="line">success</span><br><span class="line">#firewall-cmd --reload</span><br></pre></td></tr></table></figure>

<h2 id="强制重启系统"><a href="#强制重启系统" class="headerlink" title="强制重启系统"></a>强制重启系统</h2><p><img src="/images/oss/ee2e438907fa72c70d5393a651dc9113.png" alt="image.png"></p>
<h2 id="hostname"><a href="#hostname" class="headerlink" title="hostname"></a>hostname</h2><p>hostname -i 是根据机器的hostname去解析ip，如果 &#x2F;etc&#x2F;hosts里面没有指定hostname对应的ip就会走dns 流程然后libnss_myhostname 返回所有ip</p>
<p>getHostName获取的机器名如果对应的ip不是127.0.0.1，那么就用这个ip，否则就需要通过getHostByName获取所有网卡选择一个</p>
<h2 id="tsar-Floating-point-execption"><a href="#tsar-Floating-point-execption" class="headerlink" title="tsar Floating point execption"></a>tsar Floating point execption</h2><p><img src="/images/oss/72197d600425656ec9a8ed18bcc5853b.png" alt="image.png"></p>
<p>因为 &#x2F;etc&#x2F;localtime 是deleted状态</p>
<h2 id="奇怪的文件大小-sparse-file"><a href="#奇怪的文件大小-sparse-file" class="headerlink" title="奇怪的文件大小 sparse file"></a>奇怪的文件大小 <a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/259932/strange-discrepancy-of-file-sizes-from-ls">sparse file</a></h2><p><img src="/images/oss/720f618d-2911-4bfd-a63e-33399532b6e5.png" alt="img"></p>
<p>如上图 gc.log 实际为5.6M，但是通过 ls -lh 就变成74G了，但实际上总文件夹才63M。因为写文件的时候lseek了74G的地方写入5.6M的内容就看到是这个样子了，而前面lseek的74G是不需要从磁盘上分配出来的.</p>
<p><a target="_blank" rel="noopener" href="https://www.lisenet.com/2014/so-what-is-the-size-of-that-file/">而 ls -s 中的 -s就是只看实际大小</a></p>
<p><img src="/images/oss/19b5f6cc-6fc4-4ad6-854c-6164705d343a.png" alt="img"></p>
<p><a target="_blank" rel="noopener" href="https://www.systutorials.com/handling-sparse-files-on-linux/">图片来源</a></p>
<p><a target="_blank" rel="noopener" href="https://www.mankier.com/1/fallocate">回收文件中的空洞</a>：sudo fallocate -c –length 70G gc.log</p>
<p>如果文件一直打开写入中是没法回收的，因为一回收又被重新lseek到之前的末尾重新写入了！</p>
<h2 id="增加dmesg-buffer"><a href="#增加dmesg-buffer" class="headerlink" title="增加dmesg buffer"></a>增加dmesg buffer</h2><p>If dmesg does not show any information about NUMA, then increase the Ring Buffer size:<br>Boot with ‘log_buf_len&#x3D;16M’ (or some other big value). Refer the following kbase article <a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/47276">How do I increase the kernel log ring buffer size?</a> for steps on how to increase the ring buffer</p>
<h2 id="yum-源问题处理"><a href="#yum-源问题处理" class="headerlink" title="yum 源问题处理"></a>yum 源问题处理</h2><p><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/641093">Yum commands error “pycurl.so: undefined symbol”</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># yum check update</span><br><span class="line">There was a problem importing one of the Python modules</span><br><span class="line">required to run yum. The error leading to this problem was:</span><br><span class="line"></span><br><span class="line">/usr/lib64/python2.6/site-packages/pycurl.so: undefined symbol: CRYPTO_set_locking_callback</span><br><span class="line"></span><br><span class="line">Please install a package which provides this module, or</span><br><span class="line">verify that the module is installed correctly.</span><br><span class="line"></span><br><span class="line">It&#x27;s possible that the above module doesn&#x27;t match the</span><br><span class="line">current version of Python, which is:</span><br><span class="line">2.6.6 (r266:84292, Sep  4 2013, 07:46:00)</span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-3)]</span><br><span class="line"></span><br><span class="line">If you cannot solve this problem yourself, please go to</span><br><span class="line">the yum faq at:</span><br><span class="line">http://yum.baseurl.org/wiki/Faq</span><br></pre></td></tr></table></figure>

<ul>
<li>Check and fix the related library paths or remove 3rd party libraries, usually <code>libcurl</code> or <code>libssh2</code>. On a x86_64 system, the standard paths for those libraries are <code>/usr/lib64/libcurl.so.4</code> and <code>/usr/lib64/libssh2.so.1</code></li>
</ul>
<h2 id="软中断、系统调用和上下文切换"><a href="#软中断、系统调用和上下文切换" class="headerlink" title="软中断、系统调用和上下文切换"></a>软中断、系统调用和上下文切换</h2><p>“你可以把内核看做是不断对请求进行响应的服务器，这些请求可能来自在CPU上执行的进程，也可能来自发出中断的外部设备。老板的请求相当于中断，而顾客的请求相当于用户态进程发出的系统调用”。</p>
<p>软中断和系统调用一样，都是CPU停止掉当前用户态上下文，保存工作现场，然后陷入到内核态继续工作。二者的唯一区别是系统调用是切换到同进程的内核态上下文，而软中断是则是切换到了另外一个内核进程ksoftirqd上。</p>
<blockquote>
<p>系统调用开销是200ns起步</p>
<p>从实验数据来看，一次软中断CPU开销大约3.4us左右</p>
<p>实验结果显示进程上下文切换平均耗时 3.5us，lmbench工具显示的进程上下文切换耗时从2.7us到5.48之间</p>
<p>大约每次线程切换开销大约是3.8us左右。<strong>从上下文切换的耗时上来看，Linux线程（轻量级进程）其实和进程差别不太大</strong>。</p>
</blockquote>
<p>软中断和进程上下文切换比较起来，进程上下文切换是从用户进程A切换到了用户进程B。而软中断切换是从用户进程A切换到了内核线程ksoftirqd上。而ksoftirqd作为一个内核控制路径，其处理程序比一个用户进程要轻量，所以上下文切换开销相对比进程切换要少一些（实际数据基本差不多）。</p>
<p>系统调用只是在进程内将用户态切换到内核态，然后再切回来，而上下文切换可是直接从进程A切换到了进程B。显然这个上下文切换需要完成的工作量更大。</p>
<h3 id="软中断开销计算"><a href="#软中断开销计算" class="headerlink" title="软中断开销计算"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247483827&idx=3&sn=8b897c8d6d3038ea79bd156a0e88db10&scene=21#wechat_redirect">软中断开销计算</a></h3><ul>
<li><strong>查看软中断总耗时</strong>， 首先用top命令可以看出每个核上软中断的开销占比，是在si列（1.2%–1秒[1000ms]中的1.2%）</li>
<li><strong>查看软中断次数</strong>，再用vmstat命令可以看到软中断的次数（in列 56000）</li>
<li><strong>计算每次软中断的耗时</strong>，该机器是16核的物理实机，故可以得出每个软中断需要的CPU时间是&#x3D;12ms&#x2F;(56000&#x2F;16)次&#x3D;3.428us。从实验数据来看，一次软中断CPU开销大约3.4us左右</li>
</ul>
<h2 id="Linux-启动进入紧急模式"><a href="#Linux-启动进入紧急模式" class="headerlink" title="Linux 启动进入紧急模式"></a>Linux 启动进入紧急模式</h2><p>可能是因为磁盘挂载不上，检查 &#x2F;etc&#x2F;fstab 中需要挂载的磁盘，尝试 mount -a 是否能全部挂载，麒麟下容易出现弄丢磁盘的标签和uuid</p>
<p>否则的话debug为啥，比如检查设备标签（e2label）是否冲突之类的</p>
<h2 id="进程状态"><a href="#进程状态" class="headerlink" title="进程状态"></a>进程状态</h2><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/401910162">https://zhuanlan.zhihu.com/p/401910162</a></p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">PROCESS STATE CODES</span><br><span class="line">  Here are the different values that the s, stat and state output specifiers(header &quot;STAT&quot; or &quot;S&quot;) will display to describe the state of a process:</span><br><span class="line"> </span><br><span class="line">    D    uninterruptible sleep (usually IO)  #不可中断睡眠 不接受任何信号，因此kill对它无效，一般是磁盘io,网络io读写时出现</span><br><span class="line">    R    running or runnable (on run queue)  #可运行状态或者运行中，可运行状态表明进程所需要的资源准备就绪，待内核调度</span><br><span class="line">    S    interruptible sleep (waiting for an event to complete) #可中断睡眠，等待某事件到来而进入睡眠状态</span><br><span class="line">    T    stopped by job control signal #进程暂停状态 平常按下的ctrl+z,实际上是给进程发了SIGTSTP 信号 （kill -l可查看系统所有的信号量）</span><br><span class="line">    t    stopped by debugger during the tracing #进程被ltrace、strace attach后就是这种状态</span><br><span class="line">    W    paging (not valid since the 2.6.xx kernel) #没有用了</span><br><span class="line">    X    dead (should never be seen) #进程退出时的状态</span><br><span class="line">    Z    defunct (&quot;zombie&quot;) process, terminated but not reaped by its parent #进程退出后父进程没有正常回收，俗称僵尸进程</span><br></pre></td></tr></table></figure>

<h3 id="D状态的进程"><a href="#D状态的进程" class="headerlink" title="D状态的进程"></a><a target="_blank" rel="noopener" href="https://gohalo.me/post/linux-kernel-hang-task-panic-introduce.html">D状态的进程</a></h3><blockquote>
<p> Process D是指进程处于不可中断状态。即uninterruptible sleep，通常我们比较常遇到的就是进程自旋等到进入竞争区等，刷脏页，进程同步等</p>
<p>D： Disk sleep（task_uninterruptible)–比如，磁盘满，导致进程D，无法kill</p>
</blockquote>
<p>相关设置</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt;  /proc/sys/kernel/hung_task_panic  </span><br><span class="line"></span><br><span class="line">----- 处于D状态的超时时间，默认是120s</span><br><span class="line">$ cat /proc/sys/kernel/hung_task_timeout_secs</span><br><span class="line"></span><br><span class="line">----- 发现hung task之后是否触发panic操作</span><br><span class="line">$ cat /proc/sys/kernel/hung_task_panic</span><br><span class="line"></span><br><span class="line">----- 每次检查的进程数</span><br><span class="line">$ cat /proc/sys/kernel/hung_task_check_count</span><br><span class="line"></span><br><span class="line">----- 为了防止日志被刷爆，设置最多的打印次数</span><br><span class="line">$ cat /proc/sys/kernel/hung_task_warnings</span><br></pre></td></tr></table></figure>

<p>这个参数可以用来处理 D 状态进程 </p>
<p>内核在 3.10.0 版本之后提供了 hung task 机制，用来检测系统中长期处于 D 状体的进程，如果存在，则打印相关警告和进程堆栈。</p>
<p>如果配置了 <code>hung_task_panic</code> ，则会直接发起 panic 操作，然后结合 kdump 可以搜集到相关的 vmcore 文件，用于定位分析。</p>
<p>其基本原理也很简单，系统启动时会创建一个内核线程 <code>khungtaskd</code>，定期遍历系统中的所有进程，检查是否存在处于 D 状态且超过 120s 的进程，如果存在，则打印相关警告和进程堆栈，并根据参数配置决定是否发起 panic 操作。</p>
<p>查看D进程出现的原因：</p>
<p><img src="/images/951413iMgBlog/image-20230814112500971.png" alt="image-20230814112500971"></p>
<p>这个堆栈能看到进程在哪里 D 住了，但不一定是根本原因，有可能是被动进入 D 状态的</p>
<p><img src="/images/951413iMgBlog/image-20230814112742429.png" alt="image-20230814112742429"></p>
<h3 id="T-状态进程"><a href="#T-状态进程" class="headerlink" title="T 状态进程"></a>T 状态进程</h3><p>kill -CONT pid 来恢复</p>
<p>jmap -heap&#x2F;histo和大家使用-F参数是一样的，底层都是通过serviceability agent来实现的，并不是jvm attach的方式，通过sa连上去之后会挂起进程，在serviceability agent里存在bug可能<strong>导致detach的动作不会被执行</strong>，从而会让进程一直挂着，可以通过top命令验证进程是否处于T状态，如果是说明进程被挂起了，如果进程被挂起了，可以通过kill -CONT [pid]来恢复。</p>
<h2 id="路由"><a href="#路由" class="headerlink" title="路由"></a>路由</h2><p>『你所规划的路由必须要是你的网卡 (如 eth0) 或 IP 可以直接沟通 (broadcast) 的情况』才行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</span><br><span class="line">SIOCDELRT: No such process // 从bond0没法广播到 11.164.191.247</span><br><span class="line"></span><br><span class="line">$sudo route add -net 11.164.191.0  gw 100.81.183.247 netmask 255.255.255.0 bond0.700</span><br><span class="line">SIOCADDRT: Network is unreachable //从bond0.700 没法广播到 100.81.183.247，实际目前从bond0.700没法广播到任何地方</span><br><span class="line"></span><br><span class="line">$sudo route add** **11.164.191.247** **dev** **bond0.700</span><br><span class="line"></span><br><span class="line">$sudo route add -net 11.164.191.0  **gw 100.81.183.247** netmask 255.255.255.0 bond0.700</span><br><span class="line">SIOCADDRT: Network is unreachable  //从bond0.700 没法广播到 100.81.183.247</span><br><span class="line"></span><br><span class="line">$sudo route add -net 11.164.191.0  gw 11.164.191.247 netmask 255.255.255.0 bond0</span><br><span class="line">SIOCADDRT: Network is unreachable//从bond0没法广播到 11.164.191.247但是从bond0.700可以</span><br><span class="line"></span><br><span class="line">$sudo route add -net 11.164.191.0  **gw 11.164.191.247** netmask 255.255.255.0 bond0.700</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable">https://serverfault.com/questions/581159/unable-to-add-a-static-route-sioaddrt-network-is-unreachable</a></p>
<h2 id="linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”"><a href="#linux-2-6-32内核高精度定时器带来的cpu-sy暴涨的“问题”" class="headerlink" title="linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”"></a>linux 2.6.32内核高精度定时器带来的cpu sy暴涨的“问题”</h2><p>在 2.6.32 以前的内核里，即使你在java里写queue.await(1ns)之类的代码，其实都是需要1ms左右才会执行的，但.32以后则可以支持ns级的调度，对于实时性要求非常非常高的性能而言，这本来是个好特性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/timer_list | grep .resolution</span><br></pre></td></tr></table></figure>

<p>可以通过在 &#x2F;boot&#x2F;grub2&#x2F;grub.cfg 中相应的kernel行的最后增加highres&#x3D;off nohz&#x3D;off来关闭高精度（不建议这样做，最好还是程序本身做相应的修改）</p>
<h2 id="后台执行"><a href="#后台执行" class="headerlink" title="后台执行"></a>后台执行</h2><p>将任务放到后台，断开ssh后还能运行：</p>
<ol>
<li>“ctrl-Z”将当前任务挂起；</li>
<li>“disown -h”让该任务忽略 SIGHUP 信号（不会因为掉线而终止执行）；</li>
<li>“bg”让该任务在后台恢复运行。</li>
</ol>
<h2 id="Linux-进程调度"><a href="#Linux-进程调度" class="headerlink" title="Linux 进程调度"></a>Linux 进程调度</h2><p>Linux的进程调度有一个不太为人熟知的特性，叫做<strong>wakeup affinity</strong>，它的初衷是这样的：如果两个进程频繁互动，那么它们很有可能共享同样的数据，把它们放到亲缘性更近的scheduling domain有助于提高缓存和内存的访问性能，所以当一个进程唤醒另一个的时候，被唤醒的进程可能会被放到相同的CPU core或者相同的NUMA节点上。</p>
<p>这个特性缺省是打开的，它有时候很有用，但有时候却对性能有伤害作用。设想这样一个应用场景：一个主进程给成百上千个辅进程派发任务，这成百上千个辅进程被唤醒后被安排到与主进程相同的CPU core或者NUMA节点上，就会导致负载严重失衡，CPU忙的忙死、闲的闲死，造成性能下降。<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA">https://mp.weixin.qq.com/s/DG1v8cUjcXpa0x2uvrRytA</a></p>
<h2 id="tty"><a href="#tty" class="headerlink" title="tty"></a><a target="_blank" rel="noopener" href="https://www.cnblogs.com/liqiuhao/p/9031803.html">tty</a></h2><p>tty（teletype–最早的一种终端设备，远程打字机） stty 设置tty的相关参数</p>
<p>tty都在 &#x2F;dev 下，通过 ps -ax 可以看到进程的tty；通过tty 可以看到本次的终端</p>
<p>&#x2F;dev&#x2F;pty（Pseudo Terminal） 伪终端</p>
<p>&#x2F;dev&#x2F;tty 控制终端</p>
<p>远古时代tty是物理形态的存在</p>
<p><img src="/images/951413iMgBlog/v2-7aa6997d017d876543671e4113048a62_1440w.jpg" alt="img"></p>
<p>PC时代，物理上的terminal已经没有了（用虚拟的伪终端代替，pseudo tty, 简称pty），相对kernel增加了shell，这是terminal和shell容易混淆，他们的含义</p>
<p><img src="/images/951413iMgBlog/v2-63cdd117f1026c2bbf455920b29c4454_1440w.jpg" alt="img"></p>
<p>实际像如下图的工作协作:</p>
<p><img src="/images/951413iMgBlog/case3.png" alt="Diagram"></p>
<h2 id="rsync"><a href="#rsync" class="headerlink" title="rsync"></a><a target="_blank" rel="noopener" href="https://wangdoc.com/ssh/rsync.html">rsync</a></h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">将本地yum备份到150上的/data/yum/ 下</span><br><span class="line">rsync -arv ./yum/ root@11.167.60.150:/data/yum/</span><br><span class="line"></span><br><span class="line">走ssh的8022端口把目录备份到本地</span><br><span class="line">rsync -e &#x27;ssh -p 8022&#x27; -arv gcsql@10.2.3.4:/home/gcsql/doc/ ./</span><br><span class="line"></span><br><span class="line">rsync -arv -e &quot;ssh -i /home/admin/.ssh/id_dsa.per  &quot; root@1.1.20.24:/home/xijun.rxj/ /home/admin/bak/</span><br></pre></td></tr></table></figure>

<p><code>-a</code>、<code>--archive</code>参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。</p>
<p><code>--delete</code>参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。</p>
<p><code>-i</code>参数表示输出源目录与目标目录之间文件差异的详细情况。</p>
<p><code>--link-dest</code>参数指定增量备份的基准目录。</p>
<p><code>-n</code>参数或<code>--dry-run</code>参数模拟将要执行的操作，而并不真的执行。配合<code>-v</code>参数使用，可以看到哪些内容会被同步过去。</p>
<p><code>--partial</code>参数允许恢复中断的传输。不使用该参数时，<code>rsync</code>会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与<code>--append</code>或<code>--append-verify</code>配合使用。</p>
<p><code>--progress</code>参数表示显示进展。</p>
<p><code>-r</code>参数表示递归，即包含子目录。</p>
<p><code>-v</code>参数表示输出细节。<code>-vv</code>表示输出更详细的信息，<code>-vvv</code>表示输出最详细的信息。</p>
<h2 id="Shebang"><a href="#Shebang" class="headerlink" title="Shebang"></a>Shebang</h2><p>Shebang 的东西 <code>#!/bin/bash</code></p>
<p>对 Shebang 的处理是内核在进行。当内核加载一个文件时，会首先读取文件的前 128 个字节，根据这 128 个字节判断文件的类型，然后调用相应的加载器来加载。</p>
<h3 id="ELF（Executable-and-Linkable-Format）"><a href="#ELF（Executable-and-Linkable-Format）" class="headerlink" title="ELF（Executable and Linkable Format）"></a>ELF（Executable and Linkable Format）</h3><p>对应windows下的exe</p>
<h2 id="修改启动参数"><a href="#修改启动参数" class="headerlink" title="修改启动参数"></a>修改启动参数</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$cat change_kernel_parameter.sh </span><br><span class="line">#cat /sys/devices/system/cpu/vulnerabilities/*</span><br><span class="line">#grep &#x27;&#x27; /sys/devices/system/cpu/vulnerabilities/*</span><br><span class="line">#https://help.aliyun.com/document_detail/102087.html?spm=a2c4g.11186623.6.721.4a732223pEfyNC</span><br><span class="line"></span><br><span class="line">#cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">#transparent_hugepage=always</span><br><span class="line">#noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off</span><br><span class="line">#追加nopti nospectre_v2到内核启动参数中</span><br><span class="line">sudo sed -i &#x27;s/\(GRUB_CMDLINE_LINUX=&quot;.*\)&quot;/\1 nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off transparent_hugepage=always&quot;/&#x27; /etc/default/grub</span><br><span class="line"></span><br><span class="line">//从修改的 /etc/default/grub 生成 /boot/grub2/grub.cfg 配置</span><br><span class="line">sudo grub2-mkconfig -o /boot/grub2/grub.cfg</span><br><span class="line"></span><br><span class="line">#limit the journald log to 500M</span><br><span class="line">sed -i &#x27;s/^#SystemMaxUse=$/SystemMaxUse=500M/g&#x27; /etc/systemd/journald.conf</span><br><span class="line">#重启系统</span><br><span class="line">#sudo reboot</span><br><span class="line"></span><br><span class="line">## 选择不同的kernel启动</span><br><span class="line">#sudo grep &quot;menuentry &quot; /boot/grub2/grub.cfg | grep -n menu</span><br><span class="line">##grub认的index从0开始数的</span><br><span class="line">#sudo grub2-reboot 0; sudo reboot</span><br><span class="line"></span><br><span class="line">$cat /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">always [madvise] never</span><br></pre></td></tr></table></figure>

<h2 id="制作启动盘"><a href="#制作启动盘" class="headerlink" title="制作启动盘"></a>制作启动盘</h2><p>Windows 上用 UltraISO 烧制，Mac 上就比较简单了，直接用 dd 就可以搞</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ diskutil list</span><br><span class="line">/dev/disk6 (external, physical):</span><br><span class="line">   #:                       TYPE NAME                    SIZE       IDENTIFIER</span><br><span class="line">   0:                                                   *31.5 GB    disk6</span><br><span class="line">                        </span><br><span class="line"># 找到 U 盘的那个设备，umount</span><br><span class="line">$ diskutil unmountDisk /dev/disk3</span><br><span class="line"></span><br><span class="line"># 用 dd 把 ISO 文件写进设备，注意这里是 rdisk3 而不是 disk3，在 BSD 中 r(IDENTIFIER)</span><br><span class="line"># 代表了 raw device，会快很多</span><br><span class="line">$ sudo dd if=/path/image.iso of=/dev/rdisk3 bs=1m</span><br><span class="line"></span><br><span class="line"># 弹出 U 盘</span><br><span class="line">$ sudo diskutil eject /dev/disk3</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://linuxiac.com/how-to-create-bootable-usb-drive-using-dd-command/">Linux 下制作步骤</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">umount /dev/sdn1</span><br><span class="line">sudo mkfs.vfat /dev/sdn1</span><br><span class="line">dd if=/polarx/uniontechos-server-20-1040d-amd64.iso of=/dev/sdn1 status=progress</span><br></pre></td></tr></table></figure>

<h2 id="性能"><a href="#性能" class="headerlink" title="性能"></a>性能</h2><p>为保证服务性能应选用 performance 模式，将 CPU 频率固定工作在其支持的最高运行频率上，不进行动态调节，操作命令为 <code>cpupower frequency-set --governor performance</code>。</p>
<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><ul>
<li>dmesg | tail</li>
<li>vmstat 1</li>
<li>mpstat -P ALL 1</li>
<li>pidstat 1</li>
<li>iostat -xz 1</li>
<li>free -m</li>
<li>sar -n DEV 1</li>
<li>sar -n TCP,ETCP 1</li>
</ul>
<h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//检查sda磁盘中哪个应用程序占用的io比较高</span><br><span class="line">pidstat -d  1</span><br><span class="line"></span><br><span class="line">//分析应用程序中哪一个线程占用的io比较高</span><br><span class="line">pidstat -dt -p 73739 1  执行两三秒即可,得到74770线程io高</span><br><span class="line"></span><br><span class="line">//分析74770这个线程在干什么</span><br><span class="line">perf trace -t 74770 -o /tmp/tmp_aa.pstrace</span><br><span class="line">cat /tmp/tmp_aa.pstrace</span><br><span class="line">  2850.656 ( 1.915 ms): futex(uaddr: 0x653ae9c4, op: WAIT|PRIVATE_FLAG, val: 1)               = 0</span><br><span class="line">  2852.572 ( 0.001 ms): futex(uaddr: 0x653ae990, op: WAKE|PRIVATE_FLAG, val: 1)               = 0</span><br><span class="line">  2852.601 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</span><br><span class="line">  2852.690 ( 0.040 ms): write(fd: 159, buf: 0xd7a30020, count: 65536)                         = 65536</span><br><span class="line">  2852.796 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</span><br><span class="line">  2852.798 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</span><br><span class="line">  2852.939 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</span><br><span class="line">  2852.950 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</span><br><span class="line">  2852.977 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</span><br><span class="line">  2853.029 ( 0.035 ms): write(fd: 64, buf: 0xcd51e020, count: 65536)                          = 65536</span><br><span class="line">  2853.164 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f68)             = 0</span><br><span class="line">  2853.167 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f58)             = 0</span><br><span class="line">  2853.302 ( 0.001 ms): clock_gettime(which_clock: MONOTONIC, tp: 0xfff7bd470f38)             = 0</span><br></pre></td></tr></table></figure>

<h3 id="内存——虚拟内存参数"><a href="#内存——虚拟内存参数" class="headerlink" title="内存——虚拟内存参数"></a>内存——虚拟内存参数</h3><ul>
<li><code>dirty_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统将开始使用 pdflush 操作将脏的 page cache 写入磁盘。默认值为 20％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，降低其值有利于提高内存回收时的效率。</li>
<li><code>dirty_background_ratio</code> 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统开始在后台将脏的 page cache 写入磁盘。默认值为 10％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，设置较低的值有利于提高内存回收时的效率。</li>
</ul>
<h3 id="I-O-调度器"><a href="#I-O-调度器" class="headerlink" title="I&#x2F;O 调度器"></a>I&#x2F;O 调度器</h3><p>I&#x2F;O 调度程序确定 I&#x2F;O 操作何时在存储设备上运行以及持续多长时间。也称为 I&#x2F;O 升降机。对于 SSD 设备，宜设置为 noop。</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> noop &gt; /sys/block/<span class="variable">$&#123;SSD_DEV_NAME&#125;</span>/queue/scheduler</span><br></pre></td></tr></table></figure>

<h3 id="磁盘挂载参数"><a href="#磁盘挂载参数" class="headerlink" title="磁盘挂载参数"></a>磁盘挂载参数</h3><p><code>noatime</code> 读取文件时，将禁用对元数据的更新。它还启用了 nodiratime 行为，该行为会在读取目录时禁用对元数据的更新。</p>
<h2 id="Unix-Linux关系"><a href="#Unix-Linux关系" class="headerlink" title="Unix Linux关系"></a>Unix Linux关系</h2><p><img src="/images/951413iMgBlog/image-20211210085124387.png" alt="image-20211210085124387"></p>
<p><img src="/images/951413iMgBlog/G2Xri.png" alt="img"></p>
<h3 id="linux-发行版关系"><a href="#linux-发行版关系" class="headerlink" title="linux 发行版关系"></a><a target="_blank" rel="noopener" href="https://blog.51cto.com/wangyafei/1881605">linux 发行版关系</a></h3><p><img src="/images/951413iMgBlog/5cc164f5d79a11261.jpg_fo742.jpg" alt="细数各家linux之间的区别_软件应用_什么值得买"></p>
<p>Fedora：基于Red Hat Linux，在Red Hat Linux终止发行后，红帽公司计划以Fedora来取代Red Hat Linux在个人领域的应用，而另外发行的Red Hat Enterprise Linux取代Red Hat Linux在商业应用的领域。Fedora的功能对于用户而言，它是一套功能完备、更新快速的免费操作系统，而对赞助者Red Hat公司而言，它是许多新技术的测试平台，被认为可用的技术最终会加入到Red Hat Enterprise Linux中。Fedora大约每六个月发布新版本。</p>
<p>不同发行版几乎采用了不同包管理器（SLES、Fedora、openSUSE、centos、RHEL使用rmp包管理系统，包文件以RPM为扩展名；Ubuntu系列，Debian系列使用基于DPKG包管理系统，包文件以deb为扩展名。)</p>
<p>69年Unix诞生在贝尔实验室，80年 DARPA（国防部高级计划局）请人在Unix实现全新的TCP、IP协议栈。ARPANET最先搞出以太网</p>
<p>Linux 从91年到95年处于成长期，真正大规模应用是Linux+Apache提供的WEB服务被大家大规模采用</p>
<p>rpm:  centos&#x2F;fedora&#x2F;suse</p>
<p>deb:  debian&#x2F;ubuntu&#x2F;uos(早期基于ubuntu定制，后来基于debian定制，再到最近开始直接基于kernel定制)</p>
<p>ARPANET：<strong>高等研究計劃署網路</strong>（英語：Advanced Research Projects Agency Network），通称<strong>阿帕网</strong>（英語：ARPANET）是美國<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/%E5%9C%8B%E9%98%B2%E9%AB%98%E7%AD%89%E7%A0%94%E7%A9%B6%E8%A8%88%E5%8A%83%E7%BD%B2">國防高等研究計劃署</a>开发的世界上第一个运营的<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/%E5%B0%81%E5%8C%85%E4%BA%A4%E6%8F%9B">封包交换</a>网络，是全球<a target="_blank" rel="noopener" href="https://zh.m.wikipedia.org/wiki/%E4%BA%92%E8%81%94%E7%BD%91">互联网</a>的鼻祖。</p>
<p>TCP&#x2F;IP：1974年，卡恩和瑟夫带着研究成果，在IEEE期刊上，发表了一篇题为《关于分组交换的网络通信协议》的论文，正式提出TCP&#x2F;IP，用以实现计算机网络之间的互联。</p>
<p>在1983年，美国国防部高级研究计划局决定淘汰NCP协议（ARPANET最早使用的协议），TCP&#x2F;IP取而代之。</p>
<h3 id="Deepin-UOS"><a href="#Deepin-UOS" class="headerlink" title="Deepin UOS"></a>Deepin UOS</h3><p><em><strong>*Deepin 与统信 UOS 类似于红帽的 Fedora 与 RHEL 的上下游关系，Deepin 依然保持着原来的社区运营模式，而统信 UOS 则是基于社区版 Deepin 构建的商业发行版，为 Deepin 挖掘更多的商业机会和更大的商业价值，进而反哺社区，形成良性循环*</strong></em>。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/kevingrace/p/8671964.html">https://www.cnblogs.com/kevingrace/p/8671964.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/ac3e7009a764">https://www.jianshu.com/p/ac3e7009a764</a></p>
<p>B 站哈工大操作系统视频地址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1d4411v7u7?from=search&seid=2361361014547524697">https://www.bilibili.com/video/BV1d4411v7u7?from=search&amp;seid=2361361014547524697</a></p>
<p>B 站清华大学操作系统视频地址：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1js411b7vg?from=search&seid=2361361014547524697">https://www.bilibili.com/video/BV1js411b7vg?from=search&amp;seid=2361361014547524697</a></p>
<p><a target="_blank" rel="noopener" href="https://linux.cn/article-10465-1.html">Linux 工具：点的含义</a> <a target="_blank" rel="noopener" href="https://www.linux.com/training-tutorials/linux-tools-meaning-dot/">英文版</a></p>
<p><a target="_blank" rel="noopener" href="http://coolnull.com/4432.html">linux cp实现强制覆盖</a></p>
<p><a target="_blank" rel="noopener" href="https://wangdoc.com/bash/startup.html">https://wangdoc.com/bash/startup.html</a></p>
<p><a target="_blank" rel="noopener" href="https://cjting.me/2020/12/10/tiny-x64-helloworld/">编写一个最小的 64 位 Hello World</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/01/ipmitool%E4%BF%AE%E6%94%B9BIOS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/01/ipmitool%E4%BF%AE%E6%94%B9BIOS/" class="post-title-link" itemprop="url">ipmitool 和 BIOS</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-01 12:30:03" itemprop="dateCreated datePublished" datetime="2020-01-01T12:30:03+08:00">2020-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="ipmitool-和-BIOS"><a href="#ipmitool-和-BIOS" class="headerlink" title="ipmitool 和 BIOS"></a>ipmitool 和 BIOS</h1><h2 id="什么是-IPMI"><a href="#什么是-IPMI" class="headerlink" title="什么是 IPMI"></a>什么是 IPMI</h2><p>IPMI（智能平台管理接口），Intelligent Platform Management Interface 的缩写。原本是一种<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/Intel">Intel</a>架构的企业系统的周边设备所采用的一种工业标准。IPMI亦是一个开放的免费标准，用户无需支付额外的费用即可使用此标准。</p>
<p>IPMI 能够横跨不同的操作系统、固件和硬件平台，可以智能的监视、控制和自动回报大量服务器的运作状况，以降低服务器系统成本。</p>
<p>1998年Intel、DELL、HP及NEC共同提出IPMI规格，可以透过网路远端控制温度、电压。</p>
<p>2001年IPMI从1.0版改版至1.5版，新增 PCI Management Bus等功能。</p>
<p>2004年Intel发表了IPMI 2.0的规格，能够向下相容IPMI 1.0及1.5的规格。新增了Console Redirection，并可以通过Port、Modem以及Lan远端管理伺服器，并加强了安全、VLAN 和刀锋伺服器的支援性。</p>
<p>Intel&#x2F;amd&#x2F;hygon 基本都支持 ipmitool，看起来ARM 支持的接口也许不一样</p>
<p>BMC（Baseboard Management Controller）即我们常说的带外系统，是在机器上电时即完成自身初始化，开始运行。其系统可在standby电模式下工作。所以，通过带外监控服务器硬件故障，不受OS存活状态影响，可实现7*24小时无间断监控，甚至我们可以通过带外方式，精确感知带内存活，实现OS存活监控。</p>
<p>BMC在物理形态上，由一主嵌入式芯片+系列总线+末端芯片组成的一个硬件监控&amp;控制系统，嵌入式芯片中运行嵌入式Linux操作系统，负责整个BMC系统的资源协调及用户交互，核心进程是IPMImain进程，实现了全部IPMI2.0协议的消息传递&amp;处理工作。</p>
<h2 id="ipmitool-用法"><a href="#ipmitool-用法" class="headerlink" title="ipmitool 用法"></a>ipmitool 用法</h2><p>基本步骤：</p>
<ol>
<li>查看当前值：ipmitool raw 0x3e 0x5f 0x00 0x11 （非必要，列出目前BIOS中的值）</li>
<li>打开配置开关(让BIOS进入可配置，默认不可配置)：ipmitool raw 0x3e 0x5c 0x00 0x01 0x81</li>
<li>修改某个值，比如将numa 设置为on：ipmitool raw 0x3e 0x5c 0x05 0x01 0x81</li>
<li>查看修改后的值：ipmitool raw 0x3e 0x5d 0x05 0x01 (必须要)</li>
<li>最后reboot机器新的值就会apply到BIOS中</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/zygblock/article/details/53367664">ipmitool使用</a>基本语法</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">         固定-不变 0x5c修改   要修改的项    长度(一搬都是01)    新的值(0x81 表示on、0x80表示off)</span><br><span class="line">ipmitool raw 0x3e 0x5c      index       0x01                value</span><br></pre></td></tr></table></figure>

<p>第1&#x2F;2个参数raw 0x3e 固定不变</p>
<p>第三个参数表示操作可以是：</p>
<ul>
<li>0x5c 修改</li>
<li>0x5f 查看BIOS中的当前值（海光是这样，intel不是）</li>
<li>0x5d 查询即将写入的值（修改后没有写入 0x5f 看到的是老值）</li>
</ul>
<p>第四个参数Index表示需要修改的配置项（具体见后表）</p>
<p>第五个参数 0x01 表示值的长度，一般固定不需要改</p>
<p>value 表示值，0x81表示enable; 0x80表示disable</p>
<h3 id="ipmitool带外设置步骤"><a href="#ipmitool带外设置步骤" class="headerlink" title="ipmitool带外设置步骤"></a><a target="_blank" rel="noopener" href="https://promisechen.github.io/kbase/ipmi.html">ipmitool</a>带外设置步骤</h3><p>1）设置valid flag：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5c 0x00 0x01 0x81</p>
<p>2） 设置对应的选项：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5c index 0x01 Data  —- index 和Data参考下述表格；</p>
<p>3）重启CN：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 power reset</p>
<p>4）读取当前值：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10  raw 0x3e 0x5f index 0x01 </p>
<p> 如moc机型，读取CN的 Numa值：</p>
<p>ipmitool -I lan -U admin -P admin -H 192.168.1.10 raw 0x3e 0x5f 0x05 0x01 </p>
<h3 id="确认是否设置成功"><a href="#确认是否设置成功" class="headerlink" title="确认是否设置成功"></a>确认是否设置成功</h3><p>查询要写入的新值：ipmitool 0x3e 0x5d 0x00 0x11</p>
<p> 返回值，如：11 <strong>81</strong> 81 00 00 00 00 00 00 00 00 00 00 00 00 00  00 00</p>
<p>​      第一个byte 表示查询数量，表示查询0x11个设置项；</p>
<pre><code>	 第二个byte 表示index=0的值，即Configuration，必须保证是0x81，才能进行重启，否则设置不生效；
</code></pre>
<p>​      第三个byte 表示index&#x3D;1的值，即Turbo，表示要设置为0x81；</p>
<p>​      剩余byte依次类推……….</p>
<p>​      未设置新值的index对应值是00，要设置的index其对应值为Data（步骤3的设置值）；</p>
<h2 id="海光服务器修改案例"><a href="#海光服务器修改案例" class="headerlink" title="海光服务器修改案例"></a>海光服务器修改案例</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">//海光+alios下 第二列为：0x5c 修改、0x5f BIOS中的查询、0x5d 查询即将写入的值 </span><br><span class="line">//0x5f 查询BIOS中的值</span><br><span class="line"></span><br><span class="line">#ipmitool raw 0x3e 0x5f 0x00 0x11</span><br><span class="line"> 11 81 81 81 81 80 81 80 81 81 80 81 81 00 00 81</span><br><span class="line"> 80 81</span><br><span class="line"> </span><br><span class="line">//还没有写入任何新值</span><br><span class="line">#ipmitool raw 0x3e 0x5d 0x00 0x11</span><br><span class="line"> 11 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line"> 00 00 </span><br><span class="line"></span><br><span class="line">//enable numa</span><br><span class="line">#ipmitool raw 0x3e 0x5c 0x00 0x01 0x81</span><br><span class="line">#ipmitool raw 0x3e 0x5c 0x05 0x01 0x81</span><br><span class="line"></span><br><span class="line">//enable boost</span><br><span class="line">#ipmitool raw 0x3e 0x5c 0x01 0x01 0x81</span><br><span class="line"></span><br><span class="line">#ipmitool raw 0x3e 0x5d 0x01 0x01</span><br><span class="line"> 11 81 </span><br><span class="line">//关闭 SMT</span><br><span class="line">#ipmitool raw 0x3e 0x5c 0x02 0x01 0x80</span><br><span class="line"></span><br><span class="line">#ipmitool raw 0x3e 0x5d 0x02 0x01</span><br><span class="line"> 11 81 </span><br></pre></td></tr></table></figure>



<h2 id="BIOS中选项的对应关系"><a href="#BIOS中选项的对应关系" class="headerlink" title="BIOS中选项的对应关系"></a>BIOS中选项的对应关系</h2><h3 id="Intel服务器"><a href="#Intel服务器" class="headerlink" title="Intel服务器"></a>Intel服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
</tr>
</thead>
<tbody><tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)0x00  BIOS在读取设定后，会发送index&#x3D;0x00，data&#x3D;0x00的命令给BMC，BMC应清零所有参数值。</td>
</tr>
<tr>
<td>Turbo</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>HT</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>VT</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>EIST</td>
<td>0x04</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Numa</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – disable 0x81 - enable</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>VT-d</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Active Video</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
</tr>
<tr>
<td>Intel Speed Select</td>
<td>0x0C</td>
<td>1</td>
<td>0x80-Disable0x81-Config 10x82-Config 2</td>
</tr>
<tr>
<td>IMS</td>
<td>0x0D</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80-Disable0x81-Enabled0x83-Enable&amp;Clear TPM</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 – disable – 响应命令0x81 – enable –不响应命令</td>
</tr>
<tr>
<td>BIOS BOOT MODE</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
</tr>
<tr>
<td>Active Cores</td>
<td>0x11</td>
<td>1</td>
<td>0x80 - Default Core Number0x81 - Active 1 Core0x82 - Active 2 Cores0x83 - Active 3 Cores…0xFE - Active 126 Cores</td>
</tr>
<tr>
<td>C   State</td>
<td>0x12</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>HWPM</td>
<td>0x13</td>
<td>1</td>
<td>0x80-Disable0x81-Native   mode0x82-OOB   Mode0x83-Native   mode Without Legacy support</td>
</tr>
<tr>
<td>Intel   SgxSW Guard Extensions (SGX)</td>
<td>0x14</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>SGX PRMRR Size</td>
<td>0x15</td>
<td>1</td>
<td>0X80-[00]No valid PRMRR   size    0X81-[40000000]1G0X82-[80000000]2G0X83-[100000000]4G0X84-[200000000]8G0X85-[400000000]16G0X86-[800000000]32G0X87-[1000000000]64G0X88-[2000000000]128G0X89-[4000000000]256G0X8A-[8000000000]512G</td>
</tr>
<tr>
<td>SGX Factory Reset</td>
<td>0x16</td>
<td></td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td></td>
<td>0x17</td>
<td></td>
<td>预留</td>
</tr>
<tr>
<td>CPU0_IOU0 (IIO PCIe Br1)</td>
<td>0x18</td>
<td>1</td>
<td>0x80 – x4x4x4x4   0x81 – x4x4x8   0x82 – x8x4x4   0x83 – x8x8   0x84 – x16   0x85 - Auto</td>
</tr>
<tr>
<td>CPU0_IOU1 (IIO PCIe Br2)</td>
<td>0x19</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU2 (IIO PCIe Br3)</td>
<td>0x1a</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU3 (IIO PCIe Br4)</td>
<td>0x1b</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU0_IOU4 (IIO PCIe Br5)</td>
<td>0x1c</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU0 (IIO PCIe Br1)</td>
<td>0x1d</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU1 (IIO PCIe Br2)</td>
<td>0x1e</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU2 (IIO PCIe Br3)</td>
<td>0x1f</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU3 (IIO PCIe Br4)</td>
<td>0x20</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU1_IOU4 (IIO PCIe Br5)</td>
<td>0x21</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU0 (IIO PCIe Br1)</td>
<td>0x22</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU1 (IIO PCIe Br2)</td>
<td>0x23</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU2 (IIO PCIe Br3)</td>
<td>0x24</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU3 (IIO PCIe Br4)</td>
<td>0x25</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU2_IOU4 (IIO PCIe Br5)</td>
<td>0x26</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU0 (IIO PCIe Br1)</td>
<td>0x27</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU1 (IIO PCIe Br2)</td>
<td>0x28</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU2 (IIO PCIe Br3)</td>
<td>0x29</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU3 (IIO PCIe Br4)</td>
<td>0x2a</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>CPU3_IOU4 (IIO PCIe Br5)</td>
<td>0x2b</td>
<td>1</td>
<td>同上</td>
</tr>
<tr>
<td>SGXLEPUBKEYHASHx Write Enable</td>
<td>0x2C</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
</tr>
<tr>
<td>SubNuma</td>
<td>0x2D</td>
<td>1</td>
<td>0x80-Disabled0x81-SN2</td>
</tr>
<tr>
<td>VirtualNuma</td>
<td>0x2E</td>
<td>1</td>
<td>0x80-Disabled0x81-Enabled</td>
</tr>
<tr>
<td>TPM Priority</td>
<td>0x2F</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>TDX</td>
<td>0x30</td>
<td>1</td>
<td>0x80 - Disabled0x81 - Enabled</td>
</tr>
<tr>
<td>Select Owner EPOCH input type</td>
<td>0x31</td>
<td>1</td>
<td>0x81-Change to New Random Owner EPOCHs0x82-Manual User Defined Owner EPOCHs</td>
</tr>
<tr>
<td>Software Guard Extensions Epoch 0</td>
<td>0x32</td>
<td>1</td>
<td></td>
</tr>
<tr>
<td>Software Guard Extensions Epoch 1</td>
<td>0x33</td>
<td>1</td>
<td></td>
</tr>
</tbody></table>
<h3 id="AMD服务器"><a href="#AMD服务器" class="headerlink" title="AMD服务器"></a>AMD服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
<th>支持项目</th>
</tr>
</thead>
<tbody><tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Core Performance Boost</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SMT Mode</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SVM Mode</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>EIST</td>
<td>0x04</td>
<td>1</td>
<td>0x80 (AMD默认支持智能调频但无此选项)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>NUMA nodes per socket</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – NPS0 0x81 – NPS1 0x82 – NPS2 0x83 – NPS4 （开）0x87 – Auto(Auto为NPS1)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>IOMMU</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable0x8F – Auto</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Active Video</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Intel Speed Select</td>
<td>0x0C</td>
<td>1</td>
<td>0x80 (AMD无此选项)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>IMS</td>
<td>0x0D</td>
<td>1</td>
<td>0x80 (AMD暂未做IMS功能)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80 – disable0x81 – enable0x83 – enable &amp; TPM   clear</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 (AMD暂未做此功能)</td>
<td>RomeMilan</td>
</tr>
<tr>
<td>BIOS BOOT MODE</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
<td>RomeMilan</td>
</tr>
</tbody></table>
<h3 id="海光服务器"><a href="#海光服务器" class="headerlink" title="海光服务器"></a>海光服务器</h3><table>
<thead>
<tr>
<th>Name</th>
<th>Index</th>
<th>Data Length（Bytes）</th>
<th>Data （不在列表中则为无效值）</th>
<th>支持项目</th>
</tr>
</thead>
<tbody><tr>
<td>Configuration</td>
<td>0x00</td>
<td>1</td>
<td>0x81 – valid Flag 0x82 – Restore Default Value (恢复为PRD定义的默认值)</td>
<td>海光2</td>
</tr>
<tr>
<td>Core Performance Boost</td>
<td>0x01</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SMT</td>
<td>0x02</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SVM</td>
<td>0x03</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>P-State Control</td>
<td>0x04</td>
<td>1</td>
<td>0x80 – Performance0x81 – Normal</td>
<td>海光2</td>
</tr>
<tr>
<td>Memory Interleaving</td>
<td>0X05</td>
<td>1</td>
<td>0x80 – Socket (关numa) 0x81 - channel（8 node）</td>
<td>海光2</td>
</tr>
<tr>
<td>Vendor Change</td>
<td>0x06</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>IOMMU</td>
<td>0x07</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>SRIOV</td>
<td>0x08</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Onboard VGA</td>
<td>0x09</td>
<td>1</td>
<td>0x80 – Onboard0x81 – PCIe</td>
<td>海光2</td>
</tr>
<tr>
<td>Local HDD Boot</td>
<td>0x0A</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Hotkey support</td>
<td>0x0B</td>
<td>1</td>
<td>0x80 – disable0x81 – enable</td>
<td>海光2</td>
</tr>
<tr>
<td>Hygon平台没有此选项</td>
<td>0x0C</td>
<td>1</td>
<td>0x80-Disable0x81-Config 10x82-Config 2</td>
<td>不支持</td>
</tr>
<tr>
<td>Hygon平台没有此选项</td>
<td>0x0D</td>
<td>1</td>
<td>0x80-Disable0x81-Enable</td>
<td>不支持</td>
</tr>
<tr>
<td>TPM</td>
<td>0x0E</td>
<td>1</td>
<td>0x80-Disable0x81-Enabled0x83-Enable&amp;Clear TPM</td>
<td>海光2</td>
</tr>
<tr>
<td>Power off remove</td>
<td>0x0F</td>
<td>1</td>
<td>0x80 – disable – 响应命令0x81 – enable –不响应命令</td>
<td>海光2</td>
</tr>
<tr>
<td>Boot option Filter</td>
<td>0x10</td>
<td>1</td>
<td>0x80 – Legacy0x81 – UEFI</td>
<td>海光2</td>
</tr>
</tbody></table>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/01/2010%E5%88%B02020%E8%BF%9910%E5%B9%B4%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/01/2010%E5%88%B02020%E8%BF%9910%E5%B9%B4%E7%9A%84%E7%A2%8E%E7%A2%8E%E5%BF%B5%E5%BF%B5/" class="post-title-link" itemprop="url">2010到2020这10年的碎碎念念</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-01 00:30:03" itemprop="dateCreated datePublished" datetime="2020-01-01T00:30:03+08:00">2020-01-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/others/" itemprop="url" rel="index"><span itemprop="name">others</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="2010到2020这10年的碎碎念念"><a href="#2010到2020这10年的碎碎念念" class="headerlink" title="2010到2020这10年的碎碎念念"></a>2010到2020这10年的碎碎念念</h1><h2 id="来自网络的一些数据"><a href="#来自网络的一些数据" class="headerlink" title="来自网络的一些数据"></a>来自网络的一些数据</h2><p>这十年，中国的人均GDP从大约3300美金干到了9800美金。这意味着：更多的中国人脱贫，更多的中国人变成了中产。这是这一轮消费升级的核心原动力，没有之一。</p>
<p>这十年，中国的进出口的总额从2009年占GDP的44.86%，降至34.35%。</p>
<p>互联网从美国copy开始变成创新、走在前列，因为有庞大的存量市场</p>
<p>2010年，一个数据发生了逆势波动。那就是中国的适龄小学人口增速。在此之前从1997年后，基本呈负增长。这是因为中国80后家长开始登上历史舞台。这带动了诸多产业的蓬勃发展，比如互联网教育，当然还有学区房。</p>
<p>10年吉利收购沃尔沃，18年吉利收购戴姆勒10%的股份。</p>
<p>微信崛起、头条崛起、百度走下神坛。美团、pdd崛起</p>
<p>12年2月6号的王护士长外逃美国大使馆也让大家兴奋了，11年的郭美美红十字会事件快要被忘记了，但是也让大家对慈善事件更加警惕，倒是谅解了汶川地震的王石10块捐款事件，不过老王很快因为娶了年轻的影星田朴珺一下子人设坍塌，大家更热衷老王的负面言论了。</p>
<p>温州动车事件让高铁降速了</p>
<p>我爸是李刚、药家鑫、李天一、邓玉娇（09年），陈冠希艳照门、三鹿奶粉、汶川地震、奥运会（08年）</p>
<p>2018年：中美贸易站、问题疫苗、个税改革、中兴被美制裁，北京驱赶低端人口，鸿茅药酒，p2p暴雷，昆山反杀案，相互宝</p>
<p>2015年：雾霾、柴静纪录片《穹顶之下》，屠呦呦诺贝尔奖，放开二胎</p>
<p>2014年：东莞扫黄、马航370事件；周师傅被查、占中</p>
<p>2013年：劳教正式被废除，想起2003年的孙志刚事件废除收容制度</p>
<p>2012年：方韩之争，韩寒走下神坛</p>
<p>2011年：日本海啸地震，中国抢盐事件；郭美美，温州动车</p>
<p>2010年：google退出中国，上海世博会开幕，富士康N连跳楼事件；我爸是李刚，腾讯大战360</p>
<h2 id="自我记忆"><a href="#自我记忆" class="headerlink" title="自我记忆"></a>自我记忆</h2><p>刚看到有人在说乐清钱云会事件，一晃10年了，10年前微博开始流行改变了好多新闻、热点事件的引爆方式。</p>
<p>这十年BBS、门户慢慢在消亡，10年前大家都知道三大门户网站和天涯，现在的新网民应该知道的不多了。</p>
<p>影响最大的还是移动网络的崛起，这也取决于4G和山寨机以及后来的小米手机，真正给中国的移动互联网带来巨大的红利，注入的巨大的增长。<br>我自己对移动互联网的判断是极端错误的，即使09年我就开始用上了iphone手机，那又怎么样，看问题还是用静态的视觉观点。手机没有键盘、手机屏幕狭小，这些确实是限制，到2014年我还想不明白为什么要在手机上购物，比较、看物品图片太不方便了，结果便利性秒杀了这些不方便；只有手机的群体秒杀了办公室里的白领，最后大家都很高兴地用手机购物了，甚至PC端bug更多，更甚至有些网站不提供PC端。</p>
<p>移动网络的崛起和微信的成功也相辅相成的，在移动网络时代每个人都有自己的手机，所以账号系统的打通不再是问题，尤其是都被微信这个移动航母在吞噬，其它公司都活在微信的阴影里。</p>
<p>当然移动支付的崛起就理所当然了。</p>
<p>即使今天网上购物还是PC上要方便，那又怎么样，很多时候网上购物都是不在电脑前的零碎时间。</p>
<p>10多年前第一次看到智能手机是室友购买的多普达，20年前也是这个室友半夜里很兴奋地播报台湾大选，让我知道了台湾大选这个事情。</p>
<p>基本的价值观、世界观，没怎么改变，不应该是年龄大了僵化了，应该是掌握信息的手段和能力增强了，翻墙获取信息也很容易，基本的逻辑还在也没那么容易跑偏了。可能就是别人看到的年纪大了脑子僵化了吧，自我感觉不一定对。</p>
<p>最近10年经济发展的非常好，政府对言论的控制越来越精准，舆论引导也非常”成功”,所以网络上看到这5年和5年前基本差别很大，5年前公知是个褒义词，5年后居然成了贬义词。</p>
<p>房价自然是这10年最火的话题，07年大家开始感觉到房价上涨快、房价高，08年金融危机本来是最好的机会，结果4万亿刺激下09年年底房价开始翻倍，到10年面对翻倍了的房价政府、媒体、老百姓都在喊高，实际也只是横盘，13-14年小拉一波，16年涨价去库存再大拉一波。基本让很多人绝望了</p>
<p>这十年做的最错的事情除了没有早点买房外就是想搞点投资收入投了制造外加炒股，踩点能力太差了，虽然前5年像任志强一样一直看多房价的不多，这个5年都被现实教育了，房价也基本到头了。</p>
<p>工作上应该更早地、坚定地进入互联网、移动互联网，这10年互联网对人才的需求实在太大了，虽然最终能伴随公司成长的太少，毕竟活下来长大的公司不多。</p>
<p>Google退出中国、看着小杨同学和一些同事移民、360大战QQ、诺贝尔和平奖、华为251事件都算是自己在一些公众事情上投入比较多的。非常不舍google的离开，这些年也基本还是只用google，既是无奈中用下百度也还是觉得搜不到什么有效信息；好奇移民的想法和他们出去后的各种生活；360跟QQ大战的时候觉得腾讯的垄断太牛叉了，同时认为可能360有这种资源的话会更作恶和垄断的更厉害，至少腾讯还是在乎外面的看法和要面子的；LXB到现在也是敏感词，直到病死在软禁中，这些年敏感词越来越多，言论的控制更严厉了；华为251也是个奇葩事件了，暴露了资本家的粗野和枉法。</p>
<p>自己工作上跳槽一次，继续做一个北漂。公司对自己的方法论改变确实比较大，近距离看到了一些成功因素方面的逻辑（更有效的激励和企业文化）。</p>
<p>经历了从外企到私企，从小公司到大公司的不同，外企英语是天花板，也看到了华为所谓的狼性、在金钱激励下的狼性，和对企业文化的维护，不能否认90%以上的人工作是为了钱</p>
<p>这几年也开始习惯写技术文章来总结了，这得益于Markdown+截图表述的便利，也深刻感受到深抠，然后总结分享的方法真的很好（高斯学习方法），也体会到了抓手、触类旁通的运用。10年前在搜狐blog写过一两年的博客放弃的很快，很难一直有持续的高水平总结和输出。</p>
<p>10年前还在比较MSN和QQ谁更好（我是坚定站在QQ这边的），10年后MSN再也看不见了，QQ也有了更好的替代工具微信。用处不大的地方倒是站对了，对自己最有用的关键地方都站错了。</p>
<p>10年前差点要去豆瓣，10年后豆瓣还活着，依然倔强地保持自己的品味，这太难得了。相反十年前好用得不得了的RSS订阅，从抓虾转到google reader再到feedly好东西就是活得这么艰难。反过来公众号起来了、贴吧式微了，公众号运作新闻类是没问题的（看完就过），但是对技术类深度一点的就很不合适了，你看看一篇文章24小时内的阅读量占据了98%以上，再到后面就存亡了！但是公众号有流量，流量可以让大家跪在地上。</p>
<p>10年前啃老是被看不起的，10年后早结婚、多啃老也基本成了这10年更对的事情，结婚得买房，啃老买得更早，不对的事情变对了（结婚早没错）。</p>
<p>很成功地组织了一次同学20周年的聚会，也看到了远则亲、近则仇的现实情况，自己组织统筹能力还可以。</p>
<p>情绪控制能力太差、容易失眠。这十年爱上了羽毛球和滑雪，虽然最近几年滑雪少了。</p>
<p>体会到自小贫穷带来的一些抠门的坏习惯。</p>
<p>2015年的股票大跌让自己很痛苦，这个过程反馈出来的不愿意撒手、在股市上的鸵鸟方式，股市上总是踩不到正确的点。割肉太难，割掉的总是错误的。</p>
<p>15、16年我认为云计算不怎么样，觉得无非就是新瓶装旧酒，现在云计算不再有人质疑了，即使现在都还是亏钱。</p>
<p>当然我也质疑过外卖就是一跑腿的，确实撑不起那么大的盘子，虽然没有像团购一样消亡，基本跟共享单车一样了，主要因为我是共享单车的重度用户，而我极端不喜欢外卖，所以要站出来看问题、屁股坐在哪边会严重影响看法，也就是不够客观。</p>
<p>网约车和移动支付一起在硝烟中混战</p>
<p>电动车开始起来，主要受政府弯道超车的刺激，目前看取决于自有充电位（适合三四线城市），可是三四线城市用户舍不得花这个溢价，汽油车都还没爽够呢。</p>
<p>对世界杯不再那么关注，对AlphaGo的新闻倒是很在意了。魏则西事件牢牢地把百度钉死在耻辱柱上。</p>
<p>随着12306的发展和高铁的起来，终于过年回家的火车票不用再靠半夜排队了。</p>
<p>2019年年末行政强制安装ETC，让我想起20年前物理老师在课堂上跟我们描述的将来小汽车走高速公路再也不用停下来收费了，会自动感应，开过去就自动扣钱了。我一直对这个未来场景念念不忘，最近10年我经常问别人为什么不办ETC，这个年底看到的是行政命令下的各种抱怨。</p>
<h3 id="看到："><a href="#看到：" class="headerlink" title="看到："></a>看到：</h3><ul>
<li><p>老人、家人更不愿意听身边亲近人员的建议；</p>
</li>
<li><p>老人思维为什么固化、怎么样在自己老后不是那样固化；</p>
</li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/12/24/Linux%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%EF%BC%8C%E6%80%A7%E8%83%BD%E5%88%B0%E5%BA%95%E6%8F%90%E5%8D%87%E5%A4%9A%E5%B0%91%EF%BC%9F%E6%8B%BF%E6%95%B0%E6%8D%AE%E8%AF%B4%E8%AF%9D/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2019/12/24/Linux%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC%E5%8D%87%E7%BA%A7%EF%BC%8C%E6%80%A7%E8%83%BD%E5%88%B0%E5%BA%95%E6%8F%90%E5%8D%87%E5%A4%9A%E5%B0%91%EF%BC%9F%E6%8B%BF%E6%95%B0%E6%8D%AE%E8%AF%B4%E8%AF%9D/" class="post-title-link" itemprop="url">Linux内核版本升级，性能到底提升多少？拿数据说话</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-12-24 17:30:03" itemprop="dateCreated datePublished" datetime="2019-12-24T17:30:03+08:00">2019-12-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Linux/" itemprop="url" rel="index"><span itemprop="name">Linux</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Linux内核版本升级，性能到底提升多少？"><a href="#Linux内核版本升级，性能到底提升多少？" class="headerlink" title="Linux内核版本升级，性能到底提升多少？"></a>Linux内核版本升级，性能到底提升多少？</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>X 产品在公有云售卖一直使用的2.6.32的内核，有点老并且有些内核配套工具不能用，于是想升级一下内核版本。预期新内核的性能不能比2.6.32差</p>
<p>以下不作特殊说明的话都是在相同核数的Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz下得到的数据，最后还会比较相同内核下不同机型&#x2F;CPU型号的性能差异。</p>
<p>场景都是用sysbench 100个并发跑点查。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><strong>先说大家关心的数据，最终4.19内核性能比2.6.32好将近30%，建议大家升级新内核，不需要做任何改动，尤其是Java应用（不同场景会有差异）</strong></p>
<p>本次比较的场景是Java应用的Proxy类服务，主要瓶颈是网络消耗，类似于MaxScale。后面有一个简单的MySQL Server场景下2.6.32和4.19的比较，性能也有33%的提升。</p>
<h2 id="2-6-32性能数据"><a href="#2-6-32性能数据" class="headerlink" title="2.6.32性能数据"></a>2.6.32性能数据</h2><p>升级前先看看目前的性能数据好对比（以下各个场景都是CPU基本跑到85%）</p>
<p><img src="/images/oss/b57c5ee5fe50ceb81cbad158f7b7aeeb.png" alt="image.png"></p>
<h2 id="一波N折的4-19"><a href="#一波N折的4-19" class="headerlink" title="一波N折的4.19"></a>一波N折的4.19</h2><p>阿里云上默认买到的ALinux2 OS（4.19），同样配置跑起来后，tps只有16000，比2.6.32的22000差了不少，心里只能暗暗骂几句坑爹的货，看了下各项指标，看不出来什么问题，就像是CPU能力不行一样。如果这个时候直接找内核同学，估计他们心里会说 X 是个什么东西？是不是你们测试有问题，是不是你们配置的问题，不要来坑我，内核性能我们每次发布都在实验室里跑过了，肯定是你们的应用问题。</p>
<p>所以要找到一个公认的场景下的性能差异。幸好通过qperf发现了一些性能差异。</p>
<h3 id="通过qperf来比较差异"><a href="#通过qperf来比较差异" class="headerlink" title="通过qperf来比较差异"></a>通过qperf来比较差异</h3><p>大包的情况下性能基本差不多，小包上差别还是很明显</p>
<pre><code>qperf -t 40 -oo msg_size:1  4.19 tcp_bw tcp_lat
tcp_bw:
    bw  =  2.13 MB/sec
tcp_lat:
    latency  =  224 us
tcp_bw:
    bw  =  2.15 MB/sec
tcp_lat:
    latency  =  226 us

qperf -t 40 -oo msg_size:1  2.6.32 tcp_bw tcp_lat
tcp_bw:
    bw  =  82 MB/sec
tcp_lat:
    latency  =  188 us
tcp_bw:
    bw  =  90.4 MB/sec
tcp_lat:
    latency  =  229 us
</code></pre>
<p>这下不用担心内核同学怼回来了，拿着这个数据直接找他们，可以稳定重现。</p>
<p>经过内核同学排查后，发现默认镜像做了一些安全加固，简而言之就是CPU拿出一部分资源做了其它事情，比如旁路攻击的补丁之类的，需要关掉（因为 X 的OS只给我们自己用，上面部署的代码都是X 产品自己的代码，没有客户代码，客户也不能够ssh连上X 产品节点）</p>
<pre><code>去掉 melt、spec 能到20000， 去掉sonypatch能到21000 
</code></pre>
<p>关闭的办法在 &#x2F;etc&#x2F;default&#x2F;grub 里 GRUB_CMDLINE_LINUX 配置中增加这些参数：</p>
<pre><code>nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off
</code></pre>
<p>关掉之后的状态看起来是这样的：</p>
<pre><code>$sudo cat /sys/devices/system/cpu/vulnerabilities/*
Mitigation: PTE Inversion
Vulnerable; SMT Host state unknown
Vulnerable
Vulnerable
Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers
Vulnerable, STIBP: disabled
</code></pre>
<p>这块参考<a target="_blank" rel="noopener" href="https://help.aliyun.com/knowledge_detail/154567.html?spm=a2c4g.11186623.2.12.887e38843VLHkv">阿里云文档</a> 和<a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/102087.html?spm=a2c4g.11186623.6.721.4a732223pEfyNC">这个</a></p>
<p>开启漏洞补丁（性能差）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># uname -r</span><br><span class="line">4.19.91-24.8.an8.x86_64</span><br><span class="line"></span><br><span class="line"># cat /proc/cmdline</span><br><span class="line">BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.19.91-24.8.an8.x86_64 root=UUID=ac9faf02-89c6-44d8-80b2-0f8ea1084fc3 ro console=tty0 crashkernel=auto console=ttyS0,115200 crashkernel=0M-2G:0M,2G-8G:192M,8G-:256M</span><br><span class="line">[root@Anolis82 ~]# sudo cat /sys/devices/system/cpu/vulnerabilities/*</span><br><span class="line">KVM: Mitigation: Split huge pages</span><br><span class="line">Mitigation: PTE Inversion; VMX: conditional cache flushes, SMT vulnerable</span><br><span class="line">Mitigation: Clear CPU buffers; SMT vulnerable</span><br><span class="line">Mitigation: PTI</span><br><span class="line">Mitigation: Speculative Store Bypass disabled via prctl and seccomp</span><br><span class="line">Mitigation: usercopy/swapgs barriers and __user pointer sanitization</span><br><span class="line">Mitigation: Full generic retpoline, IBPB: conditional, IBRS_FW, STIBP: conditional, RSB filling</span><br><span class="line">Not affected</span><br><span class="line">Mitigation: Clear CPU buffers; SMT vulnerable</span><br></pre></td></tr></table></figure>

<p>关闭（性能好）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@Anolis82 ~]# sudo cat /sys/devices/system/cpu/vulnerabilities/*</span><br><span class="line">KVM: Vulnerable</span><br><span class="line">Mitigation: PTE Inversion; VMX: vulnerable</span><br><span class="line">Vulnerable; SMT vulnerable</span><br><span class="line">Vulnerable</span><br><span class="line">Vulnerable</span><br><span class="line">Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers</span><br><span class="line">Vulnerable, IBPB: disabled, STIBP: disabled</span><br><span class="line">Not affected</span><br><span class="line">Vulnerable</span><br><span class="line">[root@Anolis82 ~]# cat /proc/cmdline</span><br><span class="line">BOOT_IMAGE=(hd0,gpt2)/vmlinuz-4.19.91-24.8.an8.x86_64 root=UUID=ac9faf02-89c6-44d8-80b2-0f8ea1084fc3 ro console=tty0 crashkernel=auto console=ttyS0,115200 nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable no_stf_barrier mds=off mitigations=off crashkernel=0M-2G:0M,2G-8G:192M,8G-:256M</span><br></pre></td></tr></table></figure>



<h3 id="4-9版本的内核性能"><a href="#4-9版本的内核性能" class="headerlink" title="4.9版本的内核性能"></a>4.9版本的内核性能</h3><p>但是性能还是不符合预期，总是比2.6.32差点。在中间经过几个星期排查不能解决问题，陷入僵局的过程中，尝试了一下4.9内核，果然有惊喜。</p>
<p>下图中对4.9的内核版本验证发现，tps能到24000，明显比2.6.32要好，所以传说中的新内核版本性能要好看来是真的，这下坚定了升级的念头，同时也看到了兜底的方案–最差就升级到4.9</p>
<p><img src="/images/oss/2f035e145f1bc41eb4a8b8bda8ed4ea2.png" alt="image.png"></p>
<p><strong>多队列是指网卡多队列功能，也是这次升级的一个动力。看起来在没达到单核瓶颈前，网卡多队列性能反而差点，这也符合预期</strong></p>
<h3 id="继续分析为什么4-19比4-9差了这么多"><a href="#继续分析为什么4-19比4-9差了这么多" class="headerlink" title="继续分析为什么4.19比4.9差了这么多"></a>继续分析为什么4.19比4.9差了这么多</h3><p>4.9和4.19这两个内核版本隔的近，比较好对比分析内核参数差异，4.19跟2.6.32差太多，比较起来很困难。</p>
<p>最终仔细对比了两者配置的差异，发现ALinux的4.19中 transparent_hugepage 是 madvise ,这对 Java应用来说可不是太友好：</p>
<pre><code>$cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
</code></pre>
<p>将其改到 always 后4.19的tps终于稳定在了28300</p>
<p><img src="/images/oss/081c08801adb36cdfd8ff62be54fce94.png" alt="image.png"></p>
<p>这个过程中花了两个月的一些其他折腾就不多说了，主要是内核补丁和transparent_hugepage导致了性能差异。</p>
<p>transparent_hugepage，在redis、mongodb、memcache等场景（很多小内存分配）是推荐关闭的，所以要根据不同的业务场景来选择开关。</p>
<p><strong>透明大页打开后在内存紧张的时候会触发sys飙高对业务会导致不可预期的抖动，同时存在已知内存泄漏的问题，我们建议是关掉的，如果需要使用，建议使用madvise方式或者hugetlbpage</strong></p>
<h2 id="一些内核版本、机型和CPU的总结"><a href="#一些内核版本、机型和CPU的总结" class="headerlink" title="一些内核版本、机型和CPU的总结"></a>一些内核版本、机型和CPU的总结</h2><p>到此终于看到不需要应用做什么改变，整体性能将近有30%的提升。 在这个测试过程中发现不同CPU对性能影响很明显，相同机型也有不同的CPU型号（性能差异在20%以上–这个太坑了）</p>
<p>性能方面 4.19&gt;4.9&gt;2.6.32</p>
<p>没有做3.10内核版本的比较</p>
<p>以下仅作为大家选择ECS的时候做参考。</p>
<h3 id="不同机型-CPU对性能的影响"><a href="#不同机型-CPU对性能的影响" class="headerlink" title="不同机型&#x2F;CPU对性能的影响"></a>不同机型&#x2F;CPU对性能的影响</h3><p>还是先说结论：</p>
<ul>
<li>CPU:内存为1:2机型的性能排序：c6-&gt;c5-&gt;sn1ne-&gt;hfc5-&gt;s1</li>
<li>CPU:内存为1:4机型的性能排序：g6-&gt;g5-&gt;sn2ne-&gt;hfg5-&gt;sn2</li>
</ul>
<p>性能差异主要来源于CPU型号的不同</p>
<pre><code>c6/g6:                  Intel(R) Xeon(R) Platinum 8269CY CPU @ 2.50GHz
c5/g5/sn1ne/sn2ne:      Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz
</code></pre>
<p>8269比8163大概好5-10%，价格便宜一点点，8163比E5-2682好20%以上，价格便宜10%（该买什么机型你懂了吧，价格是指整个ECS，而不是单指CPU）</p>
<p>要特别注意sn1ne&#x2F;sn2ne 是8163和E5-2682 两种CPU型号随机的，如果买到的是E5-2682就自认倒霉吧</p>
<p>C5的CPU都是8163，相比sn1ne价格便宜10%，网卡性能也一样。但是8核以上的sn1ne机型就把网络性能拉开了（价格还是维持c5便宜10%），从点查场景的测试来看网络不会成为瓶颈，到16核机型网卡多队列才会需要打开。</p>
<p>顺便给一下部分机型的包月价格比较：</p>
<p><img src="/images/oss/7c8b107fb12e285c8eab2c2d136bbd4e.png" alt="image.png"></p>
<p>官方给出的CPU数据：</p>
<p><img src="/images/oss/5f57f4228621378d14ffdd124fe54626.png" alt="image.png"></p>
<h2 id="4-19内核在MySQL-Server场景下的性能比较"><a href="#4-19内核在MySQL-Server场景下的性能比较" class="headerlink" title="4.19内核在MySQL Server场景下的性能比较"></a>4.19内核在MySQL Server场景下的性能比较</h2><p>这只是sysbench点查场景粗略比较，因为本次的目标是对X 产品性能的改进</p>
<p><img src="/images/oss/4f276e93cb914b3cdd312423be63c376.png" alt="image.png"></p>
<p>（以上表格数据主要由 内核团队和我一起测试得到）</p>
<p>**重点注意2.6.32不但tps差30%，并发能力也差的比较多，如果同样用100个并发压2.6.32上的MySQL，TPS在30000左右。只有在减少并发到20个的时候压测才能达到图中最好的tps峰值：45000. **</p>
<h2 id="新内核除了性能提升外带来的便利性"><a href="#新内核除了性能提升外带来的便利性" class="headerlink" title="新内核除了性能提升外带来的便利性"></a>新内核除了性能提升外带来的便利性</h2><p>升级内核带来的性能提升只是在极端场景下才会需要，大部分时候我们希望节省开发人员的时间，提升工作效率。于是X 产品在新内核的基础上定制如下一些便利的工具。</p>
<h3 id="麻烦的网络重传率"><a href="#麻烦的网络重传率" class="headerlink" title="麻烦的网络重传率"></a>麻烦的网络重传率</h3><p>通过tsar或者其它方式发现网络重传率有点高，有可能是别的管理端口重传率高，有可能是往外连其它服务端口重传率高等，尤其是在整体流量小的情况下一点点管理端口的重传包拉升了整个机器的重传率，严重干扰了问题排查，所以需要进一步确认重传发生在哪个进程的哪个端口上，是否真正影响了我们的业务。</p>
<p>在2.6.32内核下的排查过程是：抓包，然后写脚本分析（或者下载到本地通过wireshark分析），整个过程比较麻烦，需要的时间也比较长。那么在新镜像中我们可以利用内核自带的bcc来快速得到这些信息</p>
<pre><code>sudo /usr/share/bcc/tools/tcpretrans -l
</code></pre>
<p><img src="/images/oss/c68cc22b2e6eb7dd51d8613c5e79e88c.png" alt="image.png"></p>
<p>从截图可以看到重传时间、pid、tcp四元组、状态，针对重传发生的端口和阶段（SYN_SENT握手、ESTABLISHED）可以快速推断导致重传的不同原因。</p>
<p>再也不需要像以前一样抓包、下载、写脚本分析了。</p>
<h3 id="通过perf-top直接看Java函数的CPU消耗"><a href="#通过perf-top直接看Java函数的CPU消耗" class="headerlink" title="通过perf top直接看Java函数的CPU消耗"></a>通过perf top直接看Java函数的CPU消耗</h3><p>这个大家都比较了解，不多说，主要是top的时候能够把java函数给关联上，直接看截图：</p>
<pre><code>sh ~/tools/perf-map-agent/bin/create-java-perf-map.sh pid
sudo perf top
</code></pre>
<p><img src="/images/oss/1568775788220-32745082-5155-4ecd-832a-e814a682c0df.gif"></p>
<h3 id="快速定位Java中的锁等待"><a href="#快速定位Java中的锁等待" class="headerlink" title="快速定位Java中的锁等待"></a>快速定位Java中的锁等待</h3><p>如果CPU跑不起来，可能会存在锁瓶颈，需要快速找到它们</p>
<p>如下测试中上面的11万tps是解决掉锁后得到的，下面的4万tps是没解决锁等待前的tps：</p>
<pre><code>#[ 210s] threads: 400, tps: 0.00, reads/s: 115845.43, writes/s: 0.00, response time: 7.57ms (95%)
#[ 220s] threads: 400, tps: 0.00, reads/s: 116453.12, writes/s: 0.00, response time: 7.28ms (95%)
#[ 230s] threads: 400, tps: 0.00, reads/s: 116400.31, writes/s: 0.00, response time: 7.33ms (95%)
#[ 240s] threads: 400, tps: 0.00, reads/s: 116025.35, writes/s: 0.00, response time: 7.48ms (95%)

#[ 250s] threads: 400, tps: 0.00, reads/s: 45260.97, writes/s: 0.00, response time: 29.57ms (95%)
#[ 260s] threads: 400, tps: 0.00, reads/s: 41598.41, writes/s: 0.00, response time: 29.07ms (95%)
#[ 270s] threads: 400, tps: 0.00, reads/s: 41939.98, writes/s: 0.00, response time: 28.96ms (95%)
#[ 280s] threads: 400, tps: 0.00, reads/s: 40875.48, writes/s: 0.00, response time: 29.16ms (95%)
#[ 290s] threads: 400, tps: 0.00, reads/s: 41053.73, writes/s: 0.00, response time: 29.07ms (95%)
</code></pre>
<p>下面这行命令得到如下等锁的top 10堆栈（<a target="_blank" rel="noopener" href="https://github.com/jvm-profiling-tools/async-profiler">async-profiler</a>）：</p>
<pre><code>$~/tools/async-profiler/profiler.sh -e lock -d 5 1560

--- 1687260767618 ns (100.00%), 91083 samples
 [ 0] ch.qos.logback.classic.sift.SiftingAppender
 [ 1] ch.qos.logback.core.AppenderBase.doAppend
 [ 2] ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders
 [ 3] ch.qos.logback.classic.Logger.appendLoopOnAppenders
 [ 4] ch.qos.logback.classic.Logger.callAppenders
 [ 5] ch.qos.logback.classic.Logger.buildLoggingEventAndAppend
 [ 6] ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus
 [ 7] ch.qos.logback.classic.Logger.info
 [ 8] com.*****.logger.slf4j.Slf4jLogger.info
 [ 9] com.*****.utils.logger.support.FailsafeLogger.info
 [10] com.*****.util.LogUtils.recordSql



&quot;ServerExecutor-3-thread-480&quot; #753 daemon prio=5 os_prio=0 tid=0x00007f8265842000 nid=0x26f1 waiting for monitor entry [0x00007f82270bf000]
  java.lang.Thread.State: BLOCKED (on object monitor)
	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:64)
	- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)
	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:48)
	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:282)
	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:269)
	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:470)
	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:424)
	at ch.qos.logback.classic.Logger.info(Logger.java:628)
	at com.****.utils.logger.slf4j.Slf4jLogger.info(Slf4jLogger.java:42)
	at com.****.utils.logger.support.FailsafeLogger.info(FailsafeLogger.java:102)
	at com.****.util.LogUtils.recordSql(LogUtils.java:115)

          ns  percent  samples  top
  ----------  -------  -------  ---
160442633302   99.99%    38366  ch.qos.logback.classic.sift.SiftingAppender
    12480081    0.01%       19  java.util.Properties
     3059572    0.00%        9  com.***.$$$.common.IdGenerator
      244394    0.00%        1  java.lang.Object
</code></pre>
<p>堆栈中也可以看到大量的：<br>	<br>	  - waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- locked &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)<br>		- waiting to lock &lt;0x00007f866dcec208&gt; (a ch.qos.logback.classic.sift.SiftingAppender)</p>
<p>当然还有很多其他爽得要死的命令，比如一键生成火焰图等，不再一一列举，可以从业务层面的需要从这次镜像升级的便利中将他们固化到镜像中，以后排查问题不再需要繁琐的安装、配置、调试过程了。</p>
<h2 id="跟内核无关的应用层的优化"><a href="#跟内核无关的应用层的优化" class="headerlink" title="跟内核无关的应用层的优化"></a>跟内核无关的应用层的优化</h2><p>到此我们基本不用任何改动得到了30%的性能提升，但是对整个应用来说，通过以上工具让我们看到了一些明显的问题，还可以从应用层面继续提升性能。</p>
<p>如上描述通过锁排序定位到logback确实会出现锁瓶颈，同时在一些客户场景中，因为网盘的抖动也带来了灾难性的影响，所以日志需要异步处理，经过异步化后tps 达到了32000，关键的是rt 95线下降明显，这个rt下降对X 产品这种Proxy类型的应用是非常重要的（经常被客户指责多了一层转发，rt增加了）。</p>
<p>日志异步化和使用协程后的性能数据：</p>
<p><img src="/images/oss/bec4e8105091bc4b8a263aef245c0ce9.png" alt="image.png"></p>
<h3 id="Wisp2-协程带来的红利"><a href="#Wisp2-协程带来的红利" class="headerlink" title="Wisp2 协程带来的红利"></a>Wisp2 协程带来的红利</h3><p>在整个测试过程中都很顺利，只是<strong>发现Wisp2在阻塞不明显的场景下，抖的厉害</strong>。简单来说就是压力比较大的话Wisp2表现很稳定，一旦压力一般（这是大部分应用场景），Wisp2表现像是一会是协程状态，一会是没开协程状态，系统的CS也变化很大。</p>
<p>比如同一测试过程中tps抖动明显，从15000到50000：</p>
<p><img src="/images/oss/1550cc74116a56220d25e1434a675d14.png" alt="image.png"></p>
<p>100个并发的时候cs很小，40个并发的时候cs反而要大很多：</p>
<p><img src="/images/oss/3f79909f89889459d1f0dfe4fa0a2f53.png" alt="image.png"></p>
<p>最终在 @梁希 同学的攻关下发布了新的jdk版本，问题基本都解决了。不但tps提升明显，rt也有很大的下降。</p>
<h2 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h2><p>感谢 OS&#x2F;Ali JDK 团队对这次内核版本升级的支持。</p>
<p>最终应用不需要任何改动可以得到 30%的性能提升，经过开启协程等优化后应用有将近80%的性能提升，同时平均rt下降了到原来的60%，rt 95线下降到原来的40%。</p>
<p>快点升级你们的内核，用上协程吧。同时考虑下在你们的应用中用上X 产品。</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p><a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/25378.html">https://help.aliyun.com/document_detail/25378.html</a></p>
<p><a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/55263.html">https://help.aliyun.com/document_detail/55263.html</a></p>
<p><a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/52559.html">https://help.aliyun.com/document_detail/52559.html</a> (网卡)</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/10/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/12/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
