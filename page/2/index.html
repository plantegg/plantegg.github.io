<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1">






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/2/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/2/">





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/10/03/time_zone是怎么打爆你的MySQL的/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/10/03/time_zone是怎么打爆你的MySQL的/" itemprop="url">time_zone 是怎么打爆你的MySQL的</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-10-03T17:30:03+08:00">
                2023-10-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="time-zone-是怎么打爆你的MySQL的"><a href="#time-zone-是怎么打爆你的MySQL的" class="headerlink" title="time_zone 是怎么打爆你的MySQL的"></a>time_zone 是怎么打爆你的MySQL的</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><p>这篇关于time_zone 的总结写得非常好<a href="https://opensource.actionsky.com/20211214-time_zone/" target="_blank" rel="noopener">Time_zone</a> ，建议先读完做个基础知识的打底</p>
<h3 id="Mysql日期和时间存储数据类型"><a href="#Mysql日期和时间存储数据类型" class="headerlink" title="Mysql日期和时间存储数据类型"></a><strong>Mysql日期和时间存储数据类型</strong></h3><table>
<thead>
<tr>
<th><strong>存储类型</strong></th>
<th><strong>存储值示例</strong></th>
<th><strong>解释</strong></th>
<th><strong>适用场景</strong></th>
</tr>
</thead>
<tbody><tr>
<td>Datetime</td>
<td>YYYY-MM-DD HH:MM:SS</td>
<td>时间日期类型。DB时区切换它的值不<strong>时区相关常见名词解释</strong>变, 但时区切换后代表的时间信息已改变.</td>
<td>使用简单、直观、方便。适用于无需考虑时区的业务场景，例如国内业务</td>
</tr>
<tr>
<td>Timestamp</td>
<td><strong>名词</strong>1547077063000</td>
<td><strong>解释</strong>以UTC时间戳来保存, DB时区切换它代表的时间信息值不会变，但是会随着连接会话的时区变化而变化。 内部以4个字节储存, 最大值可表示到2037年.</td>
<td>适用于客户端需要支持多时区自适应的场景，因精度有限，不推荐使用</td>
</tr>
<tr>
<td>Date</td>
<td>GMTYYYY-MM-DD</td>
<td>全称Greenwich Mean Time 格林威治（也称：格林尼治）时间，也叫世界时（Universal Time），也叫世界标准时间。是指位于英国伦敦郊区的【皇家格林尼治天文台】的标准时间，是本初子午线上的地方时，是0时区的区时。GMT格林威治时间可认为是以前的标准时间日期类型，不包含时间信息</td>
<td>不关注时区只需要显示日期的场景</td>
</tr>
<tr>
<td>Varchar</td>
<td>UTCYYYY-MM-DD HH:MM:SS</td>
<td>全称Coodinated Universal Time 协调世界时，又称世界统一时间、世界标准时间、国际协调时间。它是以原子时(物质的原子内部发射的电磁振荡频率为基准的时间计量系统)作为计量单位的时间，计算结果极其严谨和精密。它比GMT时间更来得精准，误差值必须保持在0.9秒以内，倘若大于0.9秒就会通过闰秒来”解决”。UTC时间是现在使用的世界时间标准字符串类型，可以用时间字符串来表示日期时间类型，格式可自定义，如果用ISO标准时间格式存储，则可以包含时区信息：yyyy-MM-dd’T’HH:mm:ss.SSS+HH:MM</td>
<td>自定义存储日期时间格式，可包含时区信息，适用于只需要显示时间的场景，不方便计算</td>
</tr>
<tr>
<td>Bigint</td>
<td>DST1547077063000</td>
<td>Daylight Saving Time的简称，又称“日光节约时制”和“夏令时间”，也叫夏时制。表示为了节约能源，人为规定时间的意思。在这一制度实行期间所采用的统一时间称为“夏令时间”,在欧洲和北美用得比较多数字类型，可以存储时间戳，表示某个时刻，稳定性最好</td>
<td>存储某个时刻，可以表达时间的确定性，存储&#x2F;网络传输稳定性最好</td>
</tr>
</tbody></table>
<h3 id="时区相关常见名词解释"><a href="#时区相关常见名词解释" class="headerlink" title="时区相关常见名词解释"></a><strong>时区相关常见名词解释</strong></h3><table>
<thead>
<tr>
<th><strong>名词</strong></th>
<th><strong>解释</strong></th>
</tr>
</thead>
<tbody><tr>
<td>GMT</td>
<td>全称Greenwich Mean Time 格林威治（也称：格林尼治）时间，也叫世界时（Universal Time），也叫世界标准时间。是指位于英国伦敦郊区的【皇家格林尼治天文台】的标准时间，是本初子午线上的地方时，是0时区的区时。GMT格林威治时间可认为是以前的标准时间</td>
</tr>
<tr>
<td>UTC</td>
<td>全称Coodinated Universal Time 协调世界时，又称世界统一时间、世界标准时间、国际协调时间。它是以原子时(物质的原子内部发射的电磁振荡频率为基准的时间计量系统)作为计量单位的时间，计算结果极其严谨和精密。它比GMT时间更来得精准，误差值必须保持在0.9秒以内，倘若大于0.9秒就会通过闰秒来”解决”。UTC时间是现在使用的世界时间标准</td>
</tr>
<tr>
<td>DST</td>
<td>Daylight Saving Time的简称，又称“日光节约时制”和“夏令时间”，也叫夏时制。表示为了节约能源，人为规定时间的意思。在这一制度实行期间所采用的统一时间称为“夏令时间”,在欧洲和北美用得比较多</td>
</tr>
<tr>
<td>PDT</td>
<td>全称Pacific Daylight Time太平洋夏季时间，也称夏令时。每年的3月份第二个星期日凌晨2点开始至11月份第一个星期日凌晨2点结束，第一天23个小时。「北美的西海岸太平洋沿岸地区，大城市有：温哥华，西雅图，旧金山，洛杉矶，拉斯×××，圣迭戈，萨克拉门托，波特兰等」</td>
</tr>
<tr>
<td>PST</td>
<td>全称Pacific Standard Time太平洋标准时间，也称冬令时。从11月份第一个星期日凌晨2点开始至次年3月份第二个星期日凌晨2点结束，第一天25个小时。</td>
</tr>
<tr>
<td>CST</td>
<td>CST可视为中国、古巴的标准时间或美国、澳大利亚的中部时间CST可以表示如下4个不同的时区的缩写：中国标准时间：China Standard Time UT+8:00古巴标准时间：Cuba Standard Time UT-4:00美国中部时间：Central Standard Time (USA) UT-6:00澳大利亚中部时间：Central Standard Time (Australia) UT+9:30因此具体含义需要根据上下文环境确定具体含义。在中国就表示东八区”北京时间”</td>
</tr>
<tr>
<td>UTC+08:00</td>
<td>基于UTC标准时间的时区偏移量，可表示东八区。UTC±[hh]:[mm]形式表示某个时区的区时，由UTC和偏移量组成。UTC+08:00就表示东八区时区的本地时间 &#x3D; 世界协调时间UTC + 时区偏移量(+8h)</td>
</tr>
<tr>
<td>ISO</td>
<td>在时间日期上它全称是ISO 8601，是一种日期&#x2F;时间表示方法的规范。规定了一种明确的、国际上都能理解的日历和时钟格式。在Java语言中常见格式：●<strong>ISO.DATE</strong>：yyyy-MM-dd, e.g. “2023-02-03”●<strong>ISO.TIME</strong>：HH:mm:ss.SSSXXX, e.g. “10:30:00.000-11:00”●<strong>ISO.DATE_TIME</strong>：yyyy-MM-dd’T’HH:mm:ss.SSSXXX, e.g. “2022-10-31T01:30:00.000-05:00”.</td>
</tr>
<tr>
<td>时间戳</td>
<td>时间戳一般指的UNIX时间，或类UNIX系统（比如Linux、macOS等）使用的时间表示方式。定义为：从UTC时间的1970-1-1 0:0:0起到现在的总秒数（秒是毫秒、微妙、纳秒的总称），可简单理解为某个时刻</td>
</tr>
</tbody></table>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>一般MySQL Server 实例都会设置 time_zone 为 system，方便实例部署在不同的国家、时区也都能很好兼容，这是很合理的设置。</p>
<p>如果我们的 SQL 中有一个列类型是 timestamp 的话，意味着：</p>
<blockquote>
<p>timestamp 数据类型会存储当时 session的时区信息，读取时会根据当前 session 的时区进行转换；而 datetime 数据类型插入的是什么值，再读取就是什么值，不受时区影响。也可以理解为已经存储的数据是不会变的，只是 timestamp 类型数据在读取时会根据时区转换</p>
</blockquote>
<p>如果MySQL 读取 timestamp 字段时，需要做时区转换，当 time_zone 设置为 system 时，意味着MySQL 要去follow OS系统时区，也就是把读到的timestamp 根据OS系统时区进行转换，这个转换调用OS 的glibc 的时区函数来获取 Linux OS 的时区，在这个函数中会加 mutex 锁，当并发高时，会出现 mutex 竞争激烈，每次只有一个线程获得锁，释放锁时会唤醒所有等锁线程，但最终只有一个能获取，于是一下子导致系统 sys飙高、上下文切换飙高。每读取一行带 timestamp 字段时，都会通过这个 glibc 的时区函数导致锁竞争特别激烈最终 QPS 拉胯厉害。</p>
<p>想一想，你一个SQL查1万行，10个并发这点流量其实一点都不过分，但是这里需要10*1万次转换，锁争抢就激烈了。</p>
<p>分析参考这个： <a href="https://opensource.actionsky.com/20191112-mysql/" target="_blank" rel="noopener">https://opensource.actionsky.com/20191112-mysql/</a></p>
<p>perf 以及火焰图如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230830101924021.png" alt="image-20230830101924021"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1682500623154-5834bcae-4c74-4c72-bfec-f67717f71c91.png" alt="img"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/timezone-6184549.png" alt="timezone"></p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>在中国可以将 time_zone&#x3D;’+8:00’ 将 time_zone 固定死，不再需要follow OS 时区，所以也不需要调用glibc 获取系统时区，避免了前面所说的锁竞争</p>
<p>这个经验来自无数次线上故障复盘，因为 time_zone 设置为 system 是个默认行为，所以要全部改过来还真不容易，给了我们就业机会 :)</p>
<p>当然学习总是希望交叉起来，既有深度又有宽度你才能掌握更好，所以请趁热打铁：</p>
<h2 id="进一步学习"><a href="#进一步学习" class="headerlink" title="进一步学习"></a>进一步学习</h2><p><a href="https://juejin.cn/post/7029291622537887774" target="_blank" rel="noopener">东八区CST 被JDBC 驱动错误</a>识别成了美国的中央时间，<a href="https://dev.mysql.com/doc/relnotes/connector-j/8.0/en/news-8-0-23.html" target="_blank" rel="noopener">官方修复</a></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>在我们 99块钱的 ECS 启动一个MySQL 的Docker 容器(配置简单，换MySQL版本对比也方便)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --name mysql --network host -v /plantegg/test/my.cnf:/etc/my.cnf   -e MYSQL_ROOT_PASSWORD=123 -d mysql --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci</span><br></pre></td></tr></table></figure>

<p>my.cnf配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"># cat /plantegg/test/my.cnf</span><br><span class="line">[mysqld]</span><br><span class="line">#skip_grant_tables=1</span><br><span class="line"></span><br><span class="line">#innodb</span><br><span class="line">innodb_flush_log_at_trx_commit=0</span><br><span class="line">innodb_buffer_pool_instances=8</span><br><span class="line">innodb_max_dirty_pages_pct=60</span><br><span class="line">innodb_io_capacity=6000</span><br><span class="line">innodb_open_files=615350</span><br><span class="line">innodb_buffer_pool_size = 64G</span><br><span class="line"></span><br><span class="line">#binlog</span><br><span class="line">binlog_cache_size=32K</span><br><span class="line">max_binlog_cache_size=2147483648</span><br><span class="line">max_binlog_size=1000M</span><br><span class="line">sync_binlog=0</span><br><span class="line"></span><br><span class="line">#for manager</span><br><span class="line">#lower_case_table_names=1</span><br><span class="line">#sql_mode = &apos;NO_ENGINE_SUBSTITUTION&apos;</span><br><span class="line"></span><br><span class="line">slow_query_log=0</span><br><span class="line">general_log=0</span><br><span class="line">default_authentication_plugin=mysql_native_password</span><br><span class="line"></span><br><span class="line">max_connections=2000</span><br><span class="line">max_user_connections=2000</span><br><span class="line">max_connect_errors=65536</span><br><span class="line">max_allowed_packet=1073741824</span><br><span class="line">connect_timeout=8</span><br><span class="line">net_read_timeout=30</span><br><span class="line">net_write_timeout=60</span><br><span class="line">back_log=1024</span><br><span class="line"></span><br><span class="line"># Disabling symbolic-links is recommended to prevent assorted security risks</span><br><span class="line">symbolic-links=0</span><br><span class="line"></span><br><span class="line">log-error=/var/log/mysqld.log</span><br><span class="line">pid-file=/var/run/mysqld/mysqld.pid</span><br><span class="line">datadir=/var/lib/mysql</span><br><span class="line">socket=/var/run/mysqld/mysqld.sock</span><br><span class="line">secure-file-priv=/var/lib/mysql-files</span><br><span class="line">#user=mysql</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">#socket=/var/run/mysqld/mysqld.sock</span><br></pre></td></tr></table></figure>

<p>先创建一个Database ren，然后在里面再创建一个表t，如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CREATE DATABASE `ren` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci */ /*!80016 DEFAULT ENCRYPTION=&apos;N&apos; */</span><br><span class="line"></span><br><span class="line">Create Table: CREATE TABLE `t` (</span><br><span class="line">  `ts` timestamp NULL DEFAULT NULL,</span><br><span class="line">  `dt` datetime DEFAULT NULL</span><br><span class="line">) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci</span><br></pre></td></tr></table></figure>

<p>插入一条数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">insert into t values(&apos;2021-12-02 16:45:39&apos;,&apos;2021-12-02 16:45:39&apos;);</span><br><span class="line"></span><br><span class="line">//然后反复多次执行如下SQL，让数据条数不断翻倍，直到100万条左右</span><br><span class="line">insert into t select * from t;</span><br><span class="line"></span><br><span class="line">MySQL [ren]&gt; select count(*) from t;</span><br><span class="line">+----------+</span><br><span class="line">| count(*) |</span><br><span class="line">+----------+</span><br><span class="line">|  1048576 |</span><br><span class="line">+----------+</span><br><span class="line">1 row in set (0.060 sec)</span><br><span class="line"></span><br><span class="line">MySQL [ren]&gt; show global variables like &apos;%zone%&apos;;</span><br><span class="line">+------------------+--------+</span><br><span class="line">| Variable_name    | Value  |</span><br><span class="line">+------------------+--------+</span><br><span class="line">| system_time_zone | UTC    |</span><br><span class="line">| time_zone        | SYSTEM |    —————— 注意这里默认是System,</span><br><span class="line">+------------------+--------+</span><br><span class="line">2 rows in set (0.002 sec)</span><br></pre></td></tr></table></figure>

<p>现在我们基本得到了一个100万行的测试表，接下来就要实验验证，同时开30个并发来查 ts列(timestamp， 要做时区转换) VS 查 dt 列（不需要做时区转换），对比他们的效率：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for i in &#123;1..30&#125;; do  (time mysql -h127.0.0.1 -P3306 -uroot -p123 ren -e &quot; select ts from t &quot; &gt;&gt;tmp ) &amp; done</span><br><span class="line"></span><br><span class="line">---上面30次 select每次基本需要20多秒，下面30次 select 每次基本需要4秒左右，性能差异有5倍</span><br><span class="line">for i in &#123;1..30&#125;; do  (time mysql -h127.0.0.1 -P3306 -uroot -p123 ren -e &quot; select dt from t &quot; &gt;&gt;tmp ) &amp; done</span><br></pre></td></tr></table></figure>

<p>可以清楚地看到 查ts 需要20秒左右，查 dt 需要4秒左右，差了5倍，结合我们前面的理论讲解，肯定可以想到这是在做时区转换有额外的开销，其实这还好只是开销大了几倍，有没有一种可能因为glibc 加锁导致整个系统雪崩了？大家可以试试能否搞出雪崩的场景来</p>
<p>Perf 安装和使用命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install perf -y</span><br><span class="line">perf top -p mysqld-pid</span><br></pre></td></tr></table></figure>

<p>比如用 perf top 可以看到查 timestamp 才有如下图前两行的 futex_wait_setup&#x2F;libc-2.28 的wcscoll_l , 对比一下查 datetime 是完全看不到这些内核、libc的消耗的：</p>
<p>64 核机器 CPU  使用率(CPU 核数越多反而性能越差)</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20241012175933890.png" alt="image-20241012175910202"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog//image-20241012175910202.png" alt="image-20241012175910202"></p>
<p>如果只给 MySQLD 4 核反而跑得更快了，明显在锁上的 CPU 消耗降低了很多：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20241012180110212.png" alt="image-20241012175910202"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20241012180131605.png" alt="image-20241012175910202"></p>
<p>停下来花点时间来分析他们的性能差异，多结合理论表述这篇，另外反过来想想如果你不知道这个原因，但是你看到这个现象(timestamp 和 datetime 性能差5倍的时候)，你怎么来分析是为什么？</p>
<h3 id="96核环境下对比"><a href="#96核环境下对比" class="headerlink" title="96核环境下对比"></a>96核环境下对比</h3><p>找个核数多的机器做同样的测试，比如以下数据是在96核机器上完成，让锁竞争更激烈，实际是把问题更加明显化</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240308084631703.png" alt="image-20240308084631703"></p>
<p>从 top 也可以看到CPU 都花在了sys(内核态系统调用)上，这明显是不符合逻辑的。同时也可以看到96个核基本都跑满，整个MySQLD 进程的CPU 消耗接近 9600%，又回到了我们常说的CPU 有使用没有效率，不过站在CPU的角度是有效率的，这个效率都是在做锁相关的事情，但是站在业务角度是没有效率的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240308084600599.png" alt="image-20240308084600599"></p>
<p>在这么明显的不正常情况下可以进一步通过 perf record 来采集调用关系，通过调用关系来回溯是哪个函数导致的锁争抢从而找到问题</p>
<p>从这里也可以看到这个问题在不同核数下表现完全不一样，如果只是一个核很少的实例那么看起来问题还没那么明显，只是慢了，但是到了96核下这个SQL 反而全崩了，这个SQL 完全查不出结果(CPU 都在sy 上干抢锁的内耗上，能查出才怪)，核数越多这个问题就越严重，也即内耗越严重，如果有业务流量源源不断地进来就类似雪崩了：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># mysql -h127.0.0.1 -P3306 -uroot -p123 ren -e &quot;show processlist&quot;</span><br><span class="line">+------+-----------------+-----------------+------+---------+---------+------------------------+------------------+</span><br><span class="line">| Id   | User            | Host            | db   | Command | Time    | State                  | Info             |</span><br><span class="line">+------+-----------------+-----------------+------+---------+---------+------------------------+------------------+</span><br><span class="line">| 1195 | root            | localhost:46528 | ren  | Query   |     556 | executing              | select ts from t |</span><br><span class="line">| 1196 | root            | localhost:46530 | ren  | Query   |     556 | Sending to client      | select ts from t |</span><br><span class="line">| 1197 | root            | localhost:46532 | ren  | Query   |     556 | executing              | select ts from t |</span><br><span class="line">| 1198 | root            | localhost:46538 | ren  | Query   |     556 | executing              | select ts from t |</span><br><span class="line">| 1199 | root            | localhost:46540 | ren  | Query   |     556 | executing              | select ts from t |</span><br></pre></td></tr></table></figure>

<h2 id="代码分析"><a href="#代码分析" class="headerlink" title="代码分析"></a>代码分析</h2><p>从下图可以看到 <strong>70%</strong> 的 CPU 时间消耗在 <strong>Time_zone_system::gmt_sec_to_TIME()</strong>  方法的调用上，就是以下这一段的代码。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/timezone-6184549-8719589.png" alt="timezone"></p>
<p>对应代码：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Time_zone_system::gmt_sec_to_TIME</span><span class="params">(MYSQL_TIME *tmp, <span class="keyword">my_time_t</span> t)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">  <span class="class"><span class="keyword">struct</span> <span class="title">tm</span> <span class="title">tmp_tm</span>;</span></span><br><span class="line">  <span class="keyword">time_t</span> <span class="keyword">tmp_t</span> = (<span class="keyword">time_t</span>)t;</span><br><span class="line">  localtime_r(&amp;<span class="keyword">tmp_t</span>, &amp;tmp_tm);</span><br><span class="line">  localtime_to_TIME(tmp, &amp;tmp_tm);</span><br><span class="line">  tmp-&gt;time_type = MYSQL_TIMESTAMP_DATETIME;</span><br><span class="line">  adjust_leap_second(tmp);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//继续下钻来看一下 __tz_convert()  的实现，代码如下</span></span><br><span class="line"><span class="comment">/* Return the `struct tm' representation of *TIMER in the local timezone.</span></span><br><span class="line"><span class="comment">   Use local time if USE_LOCALTIME is nonzero, UTC otherwise.  */</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">tm</span> *</span></span><br><span class="line"><span class="class">__<span class="title">tz_convert</span> (<span class="title">const</span> <span class="title">time_t</span> *<span class="title">timer</span>, <span class="title">int</span> <span class="title">use_localtime</span>, <span class="title">struct</span> <span class="title">tm</span> *<span class="title">tp</span>)</span></span><br><span class="line"><span class="class">&#123;</span></span><br><span class="line">  <span class="keyword">long</span> <span class="keyword">int</span> leap_correction;</span><br><span class="line">  <span class="keyword">int</span> leap_extra_secs;</span><br><span class="line">  <span class="keyword">if</span> (timer == <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      __set_errno (EINVAL);</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  __libc_lock_lock (tzset_lock);</span><br><span class="line">  <span class="comment">/* Update internal database according to current TZ setting.</span></span><br><span class="line"><span class="comment">     POSIX.1 8.3.7.2 says that localtime_r is not required to set tzname.</span></span><br><span class="line"><span class="comment">     This is a good idea since this allows at least a bit more parallelism.  */</span></span><br><span class="line">  tzset_internal (tp == &amp;_tmbuf &amp;&amp; use_localtime, <span class="number">1</span>);</span><br><span class="line">  <span class="keyword">if</span> (__use_tzfile)</span><br><span class="line">    __tzfile_compute (*timer, use_localtime, &amp;leap_correction,</span><br><span class="line">		      &amp;leap_extra_secs, tp);</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span> (! __offtime (timer, <span class="number">0</span>, tp))</span><br><span class="line">	tp = <span class="literal">NULL</span>;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">	__tz_compute (*timer, tp, use_localtime);</span><br><span class="line">      leap_correction = <span class="number">0L</span>;</span><br><span class="line">      leap_extra_secs = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="keyword">if</span> (tp)</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="keyword">if</span> (! use_localtime)</span><br><span class="line">	&#123;</span><br><span class="line">	  tp-&gt;tm_isdst = <span class="number">0</span>;</span><br><span class="line">	  tp-&gt;tm_zone = <span class="string">"GMT"</span>;</span><br><span class="line">	  tp-&gt;tm_gmtoff = <span class="number">0L</span>;</span><br><span class="line">	&#125;</span><br><span class="line">      <span class="keyword">if</span> (__offtime (timer, tp-&gt;tm_gmtoff - leap_correction, tp))</span><br><span class="line">        tp-&gt;tm_sec += leap_extra_secs;</span><br><span class="line">      <span class="keyword">else</span></span><br><span class="line">	tp = <span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  __libc_lock_unlock (tzset_lock);</span><br><span class="line">  <span class="keyword">return</span> tp;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代码最终会使用 atomic_compare_and_exchange_val_24_acq() 尝试对 <strong>futex</strong> 加锁。</p>
<p>而 <strong>futex</strong> 作为多个 thread 间共享的一块内存区域在多个 client thread（多个会话&#x2F;查询）竞争的场景下会引发系统调用而进入系统态，导致 SYS 系统态 CPU 使用率上升。</p>
<p>并且该临界区保护的锁机制限制了时区转换方法 <strong>__tz_convert()</strong> 的并发度，进而出现多个会话&#x2F;查询 等待获取锁进入临界区的情况，当冲突争抢激烈的场景下引发卡顿</p>
<h2 id="进一步验证"><a href="#进一步验证" class="headerlink" title="进一步验证"></a>进一步验证</h2><p>这几个算是你可以接着做的一些小任务</p>
<ul>
<li>将 time_zone 从system 改成 ‘+08:00’ 再查 timestamp 列看看是不是就不存在这个问题了，反复改来改去稳定确认</li>
<li>换MySQL 5.6&#x2F;5.7试试这个问题，默认用的MySQL 8.0</li>
<li>换低版本的OS 内核试试这个问题，我测试用的5.10，你可以试试3.10</li>
<li>构造雪崩，也就是随着并发、行数的加大系统陷入抢锁等锁，基本无法响应业务查询了</li>
<li>在这个基础上，各种折腾、折腾，会折腾就是能力</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>借着 MySQL 的时区转换我们把这个问题重现了，让我们通过实际测试来验证这个差异，下次我相信你会对这个问题印象深刻的</p>
<p>当然我们做这个实验不是为了证明这个问题，这个知识点本身价值不是特别大，而是希望你能学到：</p>
<ol>
<li>设计实验，根据你的目的设计实验</li>
<li>根据实验重现的现象反过去分析为什么——虽然你知道原因，但是如果不知道你会怎么思考</li>
<li>尝试分析问题的手段、技巧，比如 perf、比如for 循环</li>
</ol>
<p>希望能看到你们更多的不同实验现象和分析</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/09/24/localhost和127.0.0.1的区别/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/09/24/localhost和127.0.0.1的区别/" itemprop="url">localhost和127.0.0.1的区别</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-09-24T17:30:03+08:00">
                2023-09-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="localhost和127-0-0-1的区别"><a href="#localhost和127-0-0-1的区别" class="headerlink" title="localhost和127.0.0.1的区别"></a>localhost和127.0.0.1的区别</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>有人告诉我localhost和127.0.0.1的区别是localhost 不经过网卡，所以性能也高。把我惊到了，因为我还真不知道这个知识点，于是去特别去验证了一下：这是个错误的理解。正确的解释是：localhost会解析成127.0.0.1 然后接下来的流程和127.0.0.1 一模一样</p>
<p>我用Google搜了下标题，果然得到如下图:</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910100147730.png" alt="image-20230910100147730"></p>
<p>红框里是排第一、第四的文章，都大言不惭地说localhost不经过网卡、不受防火墙限制等。</p>
<p>我也看了下第二、第三的文章，这两篇都是说在MySQL命令行中连 localhost 的时候，MySQL命令行会判断 localhost 这个字符串认为是本地连接然后不走DNS 解析流程(走的话就肯定解析成了127.0.0.1)，从而绕过OS 的内核网络协议栈用MySQLD 启动的时候生成的 unix-socket 管道直接连上MySQLD，这样效率更高。</p>
<p>错误信息大概就是在MySQL这个特殊场景下演变而来的，<strong>英文搜索就没有这个错误污染信息</strong></p>
<p>但这不是我要说的重点，我想说的是自己动手去求证！这一直都是我们星球里强调的能力和目标，我把<a href="https://twitter.com/plantegg/status/1700011179324920117" target="_blank" rel="noopener">这条发到Twitter上后有无数的初学者跑出来质疑或者一知半解不去验证就丢一个结论，这是我比较痛恨的</a>。比如：</p>
<ul>
<li><p>Localhost 写死了在 &#x2F;etc&#x2F;hosts(那我就要问，你清空&#x2F;etc&#x2F;hosts localhost还能工作吗？)</p>
</li>
<li><p>Localhost 不走网卡（但凡抓个包就知道走了，我估计他们抓了，抓的是eth0. 这里有个小小的歧义 loopback 本地回环网卡算不算网卡）</p>
</li>
</ul>
<p>所以我特意再写篇文章再验证下各种质疑，并让大家看看是怎么验证的，我希望你们可以跟着验证一遍而不是只要知道个结论</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Localhost 会按<a href="https://plantegg.github.io/2019/06/09/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/">dns解析流程进行解析</a>，然后和127.0.0.1 一样。在特殊的程序中比如 MySQL 命令行会对localhost提前做特别处理。</p>
<p>完整的区别见<a href="https://www.tutorialspoint.com/difference-between-localhost-and-127-0-0-1#:~:text=The%20most%20significant%20difference%20between,look%20up%20a%20table%20somewhere." target="_blank" rel="noopener">这篇英文</a>(Google 英文第一篇就是)总结：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910101843256.png" alt="image-20230910101843256"></p>
<h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><h3 id="问题1：经过网卡吗？"><a href="#问题1：经过网卡吗？" class="headerlink" title="问题1：经过网卡吗？"></a>问题1：经过网卡吗？</h3><p>Ping localhost&#x2F;127.0.0.1，然后 tcpdump -i any icmp or icmp6  [说明：any（抓所有网卡）icmp (精确点只抓ping包) ]，可以明显抓到网络包，所以你可以理解为经过网卡。这指的是这个网络包完整地经过了内核协议栈，加tcp包头、ip包头、mac 包头等等。</p>
<p>而很多人理解的不经过网卡是指不走内核协议栈(毕竟是本机)，加tcp包头、ip包头、mac 包头然后又脱mac包头、脱ip包头、tcp包头，有点像没必要的折腾。比如你通过unix socket 连就不走内核协议栈，性能要高一些</p>
<p>但<strong>严格来说是没经过物理意义上的网卡</strong>，因为 lo 是一块虚拟网卡，不需要插网线，不会真的走到网卡、网线然后回来。如果让内核重新设计，让127.0.0.1 不过经过内核协议栈行不行？我觉得是完全可以的，当时为什么这么设计我也不懂。</p>
<p>总之，<strong>我强调经过网卡是从完整经过了内核协议栈、用 tcpdump 能抓到这个概念上来说</strong>的，为了跟别人说用127.0.0.1比用本机物理IP 性能要好而言(实际没有区别)，你如果用本机物理IP 也同样走 lo 网卡</p>
<h3 id="问题2：localhost和127-0-0-1-的关系"><a href="#问题2：localhost和127-0-0-1-的关系" class="headerlink" title="问题2：localhost和127.0.0.1 的关系"></a>问题2：localhost和127.0.0.1 的关系</h3><p>如图是我在centos、微软azure(应该是个ubuntu)、macOS下做的测试：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910103644707.png" alt="image-20230910103644707"></p>
<h3 id="问题3：如果-x2F-etc-x2F-hosts-中没有写死-localhost-127-0-0-1-会怎么样？"><a href="#问题3：如果-x2F-etc-x2F-hosts-中没有写死-localhost-127-0-0-1-会怎么样？" class="headerlink" title="问题3：如果&#x2F;etc&#x2F;hosts 中没有写死 localhost 127.0.0.1 会怎么样？"></a>问题3：如果&#x2F;etc&#x2F;hosts 中没有写死 localhost 127.0.0.1 会怎么样？</h3><p>如下图，ping的时候即使没有 &#x2F;etc&#x2F;hosts 也可以把localhost 解析成127.0.0.1，为什么呢？所以接着我就 nslookup 看一下是哪个 DNS server做的这事，最后我用114.114.114.114 这个公网的DNS 做了解析，就不认识localhost了，说明去掉 &#x2F;etc&#x2F;hosts 之后 会把localhost 发给dns server解析，标准的dns(比如114.114.114.114,8.8.8.8) 都不会返回127.0.0.1 ，但是有些特定的实现为了省事帮你解析到127.0.0.1了</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910104133832.png" alt="image-20230910104133832"></p>
<h3 id="问题4：127-0-0-1比localhost少了查-x2F-etc-x2F-hsots-到底快多少"><a href="#问题4：127-0-0-1比localhost少了查-x2F-etc-x2F-hsots-到底快多少" class="headerlink" title="问题4：127.0.0.1比localhost少了查&#x2F;etc&#x2F;hsots 到底快多少?"></a>问题4：127.0.0.1比localhost少了查&#x2F;etc&#x2F;hsots 到底快多少?</h3><p>这个问题来自这个评论：<a href="https://twitter.com/InnerHack/status/1700012845302436087" target="_blank" rel="noopener">https://twitter.com/InnerHack/status/1700012845302436087</a>  所以我去验证了一下，特别强调这个数据意义不大，但是你们可以学会用strace，命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">strace -tt ping -c 1 localhost</span><br></pre></td></tr></table></figure>

<p>然后你得到如下图，从strace时间戳你可以看到 localhost 解析成127.0.0.1 的过程，再后面就是ping 127.0.0.1(这里也说明了前面的结论，两者是一样的，就是多了域名解析)</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910104733229.png" alt="image-20230910104733229"></p>
<p>域名解析的时候，先去找&#x2F;etc&#x2F;hosts 没找到再去找 &#x2F;etc&#x2F;resolv.conf 拿dns server ip然后把localhost发给这个dns  server 解析，tcpdump抓包如下，红框是dns server返回的结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230910105107629.png" alt="image-20230910105107629"></p>
<h3 id="问题5：127-0-0-1-和127-1-的关系"><a href="#问题5：127-0-0-1-和127-1-的关系" class="headerlink" title="问题5：127.0.0.1 和127.1 的关系"></a>问题5：127.0.0.1 和127.1 的关系</h3><p>127.1 会自动补全成127.0.0.1 </p>
<h3 id="问题6：为什么还是抓不到包"><a href="#问题6：为什么还是抓不到包" class="headerlink" title="问题6：为什么还是抓不到包"></a>问题6：为什么还是抓不到包</h3><p>ping localhost的时候没有包，只有127.1有，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240505103504490.png" alt="image-20240505103504490"></p>
<p>这是对提示信息敏感度不够，仔细看上图右下角的 ::1 这是个ipv6地址；也就是localhost被默认指向了这个 ipv6(localhost其实可以随便配置指向哪里，新一点的OS 默认都是指向 ipv6了)，抓包命令多加一个 icmp6  (一个协议名字，默认不抓这个协议) 就能抓到了：tcpdump -i any icmp6</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>唯有动手能解释一切，不要空逼逼(不是说你们，是说Twitter上那帮人，我是被他们留言多了逼着写了这篇)</p>
<p>我是欢迎一切有理有据的质疑，事实文中很多信息来源于别人的质疑，然后我去验证</p>
<p>然后好多验证手段你们可以学学，比如nslookup&#x2F;tcpdump&#x2F;strace 等。</p>
<p>我给的文章链接也可以仔细读读，能学到很多东西，每一次进步都来自你深挖、展开能力。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/08/28/解决问题思路/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/08/28/解决问题思路/" itemprop="url">解决Java/MySQL性能问题的思路</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-08-28T10:30:03+08:00">
                2023-08-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="解决-Java-x2F-MySQL-性能问题的思路"><a href="#解决-Java-x2F-MySQL-性能问题的思路" class="headerlink" title="解决 Java&#x2F;MySQL 性能问题的思路"></a>解决 Java&#x2F;MySQL 性能问题的思路</h1><p>10年前写的，重新发一下</p>
<h2 id="系统性能问题"><a href="#系统性能问题" class="headerlink" title="系统性能问题"></a>系统性能问题</h2><ul>
<li>CPU（基本上WEB服务器没有多少IO，主要是CPU有瓶颈）<ul>
<li>top&#x2F;vmstat 观察CPU使用率，Load负载，r&#x2F;b线程数量等；</li>
<li>IO（数据库大多数时候瓶颈是IO，主要是索引没建好；如果数据库CPU紧张的话，检查一下是不是order by&#x2F;group by 等操作太多）</li>
<li>vmstat 观察IO&#x2F;Util吞吐，磁盘最怕随机读写了（比如：索引命中后，需要离散地从磁盘读数据）</li>
<li>对于数据库来说最怕内存不够的时候使用Swap了，所以尽量增大分配给数据库的内存，一旦有Swap就要引起注意了</li>
</ul>
</li>
</ul>
<h2 id="Java程序问题（运行慢）"><a href="#Java程序问题（运行慢）" class="headerlink" title="Java程序问题（运行慢）"></a>Java程序问题（运行慢）</h2><p>​    先通过 top 查看整个CPU资源使用情况；<br>​    通过top -Hp pid查看java进程的每一个线程占用CPU的情况；<br>​        如果有一个线程占用CPU过高，有两种可能：<br>​            没有内存了，Java垃圾回收线程不停地运行尝试回收内存，但是每次无法收回，确认：<br>​                jstat -gcutil pid 1s   观察10多秒钟就能发现了，看是不是内存使用率接近100%了<br>​            类似于死循环（hash冲突攻击），就是一个线程一直占用一个核的所有CPU资源（其实一个线程总是占用一个核超过50%的资源都是不太正常的），解决：<br>​                用我的checkPerf脚本，定位这个线程具体执行的任务（能具体到某一行），对应看代码解决。            </p>
<pre><code>    如果有很多线程，每个线程占用的CPU都不多(基本都在10%以下)，那基本是正常的，只是程序并发确实很高。

如果死锁：
    jstack -l pid 多执行几次，统计一下stack中总是在等待哪些锁，可以对锁id进行排序统计（sort uniq grep）
上面列出来的都是明显的瓶颈，最可怕的是哪里都没有明显的瓶颈，哪里都要偷一点点CPU资源走，这是可以试试JProfiler这样更专业一点的工具，同时要配合自己对业务的了解来解决。

一旦触发频繁地抛出异常，CPU占用率会急剧地上升（抛异常比正常情况下会慢2个数量级）主要是由于：Throwable的构造函数中会调用native的fillInStackTrace()，这个方法就会构造整个异常栈了。
</code></pre>
<p>Java内存的问题，如果有内存泄露（就是执行完fgc&#x2F;old gc后不能回收的内存不断地增加）：<br>    怎么确认没有内存了：<br>        jps -lmv pid 先确认你的参数，也就是你给JVM分配了多大的堆(-Xmx 比如1G); 然后jstat -gcutil pid 1s 看看GC运行情况，如果(O&#x2F;E 两列基本接近100%的话就是内存不够了)<br>            内存不够分两种：一种是真的不够，就是你们的系统很庞大需要1G以上的内存，而你只分配了1G，这个没什么好说的，增大内存，物理内存不够就投钱买；<br>            第二一种是你们的代码写的烂，有内存泄露，这样的话分配多少内存都不够，得找出是否有内存泄露，看接下的解决方案        </p>
<pre><code>快速解决：jmap -histo:live pid  来统计所有对象的个数（String/char/Integer/HashEntry 这样的对象很多很正常，主要是盯着你们公司的包名下的那些对象）
每隔一分钟执行一次上面的命令，执行5次以上，看看你们公司报名下的对象数量哪个在一直增加，那基本上就是这个对象引起了泄露；
用课堂上的工具HouseMD(java -Xbootclasspath/a:/usr/java/jdk1.6.0_29/lib/tools.jar -jar housemd-assembly-0.2.2.jar pid)来动态监控创建这个对象的地方（一般来说很多时候创建了这些对象把他们丢到一个HashMap然后就不管了），分析一下有没有释放！
    &gt;trace -s -d ClassName

上面的方法实在没法定位就用: jmap -dump:live,format=b,file=heap.bin pid 导出整个内存（耗时间，需要很大的内存的机器才能对这个导出文件进行分析，会将JVM锁住一段时间）
    在Eclipse的插件EMA中打开这个文件（2G的物理文件需要4G以上的内存，5G以上的需要将近20G的内存来分析了）
    盯着你们公司报名的那些对象，看看引用关系，谁拿着这些对象没释放（是否是必要的），可以一直追查的RootReference
</code></pre>
<h2 id="MySQL-数据库的性能问题"><a href="#MySQL-数据库的性能问题" class="headerlink" title="MySQL 数据库的性能问题"></a>MySQL 数据库的性能问题</h2><p>大部分情况下是磁盘IO的问题（索引没建好、查询太复杂）；</p>
<ul>
<li><p>索引问题的话分析慢查询日志，explain 他们挨个解决。</p>
</li>
<li><p>偶尔也有数据库CPU不够的情况，如果并发高CPU不够很正常，如果并发不高，那很可能就是group by&#x2F;order by&#x2F;random之类的操作严重消耗了数据库的CPU</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql -e &quot;show full processlist&quot; | grep -v Sleep | sort -rnk6 查看那些SQL语句执行的太长</span><br><span class="line">拿出这个SQL语句分析他们的执行计划: explain SQL 然后改进；</span><br><span class="line">分析慢查询日志，统计top10性能杀手的语句，挨个explain他们，然后改进（具体改进办法具体分析，这里只谈思路）</span><br></pre></td></tr></table></figure></li>
</ul>
<p>总结一下数据库问题就只有这三招：show full processlist&#x2F;分析慢查询日志&#x2F;explain（然后建好联合索引）</p>
<p>补充一个数据库连接数不够的问题，很多人碰到了，不知道怎么解决：</p>
<ul>
<li>在mysql 命令行里执行：show variables like ‘%max_connections%’;  看看你们的数据实际配置是多少（比如1000）</li>
<li>show full processlist 数一下多少行，一行代表一个连接，比如这里是1000行，那基本上就是连接数不够了，你要解决的为什么你的数据库需要这么多连接</li>
<li>接下来分析这些连接是从哪来的IP，然后问你自己：根据你们的服务类型的特点需要这么多连接吗？</li>
</ul>
<h3 id="数据库性能问题提问请给出："><a href="#数据库性能问题提问请给出：" class="headerlink" title="数据库性能问题提问请给出："></a>数据库性能问题提问请给出：</h3><ul>
<li>show full processlist;</li>
<li>查询语句;</li>
<li>表结构(包括索引结构);</li>
<li>数据库引擎类型;</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/08/23/如何从几百万个抓包中找到一个异常的包/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/08/23/如何从几百万个抓包中找到一个异常的包/" itemprop="url">如何从几百万个抓包中找到一个异常的包</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-08-23T12:30:03+08:00">
                2023-08-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tcpdump/" itemprop="url" rel="index">
                    <span itemprop="name">tcpdump</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何从几百万个抓包中找到一个异常的包"><a href="#如何从几百万个抓包中找到一个异常的包" class="headerlink" title="如何从几百万个抓包中找到一个异常的包"></a>如何从几百万个抓包中找到一个异常的包</h1><p>这篇算是对抓包定位原因在哪里的落地篇，没什么高深的技术，都是很low但是你一定可以照着操作的，算是星球内必须学会和带走的内容</p>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230620150119963.png" alt="image-20230620150119963"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>一次业务请求包含160个拖数据的SQL查询，通过160个连接，发给160个Database，但是过几分钟后总有报错。几分钟抓包文件10G左右，网络包几百万个，怎么找到报错的那个？</p>
<p>几个麻烦的地方</p>
<ul>
<li>虽然问题每次稳定重现，但是每次重现的Database不是固定的；</li>
<li>从开始拖到出现问题需要几分钟不等，抓包量巨大</li>
<li>有一个连接报错后剩下的其它连接也会断开</li>
<li>这么多端口怎么解析成MySQL协议，请看：<a href="https://t.zsxq.com/0f7nMlKax" target="_blank" rel="noopener">https://t.zsxq.com/0f7nMlKax</a></li>
</ul>
<h3 id="问题发生条件"><a href="#问题发生条件" class="headerlink" title="问题发生条件"></a>问题发生条件</h3><ul>
<li>一个Client同时开160条连接，发160个类似的SQL去160个MySQL Database上拖数据时必现</li>
<li>如果将拖数据的SQL拖取数量改小一点就不再出现——拖取少执行更快，没达到触发bug条件</li>
<li>网络传输得慢一点、JDBC streaming 模式下发生，比如streaming流模式拖数据是几MB每秒，去掉流模式拖数据是几十MB每秒且不报错。这里可以通过设置内核 tcp rmem&#x2F;加大rtt延时来模拟重现——和我们的<a href="https://wx.zsxq.com/dweb2/index/topic_detail/181428425525182" target="_blank" rel="noopener">必做实验callback一下</a>，无时不刻不展示下我们必做实验的用途。</li>
</ul>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><p>分析技巧和步骤：</p>
<ol>
<li>抓包，从握手到报错断开全抓下来，时间跨度3分多钟，抓下来10个G左右，怎么分析？</li>
<li>editcap -c 200000 把抓包切小，每个文件20万个包，保证wireshark打开不太慢（editcap 是安装wireshark附带的小命令，附带的还有tshark、capinfos等）</li>
<li>wireshark打开切小后的最后一个文件，搜reset&#x2F;fin 找到<strong>第一个</strong>断开的连接(如下图)，找到9913&#x2F;42909这对连接端口</li>
<li>回到10个G的抓包中，用 tshark -r .&#x2F;big.pcap -Y “tcp.port&#x3D;&#x3D;42909”   -w 42909.pcap 把42909这条连接所有包过滤出来，-r 读，-w 写</li>
<li>wireshark 打开42909.pcap 傻子也能看到问题在哪里了</li>
</ol>
<p>切完后的包，切完后的文件会加时间戳，时间戳可以和报错时间对应：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">-rw-r--r--  1 root  root   329M Jun 16 17:46 big00_00000_20230616170456.pcap</span><br><span class="line">-rw-r--r--  1 root  root   1.1G Jun 16 17:46 big00_00001_20230616170524.pcap</span><br><span class="line">-rw-r--r--  1 root  root  1022M Jun 16 17:46 big00_00002_20230616170546.pcap</span><br><span class="line">-rw-r--r--  1 root  root   1.1G Jun 16 17:46 big00_00003_20230616170608.pcap</span><br><span class="line">-rw-r--r--  1 root  root  1012M Jun 16 17:46 big00_00004_20230616170630.pcap</span><br><span class="line">-rw-r--r--  1 root  root   982M Jun 16 17:46 big00_00005_20230616170652.pcap</span><br><span class="line">-rw-r--r--  1 root  root   938M Jun 16 17:46 big00_00006_20230616170714.pcap</span><br><span class="line">-rw-r--r--  1 root  root   1.1G Jun 16 17:46 big00_00007_20230616170735.pcap</span><br><span class="line">-rw-r--r--  1 root  root   661M Jun 16 17:46 big00_00008_20230616170759.pcap</span><br></pre></td></tr></table></figure>

<p>搜reset&#x2F;fin 找到第一个断开的连接，第一个断开的连接才是罪魁祸首：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230620143248344.png" alt="image-20230620143248344"></p>
<h3 id="进一步分析发生问题的连接"><a href="#进一步分析发生问题的连接" class="headerlink" title="进一步分析发生问题的连接"></a>进一步分析发生问题的连接</h3><p>知识点：</p>
<blockquote>
<p>MySQL 协议是一来一回，也就是client发查询然后等查询结果全部返回，然后再发下一个</p>
<p>按协议在一个SQL查询的数据传输完毕前client不能再发任何请求，MySQL Server负责一直发送查询结果直到发送完毕。</p>
</blockquote>
<p>如下两个截图是从42909.pcap文件中过滤到的抓包从握手到断开的全过程，图1过滤条件：tcp.srcport eq 42909 and tcp.len&gt;0  (42909是客户端，9913是MySQL端口)，可以看到客户端 login（连数据库肯定得要user、password认证），然后是client查了MySQL的一堆服务端参数(下图第二行)，再然后是client设置了几个参数(set 那些)。关键的是倒数第二行client发了一个SQL给MySQL需要拉取大量数据(建立连接17.98秒的时候)，然后是数据传数据过程，第190秒的时候client发了 Quit断开连接</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230620140921134.png" alt="image-20230620140921134"></p>
<p>上图因为加了过滤条件，只看client端并去掉ack后的所有包，没看到全貌，这个过程9913的MySQL 服务端又做了啥呢？因为太长前面漫长的传数据就不截图了，只看最后连接的断开。</p>
<p>但是下图红框所示的地方可以看到MySQL Server 传着传着居然带了个 fin 包在里面，表示MySQL Server要断开连接了，无奈Client只能也发送quit 断开连接。红框告诉我们一个无比有力的证据MySQL Server 在不应该断开的地方断开了连接，问题在 MySQL Server 端</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230620141017987.png" alt="image-20230620141017987"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>就抓包结论来看是 MySQL 在不应该断开的时候发送了 fin 主动断开连接，可能是MySQL的bug</p>
<p>题外话，这个包证据抓了有一周了，但是MySQL研发同学始终绕来绕去(比如我的代码没记录下这个SQL就是没收到，我的代码没问题——熟悉的味道)跟我打了一周太极(异地)，我一查发现我和他老板认识且在一层楼，赶紧面对面找他老板讲清楚这个问题，且签字画押承认是MySQL的问题，然后继续推进排查，最终结果是为啥我跟你们一起期待吧，有了结果我再来update。</p>
<h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><p>找个MySQL，然后开始抓包，用mysql-client连一下MySQL Server随便发几个SQL，然后看看一来一回的响应</p>
<p>如果哪怕在星球一年你只要好好掌握这一篇用到的技能也能帮助你在日常工作中互相扯皮的时候快速给出精准定位和分析，值回星球票价，加油</p>
<p>比如这个案例我同时打开了5&#x2F;6个wireshark分析不同的流、整体搜索等</p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><p>这些技巧不只是用在MySQL 上，其它微服务、redis等涉及网络调用场景的扯皮的地方都可以用</p>
<p><a href="https://plantegg.github.io/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--WireShark%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88tshark/">wireshark 附带的一些小工具</a></p>
<blockquote>
<p>capinfos rsb2.cap</p>
</blockquote>
<blockquote>
<p>tshark -q -n -r rsb2.cap  -z “conv,ip”   分析流量总况</p>
</blockquote>
<blockquote>
<p>tshark -q -n -r rsb2.cap  -z “conv,tcp”  分析每一个连接的流量、rtt、响应时间、丢包率、重传率等等</p>
</blockquote>
<blockquote>
<p>editcap -c 100000 .&#x2F;rsb2.cap  rsb00.cap  &#x2F;&#x2F;把大文件rsb2.cap按每个文件100000个package切成小文件</p>
</blockquote>
<p>存放在这里：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">/usr/sbin/capinfos</span><br><span class="line">/usr/sbin/dftest</span><br><span class="line">/usr/sbin/dumpcap</span><br><span class="line">/usr/sbin/editcap</span><br><span class="line">/usr/sbin/mergecap</span><br><span class="line">/usr/sbin/randpkt</span><br><span class="line">/usr/sbin/rawshark</span><br><span class="line">/usr/sbin/reordercap</span><br><span class="line">/usr/sbin/text2pcap</span><br><span class="line">/usr/sbin/tshark</span><br></pre></td></tr></table></figure>

<h2 id="net-write-timeout-报错"><a href="#net-write-timeout-报错" class="headerlink" title="net_write_timeout 报错"></a>net_write_timeout 报错</h2><p>最后回答一下<a href="https://t.zsxq.com/0ftY9WNVv" target="_blank" rel="noopener">上一篇</a>中提到的流模式下 net_write_timeout 报错</p>
<p>如下图，JDBC 在 streaming 模式下，不断读取下一行，如果这个过程只要报错抛出的异常就是 StreamingNotifiable 异常</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230620173111706.png" alt="image-20230620173111706"></p>
<p>错误信息定义如下，这个报错误导太严重，从以上JDBC 代码可以看到只要读取下一行报错了就会报调大 net_write_timeout 错误，但是实际原因却是连接异常断开，和 timeout 没有一点关系，你看久经考验的 JDBC  代码也不是那么完善还得你会 Debug</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CommunicationsException.ClientWasStreaming=Application was streaming results when the connection failed. Consider raising value of &apos;&apos;net_write_timeout&apos;&apos; on the server.</span><br></pre></td></tr></table></figure>

<p>这个报错误导了排查分析方向，不知道坑了多少人了！当然如果MySQL 因为net_write_timeout 超时断开连接当然应该报如上错误，但是 JDBC 搞不清楚MySQL 为啥断开，就瞎猜是 timeout 了，然后只要是连接异常读数据错误(包含断开)就报这个错误。希望你们不要被坑</p>
<p>记住这个坑人的报错堆栈：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of &apos;net_write_timeout&apos; on the server.</span><br><span class="line">    at sun.reflect.GeneratedConstructorAccessor150.newInstance(Unknown Source)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">    at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:972)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:2123)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:374)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:354)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.close(RowDataDynamic.java:155)</span><br><span class="line">    at com.mysql.jdbc.ResultSetImpl.realClose(ResultSetImpl.java:6726)</span><br><span class="line">    at com.mysql.jdbc.ResultSetImpl.close(ResultSetImpl.java:865)</span><br><span class="line">    at com.alibaba.druid.pool.DruidPooledResultSet.close(DruidPooledResultSet.java:86)</span><br></pre></td></tr></table></figure>

<p>不过你要仔细看的话，它还是有caused by，如下，但是绝大部分工程师看到这个堆栈会忽视，上面都有 net_write_timeout 我还管个屁 Can not read response from server, 不过要是结合抓包的话就能理解：at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3186) 这个根本的原因是 JDBC 从服务端读取数据的时候报错了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.io.EOFException: Can not read response from server. Expected to read 405 bytes, read 272 bytes before connection was unexpectedly lost.</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3186)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3709)</span><br><span class="line">    ... 40 common frames omitted</span><br></pre></td></tr></table></figure>

<p>最后希望你没被绕晕，再去看看<a href="https://t.zsxq.com/0ftY9WNVv" target="_blank" rel="noopener">上一篇</a>中推荐的流模式原理，把代码和网络应用层完美地结合起来</p>
<p>完整堆栈也可以参考网络上别人碰到的：<a href="https://github.com/brettwooldridge/HikariCP/issues/1771" target="_blank" rel="noopener">https://github.com/brettwooldridge/HikariCP/issues/1771</a> </p>
<p>看 Google 里面对这个问题的分析基本都没入门：<a href="https://www.google.com/search?q=Caused+by:+com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:+Application+was+streaming+results+when+the+connection+failed.+Consider+raising+value+of+%27net_write_timeout%27+on+the+server.&hl=en&sxsrf=APwXEddTwJGjFpkKuWHyXjlTvwTo2OUMhA:1687226872136&ei=-AmRZI7gB6-C0PEPmOGbwAE&ved=0ahUKEwiOvPny4dD_AhUvATQIHZjwBhgQ4dUDCBE&uact=5&oq=Caused+by:+com.mysql.jdbc.exceptions.jdbc4.CommunicationsException:+Application+was+streaming+results+when+the+connection+failed.+Consider+raising+value+of+%27net_write_timeout%27+on+the+server.&gs_lcp=Cgxnd3Mtd2l6LXNlcnAQA0oECEEYAFAAWABgAGgAcAF4AIABAIgBAJIBAJgBAKABAqABAQ&sclient=gws-wiz-serp" target="_blank" rel="noopener">https://www.google.com/search?q=Caused+by%3A+com.mysql.jdbc.exceptions.jdbc4.CommunicationsException%3A+Application+was+streaming+results+when+the+connection+failed.+Consider+raising+value+of+%27net_write_timeout%27+on+the+server.&amp;hl=en&amp;sxsrf=APwXEddTwJGjFpkKuWHyXjlTvwTo2OUMhA%3A1687226872136&amp;ei=-AmRZI7gB6-C0PEPmOGbwAE&amp;ved=0ahUKEwiOvPny4dD_AhUvATQIHZjwBhgQ4dUDCBE&amp;uact=5&amp;oq=Caused+by%3A+com.mysql.jdbc.exceptions.jdbc4.CommunicationsException%3A+Application+was+streaming+results+when+the+connection+failed.+Consider+raising+value+of+%27net_write_timeout%27+on+the+server.&amp;gs_lcp=Cgxnd3Mtd2l6LXNlcnAQA0oECEEYAFAAWABgAGgAcAF4AIABAIgBAJIBAJgBAKABAqABAQ&amp;sclient=gws-wiz-serp</a></p>
<p>下次在你们的业务代码里如果出现查询结果太大导致JVM OOM的话你可以站出来说把拉取数据改成 流 模式会有奇效 :) , 当然随之而来的是会有 net_write_timeout 报错，嗯，你的机会来了，业务技术上按照你的指引发展，出了问题你能顶得上</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/07/23/扑朔迷离根因分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/07/23/扑朔迷离根因分析/" itemprop="url">扑朔迷离的根因分析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-07-23T12:30:03+08:00">
                2023-07-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="扑朔迷离的根因分析"><a href="#扑朔迷离的根因分析" class="headerlink" title="扑朔迷离的根因分析"></a>扑朔迷离的根因分析</h1><h2 id="原则"><a href="#原则" class="headerlink" title="原则"></a>原则</h2><p>追着RT 跑，不断加压力，到瓶颈前随着并发的增加RT很稳定。</p>
<p>但是你要对你的RT怎么来的，包含哪些环节的消耗，这样才不会出错。</p>
<p>如下图左边是QPS不停地增加，每一次台阶(增加20%流量)都是一次加压过程，右边是对应的 RT，可以看到在绿线阶段几乎是平稳的，直到最后的红色箭头 RT略微有一点点提升，但是整体也还好，说明基本没到瓶颈</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230520101758175.png" alt="image-20230520101758175"></p>
<p>当然这个图是经过长时间调优的结果，来之不易，是理想的期望系统状态，但在这之前是长时间的痛苦分析和瓶颈在哪里的定位过程。</p>
<p>凡是复杂的实际业务总是有很多干扰项出现在你的理论图上，你得很好地识别他们</p>
<h2 id="业务结构"><a href="#业务结构" class="headerlink" title="业务结构"></a>业务结构</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230517113148916.png" alt="image-20230517113148916"></p>
<p>概念说明：</p>
<p>黑色&#x3D;Database&#x3D;被依赖业务&#x3D;物理</p>
<p>蓝色&#x3D;Tomcat&#x3D;上游业务&#x3D;逻辑</p>
<p>上游响应时间&#x3D;下游业务响应时间+网络时间+上游自身处理耗时</p>
<p>响应时间&#x3D;RT&#x3D;耗时监控</p>
<p>tcprt：从内核网络取Database的响应时间</p>
<p>实际很魔幻的是同样流量有时候压测很稳定，有时候又不稳定，性能上不去(稳定时可能是压测数据、没有触发Database雪崩之类的问题)，所以导致问题</p>
<p><strong>所有压测过程中肯定是没有任何资源上的瓶颈(CPU、内存、网络带宽、磁盘等等)</strong></p>
<h2 id="监控数据"><a href="#监控数据" class="headerlink" title="监控数据"></a>监控数据</h2><p>如图，蓝线表示Tomcat，黑线表示Database被调用方，可以看到每次黑色 RT上升QPS下跌很猛(符合预期)，奇怪的是黑色RT很快降下来后蓝色RT还会维持较高一段时间，监控频率每5秒采集一次，以下所有监控图时间范围是一样的，但采集频率不一样</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230516150350614.png" alt="image-20230516150350614"></p>
<p>(图1)</p>
<p>上图的两个 RT 监控数据都是Tomcat的业务代码记录下来的，比如Database的响应时间就包含网络+Database的处理时间</p>
<p>如下图通过网络监控看响应时间(tcprt <a href="https://help.aliyun.com/document_detail/181331.html" target="_blank" rel="noopener">阿里云文档</a>，从OS 内核中取到网络包的响应时间)，蓝线表示Tomcat，紫线表示Database，监控力度每1分钟采集一次，有被平均</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230516150812485.png" alt="image-20230516150812485"></p>
<p>以上两个监控图的矛盾点：如果从网络层面记录的Database RT 可以看到上升幅度不明显，但是Tomcat 的RT上升很明显，但是Tomcat记录的RT则又是Database 上升明显。</p>
<h4 id="补充知识"><a href="#补充知识" class="headerlink" title="补充知识"></a><a href="https://help.aliyun.com/document_detail/181331.html" target="_blank" rel="noopener">补充知识</a></h4><p>tcprt和tomcat业务进程记录的 Database rt差异，tcprt记录到的是RDS&#x2F;Database的响应时间+网络时间，tomcat在这个基础上还要加入自己进程调出处理时间，比如tomcat进程取到数据库连接的时候连接需要排队等待1秒钟(后面有分析)，那么这个一秒钟对tcprt来说是不会记录进去的，但是客户端感知到的这次调用是1秒以上。当然业务记录的Database 还可以更精准，比如在连接池Druid(或者其它连接池的实现)内取记录，但是无论如何从业务进程到OS内核这里的差距总是存在的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/6f6862dec810933f34b7793018cfb0da.png" alt="image.png"></p>
<h3 id="Tomcat-CPU-监控"><a href="#Tomcat-CPU-监控" class="headerlink" title="Tomcat CPU 监控"></a>Tomcat CPU 监控</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230516150950383.png" alt="image-20230516150950383"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>可以很清楚看到 QPS 下降是因为 RT上升，那么究竟是Database的RT上升导致的还是Tomcat的RT上升导致的。</p>
<p>但是我们从监控图也能看到Database RT降下来后Tomcat RT还维持高水位，所以有点迷惑了。</p>
<p>继续看另外案例</p>
<h2 id="案例2-yy"><a href="#案例2-yy" class="headerlink" title="案例2 yy"></a>案例2 yy</h2><p>两次压测监控数据，左右两个图标是同一时间的QPS和RT，蓝线表示Tomcat，黑线表示Database被调用方</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519170255718.png" alt="image-20230519170255718"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519172225890.png" alt="image-20230519172225890"></p>
<p>从两个图来看，随着并发加高(QPS加高) 黑色RT增加明显，但是跑着跑着降下去了，可以理解成突发流量导致黑色RT增加，但是很快黑色RT稳住了阵脚，降回去了，但是蓝色 RT没降，所以表面看起来是蓝色(Tomcat)处理能力到了瓶颈</p>
<p>上图时间点内核监控的tcprt，可以看到还是Database 处理耗时增加，和上图的黑色RT下降根本不匹配，上图黑色RT最后在2.96ms，下图内核监控到的Database的tcprt在8.49，差异矛盾点</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519172519802.png" alt="image-20230519172519802"></p>
<p>第三次压测图</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519165825977.png" alt="image-20230519165825977"></p>
<p>从第一个图来看，随着并发加高(QPS加高) 黑色RT增加明显，蓝色 RT去掉黑色部分也有增加，并且黑色、蓝色都没降回去，看起来主要是黑色(Database)处理能力到了瓶颈</p>
<p>纠结的时候就在Tomcat上抓包确认一下，如下图黑色 Database服务端口是5493，可以看到Tomcat 发request总是很快，但是Database 响应都是几十毫秒(下图红色框)，和监控一致。其实监控可以说明问题，但是我担心业务记录时间不准，以及建连接时间都考虑在内，所以想抓包微观上印证一下，这种项目牵扯到很多人你搞错了方向丢人不说，大家合作联调浪费很大，所以必须稳！</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519203620163.png" alt="image-20230519203620163"></p>
<p>如果说问题在Database上，那为什么会有Database RT忽上忽下，Database RT降下去了Tomcat RT不降？我们要继续分析一下 Tomcat RT以及Database RT是怎么记录和实现的</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>问题解决后的原因分析以及数据整理</p>
<p>这个时候我们再把Tomcat部分的业务调用和RT记录再细化一下，如下图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230520111102697.png" alt="image-20230520111102697"></p>
<h3 id="Druid分析"><a href="#Druid分析" class="headerlink" title="Druid分析"></a><a href="https://github.com/alibaba/druid" target="_blank" rel="noopener">Druid分析</a></h3><p>创建和回收逻辑：<a href="https://exceting.github.io/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89Druid%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/#%E4%B9%9D%E3%80%81%E4%B8%BB%E6%B5%81%E7%A8%8B5%EF%BC%9A%E5%9B%9E%E6%94%B6%E8%BF%9E%E6%8E%A5" target="_blank" rel="noopener">https://exceting.github.io/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89Druid%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/#%E4%B9%9D%E3%80%81%E4%B8%BB%E6%B5%81%E7%A8%8B5%EF%BC%9A%E5%9B%9E%E6%94%B6%E8%BF%9E%E6%8E%A5</a></p>
<p>作为Tomcat和Database的连接点、枢纽点搞清楚Druid的逻辑对理解Tomcat和Database之间的问题的理解很关键。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240523084559029.png" alt="image-20240523084559029"></p>
<p>比如以下要说的三个Druid 错误状态如果你不放到一起比较，看到这个错误你最多反应就是连接池不够了，什么原因不知道。但是如果放到一次比较一次后你以后对详细错误提示会积极敏感，进而发现第四、第五种错误提示</p>
<p>这就是综合比较、总结的好处。</p>
<p>Druid 最核心的类是 DruidDataSource，连接的构建，入池，获取，收缩，销毁，以及核心监控数据都在这个类维护</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230721170334688.png" alt="image-20230721170334688"></p>
<p>连接池初始化流程：初始化驱动实例 -&gt; 加锁 -&gt; 初始化属性 -&gt; 初始化过滤器 -&gt; 校验参数 -&gt; <strong>创建初始化连接并校验后加入池中</strong> -&gt; 创建logStatsThread、createConnectionThread和destroyConnectionThread -&gt; 注册MBean，用于支持JMX -&gt; 如果设置了keepAlive，通知createConnectionThread创建连接对象 -&gt; 解锁</p>
<p>Druid 创建连接的堆栈如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&quot;Druid-ConnectionPool-Create-1647809146&quot; #265 daemon prio=5 os_prio=0 tid=0x00007fbcdfd5f000 nid=0x1a0 runnable [0x00007fbcdf9fd000]</span><br><span class="line">   java.lang.Thread.State: RUNNABLE</span><br><span class="line">        at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">        at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">        at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">        at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">        - locked &lt;0x00000000e71ca648&gt; (a com.mysql.jdbc.util.ReadAheadInputStream)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3001)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.readPacket(MysqlIO.java:567)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1018)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2253)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2284)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2083)</span><br><span class="line">        - locked &lt;0x00000000e7f898f0&gt; (a com.mysql.jdbc.JDBC4Connection)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:806)</span><br><span class="line">        at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)</span><br><span class="line">        at sun.reflect.GeneratedConstructorAccessor196.newInstance(Unknown Source)</span><br><span class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">        at com.mysql.jdbc.Util.handleNewInstance(Util.java:404)</span><br><span class="line">        at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:410)</span><br><span class="line">        at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:328)</span><br><span class="line">        at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1558)</span><br><span class="line">        at com.alibaba.druid.pool.DruidAbstractDataSource.createPhysicalConnection(DruidAbstractDataSource.java:1623)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2468)       </span><br><span class="line">        </span><br><span class="line">&quot;Druid-ConnectionPool-Create-1823047135&quot; #160 daemon prio=5 os_prio=0 tid=0x00007fbd60cf0000 nid=0x142 waiting on condition [0x00007fbd043fe000]</span><br><span class="line">   java.lang.Thread.State: WAITING (parking)</span><br><span class="line">        at sun.misc.Unsafe.park0(Native Method)</span><br><span class="line">        - parking to wait for  &lt;0x00000000c448b348&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)</span><br><span class="line">        at sun.misc.Unsafe.park(Unsafe.java:1036)</span><br><span class="line">        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:176)</span><br><span class="line">        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2047)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2443)</span><br></pre></td></tr></table></figure>

<h4 id="Druid-报错1"><a href="#Druid-报错1" class="headerlink" title="Druid 报错1"></a>Druid 报错1</h4><p>获取连接排队是基本不消耗CPU，下图右上角是获取失败的日志打堆栈消耗，可以看到异常非常多。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519174633172.png" alt="image-20230519174633172"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519175029396.png" alt="image-20230519175029396"></p>
<p>Druid最大连接数默认是30，多次调大，30-&gt;60-&gt;120-&gt;160，一直调下去对调大能解决问题都没有信心了，总是报错</p>
<blockquote>
<p>maxWaitThreadCount 30, current wait Thread count 0 </p>
</blockquote>
<p>调大到160后的报错堆栈，<a href="https://sourcegraph.com/github.com/alibaba/druid/-/blob/core/src/main/java/com/alibaba/druid/pool/DruidDataSource.java?L1733:92" target="_blank" rel="noopener">对应源码 </a> 这个报错说明报错时已经有160个线程在等连接了，别等了，先快速报错返回吧</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.sql.SQLException: maxWaitThreadCount 160, current wait Thread count 0</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnectionInternal(DruidDataSource.java:1620)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnectionDirect(DruidDataSource.java:1404)</span><br><span class="line">        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:5059)</span><br><span class="line">        at com.alibaba.druid.filter.FilterAdapter.dataSource_getConnection(FilterAdapter.java:2756)</span><br><span class="line">        at com.alibaba.druid.filter.FilterChainImpl.dataSource_connect(FilterChainImpl.java:5055)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1382)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:1374)</span><br><span class="line">        at com.alibaba.druid.pool.DruidDataSource.getConnection(DruidDataSource.java:98)</span><br><span class="line"></span><br><span class="line">对应代码：</span><br><span class="line">                   final Lock lock = this.lock;</span><br><span class="line">                    lock.lock();</span><br><span class="line">                    try &#123;</span><br><span class="line">                        if (activeCount &lt; maxActive) &#123;</span><br><span class="line">                            activeCount++;</span><br><span class="line">                            holder.active = true;</span><br><span class="line">                            if (activeCount &gt; activePeak) &#123;</span><br><span class="line">                                activePeak = activeCount;</span><br><span class="line">                                activePeakTime = System.currentTimeMillis();</span><br><span class="line">                            &#125;</span><br><span class="line">                            break;</span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            discard = true;</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125; finally &#123;</span><br><span class="line">                        lock.unlock();</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    if (discard) &#123;</span><br><span class="line">                        JdbcUtils.close(pyConnInfo.getPhysicalConnection());</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            final ReentrantLock lock = this.lock;</span><br><span class="line">            try &#123;</span><br><span class="line">                lock.lockInterruptibly();</span><br><span class="line">            &#125; catch (InterruptedException e) &#123;</span><br><span class="line">                connectErrorCountUpdater.incrementAndGet(this);</span><br><span class="line">                throw new SQLException(&quot;interrupt&quot;, e);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            try &#123;</span><br><span class="line">                if (maxWaitThreadCount &gt; 0</span><br><span class="line">                        &amp;&amp; notEmptyWaitThreadCount &gt;= maxWaitThreadCount) &#123;</span><br><span class="line">                    connectErrorCountUpdater.incrementAndGet(this);</span><br><span class="line">                    throw new SQLException(&quot;maxWaitThreadCount &quot; + maxWaitThreadCount + &quot;, current wait Thread count &quot;</span><br><span class="line">                            + lock.getQueueLength());//bug? lock.getQueueLength()永远为0，应该改成：lock.getWaitQueueLength(notEmpty)</span><br><span class="line">                &#125;</span><br></pre></td></tr></table></figure>

<p>以下两个Druid 报错这次压测没有出现但是可以放一起比较一下，其它项目场景经常出现</p>
<h4 id="Druid-报错2"><a href="#Druid-报错2" class="headerlink" title="Druid 报错2"></a>Druid 报错2</h4><p>Druid类似报错，明显是等了5秒最大等待时间还没有获取到连接：<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519191317489.png" alt="image-20230519191317489"></p>
<p>红色错误信息表示等了5006毫秒（设置的5000毫秒超时）还没有取到连接，所以超时了，然后抛出错误堆栈。</p>
<p>红色信息还提示我们当前连接池最大10，目前 active 0, 说明不是连接池满了取不到，而是连接池里一直是空的。</p>
<p>看到这个错误不能说明数据库、访问数据库有啥问题，只能说明Druid 连接池取不到连接，要继续分析Druid创建连接的线程栈。或者比如Druid 参数设置不合理，可以把min、init、max 连接数设置为相同的值，避免压力高峰期再去创建连接。</p>
<p>Druid通过另外一个task（thread）异步给连接池补充连接，也就是这里可能是Druid创建连接失败，比如密码错误、比如连不上数据库，比如创建的thread卡死了、报其他异常了</p>
<p><strong>Druid创建 连接 和业务取连接是两个线程，所以业务取连接报错是看不到创建连接报错的堆栈和原因的</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#grep CreateConnectionThread.run stack4.log</span><br><span class="line">	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2818)</span><br><span class="line">	at com.alibaba.druid.pool.DruidDataSource$CreateConnectionThread.run(DruidDataSource.java:2813)</span><br></pre></td></tr></table></figure>

<h4 id="Druid-报错3"><a href="#Druid-报错3" class="headerlink" title="Druid 报错3"></a>Druid 报错3</h4><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230520092224080.png" alt="image-20230520092224080"></p>
<p>借出连接为0(active 0)，creating也是0，没有新连接正在创建。</p>
<p>分析方法：</p>
<ol>
<li>dump Java应用内存，用MAT内存分析工具打开dump文件</li>
<li>使用OQL，select * from com.alibaba.druid.pool.DruidDataSource where createTaskCount&#x3D;3</li>
<li>选出来的DruidDataSource即为有问题的datasource</li>
</ol>
<p>原因</p>
<p>Druid中有个计数器createTaskCount，用来记录每个连接池当前正在创建连接的任务数，默认不能超过3。Druid中，在keepAlive&#x3D;true的情况下，这个计数器有bug，存在加了但没减的情况，导致这个值涨到3之后没有减回去，从而无法提交新的创建连接任务。</p>
<p> 注意，进入这个状态后的连接池，是无法自动恢复的。Druid升级到1.1.24可以修复这个问题。</p>
<h3 id="分片逻辑"><a href="#分片逻辑" class="headerlink" title="分片逻辑"></a>分片逻辑</h3><p>因为数据量太大，一台Database存放不下，自然会分片，或者说单表几千万之后也是建议分片。</p>
<p>分片逻辑是取业务id最后两位的字符去取string hashcode，再对16个Database分片</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">如果对id后两位字符(从00-99供100个数字，因为不排除id里面有字符，但实际主要是0-9的数字)的ascii码取hash然后按16取模的结果：</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9  --开始不正常，10-14号分片没有直接跳到15号分片</span><br><span class="line">15</span><br><span class="line">0</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">……</span><br><span class="line"></span><br><span class="line">//分片求模代码</span><br><span class="line">for(int i=0;i&lt;10;++i) //0的ascii码是48，依此类推</span><br><span class="line">	for(int j=0;j&lt;10;++j)</span><br><span class="line">		 int value=((48+i)*31+j) mod 16;</span><br></pre></td></tr></table></figure>

<p>补充个小八卦</p>
<blockquote>
<p>为什么取某几位尾数来求模？比如很多业务按user_id拆分片，然后希望这个用户的所有订单拆分也落在一个分片内。于是他们想到的办法是在订单id最后几位上追加进去下单人的user_id后几位，对订单拆分会取订单id后几位hash，这样同一个用户肯定到同一个分片</p>
<p>这样查询某个用户的所有订单时(高频需求)就只需要查一个分片，否则就要扫描所有分片。</p>
<p>掏出你的某宝、某东看看你的订单后几位</p>
</blockquote>
<p>分片后的数据，明显两头的多中间的少，这必然导致后面的 Database 负载不均衡：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519181628114.png" alt="image-20230519181628114"></p>
<p>Java源码：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230519181451384.png" alt="image-20230519181451384"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>问题的根本原因？</p>
<p>多个Database中的某几个瓶颈，为什么会这样见数据分布部分的分析</p>
<p>为什么Database RT监控能降下来？</p>
<p>业务Tomcat 帮Database拦截了流量，一旦Database响应慢 Druid 连接就会不够，请求都堵在Tomcat中，导致Tomcat RT升高(包含等待连接时间)——替人堵了枪眼，很好，Tomcat crash总比 Database crash要好，但是业务要清楚这是替人挨枪子，该往哪里去查瓶颈。</p>
<p>比如加流量20%，开始Database RT升高，很快连接不可用，可能有接近20%的流量被Tomcat拦截，这个时候Database RT能稳定，也有可能拦截的不够多，这个时候Database RT还是很高，但Tomcat RT更高，进入一种平衡状态</p>
<p>为什么有时候压测能过？</p>
<p>应该是数据分布比较巧，刚好压测流里面的数据分布没那么不均衡，没触发数据库雪崩</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/07/23/扑朔迷离的根因分析--抖动和并发/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/07/23/扑朔迷离的根因分析--抖动和并发/" itemprop="url">扑朔迷离的根因分析--抖动和并发</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-07-23T12:30:03+08:00">
                2023-07-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="扑朔迷离的根因分析–抖动和并发"><a href="#扑朔迷离的根因分析–抖动和并发" class="headerlink" title="扑朔迷离的根因分析–抖动和并发"></a>扑朔迷离的根因分析–抖动和并发</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>我们之前说过根因分析第一就是要追着 RT跑，随着并发的增加哪里RT增加快哪里就是瓶颈，这是我们的基本原则，但总有一些例外，我们今天想说说例外</p>
<h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>如下图，应用是多个Tomcat集群，Tomcat节点可以随意增加，后端是一组DB集群，有几百个Database实例，每一次业务请求都会对应多个Database查询</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230609204957690.png" alt="image-20230609204957690"></p>
<h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>开始的时候客户端压2个Tomcat集群，QPS 700，Tomcat节点CPU 90%，Database每个节点CPU 20%左右，于是增加1个Tomcat 节点这个时候QPS 还是700，Tomcat的RT增加了50%，Tomcat CPU 降低到60%，继续增加Tomcat 节点 RT、QPS保持稳定，CPU使用率下降。</p>
<p>所以这里要搞清楚哪里是瓶颈，如果Tomcat是瓶颈加Tomcat节点为什么没有效果。如果Database是瓶颈但是增加Tomcat节点的时候Database 的RT有一点点增加，远远没有到增加50%的RT 程度</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>首先最容易想到的是Tomcat 和 Database之间的网络、网关、LVS 等资源到了瓶颈，但是经过排查分析这些环节都排除了，另外也排除了Tomcat到Database的连接池、Database的磁盘等瓶颈，另外Tomcat 访问Database全是查询，没有事务。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/20230609210244.jpg" alt="image.png"></p>
<p>看起来事情比想象的复杂，于是进行了如下压测：</p>
<p>先用一个压力端压3个Tomcat中的2个，QPS 跑到700，然后新开一个压力端压第三个Tomcat(新开压力端是排查压力机的问题，新开Tomcat是想排除Tomcat 的问题)，如果Tomcat是瓶颈的话QPS应该上去，或者说后端没有问题的话那两个Tomcat 的700 QPS得保持基本稳定不变或略微下降才对。</p>
<p>实际上第二个压力端跑起来后，前两个Tomcat的QPS 铛就掉下去了，总QPS 保持稳定不变，也就是随着Tomcat给后端并发压力的增加后端肯定给了一个负反馈给那两Tomcat，导致那两Tomcat QPS掉下去了。这个负反馈明显得是Database的RT在增加，但是从监控来看Database的RT 从0.6增加到了0.8，但是Tomcat 的RT 增加更快从19.7增加到了29.8.</p>
<p>单独压DB，DB的QPS能高5倍，CPU 也可以跑到100%。看起来单压都没问题，一组合就不行了</p>
<h3 id="问题在Database"><a href="#问题在Database" class="headerlink" title="问题在Database"></a>问题在Database</h3><p>绕过Tomcat 用相同的SQL 压Database QPS 一下子就能上去，Database 的CPU 也跑到了100%，但是只要走Tomcat 就会上不去。</p>
<p>打开Tomcat 日志将所有Database的响应时间拉出来分析，发现随着并发的增加 100 ms的响应也多了很多，实际上这些查询都是1ms就应该返回</p>
<p>具体分析过程看这里：<a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/">https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/</a></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>当压力增加的时候MySQL端等锁导致的 RT 抖动或者说长尾越来越多，虽然没有数据库的写，但是查询的时候优化器也需要统计行数等数据来为查询优化器做选择依据，这个统计动作会触发加锁排队(极短)，但是因为这一代Intel CPU指令的变化导致这个锁被放大了10 被，所以最终Tomcat 端看到的长尾就多了</p>
<h2 id="为什么"><a href="#为什么" class="headerlink" title="为什么"></a>为什么</h2><h4 id="为什么同样的环境、同样的SQL-绕过Tomcat-就能压上去？"><a href="#为什么同样的环境、同样的SQL-绕过Tomcat-就能压上去？" class="headerlink" title="为什么同样的环境、同样的SQL 绕过Tomcat 就能压上去？"></a>为什么同样的环境、同样的SQL 绕过Tomcat 就能压上去？</h4><p>绕过后的压测场景没有业务逻辑，每次请求就是一条SQL，虽然有抖动但是对平均RT拉升不明显。</p>
<h4 id="走业务逻辑压Tomcat-为什么不行？"><a href="#走业务逻辑压Tomcat-为什么不行？" class="headerlink" title="走业务逻辑压Tomcat 为什么不行？"></a>走业务逻辑压Tomcat 为什么不行？</h4><p>业务逻辑是一次请求会发256条SQL，等这256条SQL全部返回来了业务请求才会返回！请反复读这句话3遍再往下看</p>
<p>如果256条SQL 中有一条花了100 ms返回那么整个业务逻辑的RT 就是100ms，假设1%的概率一条SQL是100ms，99%的SQL 是 1ms，你可以先停下来算一算这种业务模型下的平均RT是多少</p>
<h4 id="计算抖动下的平均RT"><a href="#计算抖动下的平均RT" class="headerlink" title="计算抖动下的平均RT"></a>计算抖动下的平均RT</h4><p>关于这个抖动对整体rt的影响计算：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1575880425321-79c7ea4a-fcf1-41f9-afb9-6e553d9eaf8f.png" alt="img"></p>
<p>注:假设正常查询rt 1ms，逻辑平均rt&#x3D;(1-power(1-抖动概率,物理查询次数))*抖动大小+(power(1-抖动概率,物理查询次数))*1ms </p>
<p>当前场景下，逻辑QPS:物理QPS&#x3D;1:256，假如每次查询有1%的物理（RDS）rt抖动到100ms，则会导致逻辑平均rt恶化到92.44ms.</p>
<p>在一次逻辑查询里，只有所有物理查询都不抖整体才是不抖，RT正常；如果有一个或多个物理查询抖了，那么逻辑RT就是抖动RT。</p>
<p>所以一次逻辑查询不抖的概率是： power(1-抖动概率, 物理查询次数)</p>
<p>反过来想这256条SQL都不碰上抖动这次业务请求才会1ms返回(概率极低)，否则就是256ms返回</p>
<h4 id="为什么要讲这个案例"><a href="#为什么要讲这个案例" class="headerlink" title="为什么要讲这个案例"></a>为什么要讲这个案例</h4><p>倒不是出于原因分析，这个原因几年前就分析清楚了，但是这个场景：一次业务请求会涉及多次SQL、Redis、MQ的调用，只要其中有一个有短板、抖动这次业务请求就慢了。这简直太常见了</p>
<p>但难在别人的抖动很低被平均掉了，但是业务(Tomcat) 就要替别人背锅了，因为别人的RT 几乎没有增加或者加很少，但是Tomcat RT增加很明显，瓶颈当然看着像是在Tomcat 上。背锅吧也不可怕可怕的是你增加Tomcat 节点也不能解决问题，这才是你要从这个案例里学到的。</p>
<p>如果你的Tomcat 调后端因为短板(抖动)导致压力打不到后端，因为抖动导致Tomcat不能快速返回</p>
<h5 id="上游影响下游："><a href="#上游影响下游：" class="headerlink" title="上游影响下游："></a>上游影响下游：</h5><p>和本文无关但是可以放一起综合来看上下游互相影响的复杂性</p>
<p>以前认为事务不提交的主要代价是行锁持有时间变长(这确实是个问题)，今天见识到了新代价，事务不提交会导致事务活跃链表变长，增加copy readview的代价，进而导致DB的RT 增高，实际导致DB RT高的根本原因是DB前面的业务早到了瓶颈，来不及发送commit，导致DB端事务堆积严重。也就是业务瓶颈导致了后端DB RT高，只看RT就会被蒙蔽——怎么解决？可以抓包看commit发送慢</p>
<h3 id><a href="#" class="headerlink" title></a></h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/06/30/实战瓶颈定位-我的MySQL为什么压不上去--写场景/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/06/30/实战瓶颈定位-我的MySQL为什么压不上去--写场景/" itemprop="url">实战瓶颈定位-我的MySQL为什么压不上去--写场景</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-06-30T17:30:03+08:00">
                2023-06-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实战瓶颈定位-我的MySQL为什么压不上去–写场景"><a href="#实战瓶颈定位-我的MySQL为什么压不上去–写场景" class="headerlink" title="实战瓶颈定位-我的MySQL为什么压不上去–写场景"></a>实战瓶颈定位-我的MySQL为什么压不上去–写场景</h1><p>纠结好久要不要写这篇，因为原因非常坑爹，你们基本不会遇到，想了很久觉得思路还是有些价值，所以还是写一下，我尽量简单</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>继续上文 <a href="https://plantegg.github.io/2023/06/20/%E5%AE%9E%E6%88%98%E7%93%B6%E9%A2%88%E5%AE%9A%E4%BD%8D-%E6%88%91%E7%9A%84MySQL%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8E%8B%E4%B8%8D%E4%B8%8A%E5%8E%BB/">https://plantegg.github.io/2023/06/20/%E5%AE%9E%E6%88%98%E7%93%B6%E9%A2%88%E5%AE%9A%E4%BD%8D-%E6%88%91%E7%9A%84MySQL%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8E%8B%E4%B8%8D%E4%B8%8A%E5%8E%BB/</a> ，纯读场景问题解决后，继续压纯写场景，比另外一套类似环境差了很多，大概是2折。</p>
<p>纯写肯定有预期：会有锁、磁盘瓶颈等问题</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>先看top，结果很明显CPU上不去，并且有一个单核长时间 100%，然后 top -Hp mysqld-pid 展开所有线程，果然一直有一个线程几乎一直 100%，这就太明显了，这个线程遇到了瓶颈，导致整体上不去。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230515083125494.png" alt="image-20230515083125494"></p>
<p>top -Hp mysqld-pid 看到165935 线程一直几乎是 100% 的CPU 状态</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230515083309083.png" alt="image-20230515083309083"></p>
<p>所以接下来要搞清楚这个线程在忙什么，刷盘？抢锁？</p>
<p>如果是Java应用就简单了，直接jstack一看就很清楚了，但是MySQLD没这么容易，另外环境里没有 pstack也没法安装，所以这条路走不通。</p>
<p>但是大概率能猜出来和磁盘有点关系，于是iostat -x -d 看看磁盘情况，好家伙果然ioutil 100%，磁盘 IO TPS 好几万。如下nvme0n1是MySQLD 使用的SSD 数据盘，vdb 是OS 系统盘</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#iostat  -d vdb nvme0n1 3</span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">nvme0n1       45317.33        37.33    322150.67        112     966452</span><br><span class="line">vdb               0.00         0.00         0.00          0          0</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">nvme0n1       45215.33        37.33    319228.00        112     957684</span><br><span class="line">vdb               0.00         0.00         0.00          0          0</span><br><span class="line"></span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">nvme0n1       45146.00        42.67    320677.33        128     962032</span><br><span class="line">vdb               0.00         0.00         0.00          0          0</span><br></pre></td></tr></table></figure>

<p>通过 ：iostat -x -d vdb nvme0n1 3 可以看到如下图</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230515083645463.png" alt="image-20230515083645463"></p>
<p>但这是不是正常情况不好说，于是找到家里同样的环境跑起来(没有单线程 100%问题，QPS 比问题环境高了 5倍)，于是也看一下 iostat 做一个对比，对比发现 ioutil 很小，然后磁盘 IO TPS 才我问题环境的30%，在QPS 5倍，IO TPS才 30%的情况下傻子也能看出来这两场景肯定不一样。一个QPS触发的IO TPS差了 15倍了。</p>
<p>不啰嗦，将问题环境的sysbench 脚本复制到正常环境，这下问题重现了，再diff看看两个脚本果然被人改了。问题环境使用的sysbench是别人装的，经过分析后发现里面被改动过一些东西。</p>
<p>之所以一直没有怀疑 sysbench 的问题，也有之前测试只读场景的时候符合预期，所以忽视了sysbench的差异。</p>
<p>这让我想起贝尔实验室Ken Thompson’s “cc hack” 的八卦(有兴趣的同学可以自行查证一下)：</p>
<blockquote>
<p>当年在贝尔实验室，人们都用Unix系统，但是只有Ken可以绕过密码直接登录，让其他人百思不得其解。按理说整个Unix系统是开源的，很多人检查了系统代码，尤其是登录部分， 并没有发现任何漏洞或者后门。</p>
<p>Ken的同事们不断重新编译Unix， 但是Ken依旧如幽灵一般来去自如。</p>
<p>有人怀疑编译Unix的编译器里面有代码，但是当他们反复检查编译器源码，甚至重新编译c编译器后，依旧没有任何发现。</p>
<p>多年后，在Turing Award Lecture中，Ken终于道出了事情真相，登录源码和编译器源码都是干净的。事实上，这个幽灵般的木马在编译器的可执行文件中。</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这里的思路是：单线程100%-&gt;磁盘IO TPS非常高-&gt;和正常环境对比(常用手段，也要运气好有两个环境可以对比)-&gt;一个QPS 对应的IO TPS差异巨大-&gt;压测脚本问题</p>
<p>这算是个坑爹的小问题，大家也不会碰到，比网络限速难查多了，网络限速那里我们有放之四海而皆准的 RT 逻辑+抓包，所以很好定位。但是查证分析过程我觉得有一定的参考性，所以记录下。</p>
<p>如果MySQLD能提供一个内部任何一个操作的时间就好了，实际很难实现。当然通过火焰图去看异常偏高的调用是另外一个方向。</p>
<p>跨网络我们有抓包很好界定，但是问题到进程内部的时候反而没了抓包这种一锤定影的工具了</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/06/30/等额本金和等额本息以及提前还贷/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/06/30/等额本金和等额本息以及提前还贷/" itemprop="url">等额本息和等额本金以及提前还贷误区</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-06-30T12:30:03+08:00">
                2023-06-30
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="等额本息和等额本金以及提前还贷误区"><a href="#等额本息和等额本金以及提前还贷误区" class="headerlink" title="等额本息和等额本金以及提前还贷误区"></a>等额本息和等额本金以及提前还贷误区</h1><h5 id="1-等额本金和等额本息的差异"><a href="#1-等额本金和等额本息的差异" class="headerlink" title="1 等额本金和等额本息的差异"></a>1 等额本金和等额本息的差异</h5><p>没有差异。等额本金&#x3D;等额本息+每月提前还贷一点点()</p>
<h5 id="2-为什么等额本金总利息少"><a href="#2-为什么等额本金总利息少" class="headerlink" title="2 为什么等额本金总利息少"></a>2 为什么等额本金总利息少</h5><p>因为每个月等额本金还款多，第一个月后欠本金少了，后面每个月都是这样，所还本金更多，最终总利息自然更少</p>
<blockquote>
<p>其实可以用极限思维来分析他们的差异：假设等额本息和等额本金都借100万，周期一个月(没看错，一个月还清，极限假设)，所以一个月后他们还钱一样多！所以这个时候没有任何区别；现在继续假设，假如还款周期是2个月，那么等额本金在第一个月还钱多，导致等额本金在第二个月的时候欠钱少了，到第二个月月底还清所有欠款的时候利息要少(本金少了)——这才是他们的差异，所以是没区别的。等额本金这两个月相当于欠了银行150万一个月(第一个月欠100万，第二个月欠50万) 应还利息就是150万 乘以 月利率；等额本息相当于欠了银行 151万（第一个月欠100万，第二个月51万，因为第一个月还钱的时候只还了49万本金），所以应还利息就是 151万 乘以 月利率；欠得多利息就多天经地义这里没有投机、没有人吃亏</p>
<p>再或者换个思路：第一个月借100万，月底都还清，此时利息一样多；然后再借出来99.X万，这时等额本金借得少这个X更小，所以从第二个月开始等额本金还的利息少，归根结底换利息少是因为借得少(即现实中的还本金多)</p>
</blockquote>
<p>把上面的极限思维图形化就是下图中的灰色部分面积代表你的欠款(每个月的欠款累加)，等额本息的方式欠得多，自然利息多：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230704214346239.png" alt="image-20230704214346239"></p>
<h5 id="3-为什么总有等额本金划得来的说法"><a href="#3-为什么总有等额本金划得来的说法" class="headerlink" title="3 为什么总有等额本金划得来的说法"></a>3 为什么总有等额本金划得来的说法</h5><p>同样贷款金额下等额本金总还款额少，也就是总利息少，所以给了很多人划得来的感觉，或者给人感觉利息便宜。其实这都是错的，解释如上第二个问题</p>
<h5 id="4-利息的计算方式"><a href="#4-利息的计算方式" class="headerlink" title="4 利息的计算方式"></a>4 利息的计算方式</h5><p>利息每个月计算一次，这个月所欠本金*月利率。所以利息只和你欠钱多少有关（我们假设所有人的利率都一样）。每个月的月供，都是先还掉这个月的利息，多余的再还本金</p>
<p>等额本金因为每个月还掉的本金多，所以计算下来每个月的利息更少</p>
<h5 id="5-如何理解等额本金和等额本息"><a href="#5-如何理解等额本金和等额本息" class="headerlink" title="5 如何理解等额本金和等额本息"></a>5 如何理解等额本金和等额本息</h5><p>同样贷款额+利率的话等额本金开始还款一定比等额本息要多一些，那么你可以把等额本金分成两块，一块和等额本息一样，多出来的部分你把他看成这个月额外做了一次提前还款。你提前还款了后面的总利息自然也会更少一些，然后每个月的等额本息也会减少，以此类推每个月都是这样额外做一次提前还款直到最后一个月。</p>
<p>总结：等额本金&#x3D;等额本息+提前还贷</p>
<p>额本金开始还款一定比等额本息要多一些，可以把等额本金分成两块，一块和等额本息一样，多出来的部分把他看成这个月额外做了一次提前还款。提前还款后总利息也会更少一些，然后每个月的等额本息也会减少，以此类推每个月都是这样额外做一次提前还款直到最后一个月。</p>
<h5 id="6-提前还款划不划得来"><a href="#6-提前还款划不划得来" class="headerlink" title="6 提前还款划不划得来"></a>6 提前还款划不划得来</h5><p>钱在你手里没法赚到比利息更高的收益的话(99%的人属于这种)提前还贷划得来，之所以以前不建议大家提前还贷，是因为以前普遍涨薪快、通胀厉害、房价涨得块，把钱留出来继续买二套、三套更赚钱。另外钱留在手里会有主动权和应急方便</p>
<h5 id="7-提前还贷会多付利息吗？"><a href="#7-提前还贷会多付利息吗？" class="headerlink" title="7 提前还贷会多付利息吗？"></a>7 提前还贷会多付利息吗？</h5><p>不会，见第四条利息的计算方式。担心提前还贷的时候这比贷款把后面10年的利息收走了的是脑子不好使的人。但有些银行提前还贷会有违约金</p>
<h5 id="8-为什么等额本金和等额本息给了这么多人错觉"><a href="#8-为什么等额本金和等额本息给了这么多人错觉" class="headerlink" title="8 为什么等额本金和等额本息给了这么多人错觉"></a>8 为什么等额本金和等额本息给了这么多人错觉</h5><p>从知识的第一性出发，他们都没理解第4条，受社会普遍意识影响都预先留下了错误经验。本质就是利息只和贷款额、利率有关。</p>
<h5 id="9-贷款30年和10年利率有差异吗？"><a href="#9-贷款30年和10年利率有差异吗？" class="headerlink" title="9 贷款30年和10年利率有差异吗？"></a>9 贷款30年和10年利率有差异吗？</h5><p>没有</p>
<h5 id="10贷款30年，还了6年了，然后一次还清所有欠款会多还利息吗？"><a href="#10贷款30年，还了6年了，然后一次还清所有欠款会多还利息吗？" class="headerlink" title="10贷款30年，还了6年了，然后一次还清所有欠款会多还利息吗？"></a>10贷款30年，还了6年了，然后一次还清所有欠款会多还利息吗？</h5><p>不会，你前6年只是还了前6年的利息，没为之后的6年多付一分利息</p>
<h5 id="11-那为什么利率不变我每个月利息越来越少"><a href="#11-那为什么利率不变我每个月利息越来越少" class="headerlink" title="11 那为什么利率不变我每个月利息越来越少"></a>11 那为什么利率不变我每个月利息越来越少</h5><p>因为你欠的钱越来越少了，不是你提前还了利息，是你一直在还本金</p>
<h5 id="12-网购分期合适吗？"><a href="#12-网购分期合适吗？" class="headerlink" title="12 网购分期合适吗？"></a>12 网购分期合适吗？</h5><p>大部分小贷公司的套路就是利用你每个月已经在还本金的差异，利息自然越来越少，然后计算一个大概5%的年息误导你，实际这种网购分期的实际利息要是他计算的2倍……好好想想</p>
<p>等额本金、本息都搞不清楚的人就不要去搞分期了，你的脑瓜子在这方面不好使。</p>
<p>聪明人只看第4条就能在大脑里得出所有结论，普通人除了要有第4条还需要去看每个月的还款额、还款额里面的本金、还款额里的利息等实践输入才能理解这个问题，这就是差异</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/%E6%88%BF%E8%B4%B7.jpg" alt="房贷"></p>
<p><a href="https://zhuanlan.zhihu.com/p/161405128" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/161405128</a></p>
<h2 id="提前还贷"><a href="#提前还贷" class="headerlink" title="提前还贷"></a>提前还贷</h2><h5 id="要不要提前还贷"><a href="#要不要提前还贷" class="headerlink" title="要不要提前还贷"></a>要不要提前还贷</h5><p>要</p>
<h5 id="提前还贷划得来吗？"><a href="#提前还贷划得来吗？" class="headerlink" title="提前还贷划得来吗？"></a>提前还贷划得来吗？</h5><p>划不划得来要看这笔钱在你手里能否获得比房贷利息更高的收入以及你对流动资金的需要。其实万一有个啥事也可以走消费贷啥的，现在利息都很低</p>
<h5 id="等额本息比等额本金提前还贷更划得来？"><a href="#等额本息比等额本金提前还贷更划得来？" class="headerlink" title="等额本息比等额本金提前还贷更划得来？"></a>等额本息比等额本金提前还贷更划得来？</h5><p>一样的，你这个问题等价于我欠100万，老婆欠50万，所以我提前还贷比朋友合算吗？显然你两谁提前还贷合算只和你两的贷款利率是否有差别</p>
<p>等额本息和等额本金利率一样的，所以没有差别</p>
<h5 id="20年的房贷我已经还了15年了是不是不值得提前还贷了？"><a href="#20年的房贷我已经还了15年了是不是不值得提前还贷了？" class="headerlink" title="20年的房贷我已经还了15年了是不是不值得提前还贷了？"></a>20年的房贷我已经还了15年了是不是不值得提前还贷了？</h5><p>错误！利率还是那个利率，跟你还剩10年和还剩5年没关系，提前还贷都是等价的。记住有闲钱就提前还</p>
<h5 id="问题的本质"><a href="#问题的本质" class="headerlink" title="问题的本质"></a>问题的本质</h5><p>搞清楚每个月的利息怎么计算出来的  利息&#x3D;欠款额*月利率，月供都是把本月利息还掉多出来的还本金，也就是每个月欠款额会变少</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>每个领域都有一些核心知识点代表着这些问题的本质，你只有把核心知识点或者说问题本质搞懂了才能化繁为简、不被忽悠！</p>
<p>那贷款这里的本质是什么？就是利息只和你每个月欠款以及利率有关！这简直是屁话，太简单了，但你不能理解他，就容易被套上等额本金、等额本息、提前还贷的外壳给忽悠了。</p>
<p>再或者说这里的本质就是：你去搞清楚每个月的还款是怎么计算的。月供&#x3D;本月所欠X利率+本月还掉的本金  这是个核心公式，差别在每个月还掉的本金不一样！</p>
<p>就这样吧该懂的也该看懂了，看不懂的大概怎么样也看不懂！只能说是蠢，这些人肯定理科不好、逻辑不行，必定做不了程序员。</p>
<p>比如网上流传的如图总结的所有结论都是错的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230704215506179.png" alt="image-20230704215506179"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/06/20/实战瓶颈定位-我的MySQL为什么压不上去/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/06/20/实战瓶颈定位-我的MySQL为什么压不上去/" itemprop="url">实战瓶颈定位-我的MySQL为什么压不上去</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-06-20T17:30:03+08:00">
                2023-06-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="实战瓶颈定位-我的MySQL为什么压不上去"><a href="#实战瓶颈定位-我的MySQL为什么压不上去" class="headerlink" title="实战瓶颈定位-我的MySQL为什么压不上去"></a>实战瓶颈定位-我的MySQL为什么压不上去</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>环境两台云上相同 128C的EC2(有点豪)，一台当压力机一台当服务器，用Sysbench测试MySQL纯读场景，不存在任何修改，也就几乎没有锁</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#uname -r</span><br><span class="line">5.10.84.aarch64</span><br><span class="line"></span><br><span class="line">Server:            MySQL</span><br><span class="line">Server version:        8.0.18 Source distribution</span><br></pre></td></tr></table></figure>

<p>EC2机器128核，故意只给MySQLD绑定了其中的24Core，网卡32队列</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#ethtool -l eth0</span><br><span class="line">Channel parameters for eth0:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:        0</span><br><span class="line">TX:        0</span><br><span class="line">Other:        0</span><br><span class="line">Combined:    32</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:        0</span><br><span class="line">TX:        0</span><br><span class="line">Other:        0</span><br><span class="line">Combined:    32</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/FlDlXFTuGa0BPv1YxR3KQZaP40de.png" alt="img"></p>
<h2 id="压测过程"><a href="#压测过程" class="headerlink" title="压测过程"></a>压测过程</h2><p>走同一交换机内网IP压MySQL跑不满CPU，跑压力和不跑压力时ping rtt 分别是 0.859&#x2F;0.053(RTT 有增加–注意点), 此时TPS：119956.67 1000并发 RT 8.33</p>
<p>下图是压测时 htop 看到的MySQLD 所在EC2的 CPU使用情况，右边65-88是MySQLD进程(绿色表示us, 红色表示sys+si CPU)</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511125934259.png" alt="image-20230511125934259"></p>
<p>用top查看详细的每个 core 使用(只展示MySQLD使用的24core ，top 然后按1–还可以试试2&#x2F;3，有惊喜)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">top - 13:49:55 up 160 days, 18:10,  3 users,  load average: 555.26, 720.12, 462.21</span><br><span class="line">Tasks: 1065 total,   1 running, 499 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Node1 : 10.1 us,  5.3 sy,  0.0 ni, 83.3 id,  0.0 wa,  0.0 hi,  1.3 si,  0.0 st</span><br><span class="line">%Cpu64 : 29.3 us, 16.5 sy,  0.0 ni, 54.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu65 : 37.0 us, 18.5 sy,  0.0 ni, 26.9 id,  0.0 wa,  0.0 hi, 17.5 si,  0.0 st</span><br><span class="line">%Cpu66 : 34.2 us, 17.8 sy,  0.0 ni, 47.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu67 : 26.0 us, 15.1 sy,  0.0 ni, 58.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu68 : 26.1 us, 14.8 sy,  0.0 ni, 59.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu69 : 27.2 us, 13.8 sy,  0.0 ni, 59.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu70 : 25.7 us, 11.8 sy,  0.0 ni, 62.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu71 : 18.3 us, 10.6 sy,  0.0 ni, 71.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu72 : 29.7 us, 12.6 sy,  0.0 ni, 57.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu73 : 21.2 us, 13.0 sy,  0.0 ni, 65.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu74 : 18.9 us, 10.8 sy,  0.0 ni, 70.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu75 : 28.9 us, 15.1 sy,  0.0 ni, 36.1 id,  0.0 wa,  0.0 hi, 19.9 si,  0.0 st</span><br><span class="line">%Cpu76 : 30.3 us, 15.5 sy,  0.0 ni, 54.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu77 : 25.1 us, 13.2 sy,  0.0 ni, 61.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu78 : 18.2 us, 10.3 sy,  0.0 ni, 71.5 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu79 : 14.9 us,  8.8 sy,  0.0 ni, 76.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu80 : 23.4 us, 12.2 sy,  0.0 ni, 64.4 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu81 : 35.3 us, 17.6 sy,  0.0 ni, 30.2 id,  0.0 wa,  0.0 hi, 16.9 si,  0.0 st</span><br><span class="line">%Cpu82 : 28.2 us, 16.1 sy,  0.0 ni, 55.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu83 : 37.5 us, 16.9 sy,  0.0 ni, 27.0 id,  0.0 wa,  0.0 hi, 18.6 si,  0.0 st</span><br><span class="line">%Cpu84 : 35.4 us, 18.5 sy,  0.0 ni, 46.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu85 : 27.9 us, 16.8 sy,  0.0 ni, 55.2 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu86 : 28.2 us, 13.7 sy,  0.0 ni, 58.1 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu87 : 27.2 us, 11.0 sy,  0.0 ni, 61.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu88 :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br></pre></td></tr></table></figure>

<p>继续尝试用2000并发，TPS、CPU、ping rtt都和1000并发没有区别，当然按照我们以前QPS、RT理论2000并发的时候RT应该翻倍，实际确实是16.66，<strong>所以这里的问题就是翻倍的 RT哪里来的瓶颈就在哪里</strong>。</p>
<p>也试过用两个压力机每个压力机分别用1000并发同时压，QPS一样稳定——目的快速排除压力端、链路上有瓶颈。</p>
<p>写到这里RT 刚好翻倍16.66&#x3D;8.33*2 数字精准得好像编故事一样，不得不贴一下原始数据证实一下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511130851332.png" alt="image-20230511130851332"></p>
<p>1000 并发和2000并发时的ping RTT对比(ttl 64说明内网直达)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#ping mysqld27</span><br><span class="line">PING yt27 (mysqld217) 56(84) bytes of data.</span><br><span class="line">---以下是2000并发</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=1 ttl=64 time=0.867 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=2 ttl=64 time=0.952 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=3 ttl=64 time=0.849 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=4 ttl=64 time=0.857 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=5 ttl=64 time=0.987 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=6 ttl=64 time=0.860 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=7 ttl=64 time=0.909 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=8 ttl=64 time=0.875 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=9 ttl=64 time=0.979 ms  </span><br><span class="line">---终止压测，无无压力的rtt</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=10 ttl=64 time=0.104 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=11 ttl=64 time=0.079 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=12 ttl=64 time=0.075 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=13 ttl=64 time=0.075 ms </span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=14 ttl=64 time=0.074 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=15 ttl=64 time=0.078 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=16 ttl=64 time=0.075 ms</span><br><span class="line">---开启1000并发时的rtt</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=17 ttl=64 time=0.872 ms </span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=18 ttl=64 time=0.969 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=19 ttl=64 time=0.862 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=20 ttl=64 time=0.877 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=21 ttl=64 time=0.961 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=22 ttl=64 time=0.828 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=23 ttl=64 time=0.098 ms</span><br><span class="line">64 bytes from mysqld27 (mysqld217): icmp_seq=24 ttl=64 time=0.083 ms</span><br></pre></td></tr></table></figure>

<h3 id="抓包证明"><a href="#抓包证明" class="headerlink" title="抓包证明"></a>抓包证明</h3><p>在抓保证明前推荐一个工具快速绕过抓包(原理也是通过pcap lib去分析网络包，tcpdump也会调用pcap lib)</p>
<p><a href="https://github.com/y123456yz/tcprstat" target="_blank" rel="noopener">监控tcprstat</a>，从网络层抓包来对比两个并发下的RT：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#tcprstat -p 14822 -t 1 -n 0 -l mysqld217 -f &quot;%T\t\t%n\t\t%a\n&quot;</span><br><span class="line">timestamp        count        avg</span><br><span class="line">1683785023        50743        626</span><br><span class="line">1683785024        120004        100</span><br><span class="line">1683785025        120051        103</span><br><span class="line">1683785026        120042        102</span><br><span class="line">1683785027        120031        103</span><br><span class="line">1683785028        120034        104</span><br><span class="line">1683785029        120034        104</span><br><span class="line">1683785030         55209        103    ---以上是2000并发</span><br><span class="line">1683785038        0        0</span><br><span class="line">1683785039        0        0</span><br><span class="line">1683785040         55224        614    ---以下是1000并发</span><br><span class="line">1683785041        119998        104</span><br><span class="line">1683785042        120039        105</span><br><span class="line">1683785043        120039        105</span><br><span class="line">1683785044        120026        107</span><br><span class="line">1683785045        120039        108</span><br><span class="line">1683785046        120047        108</span><br><span class="line">1683785047        120037        108</span><br><span class="line">1683785048        120032        108</span><br><span class="line">1683785049        120041        108</span><br></pre></td></tr></table></figure>

<p>也就是网卡层面<strong>确认了压不上去瓶颈不在MySQL</strong> 上，加并发后网卡的RT没变(网卡RT包含MySQLD RT)，因为ping RTT 在1000和2000并发也没有差异，推测交换机不是瓶颈，大概率出网卡的虚拟层面</p>
<p>在客户端的机器上抓包，上面我们说过了1000并发的RT是8.33毫秒：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511141508811.png" alt="image-20230511141508811"></p>
<p>注意上图，我把RT排序了，明显看到5ms到17ms 中间没有这个RT范围的包，但是有很多25ms的RT，平均下来确实是8.33毫秒，留下一个疑问：RT分布不符合正态，而且中间有很大一段范围镂空了！这是不应该的。</p>
<p>同样我们再到MySQLD 所在机器抓包分析(注：正常路径先抓MySQLD上的包就行了)：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511141925557.png" alt="image-20230511141925557"></p>
<p>同样是对RT 排序了，但是慢的RT都是对端发慢了(注意最右边的select， MySQL相应是 response)，同样对这个抓包求平均时间就是tcprstat 看到的103微秒，也就是0.1毫秒。如下图红框是请求，请求的间隔是11毫米，绿框是响应，响应的间隔都是0.2ms不到</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230513084610300.png" alt="image-20230513084610300"></p>
<p>同样在2000并发时也对MySQLD所在网卡抓包对比，response 的RT 没有变化，从这里可以看出瓶颈点在sysbench 和 MySQLD 的网卡之间的链路上，似乎有限流、管控</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230512084446715.png" alt="image-20230512084446715" style="zoom:35%;">

<h3 id="快速验证"><a href="#快速验证" class="headerlink" title="快速验证"></a>快速验证</h3><p>到这里我们已经找到了有力的证据，RT是在离开MySQLD网卡后增加上去的，先验证下走走本机127.0.0.1快速压一把，让sysbench 跑在0-7 core上，这时可以看到MySQL跑满了CPU，下图左边1-8核是压力进程，右边65-88是业务进程，TPS：239969.91 1000并发 RT 4.16</p>
<p>htop状态：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511125346066.png" alt="image-20230511125346066"></p>
<p>各CPU 详细分析：</p>
<ul>
<li>us MySQL解析SQL、处理查询</li>
<li>si  网络软中断</li>
<li>sy OS 的sys API 消耗，一般用户进程会调用系统 API, 比如读写文件、分配内存、网络访问等</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">//sysbench</span><br><span class="line">top - 13:44:27 up 160 days, 18:04,  3 users,  load average: 792.17, 619.09, 311.58</span><br><span class="line">Tasks: 1073 total,   1 running, 500 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu0  : 14.0 us, 29.1 sy,  0.0 ni, 33.3 id,  0.0 wa,  0.0 hi, 23.5 si,  0.0 st</span><br><span class="line">%Cpu1  : 12.5 us, 33.0 sy,  0.0 ni, 33.7 id,  0.0 wa,  0.0 hi, 20.8 si,  0.0 st</span><br><span class="line">%Cpu2  : 11.2 us, 32.7 sy,  0.0 ni, 34.2 id,  0.0 wa,  0.0 hi, 21.9 si,  0.0 st</span><br><span class="line">%Cpu3  : 13.4 us, 31.2 sy,  0.0 ni, 34.4 id,  0.0 wa,  0.0 hi, 21.0 si,  0.0 st</span><br><span class="line">%Cpu4  : 12.1 us, 31.3 sy,  0.0 ni, 34.2 id,  0.0 wa,  0.0 hi, 22.4 si,  0.0 st</span><br><span class="line">%Cpu5  : 10.5 us, 31.8 sy,  0.0 ni, 33.6 id,  0.0 wa,  0.0 hi, 24.1 si,  0.0 st</span><br><span class="line">%Cpu6  : 12.9 us, 31.3 sy,  0.0 ni, 34.2 id,  0.0 wa,  0.0 hi, 21.6 si,  0.0 st</span><br><span class="line">%Cpu7  : 12.3 us, 31.4 sy,  0.0 ni, 34.3 id,  0.0 wa,  0.0 hi, 22.0 si,  0.0 st</span><br><span class="line">%Cpu8  :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line"></span><br><span class="line">//MySQLD</span><br><span class="line">Tasks: 1073 total,   1 running, 505 sleeping,   0 stopped,   1 zombie</span><br><span class="line">%Node1 : 22.6 us, 10.1 sy,  0.0 ni, 62.4 id,  0.0 wa,  0.0 hi,  4.8 si,  0.0 st</span><br><span class="line">%Cpu64 : 57.9 us, 29.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.9 si,  0.0 st</span><br><span class="line">%Cpu65 : 60.3 us, 26.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 13.6 si,  0.0 st</span><br><span class="line">%Cpu66 : 57.6 us, 28.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 14.2 si,  0.0 st</span><br><span class="line">%Cpu67 : 60.9 us, 25.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 13.6 si,  0.0 st</span><br><span class="line">%Cpu68 : 59.9 us, 26.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 13.9 si,  0.0 st</span><br><span class="line">%Cpu69 : 57.9 us, 27.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 14.6 si,  0.0 st</span><br><span class="line">%Cpu70 : 61.3 us, 26.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.6 si,  0.0 st</span><br><span class="line">%Cpu71 : 64.0 us, 23.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.5 si,  0.0 st</span><br><span class="line">%Cpu72 : 61.3 us, 26.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 11.9 si,  0.0 st</span><br><span class="line">%Cpu73 : 63.0 us, 22.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 14.2 si,  0.0 st</span><br><span class="line">%Cpu74 : 61.4 us, 27.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 11.2 si,  0.0 st</span><br><span class="line">%Cpu75 : 63.9 us, 26.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  9.6 si,  0.0 st</span><br><span class="line">%Cpu76 : 61.3 us, 27.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 11.6 si,  0.0 st</span><br><span class="line">%Cpu77 : 55.0 us, 30.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 14.6 si,  0.0 st</span><br><span class="line">%Cpu78 : 60.9 us, 26.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.3 si,  0.0 st</span><br><span class="line">%Cpu79 : 58.4 us, 26.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 14.9 si,  0.0 st</span><br><span class="line">%Cpu80 : 58.7 us, 29.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.2 si,  0.0 st</span><br><span class="line">%Cpu81 : 62.6 us, 27.2 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 10.3 si,  0.0 st</span><br><span class="line">%Cpu82 : 61.9 us, 25.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.6 si,  0.0 st</span><br><span class="line">%Cpu83 : 58.7 us, 27.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 13.9 si,  0.0 st</span><br><span class="line">%Cpu84 : 59.4 us, 27.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.9 si,  0.0 st</span><br><span class="line">%Cpu85 : 58.9 us, 28.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 12.6 si,  0.0 st</span><br><span class="line">%Cpu86 : 58.4 us, 28.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 13.2 si,  0.0 st</span><br><span class="line">%Cpu87 : 61.1 us, 27.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 11.6 si,  0.0 st</span><br></pre></td></tr></table></figure>

<p>就以上sysbench VS  MySQLD 的CPU 消耗来看，因为sysbench 处理逻辑简单，就是发SQL给MySQLD，所以 sysbench自身US很少，大部分都是调用OS的网络操作，而MySQLD有 60% CPU用于US，也就是自身业务逻辑，MySQLD收到SQL要做SQL解析，要去查找数据，这些都是用户态消耗，找到数据后走网络发给Sysbench，这部分是sy </p>
<p>到这里可以拿着证据去VIP通道(土豪+专业的客户得有VIP通道)找做网络管控的了，不会再有撕逼和甩锅</p>
<h3 id="sysbench-结果不是正态分布"><a href="#sysbench-结果不是正态分布" class="headerlink" title="sysbench 结果不是正态分布"></a>sysbench 结果不是正态分布</h3><p>把所有请求RT 分布进行图形化，此时平均 RT 8.33，理论上是一个正态分布，下图是有限速时：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"> 3.615 |                                         2177</span><br><span class="line"> 3.681 |**                                       14738</span><br><span class="line"> 3.748 |*******                                  55690</span><br><span class="line"> 3.816 |*************                            109713</span><br><span class="line"> 3.885 |***************                          121830</span><br><span class="line"> 3.956 |***************                          124851</span><br><span class="line"> 4.028 |*******************                      154927</span><br><span class="line"> 4.101 |***********************                  188826</span><br><span class="line"> 4.176 |***************************              226206</span><br><span class="line"> 4.252 |************************************     302617</span><br><span class="line"> 4.329 |**************************************** 333310  //这里以4.329为中心符合正态</span><br><span class="line"> 4.407 |*******************************          257048</span><br><span class="line"> 4.487 |********************                     163100</span><br><span class="line"> 4.569 |************                             101785</span><br><span class="line"> 4.652 |********                                 63871</span><br><span class="line"> 4.737 |*****                                    43998</span><br><span class="line"> 4.823 |*****                                    40854</span><br><span class="line"> 4.910 |*****                                    42189</span><br><span class="line"> 4.999 |*****                                    41182</span><br><span class="line"> 5.090 |****                                     35652</span><br><span class="line"> 5.183 |****                                     30343</span><br><span class="line"> 5.277 |***                                      28573</span><br><span class="line"> 5.373 |***                                      24763</span><br><span class="line"> 5.470 |***                                      22210</span><br><span class="line"> 5.570 |***                                      21808</span><br><span class="line"> 5.671 |***                                      25606</span><br><span class="line"> 5.774 |***                                      26994</span><br><span class="line"> 5.879 |***                                      24672</span><br><span class="line"> 5.986 |***                                      22087</span><br><span class="line"> 6.095 |**                                       18466</span><br><span class="line"> 6.205 |**                                       14822</span><br><span class="line"> 6.318 |**                                       13688</span><br><span class="line"> 6.433 |**                                       15381</span><br><span class="line"> 6.550 |**                                       13573</span><br><span class="line"> 6.669 |*                                        11325</span><br><span class="line"> 6.790 |*                                        9442</span><br><span class="line"> 6.913 |*                                        7412</span><br><span class="line"> 省略一大堆</span><br><span class="line">20.736 |*                                        11407</span><br><span class="line">21.112 |*                                        9755</span><br><span class="line">21.496 |*                                        8957</span><br><span class="line">21.886 |*                                        9434</span><br><span class="line">22.284 |*                                        9715</span><br><span class="line">22.689 |**                                       12774</span><br><span class="line">23.101 |**                                       17000</span><br><span class="line">23.521 |***                                      22937</span><br><span class="line">23.948 |*****                                    40401</span><br><span class="line">24.384 |********                                 65370</span><br><span class="line">24.827 |**********                               82186</span><br><span class="line">25.278 |**********                               85505</span><br><span class="line">25.737 |***********                              94347 //以25.7附近大概又是一个新正态</span><br><span class="line">26.205 |**********                               82958</span><br><span class="line">26.681 |****                                     30760</span><br><span class="line">27.165 |                                         2222</span><br><span class="line">27.659 |                                         69</span><br><span class="line">28.162 |                                         16</span><br><span class="line">28.673 |                                         15</span><br><span class="line">29.194 |                                         20</span><br><span class="line">29.725 |                                         17</span><br></pre></td></tr></table></figure>

<p>去掉限速后平均 RT 3.26(比下图中大概的中位数2.71大了不少)  完美正态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">1.857 |**                                       19894</span><br><span class="line">1.891 |***                                      23569</span><br><span class="line">1.925 |***                                      27912</span><br><span class="line">1.960 |****                                     33720</span><br><span class="line">1.996 |****                                     39892</span><br><span class="line">2.032 |*****                                    48289</span><br><span class="line">2.069 |******                                   57649</span><br><span class="line">2.106 |********                                 69437</span><br><span class="line">2.145 |*********                                83611</span><br><span class="line">2.184 |***********                              99507</span><br><span class="line">2.223 |*************                            119275</span><br><span class="line">2.264 |****************                         141013</span><br><span class="line">2.305 |*******************                      165450</span><br><span class="line">2.347 |**********************                   191778</span><br><span class="line">2.389 |*************************                219706</span><br><span class="line">2.433 |****************************             250885</span><br><span class="line">2.477 |*******************************          278379</span><br><span class="line">2.522 |**********************************       303931</span><br><span class="line">2.568 |*************************************    325777</span><br><span class="line">2.615 |***************************************  342948</span><br><span class="line">2.662 |**************************************** 354029</span><br><span class="line">2.710 |**************************************** 356295</span><br><span class="line">2.760 |**************************************** 353068</span><br><span class="line">2.810 |**************************************   341345</span><br><span class="line">2.861 |************************************     324600</span><br><span class="line">2.913 |**********************************       303525</span><br><span class="line">2.966 |*******************************          280221</span><br><span class="line">3.020 |*****************************            255042</span><br><span class="line">3.075 |**************************               230861</span><br><span class="line">3.130 |***********************                  206909</span><br><span class="line">3.187 |*********************                    184616</span><br><span class="line">3.245 |*******************                      164903</span><br><span class="line">3.304 |****************                         146199</span><br><span class="line">3.364 |***************                          131427</span><br><span class="line">3.425 |*************                            117059</span><br><span class="line">3.488 |************                             104954</span><br><span class="line">3.551 |***********                              94404</span><br><span class="line">3.615 |*********                                83739</span><br><span class="line">3.681 |********                                 75705</span><br><span class="line">3.748 |********                                 67944</span><br><span class="line">3.816 |*******                                  60727</span><br><span class="line">3.885 |******                                   53757</span><br><span class="line">3.956 |*****                                    47053</span><br><span class="line">4.028 |*****                                    42130</span><br><span class="line">4.101 |****                                     38069</span><br><span class="line">4.176 |****                                     33666</span><br><span class="line">4.252 |***                                      30048</span><br><span class="line">4.329 |***                                      26923</span><br><span class="line">4.407 |***                                      23886</span><br><span class="line">4.487 |**                                       21615</span><br><span class="line">4.569 |**                                       19897</span><br><span class="line">4.652 |**                                       18458</span><br><span class="line">4.737 |**                                       17729</span><br><span class="line">4.823 |**                                       17041</span><br><span class="line">4.910 |**                                       16011</span><br><span class="line">4.999 |**                                       16099</span><br><span class="line">5.090 |**                                       16090</span><br><span class="line">5.183 |**                                       16393</span><br><span class="line">5.277 |**                                       16729</span><br><span class="line">5.373 |**                                       17412</span><br></pre></td></tr></table></figure>

<h2 id="用其他网络业务验证"><a href="#用其他网络业务验证" class="headerlink" title="用其他网络业务验证"></a>用其他网络业务验证</h2><p>先测试一下网络下载时的ping：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">--无流量</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=11 ttl=64 time=0.075 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=12 ttl=64 time=0.080 ms</span><br><span class="line">--从有网络限速的机器下载，带宽100MB</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=13 ttl=64 time=0.738 ms </span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=14 ttl=64 time=0.873 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=15 ttl=64 time=0.993 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=16 ttl=64 time=0.859 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=17 ttl=64 time=0.892 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=18 ttl=64 time=0.972 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=19 ttl=64 time=1.05 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=20 ttl=64 time=0.973 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=21 ttl=64 time=0.997 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=22 ttl=64 time=0.915 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=23 ttl=64 time=0.892 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=24 ttl=64 time=0.960 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=25 ttl=64 time=1.05 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=26 ttl=64 time=0.089 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=27 ttl=64 time=0.097 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=28 ttl=64 time=0.081 ms </span><br><span class="line">--从没有网络限速的机器下载，带宽1000MB</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=29 ttl=64 time=0.078 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=30 ttl=64 time=0.077 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=31 ttl=64 time=0.073 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=32 ttl=64 time=0.072 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=33 ttl=64 time=0.079 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=34 ttl=64 time=0.074 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=35 ttl=64 time=0.080 ms</span><br><span class="line">64 bytes from 172.16.0.205: icmp_seq=36 ttl=64 time=0.077 ms</span><br></pre></td></tr></table></figure>

<p>有限速方向，尝试了BBR和cubic 拥塞算法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#tcpperf -c 172.16.0.205 -t 100</span><br><span class="line">Connected mysqld217:51254 -&gt; 172.16.0.205:2009, congestion control: cubic</span><br><span class="line">Time (s)  Throughput   Bitrate    Cwnd    Rwnd  sndbuf  ssthresh  Retr  CA  Pacing  rtt/var</span><br><span class="line">  0.000s   0.00kB/s   0.00kbps  14.3Ki  41.3Ki  85.0Ki    2048Mi     0   0  65.2Mi  427us/213</span><br><span class="line">  1.029s    122MB/s    975Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/8</span><br><span class="line">  2.005s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/10</span><br><span class="line">  3.010s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/17</span><br><span class="line">  4.016s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/13</span><br><span class="line">  5.022s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/14</span><br><span class="line">  6.028s    105MB/s    842Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/17</span><br><span class="line">  7.003s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/15</span><br><span class="line">  8.009s    105MB/s    843Mbps  1595Ki  1595Ki  9512Ki     576Ki     0   0   123Mi  15.2ms/13</span><br><span class="line"> #tcpperf -c 172.16.0.205 -t 100</span><br><span class="line">Connected mysqld217:51932 -&gt; 172.16.0.205:2009, congestion control: bbr</span><br><span class="line">Time (s)  Throughput   Bitrate    Cwnd    Rwnd  sndbuf  ssthresh  Retr  CA  Pacing  rtt/var</span><br><span class="line">  0.000s   0.00kB/s   0.00kbps  14.3Ki  41.3Ki   128Ki    2048Mi     0   0  98.0Mi  406us/203</span><br><span class="line">  1.011s    120MB/s    957Mbps   271Ki  2281Ki  10.4Mi     560Ki  2244   0   108Mi  2427us/11</span><br><span class="line">  2.033s    104MB/s    831Mbps   271Ki  2281Ki  10.4Mi     560Ki  1056   0   109Mi  2417us/18</span><br><span class="line">  3.021s    104MB/s    830Mbps   274Ki  2281Ki  10.4Mi     560Ki  1056   0   109Mi  2428us/18</span><br><span class="line">  4.014s    103MB/s    827Mbps   271Ki  2281Ki  10.4Mi     560Ki  1452   0   108Mi  2423us/19</span><br><span class="line">  5.031s    104MB/s    835Mbps   274Ki  2281Ki  10.4Mi     560Ki   660   0  80.2Mi  2435us/22</span><br><span class="line">  6.033s    102MB/s    818Mbps   271Ki  2272Ki  10.4Mi     560Ki  2112   0   109Mi  2426us/17</span><br><span class="line">  7.030s    103MB/s    823Mbps   274Ki  2281Ki  10.4Mi     560Ki  1716   0   117Mi  2430us/18</span><br><span class="line">  8.023s    103MB/s    826Mbps   274Ki  2281Ki  10.4Mi     560Ki  1452   0   109Mi  2428us/20</span><br><span class="line">  9.016s    103MB/s    827Mbps   271Ki  2281Ki  10.4Mi     560Ki  1452   0   108Mi  2423us/15</span><br></pre></td></tr></table></figure>

<p>跑tcpperf触发限速时的监控(上下两个窗口是同一台机器)，红色是丢包率挺高的，绿色丢包就没了，应该是拥塞算法和限速管控达成了平衡</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230511215940306.png" alt="image-20230511215940306"></p>
<p>反过来限速被我去掉了(限速可以进出双向单独控制)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#tcpperf -c mysqld217 -t 1000</span><br><span class="line">Connected 172.16.0.205:32186 -&gt; mysqld217:2009, congestion control: bbr</span><br><span class="line">Time (s)  Throughput   Bitrate    Cwnd    Rwnd  sndbuf  ssthresh  Retr  CA  Pacing  rtt/var</span><br><span class="line">  0.000s   0.00kB/s   0.00kbps  14.3Ki  41.3Ki   128Ki    2048Mi     0   0   100Mi  397us/198</span><br><span class="line">  1.001s   1107MB/s   8859Mbps   471Ki   985Ki  4641Ki     277Ki     0   0  1083Mi  390us/22</span><br><span class="line">  2.001s   1103MB/s   8823Mbps   465Ki   985Ki  4641Ki     277Ki     0   0  1089Mi  393us/16</span><br><span class="line">  3.000s   1111MB/s   8892Mbps   465Ki   985Ki  4641Ki     277Ki     0   0  1072Mi  403us/25</span><br><span class="line">  4.000s   1099MB/s   8789Mbps   459Ki   985Ki  4794Ki     277Ki     0   0   799Mi  399us/18</span><br><span class="line">  5.001s   1098MB/s   8786Mbps   459Ki   985Ki  4794Ki     277Ki     0   0  1066Mi  387us/12</span><br><span class="line">  6.000s   1100MB/s   8799Mbps   462Ki   974Ki  4794Ki     277Ki     0   0  1069Mi  399us/16</span><br><span class="line">  7.001s   1135MB/s   9078Mbps   453Ki   985Ki  4794Ki     277Ki     0   0  1059Mi  377us/19</span><br></pre></td></tr></table></figure>

<p>查看限速配置如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;txcmbps:844.000, txckpps:120.000&#125;</span><br><span class="line"></span><br><span class="line">//限速解释</span><br><span class="line">0-31 我猜这是网卡队列(可以修改);</span><br><span class="line">txcmbps:844.000 105.5MB/s     每秒带宽105.5MB</span><br><span class="line">txckpps:120.000 120K packet/s 每秒12万网络包</span><br></pre></td></tr></table></figure>

<p>sysbench(主键查询-小包) 12万QPS 正好命中 txckpps:120，tcpperf (大包)稳定的105MB带宽命中txcmbps:844</p>
<p>去掉后长这样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#ovsctl -n set_out_pps -v -1  //把pps限制为-1==不限制</span><br><span class="line">#ovsctl set_tx -p &#123;&#125; -r -1;   //带宽不限制</span><br><span class="line"></span><br><span class="line">&#123;vport:  2 &#123;map:  0, prio:L, weight:   0&#125;meter: &#123;-&#125;queue: [  0- 31L]&#125;</span><br></pre></td></tr></table></figure>

<p>对这块网络管控感兴趣可以去了解一下 ovs 这个开源项目(open virtual switch)</p>
<h3 id="去掉网卡限速后的结果"><a href="#去掉网卡限速后的结果" class="headerlink" title="去掉网卡限速后的结果"></a>去掉网卡限速后的结果</h3><p>实际结构如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230513132101185.png" alt="image-20230513132101185"></p>
<p>放开所有网络控制后，1000并发压力 30万QPS，RT 3.28，此时从sysbench 以及空闲机器ping MySQLD机器的 RTT和没压力基本一致</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230512090205685.png" alt="image-20230512090205685"></p>
<p>top状态：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">%Node1 : 23.4 us, 12.3 sy,  0.0 ni, 61.4 id,  0.0 wa,  0.0 hi,  3.0 si,  0.0 st</span><br><span class="line">%Cpu64 : 63.2 us, 36.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu65 : 44.4 us, 21.9 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 33.8 si,  0.0 st</span><br><span class="line">%Cpu66 : 66.6 us, 33.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu67 : 63.4 us, 36.6 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu68 : 64.2 us, 35.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu69 : 64.9 us, 35.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu70 : 66.6 us, 33.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu71 : 65.3 us, 34.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu72 : 67.7 us, 32.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu73 : 63.6 us, 36.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu74 : 66.7 us, 33.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu75 : 42.4 us, 19.9 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 37.7 si,  0.0 st</span><br><span class="line">%Cpu76 : 63.9 us, 36.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu77 : 67.0 us, 33.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu78 : 68.3 us, 31.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu79 : 64.9 us, 35.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu80 : 65.2 us, 34.8 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu81 : 44.4 us, 21.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 34.1 si,  0.0 st</span><br><span class="line">%Cpu82 : 63.9 us, 36.1 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu83 : 44.2 us, 23.4 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi, 32.3 si,  0.0 st</span><br><span class="line">%Cpu84 : 65.7 us, 34.3 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu85 : 68.3 us, 31.7 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu86 : 67.5 us, 32.5 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu87 : 62.4 us, 37.6 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu88 :  0.0 us,  0.0 sy,  0.0 ni,100.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230512092713141.png" alt="image-20230512092713141"></p>
<p>小思考：</p>
<blockquote>
<p>我们中间尝试走本机127.0.0.1 压测时QPS 是24万，比跨机器压的 30万打了8折，想想为什么？网络延时消耗完全没影响？</p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>简单可复制的证明办法：抓包，快速撕逼和分析</p>
<p>肯定有很多人想到：内存、磁盘、线程池、队列、网络等等原因，但是这些所有原因有一个共同的爹：RT，所有这些影响因素最后体现出来就是RT 高了，你CPU资源不够、内存慢最后总表现就是在客户端看来你的 RT 太高。</p>
<p>所以我们去掉这些复杂因素先在MySQLD所在EC2 的网卡上抓一个包看看RT，再对比一下1000&#x2F;2000并发时抓包看到的 RT 有没有升高，如果有升高说明问题在MySQLD这端(含OS、MySQLD的问题)，如果 RT 不变那么问题不在MySQLD这端，并且从EC2网卡出去都是很快的，那么问题只能是在路上或者客户端的sysbench自己慢了。</p>
<p>这是我们星球里说的无招胜有招–抓包大法，扯皮过程中我还没见过几个不认网络抓包的，也有那么一两个扯上是不是网卡驱动有问题，我的代码不会有问题</p>
<p>两个限速条件：pps 120k(每秒最多12万网络包)，带宽 844mbps&#x3D;105.5MB&#x2F;s</p>
<p>Sysbench 查询都是小包，触发第一个条件，tcpperf触发第二个条件</p>
<p>ping ping神功失效了吗？也没有，我后来又测试了100、200并发，rtt 0.2ms和0.4ms，也就是说随着并发的增加rtt 增加到0.8ms后就不再增加了。上来1000并发已经到了天花板</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=159 ttl=64 time=0.226 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=160 ttl=64 time=0.334 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=161 ttl=64 time=0.336 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=162 ttl=64 time=0.213 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=163 ttl=64 time=0.104 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=164 ttl=64 time=0.096 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=165 ttl=64 time=0.101 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=166 ttl=64 time=0.116 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=167 ttl=64 time=0.104 ms--以上 100并发，QPS 119K</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=168 ttl=64 time=0.093 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=169 ttl=64 time=0.088 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=170 ttl=64 time=0.405 ms--以下 200并发，QPS 119K</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=171 ttl=64 time=0.419 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=172 ttl=64 time=0.386 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=173 ttl=64 time=0.474 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=174 ttl=64 time=0.462 ms</span><br><span class="line">64 bytes from polardbxyt27 (mysqld217): icmp_seq=175 ttl=64 time=0.410 ms</span><br></pre></td></tr></table></figure>


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/06/08/Nginx reuseport 导致偶发性卡顿/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/06/08/Nginx reuseport 导致偶发性卡顿/" itemprop="url">Nginx reuseport 导致偶发性卡顿</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-06-08T17:30:03+08:00">
                2023-06-08
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Nginx-reuseport-导致偶发性卡顿"><a href="#Nginx-reuseport-导致偶发性卡顿" class="headerlink" title="Nginx reuseport 导致偶发性卡顿"></a>Nginx reuseport 导致偶发性卡顿</h1><p>by @橘橘球</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>从2018年开始，我们有个业务陆续接到反馈 Nginx 线上集群经常出现不响应或者偶发性的“超慢”请求。这种卡顿每天都有少量出现。而只有多个集群中的一个出现，其他压力更大的集群皆未出现。<br>业务结构比较简单：LVS-&gt;Nginx-&gt;后端，如图<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230607103449616.png" alt="image-20230607103449616"></p>
<p>一些观察到的现象：</p>
<ul>
<li>出问题前不久升级 Nginx 配置，打开了 reuseport 功能</li>
<li>在压力大的后端（upstream）服务环境不容易出现，后端压力轻对应的Nginx卡顿概率更高</li>
<li>关闭 reuseport 后 问题少了很多</li>
<li>失败的请求响应时间都是 0ms（Nginx日志不靠谱了）</li>
<li>从 Nginx 日志上看，所有失败的健康检查请求都是0ms 的499 错误码（健康检查设置超时是2秒），但实际出问题的时候有5s-2分钟没有任何日志输出（Nginx卡了这么久）要么是Nginx卡住没去accept，要么是accept了没响应</li>
<li>所有超时来自同一个worker(一个Nginx服务一般按照机器核数开启多个worker)</li>
</ul>
<p>并且已知，卡顿的原因是打开 reuseport 后，新进来的请求可以由内核 hash 派发给一个 Nginx woker ，避免了锁争抢以及惊群。但如果网络条件足够好，压力足够低，Nginx worker 一直来不及读完 receive buffer 中的内容时，就无法切换并处理其他的 request，于是在新请求的客户端会观测不间断的卡顿，而压力大的后端由于网络传输慢，经常卡顿，Nginx worker 反而有时间能处理别的请求。在调小 receive buffer 人为制造卡顿后该问题得以解决。</p>
<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>由于所述场景比较复杂，缺乏直接证据，打算通过构造一个较简单的环境来复现这个问题，并且在这个过程中抓包、观测Nginx worker的具体行为，验证这个假设。</p>
<h2 id="术语"><a href="#术语" class="headerlink" title="术语"></a>术语</h2><h3 id="快连接和慢连接"><a href="#快连接和慢连接" class="headerlink" title="快连接和慢连接"></a>快连接和慢连接</h3><ul>
<li>快连接：通常是传输时间短、传输量小的连接，耗时通常是ms级别</li>
<li>慢连接：通常是传输时间长、传输量大的连接，可以维持传输状态一段时间（如30s, 1min）</li>
</ul>
<p>在本次场景复现过程中，这两种连接都是短连接，每次请求开始前都需要三次握手建立连接，结束后都需要四次挥手销毁连接</p>
<h3 id="Epoll"><a href="#Epoll" class="headerlink" title="Epoll"></a>Epoll</h3><p>Nginx使用了epoll模型，epoll 是多路复用的一种实现。在多路复用的场景下，一个task（process）会批量处理多个socket，哪个来了数据就去读那个。这就意味着要公平对待所有这些socket，不能阻塞在任何socket的”数据读”上，也就是说不能在阻塞模式下针对任何socket调用recv&#x2F;recvfrom。  </p>
<p>epoll 每次循环为O(1) 操作，循环前会得到一个就绪队列，其中包含所有已经准备好的 socket stream（有数据可读），不需要循环全部 socket stream 读取数据，在循环后会将被读取数据的 stream 重新放回睡眠队列。睡眠队列中的 socket stream 有数据可读时，再唤醒加入到 就绪队列中。</p>
<p>epoll 伪代码 （不包含唤醒、睡眠）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">while(true) &#123;  </span><br><span class="line">    streamArr = getEpollReadyStream(); // 找到准备好的stream</span><br><span class="line">    for(Stream i: streamArr) &#123;         // 循环准备好的stream</span><br><span class="line">        doSomething();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="reuseport与惊群"><a href="#reuseport与惊群" class="headerlink" title="reuseport与惊群"></a>reuseport与惊群</h3><p>Nginx reuseport 选项解决惊群的问题：在 TCP 多进程&#x2F;线程场景中（B 图），服务端如果所有新连接只保存在一个 listen socket 的全连接队列中，那么多个进程&#x2F;线程去这个队列里获取（accept）新的连接，势必会出现多个进程&#x2F;线程对一个公共资源的争抢，争抢过程中，大量资源的损耗，也就会发生惊群现象。<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/reuseport-explained.jpg" alt="img"><br>而开启reuseport后（C 图)，有多个 listener 共同 bind&#x2F;listen 相同的 IP&#x2F;PORT，也就是说每个进程&#x2F;线程有一个独立的 listener，相当于每个进程&#x2F;线程独享一个 listener 的全连接队列，新的连接请求由内核hash分配，不需要多个进程&#x2F;线程竞争某个公共资源，能充分利用多核，减少竞争的资源消耗，效率自然提高了。  </p>
<p>但同时也是由于这个分配机制，避免了上下文切换，在服务压力不大，网络情况足够好的情况下，进程&#x2F;线程更有可能专注于持续读取某个慢连接数据而忽视快连接建立的请求，从而造成快连接方卡顿。  </p>
<h2 id="复现过程"><a href="#复现过程" class="headerlink" title="复现过程"></a>复现过程</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><ol>
<li>整体的架构是N个client-&gt;1个Nginx-&gt;N个server。因为卡顿原因和reuseport机制有关，和server数量无关，server数量设为任意数字都能复现，这里为了方便设成1。client数量设为2，为了将快连接和慢连接区分开便于抓包观测</li>
<li>用慢连接制造卡顿环境，用快连接观测卡顿。在快连接客户端进行观测和抓包</li>
<li>进程数量要足够少，使得同一个 worker 有几率分配到多个连接 <code>worker_processes 2</code></li>
<li>连接数目要足够多，慢连接数目&gt;&#x3D;进程数量，使得快连接在分配时，有一定概率分配到一个正在处理慢连接的worker上</li>
<li>reuseport: 这个配置要开启，卡顿现象才能观测到。<code>listen 8000 reuseport</code></li>
</ol>
<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">linux kernal version: 6.1  </span><br><span class="line">linux image: amazon/al2023-ami-2023.0.20230419.0-kernel-6.1-x86_64  </span><br><span class="line">instance type:  </span><br><span class="line">1X AWS t2.micro (1 vCPU, 1GiB RAM) – Nginx client(fast request)  </span><br><span class="line">3X AWS t3.micro (2 vCPU, 1GiB RAM) – Http server, Nginx server, Nginx client(slow request)</span><br></pre></td></tr></table></figure>

<h3 id="复现操作"><a href="#复现操作" class="headerlink" title="复现操作"></a>复现操作</h3><ol>
<li><p>在server instance上放置一个 2GiB 大文件（0000000000000000.data）和一个 3MiB 小文件（server.pcap），并开启一个http server</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python -m http.server 8000</span><br></pre></td></tr></table></figure>
</li>
<li><p>在Nginx instance上安装、配置好Nginx，并启动Nginx (注意要绑核！)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># install</span><br><span class="line">sudo yum install nginx</span><br><span class="line"># config (/etc/nginx/nginx.conf)</span><br><span class="line">user nginx;</span><br><span class="line">worker_processes 2;</span><br><span class="line">error_log /var/log/nginx/error.log notice;</span><br><span class="line">pid /run/nginx.pid;</span><br><span class="line"></span><br><span class="line">include /usr/share/nginx/modules/*.conf;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">    worker_connections 1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">    log_format  main  &apos;$remote_addr [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">                      &apos;status=$status body_bytes_sent=$body_bytes_sent &apos;</span><br><span class="line">                      &apos;rt=$request_time uct=&quot;$upstream_connect_time&quot; uht=&quot;$upstream_header_time&quot; urt=&quot;$upstream_response_time&quot;&apos;;</span><br><span class="line"></span><br><span class="line">    access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">    sendfile            on;</span><br><span class="line">    tcp_nopush          on;</span><br><span class="line">    keepalive_timeout   60;</span><br><span class="line">    types_hash_max_size 4096;</span><br><span class="line"></span><br><span class="line">    include             /etc/nginx/mime.types;</span><br><span class="line">    default_type        application/octet-stream;</span><br><span class="line"></span><br><span class="line">    # Load modular configuration files from the /etc/nginx/conf.d directory.</span><br><span class="line">    # See http://nginx.org/en/docs/ngx_core_module.html#include</span><br><span class="line">    # for more information.</span><br><span class="line">    include /etc/nginx/conf.d/*.conf;</span><br><span class="line"></span><br><span class="line">    server &#123;</span><br><span class="line">        listen       8000 reuseport;</span><br><span class="line">        server_name  server1;</span><br><span class="line">        root         /usr/share/nginx/html;</span><br><span class="line"></span><br><span class="line">        # Load configuration files for the default server block.</span><br><span class="line">        include /etc/nginx/default.d/*.conf;</span><br><span class="line">        </span><br><span class="line">        location / &#123;</span><br><span class="line">        proxy_pass http://172.31.86.252:8000; # server ip</span><br><span class="line">        proxy_set_header Host $host;</span><br><span class="line">        proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 404 /404.html;</span><br><span class="line">        location = /404.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        error_page 500 502 503 504 /50x.html;</span><br><span class="line">        location = /50x.html &#123;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"># start nginx</span><br><span class="line">sudo taskset -c 0 nginx</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动慢连接client，开启4个下载进程并计时，测试脚本<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/script/get_big_file.sh" target="_blank" rel="noopener">在此</a> </p>
</li>
<li><p>启动快连接client，开启1个下载进程并计时，抓包，测试脚本<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/script/get_small_file.sh" target="_blank" rel="noopener">在此</a><br>需要注意的是此处使用了curl –max-time 1，意味着即使1s内文件没有下载完，也会自动终止。</p>
</li>
<li><p>进入Nginx instance观察access.log</p>
</li>
<li><p>关掉reuseport或者调小recv buffer大小，重试一次</p>
</li>
</ol>
<h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>ip maping:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">172.31.86.252: http server</span><br><span class="line">172.31.89.152: nginx server</span><br><span class="line">172.31.91.109: 快连接 client</span><br><span class="line">172.31.92.10:  慢连接 client</span><br></pre></td></tr></table></figure>

<ol>
<li><p>快连接client端：下载同一个小文件的下载时长有快有慢，方差很大，完整日志<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-bench/client-runtime.txt" target="_blank" rel="noopener">在此</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[2023-05-31 08:27:32,127] runtime=1010</span><br><span class="line">[2023-05-31 08:27:33,140] runtime=1009</span><br><span class="line">[2023-05-31 08:27:34,152] runtime=38</span><br><span class="line">[2023-05-31 08:27:34,192] runtime=1011</span><br><span class="line">[2023-05-31 08:27:35,205] runtime=37</span><br><span class="line">[2023-05-31 08:27:35,245] runtime=1008</span><br><span class="line">[2023-05-31 08:27:36,256] runtime=57</span><br><span class="line">[2023-05-31 08:27:36,315] runtime=1011</span><br></pre></td></tr></table></figure>
</li>
<li><p>快连接client：无论耗时长短，抓包结果都显示存在不同程度卡顿，抓包文件<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-bench/nginx-case-client.pcap" target="_blank" rel="noopener">在此</a>  耗时长的下载过程<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/benchmark-pkg-cature1.png" alt="img"><br>耗时短的下载过程<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/benchmark-pkg-cature2.png" alt="img"></p>
</li>
<li><p>Nginx access.log 存在大量未下载完的200请求，和少量499请求，且499请求的耗时为0，access.log文件<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-bench/access.log.txt" target="_blank" rel="noopener">在此</a><br>卡顿的日志建立连接时长（utc）在0.3-0.4ms左右，超过1s的就出现499了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">172.31.91.109 [31/May/2023:08:27:49 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=102195 rt=0.790 uct=&quot;0.413&quot; uht=&quot;0.592&quot; urt=&quot;0.791&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:50 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.058 uct=&quot;0.000&quot; uht=&quot;0.002&quot; urt=&quot;0.053&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:51 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=499 body_bytes_sent=0 rt=0.000 uct=&quot;-&quot; uht=&quot;-&quot; urt=&quot;0.000&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:51 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=102195 rt=0.763 uct=&quot;0.400&quot; uht=&quot;0.580&quot; urt=&quot;0.763&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:52 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=102195 rt=0.767 uct=&quot;0.480&quot; uht=&quot;0.768&quot; urt=&quot;0.768&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:53 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=580007 rt=0.773 uct=&quot;0.330&quot; uht=&quot;0.431&quot; urt=&quot;0.773&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:55 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=499 body_bytes_sent=0 rt=0.000 uct=&quot;-&quot; uht=&quot;-&quot; urt=&quot;0.000&quot;</span><br><span class="line">172.31.91.109 [31/May/2023:08:27:55 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=499 body_bytes_sent=0 rt=0.000 uct=&quot;-&quot; uht=&quot;-&quot; urt=&quot;0.000&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>下载中途被关闭的连接（200），可以观测到Nginx server在客户端已经请求FIN并被ACK之后仍然在发送一些网络数据包，客户端非常迷惑，向Nginx发送RST<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/benchmark-pkg-cature3.png" alt="img"><br>未和Nginx建立连接就被关闭的连接（499），可以观测到连接始终没有被建立，在等待1s后客户端超时，主动请求关连接<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/benchmark-pkg-cature4.png" alt="img"></p>
<ol start="4">
<li>限制Nginx server所在的instance的recv buffer大小，重新进行实验，可以观测到仍然有少量停顿，但整体耗时好了很多，不再有长达1s的卡顿，也不再有RST，完整日志<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-exp1/" target="_blank" rel="noopener">在此</a>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl -w net.ipv4.tcp_rmem=&quot;40960 40960 40960&quot;</span><br></pre></td></tr></table></figure></li>
</ol>
<p>client runtime log: 耗时稳定在50-100ms，比无慢连接、纯跑快连接时要大一倍（25-50ms）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[2023-06-05 06:13:22,791] runtime=120</span><br><span class="line">[2023-06-05 06:13:22,913] runtime=82</span><br><span class="line">[2023-06-05 06:13:22,997] runtime=54</span><br><span class="line">[2023-06-05 06:13:23,054] runtime=61</span><br><span class="line">[2023-06-05 06:13:23,118] runtime=109</span><br><span class="line">[2023-06-05 06:13:23,229] runtime=58</span><br><span class="line">[2023-06-05 06:13:23,290] runtime=55</span><br><span class="line">[2023-06-05 06:13:23,347] runtime=79</span><br><span class="line">[2023-06-05 06:13:23,429] runtime=65</span><br><span class="line">[2023-06-05 06:13:23,497] runtime=53</span><br></pre></td></tr></table></figure>

<p>client 抓包结果：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/exp1-pkg-cature1.png" alt="img"><br>Nginx access.log: 都发完了，而且发得很流畅，建立连接时间（utc)非常短</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">172.31.91.109 [05/Jun/2023:06:13:22 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.101 uct=&quot;0.001&quot; uht=&quot;0.004&quot; urt=&quot;0.101&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:22 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.064 uct=&quot;0.001&quot; uht=&quot;0.002&quot; urt=&quot;0.064&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.044 uct=&quot;0.000&quot; uht=&quot;0.001&quot; urt=&quot;0.044&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.047 uct=&quot;0.000&quot; uht=&quot;0.001&quot; urt=&quot;0.047&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.100 uct=&quot;0.000&quot; uht=&quot;0.001&quot; urt=&quot;0.099&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.047 uct=&quot;0.000&quot; uht=&quot;0.001&quot; urt=&quot;0.047&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.045 uct=&quot;0.001&quot; uht=&quot;0.002&quot; urt=&quot;0.045&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:13:23 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=3602590 rt=0.066 uct=&quot;0.000&quot; uht=&quot;0.002&quot; urt=&quot;0.066&quot;</span><br></pre></td></tr></table></figure>

<p>对于慢连接大文件下载时长略有影响：46s (无限制) vs 53s (有限制)</p>
<ol start="5">
<li>关闭nginx reuseport</li>
</ol>
<p>卡顿依然大量存在，但大多以连接能够建立但是下载不完的形式（200）出现，499较少，并且存在惊群现象，完整日志<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-exp2/" target="_blank" rel="noopener">在此</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">    listen 8000;</span><br></pre></td></tr></table></figure>

<p>client runtime log：存在卡顿，和benchmark没有区别</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[2023-06-05 06:38:06,682] runtime=1008</span><br><span class="line">[2023-06-05 06:38:07,692] runtime=1008</span><br><span class="line">[2023-06-05 06:38:08,703] runtime=220</span><br><span class="line">[2023-06-05 06:38:08,926] runtime=112</span><br><span class="line">[2023-06-05 06:38:09,040] runtime=60</span><br><span class="line">[2023-06-05 06:38:09,103] runtime=865</span><br><span class="line">[2023-06-05 06:38:09,970] runtime=1009</span><br><span class="line">[2023-06-05 06:38:10,982] runtime=1008</span><br><span class="line">[2023-06-05 06:38:11,992] runtime=1009</span><br></pre></td></tr></table></figure>

<p>client抓包结果：存在卡顿，存在RST，和benchmark没有区别<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/exp2-pkg-cature1.png" alt="img"><br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/exp2-pkg-cature2.png" alt="img"><br>access.log：卡顿的日志连接时间比benchmark略短，在0.2-0.3s左右，出现499的情况少了但是依然会有</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">172.31.91.109 [05/Jun/2023:06:38:02 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=204595 rt=0.844 uct=&quot;0.362&quot; uht=&quot;0.539&quot; urt=&quot;0.845&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:03 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=204595 rt=0.907 uct=&quot;0.334&quot; uht=&quot;0.476&quot; urt=&quot;0.906&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:04 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=543900 rt=0.836 uct=&quot;0.319&quot; uht=&quot;0.504&quot; urt=&quot;0.836&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:05 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=204595 rt=0.831 uct=&quot;0.161&quot; uht=&quot;0.480&quot; urt=&quot;0.830&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:06 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=552849 rt=0.820 uct=&quot;0.180&quot; uht=&quot;0.329&quot; urt=&quot;0.819&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:07 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=204595 rt=0.800 uct=&quot;0.122&quot; uht=&quot;0.462&quot; urt=&quot;0.800&quot;</span><br><span class="line">172.31.91.109 [05/Jun/2023:06:38:08 +0000] &quot;GET /server.pcap HTTP/1.1&quot; status=200 body_bytes_sent=543900 rt=0.871 uct=&quot;0.251&quot; uht=&quot;0.380&quot; urt=&quot;0.871&quot;</span><br></pre></td></tr></table></figure>

<p>存在惊群现象，以下是Nginx worker进程的cpu使用率和上下文切换频率对比</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 每5s输出一次统计结果</span><br><span class="line">pidstat -w -u 5</span><br></pre></td></tr></table></figure>

<p>两者的cpu使用率和上下文切换频率差不多，但关闭reuseport后花在wait上的cpu时间明显增加（1.3-1.6% vs 2.8-2.9%），这就是惊群带来的性能损耗。原始文件：<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-bench/pidstat.txt" target="_blank" rel="noopener">开启reuseport</a>，<a href="https://github.com/plantegg/programmer_case/blob/main/performance/Nginx%20reuseport%20%E5%AF%BC%E8%87%B4%E5%81%B6%E5%8F%91%E6%80%A7%E5%8D%A1%E9%A1%BF/log-exp2/pidstat.txt" target="_blank" rel="noopener">关闭reuseport</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 开启reuseport</span><br><span class="line">Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command</span><br><span class="line">Average:      992      2590    1.77    9.57    0.00    1.25   11.35     -  nginx</span><br><span class="line">Average:      992      2591    1.37    5.75    0.00    1.62    7.12     -  nginx</span><br><span class="line"></span><br><span class="line">Average:      UID       PID   cswch/s nvcswch/s  Command</span><br><span class="line">Average:      992      2590    179.18     49.64  nginx</span><br><span class="line">Average:      992      2591    342.51      9.87  nginx</span><br><span class="line"></span><br><span class="line"># 关闭reuseport</span><br><span class="line">Average:      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command</span><br><span class="line">Average:      992      2788    1.02    8.02    0.00    2.80    9.04     -  nginx</span><br><span class="line">Average:      992      2789    0.92    9.07    0.00    2.97    9.99     -  nginx</span><br><span class="line"></span><br><span class="line">Average:      UID       PID   cswch/s nvcswch/s  Command</span><br><span class="line">Average:      992      2788    159.06     28.68  nginx</span><br><span class="line">Average:      992      2789    250.26     22.93  nginx</span><br></pre></td></tr></table></figure>

<p>惊群对于慢连接大文件下载时长略有影响：46s (开reuseport) vs 53s (关reuseport)</p>
<ol start="6">
<li>其他的观察</li>
</ol>
<p>最初复现的场景是所有的instance都是t2.micro，但开2个慢连接进程时比较难复现，开4个进程又太容易触发限流，所以开始考虑用大一些又没那么容易限流的instance型号。考虑到aws是通过间歇掉包来限速的，慢连接进程数量并非越大越好，引发限速后反而会造成网络连接不畅，造成慢连接卡顿，使得快连接卡顿反而不容易观测。最后选择将慢连接全链路改成t3.micro，结果好复现多了.  </p>
<p>可以观察到有一些access.log上499的连接，各种计时也是0，这其实是因为计时也是通过worker进行的，只有进行epoll和上下文切换才会在日志上打入时间信息，worker如果一直不进行切换，那么计时就会失真，就会看到日志上计时也是0的现象。  </p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ol>
<li>reuseport是Nginx避免惊群的优秀feature，应该开启</li>
<li>开启reuseport后如果网络情况非常好且后端服务压力不大，且存在大量慢连接时，会造成快连接卡顿，这是Nginx的worker-epoll架构带来的，原因是recv buffer一直读不完，NGINX采用的epoll ET 触发模式在这种情况下一直无法触发暂停导致worker无法响应其它请求</li>
<li>减小recv buffer通过人为制造卡顿，提供了epoll ET切换连接的条件，可以很大程度上缓解这个问题，同时带来的负面效果是有一定性能损耗。但卡顿无法根除，只能控制在可接受范围内</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol>
<li><a href="https://wenfh2020.com/2021/09/29/nginx-thundering-herd/" target="_blank" rel="noopener">Nginx 惊群 – wenfh2020</a></li>
<li><a href="https://wenfh2020.com/2021/10/12/thundering-herd-tcp-reuseport/" target="_blank" rel="noopener">Nginx reuseport – wenfh2020</a></li>
<li><a href="https://wenfh2020.com/2021/11/21/question-nginx-epoll-et/" target="_blank" rel="noopener">Epoll – wenfh2020</a></li>
<li><a href="https://www.cnblogs.com/my_captain/p/12667016.html" target="_blank" rel="noopener">上下文切换的案例以及CPU使用率 – cnhkzyy</a></li>
</ol>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/05/26/MySQL线程池卡顿重现/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/05/26/MySQL线程池卡顿重现/" itemprop="url">MySQL线程池卡顿重现</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-05-26T17:30:03+08:00">
                2023-05-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/MySQL/" itemprop="url" rel="index">
                    <span itemprop="name">MySQL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="MySQL线程池卡顿重现"><a href="#MySQL线程池卡顿重现" class="headerlink" title="MySQL线程池卡顿重现"></a>MySQL线程池卡顿重现</h1><p>by @wych42 </p>
<h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>为了激励大家多动手少空想，我在推特发起了白嫖我的<a href="http://t.zsxq.com/0cz93XUPj" target="_blank" rel="noopener">知识星球活动</a>：</p>
<blockquote>
<p>白嫖我星球的机会来了，总有人说贵、没有优惠券，这次直接来一个完全100%免费的机会，要求： 在MySQL的基础上重现某个线程池卡的现象，给出可复制的重现过程。就是因为某个线程池满了导致落到这个池里的查询一定都慢，否则都快。 不愿意出钱就动手吧</p>
</blockquote>
<p>参考现象：<a href="https://plantegg.github.io/2020/11/17/MySQL%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BB%B6%E6%97%B6%E5%8D%A1%E9%A1%BF%E6%8E%92%E6%9F%A5/">https://plantegg.github.io/2020/11/17/MySQL%E7%BA%BF%E7%A8%8B%E6%B1%A0%E5%AF%BC%E8%87%B4%E7%9A%84%E5%BB%B6%E6%97%B6%E5%8D%A1%E9%A1%BF%E6%8E%92%E6%9F%A5/</a></p>
<hr>
<p>感谢推友<a href="https://twitter.com/wych42" target="_blank" rel="noopener">王鱼翅</a>同学，以下是他的教科书级的细致重现，你复制粘贴就能和他一样重现了</p>
<h2 id="这个案例的重要性"><a href="#这个案例的重要性" class="headerlink" title="这个案例的重要性"></a>这个案例的重要性</h2><p>这个现象对应我们年度四大案例之一，如下图左下角</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230517082106489.png" alt="image-20230517082106489"></p>
<p>重现后请思考：</p>
<ol>
<li>MySQL为什么要将多个线程分成小池子，小池子肯定容易局部资源不足</li>
<li>Nginx 一个连接固定在一个worker上，那么同样多个Worker也会有不均衡(有的worker很闲，有的很卡)</li>
<li>动手实验一下将多个小池子改成一个大线程池会怎么样</li>
<li>Java ConcurrentHashMap为什么能够高性能</li>
</ol>
<p>由 @wych42 重现 </p>
<h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>根据 <a href="https://www.brendangregg.com/usemethod.html" target="_blank" rel="noopener">USE</a> 分析套路。看到服务端执行快，但是整体RT慢的现象，大概率是中间哪个位置有排队。根据文章里的描述，原因是在thread pool group中出现了排队。</p>
<p>排队的主要原因是服务端拒绝创建新的thread（worker），导致新进来的SQL需要等待前面的执行完成。那么就需要重点分析thread(worker)的创建过程和约束条件。根据文章和文档的说明，重点在thread_pool_size, thread_pool_oversubscribe, thread_pool_max_threads, thread_pool_stall_limit这几个参数上。</p>
<p>跟据文档分析和实际执行结果，这几个参数在MySQL不同的发型版中的行为逻辑是不尽相同的。核心差异在对创建新worker的限制条件上，后面复现也会根据两个发型版的特点分别执行。</p>
<h2 id="mariadb"><a href="#mariadb" class="headerlink" title="mariadb"></a>mariadb</h2><p><a href="https://mariadb.com/kb/en/thread-groups-in-the-unix-implementation-of-the-thread-pool/" target="_blank" rel="noopener">文档</a></p>
<ul>
<li>通常情况下，新worker由listener worker创建</li>
<li>当timer worker检测到thread group 有stall时，可能会选择创建一个新的worker</li>
<li>worker的数量上限由thread_pool_max_threads限制</li>
<li>thread_pool_oversubscribe约束的是被额外创建出来的worker，在执行完任务后，最多能保留active状态的数量<blockquote>
<p>To clarify, the thread_pool_oversubscribe system variable does not play any part in the creation of new worker threads. The thread_pool_oversubscribe system variable is only used to determine how many worker threads should remain active in a thread group, once a thread group is already oversubscribed due to stalls.</p>
</blockquote>
</li>
</ul>
<h2 id="percona"><a href="#percona" class="headerlink" title="percona"></a>percona</h2><p><a href="https://docs.percona.com/percona-server/8.0/performance/threadpool.html" target="_blank" rel="noopener">文档</a></p>
<p>percona的行为更符合原文章里的说明：</p>
<ul>
<li>如果线程执行超过时间 thread_pool_stall_limit 的值，会被任务stalled，会创建一个新的线程执行排队的任务</li>
<li>thread_pool_oversubscribe 约束了每个thread group的线程数上限。</li>
</ul>
<h2 id="尝试复现"><a href="#尝试复现" class="headerlink" title="尝试复现"></a>尝试复现</h2><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>并发向DB发起请求，观察客户端耗时，这些请求应当符合这些条件：</p>
<ul>
<li>可控的并发数量：以对比数据库服务端不同参数值的情况</li>
<li>有稳定的、相同的服务端执行耗时：以对比客户端在不同场景下的耗时</li>
<li>对服务端的硬件压力较小：避免因为并发不同时，因IO、CPU资源占用，影响服务端执行耗时</li>
</ul>
<p>综合考虑使用 <code>select sleep(2); </code>作为测试SQL。并发控制使用下面的golang代码实现。</p>
<p>再控制数据库服务端参数，运行同一个并发程序进行对比，mariadb和percona分析执行运行过程：</p>
<h2 id="复现执行"><a href="#复现执行" class="headerlink" title="复现执行"></a>复现执行</h2><h3 id="mariadb-1"><a href="#mariadb-1" class="headerlink" title="mariadb"></a>mariadb</h3><p>由上面分析可以，mariadb 中造成排队的约束是thread_pool_max_threads。</p>
<h4 id="执行方案"><a href="#执行方案" class="headerlink" title="执行方案"></a>执行方案</h4><ul>
<li><p>DB配置</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| thread_pool_max_threads                 | 6               |</span><br><span class="line">| thread_pool_oversubscribe               | 1               |</span><br><span class="line">| thread_pool_size                        | 1               |</span><br><span class="line">| thread_pool_stall_limit                 | 500             |</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行SQL <code>select sleep(2)</code></p>
</li>
<li><p>执行并发：8</p>
</li>
</ul>
<p>预期结果： 6个SQL执行的客户端观察耗时为2s；2个SQL为4s</p>
<p>若调整 thread_pool_max_threads&#x3D;8，则8个SQL的执行客户端观察耗时都为2s</p>
<h4 id="执行结果"><a href="#执行结果" class="headerlink" title="执行结果"></a>执行结果</h4><ol>
<li><p>thread_pool_max_threads&#x3D;6;concurrency&#x3D;8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">go run ./main.go</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_3</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_1</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_6</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_4</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_0</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_7</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_2</span><br><span class="line">2023/05/16 13:34:51 starting taskId:task_5</span><br><span class="line">2023/05/16 13:34:53 taskId:task_0 exec cost : 2.021305666s</span><br><span class="line">2023/05/16 13:34:53 taskId:task_6 exec cost : 2.021421041s</span><br><span class="line">2023/05/16 13:34:53 taskId:task_3 exec cost : 2.021258917s</span><br><span class="line">2023/05/16 13:34:53 taskId:task_2 exec cost : 2.021275458s</span><br><span class="line">2023/05/16 13:34:53 taskId:task_4 exec cost : 2.021254083s</span><br><span class="line">2023/05/16 13:34:53 taskId:task_7 exec cost : 2.02146725s</span><br><span class="line">2023/05/16 13:34:55 taskId:task_5 exec cost : 4.021478584s</span><br><span class="line">2023/05/16 13:34:55 taskId:task_1 exec cost : 4.02192s</span><br></pre></td></tr></table></figure>
</li>
<li><p>thread_pool_max_threads&#x3D;8;concurrency&#x3D;8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">go run ./main.go</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_7</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_3</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_1</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_5</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_0</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_6</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_4</span><br><span class="line">2023/05/16 13:36:17 starting taskId:task_2</span><br><span class="line">2023/05/16 13:36:19 taskId:task_6 exec cost : 2.045480167s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_2 exec cost : 2.045405667s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_7 exec cost : 2.045507334s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_1 exec cost : 2.04553075s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_3 exec cost : 2.04554975s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_0 exec cost : 2.045697375s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_4 exec cost : 2.046417375s</span><br><span class="line">2023/05/16 13:36:19 taskId:task_5 exec cost : 2.046453792s</span><br></pre></td></tr></table></figure></li>
</ol>
<p>均符合预期。</p>
<h3 id="percona-1"><a href="#percona-1" class="headerlink" title="percona"></a>percona</h3><p>由上面分析可以，percona中造成排队的约束是thread_pool_oversubscribe。</p>
<h4 id="执行方案-1"><a href="#执行方案-1" class="headerlink" title="执行方案"></a>执行方案</h4><ul>
<li><p>DB配置: thread_pool_max_threads设置一个较大的值，以排除影响。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| thread_pool_max_threads                 | 1000               |</span><br><span class="line">| thread_pool_oversubscribe               | 1               |</span><br><span class="line">| thread_pool_size                        | 1               |</span><br><span class="line">| thread_pool_stall_limit                 | 500             |</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行SQL <code>select sleep(2)</code></p>
</li>
<li><p>执行并发：8</p>
</li>
</ul>
<p>预期结果： 客户端观察到的耗时分四个批次输出，每个批次2个SQL，耗时分别为2s,4s,6s,8s.</p>
<p>若调整 thread_pool_oversubscribe&#x3D;2，则三个批次输出，分别为3条SQL耗时均为2s，3条SQL耗时均为4s，2条SQL耗时均为6s</p>
<h4 id="执行结果-1"><a href="#执行结果-1" class="headerlink" title="执行结果"></a>执行结果</h4><ol>
<li><p>thread_pool_oversubscribe&#x3D;1,concurrency&#x3D;8</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">go run ./main.go</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_2</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_4</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_3</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_5</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_6</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_0</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_1</span><br><span class="line">2023/05/16 13:39:35 starting taskId:task_7</span><br><span class="line">2023/05/16 13:39:37 taskId:task_7 exec cost : 2.063547416s</span><br><span class="line">2023/05/16 13:39:37 taskId:task_0 exec cost : 2.064091541s</span><br><span class="line">2023/05/16 13:39:39 taskId:task_5 exec cost : 4.06672125s</span><br><span class="line">2023/05/16 13:39:39 taskId:task_6 exec cost : 4.066822583s</span><br><span class="line">2023/05/16 13:39:41 taskId:task_3 exec cost : 6.067720292s</span><br><span class="line">2023/05/16 13:39:41 taskId:task_2 exec cost : 6.069995s</span><br><span class="line">2023/05/16 13:39:43 taskId:task_4 exec cost : 8.069296042s</span><br><span class="line">2023/05/16 13:39:43 taskId:task_1 exec cost : 8.071391709s</span><br></pre></td></tr></table></figure>
</li>
<li><p>thread_pool_oversubscribe&#x3D;2,concurrency&#x3D;8 </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">go run ./main.go</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_7</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_1</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_3</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_2</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_5</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_6</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_4</span><br><span class="line">2023/05/16 13:41:02 starting taskId:task_0</span><br><span class="line">2023/05/16 13:41:04 taskId:task_1 exec cost : 2.057093667s</span><br><span class="line">2023/05/16 13:41:04 taskId:task_3 exec cost : 2.057156334s</span><br><span class="line">2023/05/16 13:41:04 taskId:task_5 exec cost : 2.057170667s</span><br><span class="line">2023/05/16 13:41:06 taskId:task_6 exec cost : 4.066917041s</span><br><span class="line">2023/05/16 13:41:06 taskId:task_7 exec cost : 4.066944125s</span><br><span class="line">2023/05/16 13:41:06 taskId:task_2 exec cost : 4.066976875s</span><br><span class="line">2023/05/16 13:41:08 taskId:task_4 exec cost : 6.070653125s</span><br><span class="line">2023/05/16 13:41:08 taskId:task_0 exec cost : 6.070612083s</span><br></pre></td></tr></table></figure></li>
</ol>
<p>均符合预期。</p>
<h3 id="real-world-模拟（percona）版本"><a href="#real-world-模拟（percona）版本" class="headerlink" title="real-world 模拟（percona）版本"></a>real-world 模拟（percona）版本</h3><p>现实场景中，很少会有大批量的2s在SQL在生产环境执行（限互联网业务)，上述的分析过程能否在真实场景中验证呢？尝试用一个执行200ms的SQL来模拟下：</p>
<ul>
<li><p>DB配置: thread_pool_max_threads设置一个较大的值，以排除影响。</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">| thread_pool_max_threads                 | 1000               |</span><br><span class="line">| thread_pool_oversubscribe               | 1               |</span><br><span class="line">| thread_pool_size                        | 1               |</span><br><span class="line">| thread_pool_stall_limit                 | 500             |</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行SQL <code>select sleep(0.2)</code></p>
</li>
<li><p>执行并发：10</p>
</li>
</ul>
<p>从执行结果中可以看到，只有第一条SQL按照预期的时间执行完成了。<br>从抓包结果中可以看到，所有SQL几乎是同时发出。观察最慢的一条SQL,但是从客户端发包到服务端响应包发出的耗时，与客户端观察到的耗时也能对应上。</p>
<p>可以验证上述分析过程。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2023/05/16 14:47:47 taskId:task_1 exec cost : 239.34925ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_9 exec cost : 239.560833ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_5 exec cost : 453.795084ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_3 exec cost : 458.0005ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_6 exec cost : 659.441541ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_8 exec cost : 659.660917ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_0 exec cost : 862.526375ms</span><br><span class="line">2023/05/16 14:47:47 taskId:task_7 exec cost : 864.450042ms</span><br><span class="line">2023/05/16 14:47:48 taskId:task_2 exec cost : 1.063766875s</span><br><span class="line">2023/05/16 14:47:48 taskId:task_4 exec cost : 1.066266041s</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/238557399-c92e2c4e-436f-4f89-ba87-48c49b5393ac.png" alt="send_sql"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/238557635-1209f057-0c3c-4cfd-9072-12bfc112b4c6.png" alt="response_delay"></p>
<h3 id="复现文章中部分线程池卡的现象"><a href="#复现文章中部分线程池卡的现象" class="headerlink" title="复现文章中部分线程池卡的现象"></a>复现文章中部分线程池卡的现象</h3><p>配置两个线程池，在其中一个线程池上,通过<code>select sleep()</code>较长时间模拟线程池被慢SQL或者大量任务堵塞的情况，具体配置方案如下：</p>
<ul>
<li>thread_pool_size&#x3D;2: 保留两个线程池，验证一个卡顿，一个不卡</li>
<li>thread_pool_oversubscribe&#x3D;1: 允许多创建一个线程，每个线程池中可以同时运行1+1&#x3D;2个线程</li>
<li>thread_pool_max_threads&#x3D;2: 每个线程池的线程数量上限，为thread_pool_oversubscribe的配置约束加一个硬限制，每个线程池中最多允许运行2个线程</li>
</ul>
<p>操作步骤如下:</p>
<ul>
<li>通过mysql client在终端发起链接，通过 <code>show processlist</code>语句获取到链接Id, 该链接会分配到 id%2 的线程池中。</li>
<li>用偶数id的链接验证卡顿线程池，用奇数id的链接验证不卡的线程池，链接情况如下:</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">  show processlist;</span><br><span class="line">+-----+-----------------+----------------+------+---------+-------+------------------------+------------------+----------+-----------+---------------+</span><br><span class="line">| Id  | User            | Host           | db   | Command | Time  | State                  | Info             | Time_ms  | Rows_sent | Rows_examined |</span><br><span class="line">+-----+-----------------+----------------+------+---------+-------+------------------------+------------------+----------+-----------+---------------+</span><br><span class="line">|   5 | event_scheduler | localhost      | NULL | Daemon  | 23664 | Waiting on empty queue | NULL             | 23663650 |         0 |             0 |</span><br><span class="line">| 404 | root            | _gateway:51310 | NULL | Sleep   |  7256 |                        | NULL             |  7256057 |         1 |             1 |</span><br><span class="line">| 405 | root            | _gateway:48860 | NULL | Sleep   |  7295 |                        | NULL             |  7295342 |         1 |             1 |</span><br><span class="line">| 406 | root            | _gateway:41144 | NULL | Sleep   |  7254 |                        | NULL             |  7254236 |         1 |             1 |</span><br><span class="line">| 410 | root            | _gateway:46794 | NULL | Sleep   |  7196 |                        | NULL             |  7196042 |         1 |             1 |</span><br><span class="line">+-----+-----------------+----------------+------+---------+-------+------------------------+------------------+----------+-----------+---------------+</span><br></pre></td></tr></table></figure>

<ul>
<li>在 id&#x3D;404, id&#x3D;406的链接上，执行 <code>select sleep(30)</code>，再到 id&#x3D;410 的链接上执行 <code>select 1</code>，预计 <code>select &#39;slow&#39;</code>会直接卡顿约30s再执行完成。</li>
<li>同时，在id&#x3D;405的链接上，反复执行 <code>select &#39;fast&#39;</code>,都可以很快执行完成。</li>
</ul>
<p>执行结果:</p>
<ul>
<li>id&#x3D;410 上的语句执行约25s返回结果（终端操作手速影响导致了5s误差）,语句执行时数据库实例输出报错日志，提示线程不足:<blockquote>
<p>2023-05-16T11:27:09.997916Z 406 [ERROR] [MY-000000] [Server] Threadpool could not create additional thread to handle queries, because the number of allowed threads was reached. Increasing ‘thread_pool_max_threads’ parameter can help in this situation.  If ‘admin_port’ parameter is set, you can still connect to the database with superuser account (it must be TCP connection using admin_port as TCP port) and troubleshoot the situation. A likely cause of pool blocks are clients that lock resources for long time. ‘show processlist’ or ‘show engine innodb status’ can give additional hints.</p>
</blockquote>
</li>
<li>id&#x3D;405链接上的执行都行快。可参考下面抓包截图。</li>
</ul>
<p>抓包结果:</p>
<p>id&#x3D;410 上的阻塞SQL,可以看到:</p>
<ol>
<li>三条语句在3s内接连发出,但是由于线程池阻塞， <code>select &#39;slow&#39;</code>原本应该很快返回结果，被卡住</li>
<li>在30s时，第一个<code>select sleep(30)</code>语句执行完成，空出的线程立刻执行了 <code>select &#39;slow&#39;</code>并返回结果<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/238638548-c4161d72-b94c-43ef-b698-3acc1002eb43.png" alt="slow query"></li>
</ol>
<p>id&#x3D;405链接上的执行结果可以看到，每条语句执行都很快。<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/238637635-323fca3b-edf1-4ae7-8d68-d9db9811c692.png" alt="fast query"></p>
<h1 id="参数合理值-x2F-已知参数的容量评估"><a href="#参数合理值-x2F-已知参数的容量评估" class="headerlink" title="参数合理值&#x2F;已知参数的容量评估"></a>参数合理值&#x2F;已知参数的容量评估</h1><p>percona 的默认配置中，thread_pool_size&#x3D;核心数，thread_pool_oversubscribe&#x3D;3.假设在一台 16core 的服务器上运行percona，默认配置下最多可以有 16*(1+3)&#x3D;64个worker同时接受请求。也就是最大可并行处理的SQL数量为 64 个。</p>
<p>假设同时有65个执行耗时为10ms的SQL到达服务端，理论上，会有一个进入排队。排查网络、解析等阶段，在客户端观察到的64个SQL执行耗时10ms，1个SQL执行耗时约20ms。这也会导致耗时监控中出现毛刺、耗时分布不符合正态分布。</p>
<p>反之，根据硬件配置、查询的量、耗时等特点，也可以推算合理的参数值。</p>
<h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="过程回顾"><a href="#过程回顾" class="headerlink" title="过程回顾"></a>过程回顾</h2><h3 id="阶段一-确定原因"><a href="#阶段一-确定原因" class="headerlink" title="阶段一 确定原因"></a>阶段一 确定原因</h3><p>看到文章时，基本确认问题根源在执行线程(worker)不够，导致排队，出于以下几点分析:</p>
<ul>
<li>开头提到的 <a href="https://www.brendangregg.com/usemethod.html" target="_blank" rel="noopener">USE</a> 分析套路，结合排查过类似问题(非SQL)的经验</li>
<li>看到文章作者调大thread_pool_oversubscribe便解决问题, 结合文章中对该参数作用的文档引用，基本可以确定</li>
</ul>
<h3 id="阶段二-走上弯路"><a href="#阶段二-走上弯路" class="headerlink" title="阶段二 走上弯路"></a>阶段二 走上弯路</h3><p>尝试复现时，要先启动一个DB实例，便查询文档该参数如何在配置文件中配置，查了MySQL的文档，似乎只在enterprise版本中才有该配置项，便转头去看mariadb的配置说明(这一步给走弯路埋下了伏笔)。</p>
<p>用docker在本地启动了mariadb实例(thread_pool_size&#x3D;2 thread_pool_oversubscribe&#x3D;1)</p>
<p>先尝试用 <code>select sleep(30)</code> 模拟阻塞，用 sysbench 模拟正常流量，结果失败：</p>
<ol>
<li>正常流量中有慢的，但是整体还符合正态分布，没有出现都卡的情况。</li>
<li>加大了  <code>select sleep(30)</code> 查询的并发量，现象同上。</li>
</ol>
<p>又翻阅了一些文档，看到DB在调度时，对不同类型的SQL调度优先级会有所区别，类似sleep这种啥也不干的SQL，会不会被降低调度优先级，才导致了没有复现呢？(走上了弯路)</p>
<p>尝试人工制造慢查询:</p>
<ol>
<li>用 sysbench 制造百万量级的表</li>
<li>执行 offset limit 的排序查询，并且不走索引</li>
</ol>
<p>复现结果仍不满意：</p>
<ol>
<li>整体耗时上升了，出现几笔长尾的耗时特别长的请求</li>
<li>但是整体仍然符合正态分布</li>
</ol>
<p>此时分析了下，整体耗时上升是人工制造的慢查询，占用了过多IO和CPU资源，影响了sysbench SQL执行的效率。</p>
<h3 id="阶段三-柳岸花明"><a href="#阶段三-柳岸花明" class="headerlink" title="阶段三 柳岸花明"></a>阶段三 柳岸花明</h3><p>回头又仔细看了下 mariadb关于线程池的<a href="https://mariadb.com/kb/en/thread-groups-in-the-unix-implementation-of-the-thread-pool/" target="_blank" rel="noopener">文档</a>，注意到文档中提到 thread_pool_oversubscribe 不决定同时有多少线程池被创建出来并执行任务，这个行为逻辑与文章中作者引用的并不相同。<br>又去查看了另一个MySQL发行版 percona 的文档，对该配置的行为描述与文章中的相符，基本就确定前面复现失败的原因了。</p>
<p>确定了前面提到的复现思路：用有稳定服务端执行耗时、并且不消耗大量硬件资源的SQL,用可控的并发进行模拟流量，到具体执行时：</p>
<ul>
<li>SQL就用 <code>select sleep(N)</code></li>
<li>可控的并发就用 golang写个小脚本(事后看直接在终端手动操作也是可以的,不过写个脚本也不费事就是了)</li>
</ul>
<h2 id="mariadb-启动命令和配置"><a href="#mariadb-启动命令和配置" class="headerlink" title="mariadb 启动命令和配置"></a>mariadb 启动命令和配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir mariadb</span><br><span class="line">cat &gt; mariadb/my.cnf &lt;&lt; EOF</span><br><span class="line">[mariadb]</span><br><span class="line">#thread pool</span><br><span class="line">thread_handling=pool-of-threads</span><br><span class="line">thread_pool_oversubscribe=1</span><br><span class="line">thread_pool_size=1</span><br><span class="line">thread_pool_max_threads=6</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line">docker run --name mariadb -v ./mariadb:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=password -p3306:3306 mariadb:10.3</span><br></pre></td></tr></table></figure>

<h2 id="percona-启动命令和配置"><a href="#percona-启动命令和配置" class="headerlink" title="percona 启动命令和配置"></a>percona 启动命令和配置</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mkdir percona</span><br><span class="line">cat &gt; percona/my.cnf &lt;&lt; EOF</span><br><span class="line">[mysqld]</span><br><span class="line">#thread pool</span><br><span class="line">thread_handling=pool-of-threads</span><br><span class="line">thread_pool_oversubscribe=1</span><br><span class="line">thread_pool_size=1</span><br><span class="line">thread_pool_max_threads=1000</span><br><span class="line">default_authentication_plugin=mysql_native_password</span><br><span class="line">EOF</span><br><span class="line">docker run --name percona -v ./percona:/etc/my.cnf.d -e MYSQL_ROOT_PASSWORD=123 -p33060:3306 percona:ps-8</span><br></pre></td></tr></table></figure>

<p>注：Mac M1启动percona时，需要在 docker run 后面添加 <code>--platform linux/x86_64</code> 参数。(percona 未提供arm架构的image)</p>
<h2 id="其他人的重现和分析"><a href="#其他人的重现和分析" class="headerlink" title="其他人的重现和分析"></a>其他人的重现和分析</h2><p><a href="https://lotabout.me/2023/Verification-of-Percona-Thread-Pool-Behavior/" target="_blank" rel="noopener">https://lotabout.me/2023/Verification-of-Percona-Thread-Pool-Behavior/</a>  从源代码debug上来分析</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/05/10/程序员案例星球介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/05/10/程序员案例星球介绍/" itemprop="url">程序员案例星球介绍</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-05-10T17:30:03+08:00">
                2023-05-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/others/" itemprop="url" rel="index">
                    <span itemprop="name">others</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="程序员案例星球介绍"><a href="#程序员案例星球介绍" class="headerlink" title="程序员案例星球介绍"></a>程序员案例星球介绍</h1><h2 id="【星球宗旨】"><a href="#【星球宗旨】" class="headerlink" title="【星球宗旨】"></a>【星球宗旨】</h2><p>平时一学就懂，但是实践总是不会，这是因为学习时<strong>缺少实践案例</strong>、场景导致学起来没有体感。我们总是习惯通过课程、教科书想要一次系统性地掌握很多东西，但最终什么都没掌握好。所以星球想通过案例打透一个或几个知识点，让你通过这几个知识点再去生长发芽形成体系</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230510191422496.png" alt="image-20230510191422496"></p>
<p>以上四个案例中的三个都给出了完整的重现环境、配置、重现步骤、抓包分析、日志结果、现场截图等等资料，保证100%重现问题并带你进行分析，让你感受实际工作一样的场景，真正做到学透一个案例顶3年工作经验</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230607085652520.png" alt="image-20230607085652520"></p>
<h2 id="【关于案例】"><a href="#【关于案例】" class="headerlink" title="【关于案例】"></a>【关于案例】</h2><p>本星球剖析各种程序员疑难经典案例，搞清楚一个案例基本能横扫一个领域，其次在一个案例后再带3&#x2F;5个相关小案例可以帮你丰富场景，多角度理解。用做会来解决学不会的问题。 案例典型普适性强，代表基础组件基本原理等知识。分析手段尽量通用，分析过程一定要逻辑合理每个疑问都能回答清晰。 最终实现在新领域用旧知识旧工具解决疑难问题，无招胜有招</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230510191512744.png" alt="image-20230510191512744"></p>
<h2 id="【关于星主】"><a href="#【关于星主】" class="headerlink" title="【关于星主】"></a>【关于星主】</h2><p>星主20多年的编程实践经历，疑难问题无数，擅长网络，性能，复杂系统的疑难问题分析，BAT背景，目前还在一线撕逼，作者的故事： <a href="https://plantegg.github.io/2022/01/01/%E4%B8%89%E4%B8%AA%E6%95%85%E4%BA%8B/">https://plantegg.github.io/2022/01/01/%E4%B8%89%E4%B8%AA%E6%95%85%E4%BA%8B/</a></p>
<h2 id="【星球成员成果】"><a href="#【星球成员成果】" class="headerlink" title="【星球成员成果】"></a>【星球成员成果】</h2><ul>
<li><p><a href="https://yishenggong.com/2023/05/06/why-does-my-network-speed-drop-cn/" target="_blank" rel="noopener">强龙难压地头蛇的故事</a> 这位星球成员刚大学毕业几个月，加入星球不到2个月</p>
</li>
<li><p>成员故事 <a href="https://liarlee.site/2023/05/08/Linux/Linux_RDS%20QPS%20%E4%B8%8B%E9%99%8D%E5%BC%95%E5%8F%91%E7%9A%84%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95/" target="_blank" rel="noopener">tcp协议和 os 网络系统的分析我之前真是一句都说不出来， 这次确实完整的走了一遍网络的部分。</a> 这位星球成员目前是 AWS 中国区员工</p>
</li>
</ul>
<p>强龙难压地头蛇的故事也引起各路技术大佬纷纷下场教年轻人如何学习：<a href="https://t.co/IBLCRzJem2" target="_blank" rel="noopener">treeverse.app&#x2F;view&#x2F;RDzsOXjO</a></p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230510193840999.png" alt="image-20230510193840999" style="zoom: 33%;">



<h2 id="一年感悟"><a href="#一年感悟" class="headerlink" title="一年感悟"></a>一年感悟</h2><p>用心做了一年知识星球后总结了一个我才懂的道理：发现大部分情况下看文章或者真实案例还是不够(更别提看教材了，教材更抽象)，大部分人即使当时看明白文章、案例了，事后还是不会用发懵</p>
<p>那经过这一年的总结怎么解决这个问题呢？我的摸索是：一定要把案例重现出来，把现场交给大家。通过这样三步：</p>
<p>\1. 看纸上案例，以及别人怎么分析、诊断</p>
<p>\2. 到自己跟着重现这个现象，自己试试去把案例里的分析步骤走一遍</p>
<p>\3. 最后通过视频或者直播我再演示一遍</p>
<p>这样才是真正和你经历了一样，但是这个过程完美了吗？不，人都是懒惰的，畏惧撘环境，碰到一点坑就掉里面出不来了，还怎么重现、诊断？</p>
<p>所以对于撘环境我也总结了一条经验：大家都买个一样的 ECS，OS 镜像也一样，这回老师学生就对齐(黑化)了操作环境，所以只要我把重现步骤写清楚基本你就能重现出来</p>
<p>剩下的就是跟着做了，如果这样你还学不会，那我就要退出江湖了，明年到期直接把星球关了</p>
<p>最后我准备了一张优惠券送给大家(or 链接复制到微信里打开有优惠券：<a href="https://t.zsxq.com/18PM8m2ln" target="_blank" rel="noopener">https://t.zsxq.com/18PM8m2ln</a> )，强烈建议你试试</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240324161113874.png" alt="image-20240324161113874"></p>
<p>讲个笑话：如果你问一个老程序员，在你的系统上看到超多 CLOSE_WAIT 的连接，大概99%的会建议你去改：tcp_tw_reuse 还不行再改tcp_tw_recycle，然后让你改 tcp_timestamps、tcp_max_tw_buckets等等，这老工程师懂得还真多、但又屁都不懂</p>
<h2 id="【加入星球】"><a href="#【加入星球】" class="headerlink" title="【加入星球】"></a>【加入星球】</h2><p>知识星球：<a href="https://t.zsxq.com/0cSFEUh2J" target="_blank" rel="noopener">https://t.zsxq.com/0cSFEUh2J</a>，在这里有600多成员等着你和你一起分享案例</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230607090024270.png" alt="image-20230607090024270"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/05/06/我的网络传输速度为什么突然下降了/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/05/06/我的网络传输速度为什么突然下降了/" itemprop="url">我的网络传输速度为什么突然下降了</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-05-06T12:30:03+08:00">
                2023-05-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="我的网络传输速度为什么突然下降了"><a href="#我的网络传输速度为什么突然下降了" class="headerlink" title="我的网络传输速度为什么突然下降了"></a>我的网络传输速度为什么突然下降了</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这个问题是我星球成员做星球里面的必做实验时碰到的一个问题</p>
<p>最后的分析用到了我们的抓包大法+ping一ping真牛逼的证明方案，所以特意放出来供大家试试，同时也检验大家对知识的掌握和运用。</p>
<p>这个问题很好，在EC2上稳定重现，假如你们的业务碰到了这个问题你怎么解决？</p>
<p>或者换个场景描述：你有一个SQL单独执行很快，2个SQL并行一起查速度就降到了原来的10%，是DB还是谁的锅？</p>
<p>推特上大佬们的讨论，看看别人都是怎么思考和推理的：<a href="https://treeverse.app/view/RDzsOXjO" target="_blank" rel="noopener">https://treeverse.app/view/RDzsOXjO</a></p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>一个降速问题，在AWS的 <a href="t2.micro">t2.micro</a>机器上几乎100％复现，操作: </p>
<ol>
<li>开三台aws <a href="t2.micro">t2.micro</a>机器，一台做server两台做client, 已知正常情况rtt是0.5ms，bandwidth 60mbps，文件大小2g</li>
<li>client1 去curl get server 文件，等一段时间速度稳定在 60mbps</li>
<li>然后用 client2 去curl get server 文件</li>
<li>可以观察到两个client都降速到3.5mbps，这种情况就是算把server跑坏了。</li>
<li>关掉client2, 观察到client1恢复到7-8mbps，但是远低于60mbps的带宽上限，也就是速度被限制到了标称的10%</li>
<li>server搞坏之后，client重新下载就会出现一开始还行，但过10s就会掉到7-8mbps的情况，需要重启server才能恢复到60mbps</li>
</ol>
<p>星球不能发多图，和pcap文件，重现的详细抓包、截图等都放在 google driver上了: <a href="https://drive.google.com/drive/folders/13rsOQ-6VZhXu0JRMLlUypRposRRcRN-a" target="_blank" rel="noopener">https://drive.google.com/drive/folders/13rsOQ-6VZhXu0JRMLlUypRposRRcRN-a</a> （<strong>建议大家去下载client2.pcap抓包看看</strong>）</p>
<p>抓包发现大量dup ack, 且bytes in flight很大，server send buffer很大。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/FryRnESX2vOUCICndaLZ3MuaqSmH.png" alt="img"></p>
<p><strong>&#x3D;&#x3D;强烈建议你先别往下看，去下载上面链接中的抓包文件分析看看，然后和下面的分析对比一下&#x3D;&#x3D;</strong></p>
<hr>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>有网络大牛陈硕老师 <a href="https://twitter.com/bnu_chenshuo/status/1654288717673291776" target="_blank" rel="noopener">在EC2上重现了这个问题</a> 以及他的分析，速度由300Mbps下降到了60Mbps：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/Fnl-CGFUBMjLwQWa2i6kPo7MuJFc.png" alt="img"></p>
<p>以及 左耳朵耗子 老师也做了实验以及分析：<a href="https://twitter.com/haoel/status/1654655067365179393" target="_blank" rel="noopener">https://twitter.com/haoel/status/1654655067365179393</a></p>
<p>我的分析：<a href="https://articles.zsxq.com/id_iq5a872u8sux.html" target="_blank" rel="noopener">https://articles.zsxq.com/id_iq5a872u8sux.html</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230506132001274.png" alt="image-20230506132001274"></p>
<h3 id="证明"><a href="#证明" class="headerlink" title="证明"></a>证明</h3><p>证明原理如这个图</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230506131422140.png" alt="image-20230506131422140"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230506131709216.png" alt="image-20230506131709216"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>99%的人不会弄脏双手去实验，哪怕是只需要下载一个抓包就能分析出来都不会去下载。但是为什么刚好是两位陈老师会去测试重现一下呢(原谅你没有AWS机器，但是不接受你不去google driver下载抓包文件 :) )，大概率他们的时间、经验、知识都比你要丰富一些，但是他们不忌惮弄脏手而你只想做个过客看看就懂，但最后你真的啥都没看懂！</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/04/20/scapy使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/20/scapy使用/" itemprop="url">从一个fin 卡顿问题到 scapy 的使用</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-04-20T17:30:03+08:00">
                2023-04-20
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="从一个fin-卡顿问题到-scapy-的使用"><a href="#从一个fin-卡顿问题到-scapy-的使用" class="headerlink" title="从一个fin 卡顿问题到 scapy 的使用"></a>从一个fin 卡顿问题到 scapy 的使用</h1><h2 id="scapy-使用"><a href="#scapy-使用" class="headerlink" title="scapy 使用"></a>scapy 使用</h2><p>scapy 可以绕过内核构造任意网络包</p>
<p>使用比较简单，git clone  <a href="https://github.com/secdev/scapy" target="_blank" rel="noopener">https://github.com/secdev/scapy</a> 然后在有python3的环境直接可以跑(python2官方说也支持)</p>
<p>注意：</p>
<p>scapy会触发内核发送reset，所以先要在iptables条件一条规则把内核的reset干掉，要不影响scapy的测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">iptables -A OUTPUT -p tcp --tcp-flags RST RST -s 192.168.0.1 -j DROP</span><br><span class="line"></span><br><span class="line">iptables -A OUTPUT -p tcp --tcp-flags RST RST -s 192.168.0.1 -d 192.168.0.2 --sport 1234 --dport 12347 -j DROP</span><br></pre></td></tr></table></figure>

<p>因为包是 scapy 绕过OS 够早的，导致 OS 不认 scapy 模拟发出的包，内核里面没有你这个socket记录，只能 reset你，所以还得用iptables把这个OS 触发的reset，测试才能顺利进行</p>
<h2 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h2><p>用scapy 模拟客户端来进行3次握手</p>
<p>代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sport=random.randint(1024,65535)</span><br><span class="line">ip=IP(dst=&quot;127.0.0.1&quot;)</span><br><span class="line">SYN=TCP(sport=sport, dport=22345, flags=&apos;S&apos;, seq=123451000)</span><br><span class="line">c=sr1(ip/SYN)</span><br></pre></td></tr></table></figure>

<p>完整案例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ./run_scapy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sport=random.randint(<span class="number">1024</span>,<span class="number">65535</span>) //初始化一个本地随机端口</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>SYN=TCP(sport=sport, dport=<span class="number">22345</span>, flags=<span class="string">'S'</span>, seq=<span class="number">123451000</span>) //构造一个连 <span class="number">22345</span> 目标端口的 SYN 包 </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ip=IP(dst=<span class="string">"192.168.0.2"</span>) //构造目标地址</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sr1(ip/SYN) 发送包</span><br><span class="line">Begin emission:</span><br><span class="line">Finished sending <span class="number">1</span> packets.</span><br><span class="line">.*</span><br><span class="line">Received <span class="number">2</span> packets, got <span class="number">1</span> answers, remaining <span class="number">0</span> packets</span><br><span class="line">&lt;IP  version=<span class="number">4</span> ihl=<span class="number">5</span> tos=<span class="number">0x0</span> len=<span class="number">44</span> id=<span class="number">0</span> flags=DF frag=<span class="number">0</span> ttl=<span class="number">64</span> proto=tcp chksum=<span class="number">0xb978</span> src=<span class="number">192.168</span><span class="number">.0</span><span class="number">.2</span> dst=<span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span> |&lt;TCP  sport=<span class="number">22345</span> dport=<span class="number">45814</span> seq=<span class="number">1599494827</span> ack=<span class="number">123451001</span> dataofs=<span class="number">6</span> reserved=<span class="number">0</span> flags=SA window=<span class="number">64240</span> chksum=<span class="number">0x99bb</span> urgptr=<span class="number">0</span> options=[(<span class="string">'MSS'</span>, <span class="number">1460</span>)] |&lt;Padding  load=<span class="string">b'\x00\x00'</span> |&gt;&gt;&gt;</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>

<p>上面的代码是给对端22345 发了个syn包，然后收到了 syn+ack 包并展示在最后，这一来一回的两个握手包都可以用tcpdump 抓到</p>
<p>服务端的话可以起一个标准http server来验证：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m http.server 22345</span><br></pre></td></tr></table></figure>

<p>对应抓包：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">15:58:43.301867 IP localhost.44633 &gt; ky2.22345: Flags [S], seq 123451000, win 8192, length 0</span><br><span class="line">15:58:43.301929 IP ky2.22345 &gt; localhost.44633: Flags [S.], seq 106274946, ack 123451001, win 64240, options [mss 1460], length 0</span><br><span class="line">15:58:44.361834 IP ky2.22345 &gt; localhost.44633: Flags [S.], seq 106274946, ack 123451001, win 64240, options [mss 1460], length 0</span><br><span class="line">15:58:46.441862 IP ky2.22345 &gt; localhost.44633: Flags [S.], seq 106274946, ack 123451001, win 64240, options [mss 1460], length 0</span><br></pre></td></tr></table></figure>

<h2 id="fin-挥手端口卡顿案例"><a href="#fin-挥手端口卡顿案例" class="headerlink" title="fin 挥手端口卡顿案例"></a>fin 挥手端口卡顿案例</h2><p>一个奇葩的tcp连接断开的卡顿问题(来自这里 <a href="https://mp.weixin.qq.com/s/BxU246Btm2FLt1pppBgYQg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/BxU246Btm2FLt1pppBgYQg</a> )，下面是我对这篇文章问题描述的总结：</p>
<blockquote>
<p>1 两端几乎同时发fin, client收到fin回了一个ack </p>
<p>2 client发的ack先fin到达server，server收到ack直接进入time_wait </p>
<p>3 fin到达server被扔掉—-接下来就是要用scapy验证这个fin包被扔掉&#x2F;忽略了，导致client不能立即断开要等200ms</p>
<p>4 client认为关闭失败，等了200ms重传fin然后关闭成功 </p>
</blockquote>
<p>这个问题的总结就是：TCP连接断开的四次挥手中，由于fin包和ack包乱序，导致等了一次timeout才关闭连接，但是上层业务设置了200ms超时，导致业务报错了，现在需要重现这个问题！</p>
<blockquote>
<p>出现这种问题的场景：比如在 OVS&#x2F;MOC 等网络场景下，SYN&#x2F;FIN 等关键包需要更新路由(session flow)，会走 slowpath(送到更高层的复杂逻辑处理，比如 SYN 就创建一个新的 session 记录，以后普通包直接命中这个 session 就快了；同样 FIN 需要走高层逻辑去释放这条 session 记录)</p>
<p>影响：因为 ack 比 FIN 先到，导致应用连接已被释放，但是后面的 FIN 被重传，从而可能使得应用记录的连接断开时间要晚甚至超时</p>
</blockquote>
<p>原作者怎么分析定位，花了几周，这个过程大家可以去看上面的原因，本篇的目的是对这个问题用Scapy 来重现，目标掌握好 Scapy 这个工具，以后对各种其他问题大家自己都能快速定位</p>
<p>用scapy来模拟这个问题，server端用python实现，重点注意server端的断开方式有两个shutdown&#x2F;close:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line">server_ip = <span class="string">"0.0.0.0"</span></span><br><span class="line">server_port = <span class="number">22345</span></span><br><span class="line"></span><br><span class="line">server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">server_socket.bind((server_ip, server_port))</span><br><span class="line">server_socket.listen(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">connection, client_address = server_socket.accept()</span><br><span class="line"></span><br><span class="line">connection.shutdown(socket.SHUT_RDWR) //比较shutdown和close的不同</span><br><span class="line"><span class="comment">#connection.close()</span></span><br><span class="line"></span><br><span class="line">time.sleep(<span class="number">3</span>)</span><br><span class="line">server_socket.close()</span><br></pre></td></tr></table></figure>

<p>对应的client 测试代码，引入了scapy，代码首先是和服务端3次握手，然后抓取(sniff)所有服务端的来包，看看是不是fin，是的话故意先回ack再回fin人为制造乱序：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scapy.all <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">target_ip = <span class="string">"127.0.0.1"</span></span><br><span class="line">target_port = <span class="number">22345</span></span><br><span class="line"><span class="comment">#src_port=random.randint(1024,65535)</span></span><br><span class="line">src_port=<span class="number">54322</span></span><br><span class="line"></span><br><span class="line">ip = IP(dst=target_ip)</span><br><span class="line">syn = TCP(sport=src_port, dport=target_port, flags=<span class="string">"S"</span>, seq=<span class="number">4294967293</span>)</span><br><span class="line">syn_ack = sr1(ip / syn)</span><br><span class="line"><span class="keyword">if</span> syn_ack <span class="keyword">and</span> TCP <span class="keyword">in</span> syn_ack <span class="keyword">and</span> syn_ack[TCP].flags == <span class="string">"SA"</span>:</span><br><span class="line">    print(<span class="string">"Received SYN-ACK"</span>)</span><br><span class="line">    ack = TCP(sport=src_port, dport=target_port,</span><br><span class="line">              flags=<span class="string">"A"</span>, seq=<span class="number">4322</span>, ack=syn_ack.seq+<span class="number">1</span>)</span><br><span class="line">    send(ip / ack)</span><br><span class="line">    data=<span class="string">"rrrrrrrrrrrrrrrrrrrr"</span></span><br><span class="line">    payload=TCP(sport=src_port, dport=<span class="number">22345</span>, flags=<span class="string">'S'</span>, seq=<span class="number">4294967294</span>)</span><br><span class="line">    send(ip/payload()/Raw(load=data))</span><br><span class="line">    print(<span class="string">"Send ACK"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    print(<span class="string">"Failed to establish TCP connection"</span>)</span><br><span class="line">    </span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_packet</span><span class="params">(packet)</span>:</span></span><br><span class="line">    print(<span class="string">"handle fin packet"</span>)</span><br><span class="line">    print(Ether(raw(packet)))</span><br><span class="line">    <span class="keyword">if</span>  TCP <span class="keyword">in</span> packet <span class="keyword">and</span> packet[TCP].flags &amp; <span class="number">0x011</span> <span class="keyword">and</span> packet[TCP].sport == <span class="number">22345</span>:</span><br><span class="line">        print(<span class="string">"Received FIN packet"</span>)</span><br><span class="line">        ack = TCP(sport=src_port, dport=target_port,</span><br><span class="line">		  flags=<span class="string">"A"</span>, seq=packet.ack+<span class="number">1</span>, ack=packet.seq+<span class="number">1</span>)</span><br><span class="line">        send(ip / ack)</span><br><span class="line"></span><br><span class="line">        time.sleep(<span class="number">0.1</span>)</span><br><span class="line">        fin = TCP(sport=src_port, dport=target_port,</span><br><span class="line">                  flags=<span class="string">"FA"</span>, seq=packet.ack, ack=packet.seq)</span><br><span class="line">        send(ip / fin)</span><br><span class="line">        sys.exit(<span class="number">0</span>)</span><br><span class="line"><span class="comment">#抓包，抓到的包给handle_packet处理</span></span><br><span class="line"><span class="comment">#sniff(filter="tcp port 22345", prn=handle_packet)</span></span><br><span class="line">sniff(filter=<span class="string">"tcp port 22345"</span>, iface=<span class="string">"lo"</span>,prn=handle_packet)</span><br></pre></td></tr></table></figure>

<p>下图是服务端 shutdown时模拟挥手断开时ack包和fin包乱序了，也就是先回ack，sleep一段时间后再回fin包，如图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231009101444587.png" alt="image-20231009101444587"></p>
<p>如果server端代码中将shutdown改成close 并做个对比，关键是上面绿框回复ack是4322(challenge_ack 表示seq&#x3D;4322的fin包被忽略了)，而下面close时的seq&#x3D;4322的fin包会被正确接收并回复ack 4323 确认，那么这时client 可以断开了。而上图绿框表示fin 被忽略了，那么内核要继续等200ms 再次发 fin，等收到ack后client 才能断开</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231008175355835.png" alt="image-20231008175355835"></p>
<p>server上通过 netstat 观察连接的状态变化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">//shutdown，可以看到server 发fin进入FIN_WAIT1，然后收到ack 进入 FIN_WAIT2，此时收到fin了，但是被扔掉了，无法断开进入TIME_WAIT</span><br><span class="line"># sh test.sh</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       SYN_RECV</span><br><span class="line">tcp        0      1 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT1</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line"></span><br><span class="line">//close 可以看到server 发fin进入FIN_WAIT1，然后收到ack 进入 FIN_WAIT2，此时收到fin了没有被扔掉，所以很快连接断开进入了TIME_WAIT </span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       SYN_RECV</span><br><span class="line">tcp        0      1 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT1</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       FIN_WAIT2</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       TIME_WAIT</span><br><span class="line">tcp        0      0 192.168.0.2:22345       192.168.0.1:54322       TIME_WAIT</span><br></pre></td></tr></table></figure>

<h2 id="seq-回绕到0-会导致丢包吗？"><a href="#seq-回绕到0-会导致丢包吗？" class="headerlink" title="seq 回绕到0 会导致丢包吗？"></a>seq 回绕到0 会导致丢包吗？</h2><p>首先学习下 seq 是一个无符号32位的整数，最大值是4294967295 </p>
<p>如图，有人发现探活连接不通，导致了一次非正常切换，所以需要分析连接为什么断开，抓包发现重传的时候正好seq 为0，于是他就奇怪了是不是这个seq溢出搞的鬼？怎么这么巧seq 刚好为0了？</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231130113732507.png" alt="image-20231130113732507"></p>
<p>要想正好巧合在 seq 回绕的时候刚好回绕到 0 还是非常不容易的，不过好在用 scapy 来模拟这事就简单了</p>
<p>重现代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">from scapy.all import *</span><br><span class="line">import time</span><br><span class="line">import sys</span><br><span class="line"></span><br><span class="line">target_ip = &quot;server_host_ip&quot;</span><br><span class="line">target_port = 22345</span><br><span class="line">src_port=random.randint(1024,65535)</span><br><span class="line"></span><br><span class="line">#接近最大值 4294967295</span><br><span class="line">start_seq=4294967292 </span><br><span class="line"></span><br><span class="line">ip = IP(dst=target_ip)</span><br><span class="line">syn = TCP(sport=src_port, dport=target_port, flags=&quot;S&quot;, seq=start_seq)</span><br><span class="line">syn_ack = sr1(ip / syn)</span><br><span class="line">if syn_ack and TCP in syn_ack and syn_ack[TCP].flags == &quot;SA&quot;:</span><br><span class="line">    print(&quot;Received SYN-ACK&quot;)</span><br><span class="line">    ack = TCP(sport=src_port, dport=target_port,</span><br><span class="line">              flags=&quot;A&quot;, seq=syn_ack.ack, ack=syn_ack.seq+1)</span><br><span class="line">    print(syn_ack.seq)</span><br><span class="line">    print(syn_ack.ack)</span><br><span class="line">    print(ack)</span><br><span class="line">    send(ip/ack)</span><br><span class="line">    print(&quot;Send ACK&quot;)</span><br><span class="line">else:</span><br><span class="line">    print(&quot;Failed to establish TCP connection&quot;)</span><br><span class="line"></span><br><span class="line">print(&quot;send payload&quot;)</span><br><span class="line">#4294967293+3(3个r) 正好是无符号整数溢出(最大 4294967295)，回绕到0</span><br><span class="line">data=&quot;rrr&quot;</span><br><span class="line">payload=TCP(sport=src_port, dport=22345,flags=&quot;AP&quot;, seq=syn_ack.ack,  ack=syn_ack.seq+1)</span><br><span class="line">payload2=TCP(sport=src_port, dport=22345,flags=&quot;AP&quot;, seq=0,  ack=syn_ack.seq+1)</span><br><span class="line">syn_ack=send(ip/payload/Raw(load=data))</span><br><span class="line">syn_ack=send(ip/payload2/Raw(load=data))</span><br></pre></td></tr></table></figure>

<p>对应的抓包：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231130114200515.png" alt="image-20231130114200515"></p>
<p>从上面的实验来看内核能正确处理这种 seq 回绕到 0 的场景，所以问题不在内核处理上。进一步分析发现是交换机的 bug 导致的</p>
<h3 id="最终确认原因：交换机bug-丢掉-seq-x3D-0的包"><a href="#最终确认原因：交换机bug-丢掉-seq-x3D-0的包" class="headerlink" title="最终确认原因：交换机bug 丢掉 seq&#x3D;0的包"></a>最终确认原因：交换机bug 丢掉 seq&#x3D;0的包</h3><p>最后发现这个问题是中兴 9900系列交换机存在seq&#x3D;0 push 发送模式下报文存在丢失缺陷， 丢包原因是中兴交换机从安全角度考量是支持antidos防攻击功能，该功能开启后会将 TCP seq 为 0 的报文作为非法报文并丢弃。中兴 9900交换机上该功能默认是关闭的但是未生效，需要重新触发关闭（现场看配置是关闭的，实际是开启的，现在执行将配置先打开再关闭）。<br>临时规避方案：（中兴内部验证测试针对此报文有效，现场环境可能有差异，需要现场验证确认）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">先执行 (config)#anti-dos abnormal enable</span><br><span class="line">再执行 (config)#anti-dos abnormal disable</span><br></pre></td></tr></table></figure>

<p>现场实施完毕后，发包验证恢复正常，后续持续观察业务。</p>
<p>彻底解决方案：将该版本升级至V2.00.00R8P16  </p>
<h4 id="华为交换机针对默认连接丢包-bug"><a href="#华为交换机针对默认连接丢包-bug" class="headerlink" title="华为交换机针对默认连接丢包 bug"></a>华为交换机针对默认连接丢包 bug</h4><p>借着中兴这个交换机问题，说一下华为交换机的的 bug，华为 CE12800系列，V200R022C00SPC500之前的版本</p>
<p>当大规格路由反复震荡场景下会小概率</p>
<ol>
<li>出现优先级更高的(子网掩码范围更小)路由表项残留，导致整个子网不通</li>
<li>某个接口下发的路由表和其他接口(芯片)不一致，导致某条连接一直丢包</li>
</ol>
<p>可以重启单板修复表项异常问题，该问题在新版本上（V200R022C00SPC500）已经补丁修复（SPH220）</p>
<p>根因：残留表项或表项异常导致，老版本在大规格路由反复震荡场景下会小概率触发，是软件多线程处理路由下发或删除时，出现线程读取数据异常，导致芯片表项错误。</p>
<h2 id="scapy-构造全连接队列溢出"><a href="#scapy-构造全连接队列溢出" class="headerlink" title="scapy 构造全连接队列溢出"></a>scapy 构造全连接队列溢出</h2><p>server 端用python 起一个WEB 服务：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nohup python3 -m http.server 22345 &amp;</span><br></pre></td></tr></table></figure>

<p>然后client端用如下scapy 代码不断去3次握手建立连接，试几次后就抓到如下现象：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231130111028268.png" alt="image-20231130111028268"></p>
<p>抓包效果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231130133248747.png" alt="image-20231130133248747"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我觉得scapy还是挺好用的，比packetdrill好用一万倍，直观明了，还有命令行可以交互测试</p>
<p>但是要注意 scapy 是绕过内核在模拟发包，收包靠 sniff，所以内核收到这些回包会认为连接不存在，直接reset，需要在iptables 里处理一下</p>
<p>这个问题是别人推荐给我看的，一般10分钟就看完了，但是我差不多花了2天时间，不断地想和去实验重现</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://wizardforcel.gitbooks.io/scapy-docs/content/3.html" target="_blank" rel="noopener">https://wizardforcel.gitbooks.io/scapy-docs/content/3.html</a></p>
<p><a href="https://www.osgeo.cn/scapy/usage.html" target="_blank" rel="noopener">https://www.osgeo.cn/scapy/usage.html</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/51002301" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/51002301</a></p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a href="https://twitter.com/plantegg" target="_blank" rel="noopener">@plantegg</a></p>
<p>知识星球：<a href="https://t.zsxq.com/0cSFEUh2J" target="_blank" rel="noopener">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240324161113874.png" alt="image-20240324161113874" style="zoom:50%;">


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/04/16/比较不同CPU下的分支预测/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/16/比较不同CPU下的分支预测/" itemprop="url">比较不同CPU下的分支预测</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-04-16T12:30:03+08:00">
                2023-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CPU/" itemprop="url" rel="index">
                    <span itemprop="name">CPU</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="比较不同CPU下的分支预测"><a href="#比较不同CPU下的分支预测" class="headerlink" title="比较不同CPU下的分支预测"></a>比较不同CPU下的分支预测</h1><h2 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h2><p>本文通过一段对分支预测是否友好的代码来验证 branch load miss 差异，已经最终带来的 性能差异。同时在x86和aarch64 下各选几款CPU共5款进行差异性对比</p>
<h2 id="CPU-情况"><a href="#CPU-情况" class="headerlink" title="CPU 情况"></a>CPU 情况</h2><h3 id="intel-x86"><a href="#intel-x86" class="headerlink" title="intel x86"></a>intel x86</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                48</span><br><span class="line">On-line CPU(s) list:   0-47</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    24</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 85</span><br><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2500.195</span><br><span class="line">CPU max MHz:           3100.0000</span><br><span class="line">CPU min MHz:           1000.0000</span><br><span class="line">BogoMIPS:              4998.89</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              33792K</span><br><span class="line">NUMA node0 CPU(s):     0-23</span><br><span class="line">NUMA node1 CPU(s):     24-47</span><br><span class="line">Flags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch ida arat epb invpcid_single pln pts dtherm spec_ctrl ibpb_support tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt avx512f avx512dq rdseed adx smap clflushopt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local cat_l3 mba</span><br></pre></td></tr></table></figure>

<h3 id="hygon-7260"><a href="#hygon-7260" class="headerlink" title="hygon 7260"></a>hygon 7260</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:        x86_64</span><br><span class="line">CPU op-mode(s):      32-bit, 64-bit</span><br><span class="line">Byte Order:          Little Endian</span><br><span class="line">Address sizes:       43 bits physical, 48 bits virtual</span><br><span class="line">CPU(s):              48</span><br><span class="line">On-line CPU(s) list: 0-47</span><br><span class="line">Thread(s) per core:  1</span><br><span class="line">Core(s) per socket:  24</span><br><span class="line">Socket(s):           2</span><br><span class="line">NUMA node(s):        8</span><br><span class="line">Vendor ID:           HygonGenuine</span><br><span class="line">CPU family:          24</span><br><span class="line">Model:               1</span><br><span class="line">Model name:          Hygon C86 7260 24-core Processor</span><br><span class="line">Stepping:            1</span><br><span class="line">Frequency boost:     enabled</span><br><span class="line">CPU MHz:             1069.534</span><br><span class="line">CPU max MHz:         2200.0000</span><br><span class="line">CPU min MHz:         1200.0000</span><br><span class="line">BogoMIPS:            4399.38</span><br><span class="line">Virtualization:      AMD-V</span><br><span class="line">L1d cache:           32K</span><br><span class="line">L1i cache:           64K</span><br><span class="line">L2 cache:            512K</span><br><span class="line">L3 cache:            8192K</span><br><span class="line">NUMA node0 CPU(s):   0-5</span><br><span class="line">NUMA node1 CPU(s):   6-11</span><br><span class="line">NUMA node2 CPU(s):   12-17</span><br><span class="line">NUMA node3 CPU(s):   18-23</span><br><span class="line">NUMA node4 CPU(s):   24-29</span><br><span class="line">NUMA node5 CPU(s):   30-35</span><br><span class="line">NUMA node6 CPU(s):   36-41</span><br><span class="line">NUMA node7 CPU(s):   42-47</span><br></pre></td></tr></table></figure>

<h3 id="ARM-鲲鹏920"><a href="#ARM-鲲鹏920" class="headerlink" title="ARM 鲲鹏920"></a>ARM 鲲鹏920</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    48</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          4</span><br><span class="line">Model:                 0</span><br><span class="line">CPU max MHz:           2600.0000</span><br><span class="line">CPU min MHz:           200.0000</span><br><span class="line">BogoMIPS:              200.00</span><br><span class="line">L1d cache:             64K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              512K</span><br><span class="line">L3 cache:              24576K</span><br><span class="line">NUMA node0 CPU(s):     0-23</span><br><span class="line">NUMA node1 CPU(s):     24-47</span><br><span class="line">NUMA node2 CPU(s):     48-71</span><br><span class="line">NUMA node3 CPU(s):     72-95</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma dcpop asimddp asimdfhm</span><br></pre></td></tr></table></figure>

<h3 id="ARM-M710"><a href="#ARM-M710" class="headerlink" title="ARM M710"></a>ARM M710</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    128</span><br><span class="line">Socket(s):             1</span><br><span class="line">NUMA node(s):          2</span><br><span class="line">Model:                 0</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             64K</span><br><span class="line">L1i cache:             64K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              65536K</span><br><span class="line">NUMA node0 CPU(s):     0-63</span><br><span class="line">NUMA node1 CPU(s):     64-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh</span><br></pre></td></tr></table></figure>

<h3 id="ARM-FT-S2500"><a href="#ARM-FT-S2500" class="headerlink" title="ARM FT S2500"></a>ARM FT S2500</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          aarch64</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                128</span><br><span class="line">On-line CPU(s) list:   0-127</span><br><span class="line">Thread(s) per core:    1</span><br><span class="line">Core(s) per socket:    64</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          16</span><br><span class="line">Model:                 3</span><br><span class="line">BogoMIPS:              100.00</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              2048K</span><br><span class="line">L3 cache:              65536K</span><br><span class="line">NUMA node0 CPU(s):     0-7</span><br><span class="line">NUMA node1 CPU(s):     8-15</span><br><span class="line">NUMA node2 CPU(s):     16-23</span><br><span class="line">NUMA node3 CPU(s):     24-31</span><br><span class="line">NUMA node4 CPU(s):     32-39</span><br><span class="line">NUMA node5 CPU(s):     40-47</span><br><span class="line">NUMA node6 CPU(s):     48-55</span><br><span class="line">NUMA node7 CPU(s):     56-63</span><br><span class="line">NUMA node8 CPU(s):     64-71</span><br><span class="line">NUMA node9 CPU(s):     72-79</span><br><span class="line">NUMA node10 CPU(s):    80-87</span><br><span class="line">NUMA node11 CPU(s):    88-95</span><br><span class="line">NUMA node12 CPU(s):    96-103</span><br><span class="line">NUMA node13 CPU(s):    104-111</span><br><span class="line">NUMA node14 CPU(s):    112-119</span><br><span class="line">NUMA node15 CPU(s):    120-127</span><br><span class="line">Flags:                 fp asimd evtstrm aes pmull sha1 sha2 crc32 cpuid</span><br></pre></td></tr></table></figure>

<h2 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a><a href="https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-processing-an-unsorted-array" target="_blank" rel="noopener">测试代码</a></h2><p>对一个数组中较大的一半的值累加：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;ctime&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 随机产生整数，用分区函数填充，以避免出现分桶不均</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">unsigned</span> arraySize = <span class="number">32768</span>;</span><br><span class="line">    <span class="keyword">int</span> data[arraySize];</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> c = <span class="number">0</span>; c &lt; arraySize; ++c)</span><br><span class="line">        data[c] = <span class="built_in">std</span>::rand() % <span class="number">256</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//排序后数据有序，CPU可以准确预测到if的分支</span></span><br><span class="line">    <span class="built_in">std</span>::sort(data, data + arraySize); <span class="comment">//预先排序，也可以注释掉，注释掉表示随机乱序几乎无法预测</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 测试部分</span></span><br><span class="line">    <span class="keyword">clock_t</span> start = clock();</span><br><span class="line">    <span class="keyword">long</span> <span class="keyword">long</span> sum = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> i = <span class="number">0</span>; i &lt; <span class="number">100000</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// 主要计算部分，选一半元素参与计算</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">unsigned</span> c = <span class="number">0</span>; c &lt; arraySize; ++c)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span> (data[c] &gt;= <span class="number">128</span>)</span><br><span class="line">                sum += data[c];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">double</span> elapsedTime = <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(clock() - start) / CLOCKS_PER_SEC;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; elapsedTime &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"sum = "</span> &lt;&lt; sum &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>以上代码可以注释掉第15行，也就是不对代码排序直接累加，不排序的话 if (data[c] &gt;&#x3D; 128) 有50% 概率成立，排序后前一半元素if都不成立，后一半元素if都成立，导致CPU流水线很好预测后面的代码，可以提前加载运算打高IPC</p>
<h2 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h2><h3 id="aarch64-鲲鹏920"><a href="#aarch64-鲲鹏920" class="headerlink" title="aarch64 鲲鹏920"></a>aarch64 鲲鹏920</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./aftersort</span><br><span class="line">11.44</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">           470,740      branch-misses                                                 (59.99%)</span><br><span class="line">    29,716,627,485      bus-cycles                # 2595.890 M/sec                    (60.03%)</span><br><span class="line">        96,469,435      cache-misses              #    0.420 % of all cache refs      (60.03%)</span><br><span class="line">    22,984,316,728      cache-references          # 2007.791 M/sec                    (60.03%)</span><br><span class="line">    29,716,018,641      cpu-cycles                #    2.596 GHz                      (65.02%)</span><br><span class="line">    83,666,813,837      instructions              #    2.82  insn per cycle</span><br><span class="line">                                                  #    0.10  stalled cycles per insn  (65.02%)</span><br><span class="line">     8,765,807,804      stalled-cycles-backend    #   29.50% backend cycles idle      (65.02%)</span><br><span class="line">         8,917,112      stalled-cycles-frontend   #    0.03% frontend cycles idle     (65.02%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">                 5      context-switches          #    0.000 K/sec</span><br><span class="line">         11,447.57 msec cpu-clock                 #    1.000 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               132      minor-faults              #    0.012 K/sec</span><br><span class="line">               132      page-faults               #    0.012 K/sec</span><br><span class="line">         11,447.57 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">        96,471,779      L1-dcache-load-misses     #    0.42% of all L1-dcache accesses  (65.02%)</span><br><span class="line">    22,985,408,745      L1-dcache-loads           # 2007.886 M/sec                    (65.02%)</span><br><span class="line">        96,472,614      L1-dcache-store-misses    #    8.427 M/sec                    (65.02%)</span><br><span class="line">    22,986,056,706      L1-dcache-stores          # 2007.943 M/sec                    (65.02%)</span><br><span class="line">           184,402      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (65.02%)</span><br><span class="line">    14,779,996,797      L1-icache-loads           # 1291.104 M/sec                    (64.99%)</span><br><span class="line">           330,651      branch-load-misses        #    0.029 M/sec                    (64.96%)</span><br><span class="line">     6,561,353,921      branch-loads              #  573.166 M/sec                    (64.96%)</span><br><span class="line">         3,464,612      dTLB-load-misses          #    0.02% of all dTLB cache accesses  (64.96%)</span><br><span class="line">    23,008,097,187      dTLB-loads                # 2009.868 M/sec                    (59.96%)</span><br><span class="line">               745      iTLB-load-misses          #    0.00% of all iTLB cache accesses  (59.96%)</span><br><span class="line">    14,779,577,851      iTLB-loads                # 1291.067 M/sec                    (59.96%)</span><br><span class="line">    </span><br><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./beforesort</span><br><span class="line">30.92</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     1,639,558,981      branch-misses                                                 (59.96%)</span><br><span class="line">    80,284,783,419      bus-cycles                # 2595.949 M/sec                    (59.96%)</span><br><span class="line">       118,459,436      cache-misses              #    0.356 % of all cache refs      (59.96%)</span><br><span class="line">    33,285,701,200      cache-references          # 1076.269 M/sec                    (59.96%)</span><br><span class="line">    80,283,427,379      cpu-cycles                #    2.596 GHz                      (64.96%)</span><br><span class="line">    83,694,841,472      instructions              #    1.04  insn per cycle</span><br><span class="line">                                                  #    0.11  stalled cycles per insn  (64.98%)</span><br><span class="line">     8,849,746,372      stalled-cycles-backend    #   11.02% backend cycles idle      (64.99%)</span><br><span class="line">     8,064,207,583      stalled-cycles-frontend   #   10.04% frontend cycles idle     (65.00%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">                10      context-switches          #    0.000 K/sec</span><br><span class="line">         30,926.95 msec cpu-clock                 #    1.000 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               133      minor-faults              #    0.004 K/sec</span><br><span class="line">               133      page-faults               #    0.004 K/sec</span><br><span class="line">         30,926.95 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">       118,445,576      L1-dcache-load-misses     #    0.36% of all L1-dcache accesses  (65.02%)</span><br><span class="line">    33,286,586,418      L1-dcache-loads           # 1076.297 M/sec                    (65.03%)</span><br><span class="line">       118,441,599      L1-dcache-store-misses    #    3.830 M/sec                    (65.04%)</span><br><span class="line">    33,286,751,407      L1-dcache-stores          # 1076.302 M/sec                    (65.05%)</span><br><span class="line">           410,040      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (65.05%)</span><br><span class="line">    51,611,031,810      L1-icache-loads           # 1668.805 M/sec                    (65.04%)</span><br><span class="line">     1,639,731,725      branch-load-misses        #   53.020 M/sec                    (65.03%)</span><br><span class="line">     7,520,634,791      branch-loads              #  243.174 M/sec                    (65.02%)</span><br><span class="line">         3,536,061      dTLB-load-misses          #    0.01% of all dTLB cache accesses  (65.00%)</span><br><span class="line">    47,898,134,543      dTLB-loads                # 1548.751 M/sec                    (59.99%)</span><br><span class="line">             2,529      iTLB-load-misses          #    0.00% of all iTLB cache accesses  (59.97%)</span><br><span class="line">    51,612,575,118      iTLB-loads                # 1668.854 M/sec                    (59.96%)</span><br></pre></td></tr></table></figure>

<p>以上在相同CPU下数据对比可以看到核心差异是branch-load-misses和branch-misses，当然最终也体现在 IPC 数值上，排序后IPC更高不是因为数据有序取起来更快，而是因为执行逻辑更容易提前预测，也就是可以提前加载if代码到cache中。符合预期</p>
<h3 id="aarch64-M710"><a href="#aarch64-M710" class="headerlink" title="aarch64 M710"></a>aarch64 M710</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses,L1-icache-loads,LLC-load-misses,LLC-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./aftersort</span><br><span class="line">8.20237</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">           912,836      branch-misses                                                 (29.86%)</span><br><span class="line">    22,560,165,604      bus-cycles                # 2748.461 M/sec                    (29.91%)</span><br><span class="line">       205,068,961      cache-misses              #    0.892 % of all cache refs      (29.96%)</span><br><span class="line">    22,998,186,284      cache-references          # 2801.824 M/sec                    (30.01%)</span><br><span class="line">    22,559,518,941      cpu-cycles                #    2.748 GHz                      (35.03%)</span><br><span class="line">    77,068,271,833      instructions              #    3.42  insn per cycle</span><br><span class="line">                                                  #    0.06  stalled cycles per insn  (35.08%)</span><br><span class="line">     4,892,933,264      stalled-cycles-backend    #   21.69% backend cycles idle      (35.13%)</span><br><span class="line">     1,103,203,963      stalled-cycles-frontend   #    4.89% frontend cycles idle     (35.13%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">                17      context-switches          #    0.002 K/sec</span><br><span class="line">          8,208.29 msec cpu-clock                 #    1.000 CPUs utilized</span><br><span class="line">                 3      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               227      minor-faults              #    0.028 K/sec</span><br><span class="line">               227      page-faults               #    0.028 K/sec</span><br><span class="line">          8,208.30 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">       205,384,990      L1-dcache-load-misses     #    0.89% of all L1-dcache accesses  (35.13%)</span><br><span class="line">    22,997,494,522      L1-dcache-loads           # 2801.739 M/sec                    (35.13%)</span><br><span class="line">            66,804      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (35.13%)</span><br><span class="line">    15,486,704,750      L1-icache-loads           # 1886.715 M/sec                    (30.12%)</span><br><span class="line">            76,066      LLC-load-misses           #    0.00% of all LL-cache accesses  (30.09%)</span><br><span class="line">                 0      LLC-loads                 #    0.000 K/sec                    (30.03%)</span><br><span class="line">           672,231      branch-load-misses        #    0.082 M/sec                    (29.98%)</span><br><span class="line">     9,844,109,024      branch-loads              # 1199.288 M/sec                    (29.93%)</span><br><span class="line">           107,198      dTLB-load-misses          #    0.00% of all dTLB cache accesses  (29.89%)</span><br><span class="line">    22,998,647,232      dTLB-loads                # 2801.880 M/sec                    (29.84%)</span><br><span class="line">             9,497      iTLB-load-misses          #    0.08% of all iTLB cache accesses  (29.81%)</span><br><span class="line">        11,755,825      iTLB-loads                #    1.432 M/sec                    (29.82%)</span><br><span class="line"></span><br><span class="line">       8.210235171 seconds time elapsed</span><br><span class="line"></span><br><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses,L1-icache-loads,LLC-load-misses,LLC-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./beforesort</span><br><span class="line">16.8872</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     1,229,182,485      branch-misses                                                 (29.93%)</span><br><span class="line">    46,401,675,872      bus-cycles                # 2747.200 M/sec                    (29.95%)</span><br><span class="line">       206,116,950      cache-misses              #    0.546 % of all cache refs      (29.97%)</span><br><span class="line">    37,773,036,315      cache-references          # 2236.343 M/sec                    (30.01%)</span><br><span class="line">    46,410,071,081      cpu-cycles                #    2.748 GHz                      (35.03%)</span><br><span class="line">    77,083,625,280      instructions              #    1.66  insn per cycle</span><br><span class="line">                                                  #    0.06  stalled cycles per insn  (35.07%)</span><br><span class="line">     1,961,071,890      stalled-cycles-backend    #    4.23% backend cycles idle      (35.11%)</span><br><span class="line">     4,988,241,014      stalled-cycles-frontend   #   10.75% frontend cycles idle     (35.11%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">             1,100      context-switches          #    0.065 K/sec</span><br><span class="line">         16,890.39 msec cpu-clock                 #    0.997 CPUs utilized</span><br><span class="line">                 7      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               229      minor-faults              #    0.014 K/sec</span><br><span class="line">               229      page-faults               #    0.014 K/sec</span><br><span class="line">         16,890.69 msec task-clock                #    0.997 CPUs utilized</span><br><span class="line">       205,761,970      L1-dcache-load-misses     #    0.54% of all L1-dcache accesses  (35.09%)</span><br><span class="line">    37,832,336,945      L1-dcache-loads           # 2239.854 M/sec                    (35.06%)</span><br><span class="line">           207,158      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (35.04%)</span><br><span class="line">    41,944,228,741      L1-icache-loads           # 2483.298 M/sec                    (30.00%)</span><br><span class="line">           135,144      LLC-load-misses           #    0.00% of all LL-cache accesses  (29.97%)</span><br><span class="line">                 0      LLC-loads                 #    0.000 K/sec                    (29.97%)</span><br><span class="line">     1,232,325,180      branch-load-misses        #   72.960 M/sec                    (29.96%)</span><br><span class="line">    14,776,289,690      branch-loads              #  874.827 M/sec                    (29.96%)</span><br><span class="line">           177,790      dTLB-load-misses          #    0.00% of all dTLB cache accesses  (29.97%)</span><br><span class="line">    37,839,288,998      dTLB-loads                # 2240.266 M/sec                    (29.95%)</span><br><span class="line">            46,301      iTLB-load-misses          #    0.00% of all iTLB cache accesses  (29.94%)</span><br><span class="line">    12,631,307,441      iTLB-loads                #  747.833 M/sec                    (29.92%)</span><br><span class="line"></span><br><span class="line">      16.943678377 seconds time elapsed</span><br></pre></td></tr></table></figure>

<p>M710上排序与否和鲲鹏差不多，但是 M710比 鲲鹏要快一些，差别只要有主频高一点点(6%)，另外M710编译后的指令数量也略少(8%)。</p>
<p>最大的差别是没有排序的话 branch-load-misses(1,232,325,180)&#x2F;branch-loads(14,776,289,690) 比例只有鲲鹏的50%，导致整体 IPC 比鲲鹏高不少(1.66 VS 1.04)</p>
<p>如果是排序后的数据来看 M710比鲲鹏好40%，IPC 好了20%，iTLB-loads 差异特别大</p>
<h3 id="aarch64-FT-S2500"><a href="#aarch64-FT-S2500" class="headerlink" title="aarch64 FT S2500"></a>aarch64 FT S2500</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses ./aftersort</span><br><span class="line">16.63</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">         1,298,873      branch-misses             #    0.078 M/sec                    (37.49%)</span><br><span class="line">    34,893,306,641      bus-cycles                # 2096.049 M/sec                    (37.51%)</span><br><span class="line">       211,447,452      cache-misses              #    0.913 % of all cache refs      (37.53%)</span><br><span class="line">    23,154,909,673      cache-references          # 1390.921 M/sec                    (37.54%)</span><br><span class="line">    34,891,766,353      cpu-cycles                #    2.096 GHz                      (43.79%)</span><br><span class="line">    83,918,069,835      instructions              #    2.41  insns per cycle          (43.79%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">               102      context-switches          #    0.006 K/sec</span><br><span class="line">      16647.131540      cpu-clock (msec)</span><br><span class="line">                35      cpu-migrations            #    0.002 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               384      minor-faults              #    0.023 K/sec</span><br><span class="line">               384      page-faults               #    0.023 K/sec</span><br><span class="line">      16647.178560      task-clock (msec)         #    1.000 CPUs utilized</span><br><span class="line">       211,277,069      L1-dcache-load-misses     #    0.91% of all L1-dcache hits    (43.79%)</span><br><span class="line">    23,168,806,437      L1-dcache-loads           # 1391.756 M/sec                    (43.77%)</span><br><span class="line">       211,376,611      L1-dcache-store-misses    #   12.697 M/sec                    (43.75%)</span><br><span class="line">    23,172,492,978      L1-dcache-stores          # 1391.977 M/sec                    (43.73%)</span><br><span class="line">         6,060,438      L1-icache-load-misses     #    0.364 M/sec                    (43.72%)</span><br><span class="line">    23,283,174,318      L1-icache-loads           # 1398.626 M/sec                    (37.48%)</span><br><span class="line">         1,201,268      branch-load-misses        #    0.072 M/sec                    (37.47%)</span><br><span class="line">     6,626,003,512      branch-loads              #  398.026 M/sec                    (37.47%)</span><br><span class="line">         4,417,981      dTLB-load-misses          #    0.265 M/sec                    (37.47%)</span><br><span class="line">            58,242      iTLB-load-misses          #    0.003 M/sec                    (37.47%)</span><br><span class="line"></span><br><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-store-misses,L1-dcache-stores,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,iTLB-load-misses ./beforesort</span><br><span class="line">39.8</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     1,641,714,982      branch-misses             #   41.244 M/sec                    (37.50%)</span><br><span class="line">    83,450,971,727      bus-cycles                # 2096.514 M/sec                    (37.51%)</span><br><span class="line">       209,942,920      cache-misses              #    0.625 % of all cache refs      (37.51%)</span><br><span class="line">    33,584,108,027      cache-references          #  843.724 M/sec                    (37.51%)</span><br><span class="line">    83,446,991,284      cpu-cycles                #    2.096 GHz                      (43.76%)</span><br><span class="line">    83,872,213,462      instructions              #    1.01  insns per cycle          (43.75%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">               165      context-switches          #    0.004 K/sec</span><br><span class="line">      39804.395840      cpu-clock (msec)</span><br><span class="line">               104      cpu-migrations            #    0.003 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               728      minor-faults              #    0.018 K/sec</span><br><span class="line">               728      page-faults               #    0.018 K/sec</span><br><span class="line">      39804.626860      task-clock (msec)         #    1.000 CPUs utilized</span><br><span class="line">       209,884,485      L1-dcache-load-misses     #    0.62% of all L1-dcache hits    (43.75%)</span><br><span class="line">    33,591,847,895      L1-dcache-loads           #  843.918 M/sec                    (43.75%)</span><br><span class="line">       209,796,158      L1-dcache-store-misses    #    5.271 M/sec                    (43.75%)</span><br><span class="line">    33,595,628,139      L1-dcache-stores          #  844.013 M/sec                    (43.75%)</span><br><span class="line">         5,575,802      L1-icache-load-misses     #    0.140 M/sec                    (43.75%)</span><br><span class="line">    68,272,798,305      L1-icache-loads           # 1715.198 M/sec                    (37.50%)</span><br><span class="line">     1,642,653,627      branch-load-misses        #   41.268 M/sec                    (37.50%)</span><br><span class="line">     6,846,418,902      branch-loads              #  172.001 M/sec                    (37.50%)</span><br><span class="line">         4,162,728      dTLB-load-misses          #    0.105 M/sec                    (37.50%)</span><br><span class="line">            57,375      iTLB-load-misses          #    0.001 M/sec                    (37.50%)</span><br></pre></td></tr></table></figure>

<h3 id="Intel-x86-8163"><a href="#Intel-x86-8163" class="headerlink" title="Intel x86 8163"></a>Intel x86 8163</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores ./aftersort</span><br><span class="line">9.77</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     6,541,060,672      branch-instructions       #  669.204 M/sec                    (10.72%)</span><br><span class="line">           727,847      branch-misses             #    0.01% of all branches          (14.30%)</span><br><span class="line">       241,730,862      bus-cycles                #   24.731 M/sec                    (17.88%)</span><br><span class="line">           275,443      cache-misses              #   44.685 % of all cache refs      (21.45%)</span><br><span class="line">           616,413      cache-references          #    0.063 M/sec                    (25.03%)</span><br><span class="line">    24,186,369,646      cpu-cycles                #    2.474 GHz                      (28.60%)</span><br><span class="line">    29,491,804,977      instructions              #    1.22  insns per cycle          (32.18%)</span><br><span class="line">    24,198,780,299      ref-cycles                # 2475.731 M/sec                    (35.75%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                16      context-switches          #    0.002 K/sec</span><br><span class="line">       9774.393202      cpu-clock (msec)</span><br><span class="line">                 8      cpu-migrations            #    0.001 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               490      minor-faults              #    0.050 K/sec</span><br><span class="line">               490      page-faults               #    0.050 K/sec</span><br><span class="line">       9774.396556      task-clock (msec)         #    1.001 CPUs utilized</span><br><span class="line">        74,078,676      L1-dcache-load-misses     #    1.64% of all L1-dcache hits    (189256748561.94%)</span><br><span class="line">     4,515,522,850      L1-dcache-loads           #  461.975 M/sec                    (189237344482.16%)</span><br><span class="line">         3,798,032      L1-dcache-stores          #    0.389 M/sec                    (189217941721.85%)</span><br><span class="line">         1,077,146      L1-icache-load-misses     #    0.110 M/sec                    (189198537875.18%)</span><br><span class="line">            89,144      LLC-load-misses           #   74.54% of all LL-cache hits     (189179139811.86%)</span><br><span class="line">           119,586      LLC-loads                 #    0.012 M/sec                    (189159737036.24%)</span><br><span class="line">             3,450      LLC-store-misses          #    0.353 K/sec                    (189140342885.02%)</span><br><span class="line">           105,021      LLC-stores                #    0.011 M/sec                    (7.15%)</span><br><span class="line">           458,465      branch-load-misses        #    0.047 M/sec                    (10.73%)</span><br><span class="line">     6,557,558,579      branch-loads              #  670.891 M/sec                    (14.30%)</span><br><span class="line">               733      dTLB-load-misses          #    0.00% of all dTLB cache hits   (17.87%)</span><br><span class="line">    12,039,967,837      dTLB-loads                # 1231.786 M/sec                    (21.44%)</span><br><span class="line">               104      dTLB-store-misses         #    0.011 K/sec                    (25.01%)</span><br><span class="line">         7,040,783      dTLB-stores               #    0.720 M/sec                    (28.58%)</span><br><span class="line">               763      iTLB-load-misses          #   62.80% of all iTLB cache hits   (28.56%)</span><br><span class="line">             1,215      iTLB-loads                #    0.124 K/sec                    (28.55%)</span><br><span class="line">           168,588      node-load-misses          #    0.017 M/sec                    (28.55%)</span><br><span class="line">           131,578      node-loads                #    0.013 M/sec                    (28.55%)</span><br><span class="line">             4,484      node-store-misses         #    0.459 K/sec                    (7.14%)</span><br><span class="line">                42      node-stores               #    0.004 K/sec                    (7.14%)</span><br><span class="line">                </span><br><span class="line">#perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores ./beforesort</span><br><span class="line">29.52</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     6,565,036,614      branch-instructions       #  222.370 M/sec                    (10.72%)</span><br><span class="line">     1,599,826,737      branch-misses             #   24.37% of all branches          (14.29%)</span><br><span class="line">       730,977,010      bus-cycles                #   24.760 M/sec                    (17.86%)</span><br><span class="line">           920,858      cache-misses              #   48.057 % of all cache refs      (21.43%)</span><br><span class="line">         1,916,178      cache-references          #    0.065 M/sec                    (25.00%)</span><br><span class="line">    73,123,904,158      cpu-cycles                #    2.477 GHz                      (28.57%)</span><br><span class="line">    29,618,485,912      instructions              #    0.41  insns per cycle          (32.14%)</span><br><span class="line">    73,152,861,566      ref-cycles                # 2477.828 M/sec                    (35.72%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                26      context-switches          #    0.001 K/sec</span><br><span class="line">      29522.972689      cpu-clock (msec)</span><br><span class="line">                13      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               593      minor-faults              #    0.020 K/sec</span><br><span class="line">               593      page-faults               #    0.020 K/sec</span><br><span class="line">      29522.976661      task-clock (msec)         #    1.001 CPUs utilized</span><br><span class="line">        76,164,025      L1-dcache-load-misses     #    1.68% of all L1-dcache hits    (62596004730.92%)</span><br><span class="line">     4,521,935,099      L1-dcache-loads           #  153.167 M/sec                    (62593882213.79%)</span><br><span class="line">         1,170,288      L1-dcache-stores          #    0.040 M/sec                    (62591759384.11%)</span><br><span class="line">         2,975,318      L1-icache-load-misses     #    0.101 M/sec                    (62591281765.29%)</span><br><span class="line">           178,510      LLC-load-misses           #   66.98% of all LL-cache hits     (62591281765.30%)</span><br><span class="line">           266,514      LLC-loads                 #    0.009 M/sec                    (62591281765.18%)</span><br><span class="line">             6,841      LLC-store-misses          #    0.232 K/sec                    (62591578887.87%)</span><br><span class="line">           335,369      LLC-stores                #    0.011 M/sec                    (7.15%)</span><br><span class="line">     1,600,893,693      branch-load-misses        #   54.225 M/sec                    (10.72%)</span><br><span class="line">     6,565,516,562      branch-loads              #  222.387 M/sec                    (14.29%)</span><br><span class="line">            33,070      dTLB-load-misses          #    0.00% of all dTLB cache hits   (17.87%)</span><br><span class="line">    12,043,088,689      dTLB-loads                #  407.923 M/sec                    (21.44%)</span><br><span class="line">               180      dTLB-store-misses         #    0.006 K/sec                    (25.01%)</span><br><span class="line">         2,359,365      dTLB-stores               #    0.080 M/sec                    (28.58%)</span><br><span class="line">             9,399      iTLB-load-misses          #  849.82% of all iTLB cache hits   (28.58%)</span><br><span class="line">             1,106      iTLB-loads                #    0.037 K/sec                    (28.58%)</span><br><span class="line">           439,052      node-load-misses          #    0.015 M/sec                    (28.58%)</span><br><span class="line">           367,546      node-loads                #    0.012 M/sec                    (28.58%)</span><br><span class="line">             7,539      node-store-misses         #    0.255 K/sec                    (7.15%)</span><br><span class="line">             1,736      node-stores               #    0.059 K/sec                    (7.14%)</span><br></pre></td></tr></table></figure>

<p>从 x86 和 aarch 对比来看，x86 编译后的指令数是 aarch 的35%，ARM 是精简指令，数量多比较好理解。主频2.5 GHz 较 M710低了11%。</p>
<p>IPC 差异比较大，有一部分是因为 ARM 精简指令本来有较高的 IPC。</p>
<p>从排序前的差异来看除了指令集外导致 IPC 较高的原因主要也是 branch-load-misses(1,232,325,180)&#x2F;branch-loads(14,776,289,690)  比 intel的 1,602,020,038&#x2F;6,568,921,480, 也就是 M710的 branch miss 率比 intel 低了一倍。</p>
<p>排序后去掉了 branch miss 差异，M710 比 intel 快了 10%，只要是因为主频的差异</p>
<p>on 8269 3.2GHz</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-instructions,branch-misses,cpu-cycles,instructions,branch-load-misses,branch-loads,task-clock,cpu-clock ./beforesort</span><br><span class="line">22.96</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &apos;./beforesort&apos;:</span><br><span class="line"></span><br><span class="line">     6,573,626,859      branch-instructions       #  286.177 M/sec                    (83.33%)</span><br><span class="line">     1,602,898,541      branch-misses             #   24.38% of all branches          (83.33%)</span><br><span class="line">    73,189,204,878      cpu-cycles                #    3.186 GHz                      (66.67%)</span><br><span class="line">    29,627,520,323      instructions              #    0.40  insns per cycle          (83.33%)</span><br><span class="line">     1,602,848,454      branch-load-misses        #   69.779 M/sec                    (83.33%)</span><br><span class="line">     6,572,915,651      branch-loads              #  286.146 M/sec                    (83.33%)</span><br><span class="line">      22970.482491      task-clock (msec)         #    1.001 CPUs utilized</span><br><span class="line">      22970.482557      cpu-clock (msec)</span><br></pre></td></tr></table></figure>

<h3 id="hygon-7260-1"><a href="#hygon-7260-1" class="headerlink" title="hygon 7260"></a>hygon 7260</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-instructions,branch-misses,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./aftersort</span><br><span class="line">10.9479</span><br><span class="line">sum = 314931600000</span><br><span class="line">     9,848,123,830      branch-instructions       #  898.161 M/sec                    (26.26%)</span><br><span class="line">           496,734      branch-misses             #    0.01% of all branches          (26.30%)</span><br><span class="line">           713,235      cache-misses              #    0.336 % of all cache refs      (26.34%)</span><br><span class="line">       212,455,257      cache-references          #   19.376 M/sec                    (26.37%)</span><br><span class="line">    27,277,461,559      cpu-cycles                #    2.488 GHz                      (26.41%)</span><br><span class="line">    32,785,270,866      instructions              #    1.20  insn per cycle</span><br><span class="line">                                                  #    0.58  stalled cycles per insn  (26.43%)</span><br><span class="line">    19,069,766,918      stalled-cycles-backend    #   69.91% backend cycles idle      (26.43%)</span><br><span class="line">         6,560,109      stalled-cycles-frontend   #    0.02% frontend cycles idle     (26.42%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">             1,086      context-switches          #    0.099 K/sec</span><br><span class="line">         10,964.61 msec cpu-clock                 #    0.999 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               154      minor-faults              #    0.014 K/sec</span><br><span class="line">               154      page-faults               #    0.014 K/sec</span><br><span class="line">         10,964.91 msec task-clock                #    0.999 CPUs utilized</span><br><span class="line">       206,294,123      L1-dcache-load-misses     #    1.14% of all L1-dcache hits    (26.38%)</span><br><span class="line">    18,083,269,173      L1-dcache-loads           # 1649.217 M/sec                    (26.35%)</span><br><span class="line">       205,499,292      L1-dcache-prefetches      #   18.742 M/sec                    (26.31%)</span><br><span class="line">           749,548      L1-icache-load-misses     #    8.67% of all L1-icache hits    (26.27%)</span><br><span class="line">         8,643,478      L1-icache-loads           #    0.788 M/sec                    (26.25%)</span><br><span class="line">           305,577      branch-load-misses        #    0.028 M/sec                    (26.25%)</span><br><span class="line">     9,850,674,490      branch-loads              #  898.394 M/sec                    (26.25%)</span><br><span class="line">             6,736      dTLB-load-misses          #    6.85% of all dTLB cache hits   (26.25%)</span><br><span class="line">            98,327      dTLB-loads                #    0.009 M/sec                    (26.25%)</span><br><span class="line">               114      iTLB-load-misses          #   78.62% of all iTLB cache hits   (26.25%)</span><br><span class="line">               145      iTLB-loads                #    0.013 K/sec                    (26.25%)</span><br><span class="line">               </span><br><span class="line">#perf stat -e branch-instructions,branch-misses,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-prefetches,L1-icache-load-misses,L1-icache-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads ./beforesort</span><br><span class="line">23.3648</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     9,843,358,378      branch-instructions       #  421.186 M/sec                    (26.26%)</span><br><span class="line">     1,156,804,801      branch-misses             #   11.75% of all branches          (26.28%)</span><br><span class="line">           754,542      cache-misses              #    0.351 % of all cache refs      (26.29%)</span><br><span class="line">       215,234,724      cache-references          #    9.210 M/sec                    (26.31%)</span><br><span class="line">    58,274,116,562      cpu-cycles                #    2.493 GHz                      (26.33%)</span><br><span class="line">    32,850,416,330      instructions              #    0.56  insn per cycle</span><br><span class="line">                                                  #    0.06  stalled cycles per insn  (26.34%)</span><br><span class="line">     1,838,222,200      stalled-cycles-backend    #    3.15% backend cycles idle      (26.34%)</span><br><span class="line">     1,187,291,146      stalled-cycles-frontend   #    2.04% frontend cycles idle     (26.34%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">             2,326      context-switches          #    0.100 K/sec</span><br><span class="line">         23,370.23 msec cpu-clock                 #    0.999 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               150      minor-faults              #    0.006 K/sec</span><br><span class="line">               150      page-faults               #    0.006 K/sec</span><br><span class="line">         23,370.97 msec task-clock                #    0.999 CPUs utilized</span><br><span class="line">       207,451,839      L1-dcache-load-misses     #    0.82% of all L1-dcache hits    (26.34%)</span><br><span class="line">    25,180,673,249      L1-dcache-loads           # 1077.451 M/sec                    (26.34%)</span><br><span class="line">       205,669,557      L1-dcache-prefetches      #    8.800 M/sec                    (26.34%)</span><br><span class="line">         1,725,971      L1-icache-load-misses     #    8.12% of all L1-icache hits    (26.34%)</span><br><span class="line">        21,265,604      L1-icache-loads           #    0.910 M/sec                    (26.34%)</span><br><span class="line">     1,157,454,249      branch-load-misses        #   49.526 M/sec                    (26.34%)</span><br><span class="line">     9,843,015,406      branch-loads              #  421.171 M/sec                    (26.33%)</span><br><span class="line">            22,287      dTLB-load-misses          #    7.08% of all dTLB cache hits   (26.31%)</span><br><span class="line">           314,618      dTLB-loads                #    0.013 M/sec                    (26.29%)</span><br><span class="line">               445      iTLB-load-misses          #   44.95% of all iTLB cache hits   (26.28%)</span><br><span class="line">               990      iTLB-loads                #    0.042 K/sec                    (26.26%)</span><br></pre></td></tr></table></figure>

<p>hygon 在这两个场景中排序前比 intel 好了 20%，IPC 好30%，但是指令数多了10%，最关键的也是因为hygon的 branch-load-misses 率较低。</p>
<p>排序后 hygon 略慢10%，主要是指令数多了将近10%。</p>
<p>如果直接将 intel 下 编译好的二进制放到 hygon 下运行，完全可以跑通，指令数也显示和 intel 一样了，但是总时间较在hygon下编译的二进制没有变化</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230308145915585.png" alt="image-20230308145915585"></p>
<h2 id="开启-gcc-O3-优化"><a href="#开启-gcc-O3-优化" class="headerlink" title="开启 gcc O3 优化"></a>开启 gcc O3 优化</h2><h3 id="intel-8163"><a href="#intel-8163" class="headerlink" title="intel 8163"></a>intel 8163</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores ./beforesort</span><br><span class="line">2.94</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     3,268,501,946      branch-instructions       # 1109.263 M/sec                    (10.74%)</span><br><span class="line">           226,833      branch-misses             #    0.01% of all branches          (14.33%)</span><br><span class="line">        72,998,727      bus-cycles                #   24.774 M/sec                    (17.90%)</span><br><span class="line">            89,636      cache-misses              #   34.026 % of all cache refs      (21.47%)</span><br><span class="line">           263,434      cache-references          #    0.089 M/sec                    (25.03%)</span><br><span class="line">     7,301,839,495      cpu-cycles                #    2.478 GHz                      (28.59%)</span><br><span class="line">    26,180,809,574      instructions              #    3.59  insns per cycle          (32.16%)</span><br><span class="line">     7,304,150,283      ref-cycles                # 2478.880 M/sec                    (35.73%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                10      context-switches          #    0.003 K/sec</span><br><span class="line">       2946.550492      cpu-clock (msec)</span><br><span class="line">                 7      cpu-migrations            #    0.002 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               370      minor-faults              #    0.126 K/sec</span><br><span class="line">               370      page-faults               #    0.126 K/sec</span><br><span class="line">       2946.552985      task-clock (msec)         #    1.001 CPUs utilized</span><br><span class="line">        73,550,829      L1-dcache-load-misses     #    8.97% of all L1-dcache hits    (627063379426.55%)</span><br><span class="line">       820,264,255      L1-dcache-loads           #  278.381 M/sec                    (627063379426.55%)</span><br><span class="line">             6,301      L1-dcache-stores          #    0.002 M/sec                    (627063379426.52%)</span><br><span class="line">           344,639      L1-icache-load-misses     #    0.117 M/sec                    (627063379426.51%)</span><br><span class="line">            70,181      LLC-load-misses           #   84.80% of all LL-cache hits     (630745019998.59%)</span><br><span class="line">            82,757      LLC-loads                 #    0.028 M/sec                    (630529428492.86%)</span><br><span class="line">               592      LLC-store-misses          #    0.201 K/sec                    (630313967916.99%)</span><br><span class="line">            33,362      LLC-stores                #    0.011 M/sec                    (7.17%)</span><br><span class="line">           153,522      branch-load-misses        #    0.052 M/sec                    (10.75%)</span><br><span class="line">     3,263,884,498      branch-loads              # 1107.696 M/sec                    (14.33%)</span><br><span class="line">               274      dTLB-load-misses          #    0.00% of all dTLB cache hits   (17.90%)</span><br><span class="line">     2,179,821,780      dTLB-loads                #  739.787 M/sec                    (21.47%)</span><br><span class="line">                 8      dTLB-store-misses         #    0.003 K/sec                    (25.04%)</span><br><span class="line">            12,708      dTLB-stores               #    0.004 M/sec                    (28.61%)</span><br><span class="line">                59      iTLB-load-misses          #   52.68% of all iTLB cache hits   (28.60%)</span><br><span class="line">               112      iTLB-loads                #    0.038 K/sec                    (28.59%)</span><br><span class="line">             5,919      node-load-misses          #    0.002 M/sec                    (28.59%)</span><br><span class="line">             1,648      node-loads                #    0.559 K/sec                    (28.58%)</span><br><span class="line">               560      node-store-misses         #    0.190 K/sec                    (7.15%)</span><br><span class="line">                14      node-stores               #    0.005 K/sec                    (7.14%)</span><br><span class="line">                </span><br><span class="line">#perf stat -e branch-instructions,branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,ref-cycles,alignment-faults,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-dcache-stores,L1-icache-load-misses,LLC-load-misses,LLC-loads,LLC-store-misses,LLC-stores,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,dTLB-store-misses,dTLB-stores,iTLB-load-misses,iTLB-loads,node-load-misses,node-loads,node-store-misses,node-stores ./aftersort</span><br><span class="line">2.95</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">     3,255,184,180      branch-instructions       # 1102.320 M/sec                    (10.74%)</span><br><span class="line">           791,180      branch-misses             #    0.02% of all branches          (14.35%)</span><br><span class="line">        73,001,075      bus-cycles                #   24.721 M/sec                    (17.93%)</span><br><span class="line">           520,140      cache-misses              #   82.262 % of all cache refs      (21.52%)</span><br><span class="line">           632,298      cache-references          #    0.214 M/sec                    (25.11%)</span><br><span class="line">     7,309,286,959      cpu-cycles                #    2.475 GHz                      (28.69%)</span><br><span class="line">    26,120,077,275      instructions              #    3.57  insns per cycle          (32.28%)</span><br><span class="line">     7,316,568,954      ref-cycles                # 2477.649 M/sec                    (35.86%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                10      context-switches          #    0.003 K/sec</span><br><span class="line">       2953.027151      cpu-clock (msec)</span><br><span class="line">                 3      cpu-migrations            #    0.001 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               370      minor-faults              #    0.125 K/sec</span><br><span class="line">               370      page-faults               #    0.125 K/sec</span><br><span class="line">       2953.029425      task-clock (msec)         #    1.001 CPUs utilized</span><br><span class="line">        73,778,174      L1-dcache-load-misses     #    8.94% of all L1-dcache hits    (625801033059.49%)</span><br><span class="line">       825,038,324      L1-dcache-loads           #  279.387 M/sec                    (625693600466.98%)</span><br><span class="line">             6,137      L1-dcache-stores          #    0.002 M/sec                    (625693600466.94%)</span><br><span class="line">           339,275      L1-icache-load-misses     #    0.115 M/sec                    (625693600466.87%)</span><br><span class="line">             7,611      LLC-load-misses           #   52.34% of all LL-cache hits     (625693600466.22%)</span><br><span class="line">            14,542      LLC-loads                 #    0.005 M/sec                    (625693600466.18%)</span><br><span class="line">               975      LLC-store-misses          #    0.330 K/sec                    (625718826721.74%)</span><br><span class="line">            28,542      LLC-stores                #    0.010 M/sec                    (7.17%)</span><br><span class="line">           150,256      branch-load-misses        #    0.051 M/sec                    (10.75%)</span><br><span class="line">     3,260,765,171      branch-loads              # 1104.210 M/sec                    (14.33%)</span><br><span class="line">                84      dTLB-load-misses          #    0.00% of all dTLB cache hits   (17.91%)</span><br><span class="line">     2,177,927,665      dTLB-loads                #  737.523 M/sec                    (21.48%)</span><br><span class="line">                 0      dTLB-store-misses         #    0.000 K/sec                    (25.05%)</span><br><span class="line">            12,502      dTLB-stores               #    0.004 M/sec                    (28.62%)</span><br><span class="line">                10      iTLB-load-misses          #    5.62% of all iTLB cache hits   (28.61%)</span><br><span class="line">               178      iTLB-loads                #    0.060 K/sec                    (28.60%)</span><br><span class="line">            14,538      node-load-misses          #    0.005 M/sec                    (28.59%)</span><br><span class="line">             1,527      node-loads                #    0.517 K/sec                    (28.62%)</span><br><span class="line">             2,339      node-store-misses         #    0.792 K/sec                    (7.18%)</span><br><span class="line">                 0      node-stores               #    0.000 K/sec                    (7.14%)</span><br></pre></td></tr></table></figure>

<p>可以看到 O3 优化后是否排序执行时间差不多，并且都比没有 O3 前的快几倍，指令数较优化前基本不变。</p>
<p>最明显的是排序前的 branch-load-misses 几乎都被优化掉了，这也导致 IPC 从0.41 提升到了3.59</p>
<h3 id="aarch64-M710-1"><a href="#aarch64-M710-1" class="headerlink" title="aarch64 M710"></a>aarch64 M710</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses,L1-icache-loads,LLC-load-misses,LLC-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads  ./beforesort</span><br><span class="line">1.19468</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">           178,045      branch-misses                                                 (29.84%)</span><br><span class="line">     3,290,281,574      bus-cycles                # 2748.321 M/sec                    (29.84%)</span><br><span class="line">       204,017,139      cache-misses              #   24.768 % of all cache refs      (29.84%)</span><br><span class="line">       823,700,482      cache-references          #  688.024 M/sec                    (29.84%)</span><br><span class="line">     3,290,247,311      cpu-cycles                #    2.748 GHz                      (34.85%)</span><br><span class="line">     5,730,855,778      instructions              #    1.74  insn per cycle</span><br><span class="line">                                                  #    0.26  stalled cycles per insn  (34.85%)</span><br><span class="line">     1,485,014,712      stalled-cycles-backend    #   45.13% backend cycles idle      (35.03%)</span><br><span class="line">           980,441      stalled-cycles-frontend   #    0.03% frontend cycles idle     (35.08%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">                 2      context-switches          #    0.002 K/sec</span><br><span class="line">          1,197.20 msec cpu-clock                 #    1.000 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               140      minor-faults              #    0.117 K/sec</span><br><span class="line">               140      page-faults               #    0.117 K/sec</span><br><span class="line">          1,197.20 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">       205,399,817      L1-dcache-load-misses     #   25.00% of all L1-dcache accesses  (35.08%)</span><br><span class="line">       821,607,081      L1-dcache-loads           #  686.276 M/sec                    (35.08%)</span><br><span class="line">            10,361      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (35.08%)</span><br><span class="line">     1,150,511,080      L1-icache-loads           #  961.004 M/sec                    (30.07%)</span><br><span class="line">             6,275      LLC-load-misses           #    0.00% of all LL-cache accesses  (30.07%)</span><br><span class="line">                 0      LLC-loads                 #    0.000 K/sec                    (30.07%)</span><br><span class="line">           103,368      branch-load-misses        #    0.086 M/sec                    (30.07%)</span><br><span class="line">       821,524,106      branch-loads              #  686.206 M/sec                    (30.07%)</span><br><span class="line">            15,315      dTLB-load-misses          #    0.00% of all dTLB cache accesses  (30.07%)</span><br><span class="line">       821,589,564      dTLB-loads                #  686.261 M/sec                    (30.07%)</span><br><span class="line">             1,084      iTLB-load-misses          #    0.07% of all iTLB cache accesses  (30.07%)</span><br><span class="line">         1,613,786      iTLB-loads                #    1.348 M/sec                    (29.89%)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#perf stat -e branch-misses,bus-cycles,cache-misses,cache-references,cpu-cycles,instructions,stalled-cycles-backend,stalled-cycles-frontend,alignment-faults,bpf-output,context-switches,cpu-clock,cpu-migrations,dummy,emulation-faults,major-faults,minor-faults,page-faults,task-clock,L1-dcache-load-misses,L1-dcache-loads,L1-icache-load-misses,L1-icache-loads,LLC-load-misses,LLC-loads,branch-load-misses,branch-loads,dTLB-load-misses,dTLB-loads,iTLB-load-misses,iTLB-loads  ./aftersort</span><br><span class="line">1.1949</span><br><span class="line">sum = 314931600000</span><br><span class="line"></span><br><span class="line">           656,175      branch-misses                                                 (29.91%)</span><br><span class="line">     3,293,615,450      bus-cycles                # 2748.397 M/sec                    (29.91%)</span><br><span class="line">       203,683,518      cache-misses              #   24.631 % of all cache refs      (29.91%)</span><br><span class="line">       826,934,774      cache-references          #  690.046 M/sec                    (29.91%)</span><br><span class="line">     3,293,560,111      cpu-cycles                #    2.748 GHz                      (34.92%)</span><br><span class="line">     5,732,241,288      instructions              #    1.74  insn per cycle</span><br><span class="line">                                                  #    0.29  stalled cycles per insn  (34.91%)</span><br><span class="line">     1,645,938,192      stalled-cycles-backend    #   49.97% backend cycles idle      (35.00%)</span><br><span class="line">         1,757,056      stalled-cycles-frontend   #    0.05% frontend cycles idle     (35.05%)</span><br><span class="line">                 0      alignment-faults          #    0.000 K/sec</span><br><span class="line">                 0      bpf-output                #    0.000 K/sec</span><br><span class="line">                 2      context-switches          #    0.002 K/sec</span><br><span class="line">          1,198.38 msec cpu-clock                 #    1.000 CPUs utilized</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec</span><br><span class="line">                 0      dummy                     #    0.000 K/sec</span><br><span class="line">                 0      emulation-faults          #    0.000 K/sec</span><br><span class="line">                 0      major-faults              #    0.000 K/sec</span><br><span class="line">               137      minor-faults              #    0.114 K/sec</span><br><span class="line">               137      page-faults               #    0.114 K/sec</span><br><span class="line">          1,198.38 msec task-clock                #    1.000 CPUs utilized</span><br><span class="line">       205,557,180      L1-dcache-load-misses     #   25.00% of all L1-dcache accesses  (35.05%)</span><br><span class="line">       822,366,213      L1-dcache-loads           #  686.233 M/sec                    (35.04%)</span><br><span class="line">            12,708      L1-icache-load-misses     #    0.00% of all L1-icache accesses  (35.05%)</span><br><span class="line">       987,422,733      L1-icache-loads           #  823.967 M/sec                    (30.04%)</span><br><span class="line">             6,234      LLC-load-misses           #    0.00% of all LL-cache accesses  (30.04%)</span><br><span class="line">                 0      LLC-loads                 #    0.000 K/sec                    (30.04%)</span><br><span class="line">           103,635      branch-load-misses        #    0.086 M/sec                    (30.04%)</span><br><span class="line">       822,357,251      branch-loads              #  686.226 M/sec                    (30.04%)</span><br><span class="line">            13,961      dTLB-load-misses          #    0.00% of all dTLB cache accesses  (30.04%)</span><br><span class="line">       822,374,897      dTLB-loads                #  686.241 M/sec                    (30.04%)</span><br><span class="line">               709      iTLB-load-misses          #    0.05% of all iTLB cache accesses  (30.04%)</span><br><span class="line">         1,562,083      iTLB-loads                #    1.303 M/sec                    (29.96%)</span><br></pre></td></tr></table></figure>

<p>可以看到在M710上开启 O3 优化后是否排序执行时间差不多，并且都比没有 O3 前</p>
<p>的快几倍，最明显的是指令数只有之前的7%。另外就是排序前的 branch-load-misses 几乎都被优化掉了，虽然这里 IPC 提升不大但主要在指令数的减少上。</p>
<p>O3意味着代码尽可能展开，更长的代码意味着对 L1i（以及 L2和更高级别）高速缓存的压力更高。这会导致性能降低。更短的代码可以运行得更快。幸运的是，gcc 有一个优化选项可以指定此项。如果使用-Os，则编译器将优化代码大小。使用后，能够增加代码大小的哪些优化将被禁用。使用此选项通常会产生令人惊讶的结果。特别是循环展开和内联没有实质优势时，那么此选项将是一个很好的选择。</p>
<h2 id="分支预测原理介绍"><a href="#分支预测原理介绍" class="headerlink" title="分支预测原理介绍"></a>分支预测原理介绍</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/v2-475f184ea376484878515491a120bf49_1440w.png" alt="img"></p>
<p>如上图的上面部分代表通常情况下的简单代码布局。如果区域 B（这里是内联函数 inlfct 生成的代码）经常由于条件 I 被跳过，而不会执行，处理器的预取将拉入很少使用的包含块 B 的高速缓存行。使用块重新排序可以改变这种局面，改变之后的效果可以在图的下半部分看到。经常执行的代码在内存中是线性的，而很少执行的代码被移动到不会损害预取和 L1i 效率的位置。</p>
<h2 id="Linux内核流水线优化案例"><a href="#Linux内核流水线优化案例" class="headerlink" title="Linux内核流水线优化案例"></a>Linux内核流水线优化案例</h2><p>在Linux Kernel中有大量的 likely&#x2F;unlikely</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//ip 层收到消息后，如果是tcp就调用tcp_v4_rcv作为tcp协议的入口</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tcp_v4_rcv</span><span class="params">(struct sk_buff *skb)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  ...</span><br><span class="line">	<span class="keyword">if</span> (unlikely(th-&gt;doff &lt; <span class="keyword">sizeof</span>(struct tcphdr) / <span class="number">4</span>))</span><br><span class="line">		<span class="keyword">goto</span> bad_packet; <span class="comment">//概率很小</span></span><br><span class="line">	<span class="keyword">if</span> (!pskb_may_pull(skb, th-&gt;doff * <span class="number">4</span>))</span><br><span class="line">		<span class="keyword">goto</span> discard_it;</span><br><span class="line">  </span><br><span class="line"><span class="comment">//file: net/ipv4/tcp_input.c</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">tcp_rcv_established</span><span class="params">(struct sock *sk, ...)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> <span class="keyword">if</span> (unlikely(sk-&gt;sk_rx_dst == <span class="literal">NULL</span>))</span><br><span class="line">  ......</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: include/linux/compiler.h</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> likely(x)   __builtin_expect(!!(x),1)</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> unlikely(x) __builtin_expect(!!(x),0)</span></span><br></pre></td></tr></table></figure>

<p>__builtin_expect 这个指令是 gcc 引入的。该函数作用是允许程序员将最有可能执行的分支告诉编译器，来辅助系统进行分支预测。(参见 <a href="https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html" target="_blank" rel="noopener">https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html</a>)</p>
<p>它的用法为：__builtin_expect(EXP, N)。意思是：EXP &#x3D;&#x3D; N的概率很大。那么上面 likely 和 unlikely 这两句的具体含义就是：</p>
<ul>
<li>__builtin_expect(!!(x),1) x 为真的可能性更大  &#x2F;&#x2F;0两次取反还是0，非0两次取反都是1，这样可以适配__builtin_expect(EXP, N)的N，要不N的参数没法传</li>
<li>__builtin_expect(!!(x),0) x 为假的可能性更大</li>
</ul>
<p>当正确地使用了__builtin_expect后，编译器在编译过程中会根据程序员的指令，将可能性更大的代码紧跟着前面的代码，从而减少指令跳转带来的性能上的下降。让L1i中加载的代码尽量有效紧凑</p>
<p>这样可以让 CPU流水线分支预测的时候默认走可能性更大的分支。如果分支预测错误所有流水线都要取消重新计算。</p>
<p>如果程序员利用这些宏，然后使用 <code>-freorder-blocks</code> 优化选项，则 gcc 将对块进行重新排序，如原理解图所示。该选项在 -O 2中启用，但在-Os 中禁用。还有另一种对块进行重新排序的选项（<code>-freorder-blocks-and-partition</code> ），但是它的用处有限，因为它不适用于异常处理。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不排序的代码(分支极难预测正确)运行数据对比：</p>
<table>
<thead>
<tr>
<th></th>
<th>branch-load-misses&#x2F;branch-loads</th>
<th>instructions</th>
<th>IPC</th>
<th>耗时(秒)</th>
<th>排序后耗时(秒)</th>
</tr>
</thead>
<tbody><tr>
<td>鲲鹏920</td>
<td>21.7%</td>
<td>83,694,841,472</td>
<td>1.04</td>
<td>30.92</td>
<td>11.44</td>
</tr>
<tr>
<td>M710</td>
<td>8.3%</td>
<td>77,083,625,280</td>
<td>1.66</td>
<td>16.89</td>
<td>8.20</td>
</tr>
<tr>
<td>Intel 8163</td>
<td>24.4%</td>
<td>29,618,485,912</td>
<td>0.41</td>
<td>29.52</td>
<td>9.77</td>
</tr>
<tr>
<td>hygon 7260</td>
<td>11.8%</td>
<td>32,850,416,330</td>
<td>0.56</td>
<td>23.36</td>
<td>10.95</td>
</tr>
<tr>
<td>FT S2500</td>
<td>24%</td>
<td>83,872,213,462</td>
<td>1.01</td>
<td>39.8</td>
<td>16.63</td>
</tr>
</tbody></table>
<p>排序后的代码(分支预测容易)运行数据对比：</p>
<table>
<thead>
<tr>
<th></th>
<th>instructions</th>
<th>instructions(排序前)</th>
<th>IPC</th>
<th>耗时(秒)</th>
</tr>
</thead>
<tbody><tr>
<td>鲲鹏920</td>
<td>83,666,813,837</td>
<td>83,694,841,472</td>
<td>2.82</td>
<td>11.44</td>
</tr>
<tr>
<td>M710</td>
<td>77,068,271,833</td>
<td>77,083,625,280</td>
<td>3.42</td>
<td>8.20</td>
</tr>
<tr>
<td>Intel 8163</td>
<td>29,491,804,977</td>
<td>29,618,485,912</td>
<td>1.22</td>
<td>9.77</td>
</tr>
<tr>
<td>hygon 7260</td>
<td>32,785,270,866</td>
<td>32,850,416,330</td>
<td>1.20</td>
<td>10.95</td>
</tr>
<tr>
<td>FT S2500</td>
<td>83,918,069,835</td>
<td>83,872,213,462</td>
<td>2.41</td>
<td>16.63</td>
</tr>
</tbody></table>
<ul>
<li>所有 CPU 都期望对分支预测友好的代码</li>
<li>分支预测重点关注 perf branch-load-misses&#x2F;branch-loads</li>
<li>aarch64 较 x86_64 指令数是2.6倍，同时对流水线更友好，也就是 IPC 更高(2.6倍)，测试代码单线程、无锁</li>
<li>M710的分支预测正确率是鲲鹏920、intel的3倍，hygon 是鲲鹏 、intel的分支预测率的2倍</li>
<li>10% 的分支load miss 会带来一倍的性能差异</li>
<li>gcc O3 优化效果很明显，代价就是代码展开后很大，容易造成icache不够，对小段测试代码效果最好，实践不一定</li>
<li>测试代码只是极简场景，实际生产环境更复杂，也就是预测效果不会这么明显</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/04/06/为什么你不去看文档/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/06/为什么你不去看文档/" itemprop="url">为什么你不去看文档</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-04-06T20:30:03+08:00">
                2023-04-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么你不去看文档"><a href="#为什么你不去看文档" class="headerlink" title="为什么你不去看文档"></a>为什么你不去看文档</h1><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>在推特上看到这张图片，我觉得很好，但是评论里面都在说：这是国内特有的现状。</p>
<p>好像国外就不会这样一样，实际我的看法是国外也是这个鸟样子</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/two_queue-0761423.png" alt="two_queue"></p>
<h2 id="我的看法"><a href="#我的看法" class="headerlink" title="我的看法"></a>我的看法</h2><p>这可不只是国内的问题，medium&#x2F;reddit 上最受欢迎的都是10个坑、5个技巧、3个必知这类文章。其实原因我反倒是很理解，官方文档通篇读下来感觉都看了但是有点不得要领，典型就是看完还是解决不了问题。XX系列简明直接要害，多搞清楚几个XX系列后再看官方手册那感觉完全不一样</p>
<h2 id="为什么会这样"><a href="#为什么会这样" class="headerlink" title="为什么会这样"></a>为什么会这样</h2><p>大多人没有能力只看知识就会掌握并解决问题，这就是为什么我们学数理化，知道了那些公式还是不会解题(有极少数人会)，而是需要老师手把手多讲解几道习题，把习题里面如何套用公式给大家示范并解决问题。</p>
<p>我们学程序相关知识也是这样，只看官方手册大多数人也解决不了问题，但是如果看10个坑、8个技巧后基本会解决一些问题了，如果这个时候我们再去看手册就会发现看起来有感觉多了。</p>
<p>读书的时候有老师带我们解题，做程序员后就只能靠自己了，实际这些XX系列就是我们最好的老师，但最终要记住靠XX系列入门后还是要回到文档、手册、帮助上来。</p>
<p>究其原因总结下来可以把学习分成<strong>工程效率、知识效率</strong></p>
<p>但是我们最容易犯的错误就是：没有知识效率能力确犯了知识效率的毛病。看到知识一看就懂，但实际一用就懵这才是我们的常态</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230406203738548.png" alt="image-20230406203738548"></p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，不实践还能在脑海里举一反三，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>肯定知识效率最牛逼，但是拥有这种技能的人毕竟非常少（天生的高智商吧）。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快的掌握一个新知识，非常气人。剩下的绝大部分只能拼时间+方法+总结等也能掌握一些知识</p>
<p>我自己就是工程效率型，只能羡慕那些知识效率型的学霸。所以我花了长时间去总结他们的差异，在程序员这个领域学会了用案例去学习，也就是深度学习深挖一个案例，通过案例来学习背后的知识，这种方式极大的好处就是学得牢固，并且通过案例掌握的知识点就像一根长长的钉子一样，深深地插入你的记忆里。再然后去看XX官方手册就会发现轻松多了。同时经过多个案例锤炼后举一反三也是积极自然。</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，两者之间当然没有明显的界限，知识积累多了逻辑训练好了在别人看来你的智商就高了</p>
<h2 id="其他想说的"><a href="#其他想说的" class="headerlink" title="其他想说的"></a>其他想说的</h2><p>看完故事升华一下方法论：<a href="https://plantegg.github.io/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/">如何在工作中学习</a></p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a href="https://twitter.com/plantegg" target="_blank" rel="noopener">@plantegg</a></p>
<p>知识星球：<a href="https://t.zsxq.com/0cSFEUh2J" target="_blank" rel="noopener">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230407232314969.png" alt="image-20230407232314969" style="zoom:50%;">


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2023/04/02/为什么你有知识但没有能力/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2023/04/02/为什么你有知识但没有能力/" itemprop="url">为什么你有知识但没有能力</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2023-04-02T20:30:03+08:00">
                2023-04-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="为什么你有知识但没有能力"><a href="#为什么你有知识但没有能力" class="headerlink" title="为什么你有知识但没有能力"></a>为什么你有知识但没有能力</h1><h2 id="起因"><a href="#起因" class="headerlink" title="起因"></a>起因</h2><p>有同学想抓一下访问 baidu.com 的流量，然后分析学习，抓完包后想过滤只看 baidu.com 的流量，减少干扰，于是他在 Wireshark 里面用上了过滤条件： http.host eq “baidu.com” 但是没有过滤到任何包，所以他带着这个问题来问我了</p>
<p>如下图是他的过滤结果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/http.host.png" alt="img"></p>
<p>多说一句，要是我我就只留一个条件来提问：http.host eq “baidu.com” </p>
<p>看到这个问题，虽然我从来没有用过 http.host 这种过滤方式，但我大概猜到了原因，所以我先找了一个政府网站(他们是为数不多还在用 http 的网站)，然后我轻松用同样的方式正确过滤到了我要的包：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230402200619645.png" alt="image-20230402200619645"></p>
<p>于是我回复他：</p>
<blockquote>
<p>第一，你不应该搞一堆条件，不好调试；最简单用一个条件过滤验证 </p>
<p>第二，为什么你百度过滤不了，我想留给你自己去看书、想一想，如果不行一周后我再告诉你答案。自己琢磨出来会让你的正向激励跟吸du一样更嗨，唾手可得的答案不符合本星球希望帮助成员达到无招胜有招的目的，知识是学不完的，总有你不会，但是分析能力、解决问题的能力才是我们要可以去训练，最终你要达到把你丢到一个不懂的领域你很快可以解决问题</p>
</blockquote>
<p>其实我是想引导他自己分析解决问题。</p>
<p>我认为这个同学能动手去抓包分析，学习的劲头已经有了，居然会 http.host 这种用法，这是我第一次看到这么用(我平时不和 http 打交道，别鄙视我)，我想他肯定知道https</p>
<p>但是为什么他知道这些知识但是在实践中什么阻碍了他把学到的知识和他碰到这个头疼的问题联系不起来呢？</p>
<h2 id="有知识但没有能力"><a href="#有知识但没有能力" class="headerlink" title="有知识但没有能力"></a>有知识但没有能力</h2><p>我以前在《<a href="https://plantegg.github.io/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/">如何在工作中学习</a>》就讨论过这种情况，如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230402201307165.png" alt="image-20230402201307165"></p>
<p>显然，这次这位同学的只是具备了，但是没有转化成能力，也就虽然我们都学了TCP、HTTP、HTTPS这些信息，但是没有理解透彻，更具体一点没有把 HTTPS，这层 TLS 工作结构就没理解清楚，TLS 把你原来的 http host 都给加密了，你自然没法按原来的方式过滤。</p>
<p>如下图，这是加密后的结构，你是没法知道TCP 里面是http&#x2F;redis还是MySQL协议的，如果你要理解了TLS直接作用在TCP（四层），而http这种七层协议哪还有说话的空间啊？</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230402201725641.png" alt="image-20230402201725641"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>不要总是抱怨学不会、学不懂，你就是思考的稍微少一点、浅一点。</p>
<p>不要总是抱怨自己10年工作经验实践下来还不如一年的新手，同上！</p>
<p>思维方式是最难改变的，但是是最重要的。</p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a href="https://twitter.com/plantegg" target="_blank" rel="noopener">@plantegg</a></p>
<p>知识星球：<a href="https://t.zsxq.com/0cSFEUh2J" target="_blank" rel="noopener">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230407232314969.png" alt="image-20230407232314969" style="zoom:50%;">


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/11/04/nginx性能和软中断/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/11/04/nginx性能和软中断/" itemprop="url">nginx性能和软中断</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-11-04T12:30:03+08:00">
                2022-11-04
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="nginx性能和软中断"><a href="#nginx性能和软中断" class="headerlink" title="nginx性能和软中断"></a>nginx性能和软中断</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul>
<li>如何调整软中断才能达到最优性能？</li>
<li>通过 top 观察软中断 和 si%、sy% 的关系</li>
</ul>
<h2 id="测试机型"><a href="#测试机型" class="headerlink" title="测试机型"></a>测试机型</h2><p>双路 Intel(R) Xeon(R) CPU E5-2682 v4 sh</p>
<p>两块万兆网卡：Intel Corporation 82599ES 10-Gigabit SFI&#x2F;SFP+ Network Connection (rev 01)</p>
<p>内核：3.10.0-327</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NUMA node0 CPU(s):     0-15,32-47</span><br><span class="line">NUMA node1 CPU(s):     16-31,48-63</span><br></pre></td></tr></table></figure>

<h2 id="软中断和-si"><a href="#软中断和-si" class="headerlink" title="软中断和 si%"></a>软中断和 si%</h2><p>压nginx 碰到一个奇怪的问题，将软中断绑到48-63核，如果nginx绑到这个socket下的其它核比如 16-23，我就基本上看不到 si% 的使用率；如果所有条件都不变我将nginx 绑0-7core（另外一个socket），那么我能看到0-7 core上的软中断 si%使用率达到600%以上（8core累加）。 si%使用率应该只和 PPS、流量相关，这个测试中不同绑核nginx的QPS 差了20%以内。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20221031152031791.png" alt="image-20221031152031791"><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20221031152044825.png" alt="image-20221031152044825"></p>
<p>CPU是intel E5，网卡插在node0上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</span><br><span class="line">NUMA node0 CPU(s):     0-15,32-47</span><br><span class="line">NUMA node1 CPU(s):     16-31,48-63</span><br><span class="line"></span><br><span class="line">软中断绑定：IRQBALANCE_BANNED_CPUS=0000ffff,ffffffff</span><br></pre></td></tr></table></figure>

<p>默认业务进程调用内核软中断do_softirq等来处理收发包，不需要跨core，如果将软中断绑定到具体的core后，会触发ksoftirqd 来调用do_softirq来处理收发包，整体上肯定效率不如同一个core处理业务和软中断的效率高。进一步如果软中断跨socket绑定导致处理时长进一步升高、总效率更差</p>
<p><a href="https://askubuntu.com/questions/7858/why-is-ksoftirqd-0-process-using-all-of-my-cpu" target="_blank" rel="noopener">https://askubuntu.com/questions/7858/why-is-ksoftirqd-0-process-using-all-of-my-cpu</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20221101113948809.png" alt="image-20221101113948809"></p>
<p>下图场景下，收包没有占用 si，而是占用的 sy</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20221101114217738.png" alt="image-20221101114217738"></p>
<p>将软中断和业务进程拆开绑核，均将软中断、业务基本压满的情况下，如果软中断在本node，QPS 增加20%+</p>
<p>软中断打满单核后的IPC：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#perf stat --cpu 29  //软中断所在core，si%=100%，和业务以及网卡跨node</span><br><span class="line"> Performance counter stats for &apos;CPU(s) 29&apos;:</span><br><span class="line"></span><br><span class="line">       4470.584807      task-clock (msec)         #    1.001 CPUs utilized            (100.00%)</span><br><span class="line">               252      context-switches          #    0.056 K/sec                    (100.00%)</span><br><span class="line">                 8      cpu-migrations            #    0.002 K/sec                    (100.00%)</span><br><span class="line">                 3      page-faults               #    0.001 K/sec</span><br><span class="line">    11,158,106,237      cycles                    #    2.496 GHz                      (100.00%)</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">     7,976,745,525      instructions              #    0.71  insns per cycle          (100.00%)</span><br><span class="line">     1,444,740,326      branches                  #  323.166 M/sec                    (100.00%)</span><br><span class="line">         7,073,805      branch-misses             #    0.49% of all branches</span><br><span class="line"></span><br><span class="line">       4.465613433 seconds time elapsed</span><br><span class="line"></span><br><span class="line">#perf stat --cpu 1  //软中断所在core，si%=100%，和业务以及网卡跨node</span><br><span class="line"> Performance counter stats for &apos;CPU(s) 1&apos;:</span><br><span class="line"></span><br><span class="line">       5132.639092      task-clock (msec)         #    1.002 CPUs utilized            (100.00%)</span><br><span class="line">             1,119      context-switches          #    0.218 K/sec                    (100.00%)</span><br><span class="line">                 6      cpu-migrations            #    0.001 K/sec                    (100.00%)</span><br><span class="line">                 0      page-faults               #    0.000 K/sec</span><br><span class="line">    12,773,996,227      cycles                    #    2.489 GHz                      (100.00%)</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">    12,457,832,798      instructions              #    0.98  insns per cycle          (100.00%)</span><br><span class="line">     2,243,820,953      branches                  #  437.167 M/sec                    (100.00%)</span><br><span class="line">        12,769,358      branch-misses             #    0.57% of all branches</span><br><span class="line"></span><br><span class="line">       5.124937947 seconds time elapsed</span><br></pre></td></tr></table></figure>

<p>Nginx业务进程的IPC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">#perf stat -p 30434   //软中断跨node</span><br><span class="line"></span><br><span class="line"> Performance counter stats for process id &apos;30434&apos;:</span><br><span class="line"></span><br><span class="line">       6838.088642      task-clock (msec)         #    0.953 CPUs utilized            (100.00%)</span><br><span class="line">            19,664      context-switches          #    0.003 M/sec                    (100.00%)</span><br><span class="line">                 0      cpu-migrations            #    0.000 K/sec                    (100.00%)</span><br><span class="line">                 4      page-faults               #    0.001 K/sec</span><br><span class="line">    17,027,659,259      cycles                    #    2.490 GHz                      (100.00%)</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">    14,315,679,297      instructions              #    0.84  insns per cycle          (100.00%)</span><br><span class="line">     2,919,774,303      branches                  #  426.987 M/sec                    (100.00%)</span><br><span class="line">        34,643,571      branch-misses             #    1.19% of all branches</span><br><span class="line"></span><br><span class="line">       7.176493377 seconds time elapsed      </span><br><span class="line">       </span><br><span class="line">#perf stat -p 30434    //软中断和nginx、网卡在同一node</span><br><span class="line">^C</span><br><span class="line"> Performance counter stats for process id &apos;30434&apos;:</span><br><span class="line"></span><br><span class="line">       5720.308631      task-clock (msec)         #    0.979 CPUs utilized            (100.00%)</span><br><span class="line">            11,513      context-switches          #    0.002 M/sec                    (100.00%)</span><br><span class="line">                 1      cpu-migrations            #    0.000 K/sec                    (100.00%)</span><br><span class="line">                 0      page-faults               #    0.000 K/sec</span><br><span class="line">    14,234,226,577      cycles                    #    2.488 GHz                      (100.00%)</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">    14,741,777,543      instructions              #    1.04  insns per cycle          (100.00%)</span><br><span class="line">     3,009,021,477      branches                  #  526.024 M/sec                    (100.00%)</span><br><span class="line">        35,690,882      branch-misses             #    1.19% of all branches</span><br><span class="line"></span><br><span class="line">       5.845534744 seconds time elapsed</span><br></pre></td></tr></table></figure>

<p>如果将nginx绑到node1（和网卡分开），同样再将软中断绑到node0、node1上，这个时候同样是软中断和业务在同一node性能要好，也就是软中断要和业务在一个node和网卡在哪里没关系。</p>
<p>网络包收发涉及两块内存分配：描述符(指针)和data buffer（存放网络包数据）；</p>
<p><a href="https://ata.alibaba-inc.com/articles/230545" target="_blank" rel="noopener">网卡的描述符、data buffer申请的内存都在设备所在的numa上</a>， 如果将队列的中断绑定到其他cpu上， 那么队列申请的data buffer的节点也会跟着中断迁移，但是描述符是和网卡所在的node绑定不会迁移的。</p>
<p>Top 看到的 ksoftirqd 占用cpu不高，但是去看对应的 CPU core si消耗比较高，这是因为 ksoftirqd 只是触发软中断后的入口，进而会调用do_softirq&#x2F;net_rx_action 等内核函数，在 si% 的消耗中包含了这些被调用的消耗</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li><p>软中断绑定优先让irqbalance自己决定，默认系统倾向于自动在业务中调用软中断，代价最低</p>
</li>
<li><p>尽量不要让包溢出net.core.netdev_budget，溢出后触发ksoftirqd 来处理效率更低</p>
</li>
<li><p>尽量控制不要让 ksoftirqd 打满，所以可以绑定更多core来</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/10/24/weibo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/10/24/weibo/" itemprop="url">微博备份</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-10-24T16:30:03+08:00">
                2022-10-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/others/" itemprop="url" rel="index">
                    <span itemprop="name">others</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="微博备份"><a href="#微博备份" class="headerlink" title="微博备份"></a>微博备份</h1><p>原因：2022 10月的时候微博被封杀了，之前的重点内容做下备份，评论、转发就没有了</p>
<p>马克思是搞阶级斗争的；列宁是搞革命的；对的，他们就是没有能力治国、搞经济。你见过哪个唯马、列独尊的制度下搞好了的？改革前我们也没搞好，后来有了邓放弃阶级斗争，提出白猫黑猫、科学是第一生产力，放弃马和列的那套阶级斗争，才迎来40年的发展。到了江这里他自评做了三件大事，其中两件是：将邓小平理论写入党章（不要搞斗争，好好搞经济大家都有饭吃），另外就是三个代表，团结一切力量和阶层，尤其是科学（臭老九），真正地让党从斗争转入了治国的轨道上，从此也就不应该有那么多敌对势力了</p>
<p>nginx sys CPU消耗到95%这个是非常不正常的，似乎测试的是短连接，那么惊群问题很严重。打开reuseport看看（listen 80 reuseport;）我测试8163CPU单core压index页面能到7万 QPS，期待定制系统比nginx性能好。反过来想你的Gateway用了70% US CPU在和nginx 5%的US比（bypass的话忽略这句）</p>
<p>经常被问到Apple M1的购买建议，以及M1和Intel 12代谁强，于是我跑到Apple官网查了下，发现性价比超高的一款M1，如图1</p>
<p>M1 Pro都是10+16核的(图二)，突然出来一个8+14核不太正常，就两个核的差异再搞条生长线一起生产良品率都不高，并且10核中有很多次品，比如坏掉了1C还剩下9C你扔掉还是?于是聪明的工程师设计的时候就做好了软开关，台积电下线后检测出略微坏的就尽量当8c卖，提升总的良品率降低成本，所以你看看这款的差价其实买到就是赚到.</p>
<p>说回芯片成本，Die（裸片，一般大拇指指甲大，你买到的都是封装后的火柴盒大）越大良品率越低成本越高如图三(发热控制另说)，Intel也一直这么干，如图四，拿出大拇指感受下Die的大小，注意里面L3的大小，现在的CPU cache大小超过了Die一半的面积了，下次说这个。</p>
<p>Intel 无论哪代的I5、I7、I9基本都是一条生产线在玩关核的把戏，Die的面积都是 215.25 mm²，详细参数参考这里<a href="https://en.wikichip.org/wiki/intel/core_i5/i5-12600k" target="_blank" rel="noopener">https://en.wikichip.org/wiki/intel/core_i5/i5-12600k</a> ，如图5，把I9放到显微镜下看到如图6</p>
<p>购买建议如图8</p>
<p>N年前我刚加入一家公司几个月，有一个客户购买了我们的产品上线后金额对不上（1类生产事故），于是经理带着我们几个技术去现场看看是什么原因，路上经理说你们不要有什么心理压力，我不懂技术但是我过去就是帮助你们挨骂的，我好好跪在客户那你们好好安心解决问题。</p>
<p>问题大概就是客户代码在一段事务中，但是提交到后端我们的服务上后前对不上了，客户认为我们产品事务有问题。</p>
<p>到了现场客户不让下载他们代码，只能人肉趴在他们指定的机器上用眼睛看问题在哪里，看了三天大家非常沮丧地回来了，自然产品被下线，客户直接用MySQL了，但是三天后一个振奋人心的消息传过来了：金额还是对不上 ……</p>
<p>于是我们再度派出技术人员帮他们看为什么（这次客户配合度高了很多），最后有个同事提了一嘴tcpdump抓个包看看，到底应用代码有没有set autocommit&#x3D;0, 半个小时后传来喜讯用户代码发出的就是autocommit&#x3D;1,说明用户代码的事务配置没生效。</p>
<p>最后查出来配置文件中有中文注释，而生产环境机器不支持中文出现了乱码，导致事务没有生效！</p>
<p>事情还没完，当我听到这个结果后恨不得实际抽自己，tcpdump咱也会用，怎么当时就没想到呢！于是后来我天天看tcpdump、分析网络包，有段时间最开心的是在酒店看书了。一个月后写了几篇文章放在公司内网，再然后公司内部各个团队开始拿着各种问题找过来，我的case也越来越多，结果呢我内心自我认为阿满老师去了西半球后是不是东半球抓包我最牛了 :) </p>
<p>有一次产品调用是这样的 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;6  产品5是我们的，1说性能上不去，rt太大，扯了两天皮，然后说5有问题，于是我到5上抓了个包，明确告诉他们5的rt是多少，压力还没有到5这里来，另外按照我抓包结果的rt分析，5的能力是20万，现在还不到1万，瓶颈在1-5之间，后来我上1&#x2F;2&#x2F;3&#x2F;4用 netstat 分别看下网络状态发现1-2之间网络到了瓶颈（2回包给1的时候大量的包no ack）,不要怀疑netstat真有这么强大，只是你不会看而已。如图三 2上的9108服务端口给1发回结果的时候1那边迟迟不给ack。其实这个case用好工具只是很小的一点，关键的是我能抓包分析出rt，然后从rt推断出系统的能力（别说全链路监控之类的，有时候还得拼刺刀），进而快速定位到瓶颈</p>
<p>现在我们的产品文档必备一份tcpdump、tshark（wireshark命令行版本）救急命令箱，有时候让客户复制粘贴执行后给我们某个结果，好多问题不再是问题了，如图1&#x2F;2</p>
<p>网络这个卡点是在一个复杂、长链路的系统中非常关键的点，大家都认网络数据（抓包数据），可信度比日志高多了，除了鹰眼之类的全链路监控外，可以在Kernel的网络模块中插入代码记下网络收包、回包的时间点（大概20-30行代码），然后监控系统分析内核吐出来的日志形成监控数据。这样一个不侵入应用、0代码实现的完美监控就有了，其实不算完美，因为这种做法只能监控到同步请求一来一回的RT。当然对我们来说就够了，上线后好多次都是通过这个系统进行完美甩锅（快速发现问题）</p>
<p>一次听风扇声音来定位性能瓶颈</p>
<p>问题描述背景</p>
<p>在一次POC测试过程中，测试机构提供了两台Intel压力机来压我们的集群</p>
<ul>
<li>压力机1：两路共72core intel 5XXX系列 CPU，主频2.2GHz， 128G内存</li>
<li>压力机2：四路共196core intel 8XXX系列 CPU，主频2.5GHz， 256G内存 （8系列比5系列 CPU的性能要好、要贵）</li>
</ul>
<p>从CPU硬件指标来看压力机2都是碾压压力机1，但是实际测试是压力机2只能跑到接近压力机1的能力，两台机器CPU基本都跑满，并且都是压测进程消耗了90%以上的CPU，内核态消耗不到5%CPU</p>
<p>所以问题就是为什么196core没打过72core，关键是CPU都还用完了</p>
<p>机器在客户环境缺网络、缺各种工具（连perf都没有），于是只能趴在机箱上听风扇声音，两台机器都听了1分钟，我觉察到了问题，196core机器的CPU风扇声音更小，说明196core的CPU出工不出力，大概是流水线在频繁地Stall。</p>
<p>知识点：通过top看到CPU在运行，但是在芯片内可不一定是真正在running。比如执行一条指令，需要读取数据，如果数据没在cache中那么需要到内存中取进来，这个时候CPU就会休息（放电、降温），这个就叫Stall</p>
<p>于是做了个读写内存的带宽和时延测试，得到如下数据：</p>
<p>72core机器，  本路时延1.1，跨路时延1.4，因为是2路所以有50%的概率跨路，性能下降30%，查到内存条速度2900</p>
<p>196core机器，本路时延1.2，跨路时延1.85，因为是4路所以有75%的概率跨路，性能下降50%，查到内存条速度2100</p>
<p>赶快给196core机器换上2900的内存条速度一下子就上去了，同时这多路服务器不能这么用，要在每一路上起一个实例，不要让内存跨路访问，速度又是几十个个点的提升</p>
<p>面试官为什么喜欢问算法题（算法岗除外）：本质就是对招人方成本最低！</p>
<p>面试官和候选人很难在大部分技术点上match，也就是候选人擅长的面试官问不出深浅；面试官擅长的候选人不一定懂问了也白问。这个时候上来几道力扣算法题最轻松了，只要认识字的面试官都能看出来候选人会不会，当然面试官也没法追问候选人是刷题了还是真现场想出来的（真正现场想出来的应该不会超过5%吧），一般不敢追问怕被反杀:( 因为面试官也是背的答案</p>
<p>这个成本低的本质则是面试官水平不行、或者面试官想偷懒。</p>
<p>什么样的面试官想偷懒？一上来让你自我介绍，然后闷头看简历的，基本都是没做任何准备趁着你自我介绍的时间赶紧看两眼你的简历。好的面试官会提前看简历，针对性地准备好几个问题，然后上来只需要寒暄几句暖场一下从简历上擅长的技术或者项目开始问（最好从简单的，让候选人先把气场打开）。</p>
<p>为什么好的面试官不多呢？成本太高，准备好问题、读完你的博客结果电话一通候选人不感兴趣 :) 慢慢地大家都开始往节省成本的方向靠近</p>
<p>好的面试行为：</p>
<ul>
<li>提前看几遍简历，针对性地准备好问题</li>
<li>简历有博客的，去博客中看看</li>
<li>上来寒暄下，从候选人最熟悉的地方开始发问</li>
<li>针对一些技术点问到候选人答不出来，这样能看出候选人的深浅</li>
<li>场景式面试法，什么场景下、问题是什么、做了什么（如何做）、得到了什么结果</li>
<li>候选人表示不太熟悉的就不要追问了</li>
</ul>
<p>江湖大佬无招胜有招的故事（如何在不懂领域解决问题起来胜过该领域的工程师）</p>
<p>这位同学从chinaren出道，跟着王兴一块创业5Q，5Q在学校靠鸡腿打下大片市场和校内网竞争，最后被陈一舟的校内收购（据说被收购后5Q的好多技术都走了，最后王兴硬是呆在校内网把合约上的所有钱都拿到了–收购合约的钱都是分期付款）。</p>
<p>在我们公司负责技术（所有解决不了的问题都找他），这位同学让我最佩服的解决问题的能力，好多问题其实他也不一定就擅长，但是他就是有本事通过man、Help、Google不停地验证尝试、分析就把一个不熟悉的问题给解决了，这是我最羡慕的能力。</p>
<p>案例：应用刚启动连接到MySQL数据库的时候比较慢，但又不是慢查询，对这个问题有如下几种解决方案：</p>
<ol>
<li>这位同学的解决办法是通过tcpdump来分析网络通讯包，看具体卡在哪个步骤，把这个问题硬生生地给找到了。</li>
<li>如果是专业的DBA可能会通过show processlist 看具体连接在做什么，比如看到这些连接状态是 <strong>authentication</strong> 状态，然后再通过Google或者对这个状态的理解知道创建连接的时候MySQL需要反查IP、域名这里比较耗时，通过配置参数 <strong>skip-name-resolve</strong> 跳过去就好了。</li>
<li>如果是MySQL的老司机，一上来就知道 <strong>skip-name-resolve</strong> 这个参数要改改默认值。</li>
</ol>
<p>在我眼里这三种方式都解决了问题，最后一种最快但是纯靠积累和经验，换个问题也许就不灵了；第一种方式是最牛逼和通用的，只需要最少的业务知识+方法论就可以更普遍地解决各种问题。</p>
<p>每次碰到问题我尽量让他在我的电脑上来操作，解决后我<strong>再自己复盘，通过history调出他的所有操作记录，看他在我的电脑上用Google搜了哪些关键字，然后一个个去学习分析他每个动作，去想他为什么搜这个关键字，复盘完还有不懂的再到他面前跟他面对面的讨论他为什么要这么做，指导他这么做的知识和逻辑又是什么</strong>（这个动作没有任何难度吧，你照着做就是了，实际我发现绝对不会有10%的同学会去分析history的，而我则是通过history 搞到了各种黑科技 :) ）。</p>
<p>感觉这个实现还是不对，padding只对齐了尾巴不让合别人共享一个cache line，但是没法避免前面和别人对齐跨cache line。即使后面对齐也不对，用一个rp而不是8个就能对齐到64，刚好一个cache_line，正确做法得和Disruptor一样前后夹击对齐  <a href="https://plantegg.github.io/2021/05/16/CPU_Cache_Line%E5%92%8C%E6%80%A7%E8%83%BD/">https://plantegg.github.io/2021/05/16/CPU_Cache_Line和性能/</a></p>
<p>一语惊醒梦中人的感觉最爽，我是做了10年性能优化后碰到了一次醍醐灌顶般的醒悟</p>
<p>这之后，无数次只需要看一眼服务的RT、CPU状态就能很快给出服务的极限QPS是多少。</p>
<p>这个原理最简单的总结就是：QPS和延时的乘积是常量（复杂版总结如图1）；其次如图2</p>
<p>当别人给我压测结果数据(并发、QPS、RT)的时候大多我一眼就能看出来数据错了，比如打压力5分钟，然后给了一个5分钟的平均QPS，我一推算QPS不对，再让业务仔细一查原来是5分钟前面有3分钟热身，真正完整压力2分钟，但是QPS给了5分钟的！</p>
<p>如果QPS和延时同时下降那么一定是并发过来的压力不够了。</p>
<p>我见到99%的性能压测就像洞房的处男一顿乱鼓捣，只会不停地加并发，从来没有停下来计算一个并发的QPS是多少、对应的RT是多少、US CPU是多少，最佳并发压力是多少。</p>
<p>而我的做法是：1）用很少的线程压，收集RT、QPS、CPU数据；2）计算得到总QPS、最佳并发线程；3）用计算所得的并发数打压力</p>
<p>理解了下面两张图顶极客时间上所有性能优化的课程（我就在那些课程上找过一些错误数据），胜过你看一堆的性能优化书籍 :)</p>
<p>这也是以一挡一百的知识点，搞懂就打通一个领域</p>
<p>来说一次教科书式徒手全链路性能分析过程</p>
<p>强调徒手是缺少工具、监控的时候不能抱怨、不能甩锅，经理不听</p>
<p>强调全链路是不纠缠某段代码、业务逻辑的问题，而是需要找到问题瓶颈在哪个环节上</p>
<p>场景描述</p>
<p>某客户通过PTS（一个打压力工具）来压选号业务(HTTP服务在9108端口上），一个HTTP请求对应一次select seq-id 和 一次insert</p>
<p>PTS端看到RT900ms+，QPS大概5万（期望20万）， 数据库代理服务 rt 5ms，QPS 10万+</p>
<p>调用链路：</p>
<p>pts发起压力 -&gt; 5个eip -&gt; slb -&gt; app(300个容器运行tomcat监听9108端口上） -&gt; slb -&gt; 数据库代理服务集群 -&gt; RDS集群</p>
<p>问题</p>
<p>性能不达标，怀疑数据库代理服务或者RDS性能不行，作为数据库需要自证清白，所以从RDS和数据库代理服务开始分析问题在哪里。</p>
<p>app业务方也尝试过增加app容器对性能没啥提升，所以怀疑问题在数据库上</p>
<p>分析过程</p>
<p>这里缺各个环节的RT监控，所以定位不了哪个环节瓶颈了</p>
<p>先到app服务上抓到数据库代理服务上的包，快速确认下从app到后端有没有瓶颈，如图1</p>
<p>重点在如何分析图1的数据，我从图1可以得到 数据库代理服务 RT是 15ms，也就是单连接的TPS是1000&#x2F;15&#x3D;70, 实际一条连接一秒钟才给后面发20个请求。</p>
<p>所以结论是后端能扛40万 QPS，压力没有从app服务打给后端</p>
<p>继续在app上抓包，这次抓服务端口9108的响应时间（如图2），分析如图1，结论是压力根本就没有打到9108上</p>
<p>临门一脚得结论</p>
<p>如图三，netstat 一看就知道问题在app服务前面，总结在图4</p>
<p>这次只是在各种监控缺乏的场景下，如何借助各种工具来有理有据地甩锅，其实核心理论在图1的分析过程，也是能力的体现，这后面是对并发、RT、QPS的理解</p>
<p>性能问题最好别找我，找我我能把你们卷飞了</p>
<p>我们的服务一般都在链路的最末端，也是最容易被责问性能不行的（正常）</p>
<p>所以无数次项目需要证明你行</p>
<p>我一般都能徒手透过5&#x2F;6个中间环节一直打到发压力端。</p>
<p>有一次压力机用了12台，总QPS 总是无法逾越10000（上下波动），链路上各种加机器扩容，但是都无法突破10000 QPS</p>
<p>链路上所有环节的工程师都说自己的服务没有问题</p>
<p>于是我从最后端撸到发压力机器，发现每台压力机器的port range是10000-60000，也就是5万可用端口，12台总共60万可用端口，测试用的短连接，一个端口默认是60秒timewait，那么TPS就是 60万除以60秒，正好10000 QPS。</p>
<p>完美的1000 QPS，你说问题简单不，就是很简单，改成长连接就好了</p>
<p>比如这个case：<a href="https://weibo.com/1667773473/Lrl2vzX0Z" target="_blank" rel="noopener">https://weibo.com/1667773473/Lrl2vzX0Z</a> </p>
<p>性能优化是最能体现全栈能力的</p>
<p>曾经有一次紧急被派过去优化一个项目，3天将性能提升了10倍</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)</span><br><span class="line">- 短连接导致的local port不够                   (优化后：1000-3000TPS)</span><br><span class="line">- 生产环境snat单核导致的网络延时增大             (优化后生产环境能达到测试环境的3000TPS)</span><br><span class="line">- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)</span><br><span class="line">- 其他业务代码的优化(比如异常、agent等)          (优化后：4200-&gt;5400TPS)</span><br></pre></td></tr></table></figure>

<p>前面从500到3000是比较容易的，优化起来效果很明显，主要是把CPU从SI、SY赶到US的过程，当然各种工具配合使用要熟练，过程如图1、图2（动图）、图3</p>
<p>当然最有意思的是优化后放到生产环境压测就又不行了，这个时候经理投来不信任的眼神，你丫忽悠我啊</p>
<p>吹了一天牛逼说是从500翻了6倍，然后生产环境一验证，白瞎！</p>
<p>线上最大的差别就是会调用第三方服务，但是第三方服务监控显示rt很小、也没啥压力（又到了扯皮时间）</p>
<p>于是我设计如下三个场景证明问题在中间链路上：</p>
<ol>
<li>压测的时候在业务机器 ping 依赖第三方服务的机器；</li>
<li>将一台业务机器从负载均衡上拿下来(没有压力），ping 依赖第三方服务的机器；</li>
<li>从公网上非我们机房的机器 ping 依赖第三方服务的机器；</li>
</ol>
<p>这个时候奇怪的事情发现了，压力一上来<strong>场景1、2</strong>的两台机器ping 依赖第三方服务的机器的rt都从30ms上升到100-150ms，<strong>场景1</strong> 的rt上升可以理解，但是<strong>场景2</strong>的rt上升不应该，同时<strong>场景3</strong>中ping依赖第三方服务的机器在压力测试的情况下rt一直很稳定(说明压力下依赖第三方服务的机器没有问题），到此确认问题在我们到依赖第三方服务机房的链路上有瓶颈，而且问题在我们机房出口扛不住这么大的压力。于是从上海Passport的团队找到北京Passport的PE团队，确认在我们调用依赖第三方服务的出口上使用了snat，PE到snat机器上看到snat只能使用单核，而且对应的单核早就100%的CPU了，因为之前一直没有这么大的压力所以这个问题一直存在只是没有被发现。</p>
<p><strong>于是PE去掉snat，再压的话 TPS稳定在3000左右</strong></p>
<p>这个优化最戏剧性的就是在上线后因为snat导致性能不行的证明问题，链路很长、团队很多，直接说不是自己的问题是没有意义的，关键是要如何在一个长链路中证明不是自己的问题，并且定位问题在哪里</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/82489e801d8f7bd455053315d760614b.png" alt="image.png"></p>
<p>教科书和实践很容易脱节</p>
<p>比如讲DNS出问题总是喜欢谈到DNS的递归解析，这些是DNS工程是需要关心的，但是对程序员来说更重要的是DNS在我的服务器上是怎么一个解析流程，解析不了再发给DNS服务器，但是在发给DNS服务器之前出问题是需要程序员兜底的。比如域名不能ping通，但是nslookup能通；</p>
<p>这方面就很少有教科书、文章来讲了。</p>
<p>比如讲到LVS总是告诉你那LVS的几种模式，但是没有从技术的原理上讲通LVS是怎么样在不同的模式下工作的，从而得到他们的优缺点，教科书一上来就把优缺点告诉你，没有带你推导他们的背后原因！</p>
<p>再比如讲LVS负载均衡原理一上来就是那10种负载均衡算法，但是现实中我们经常碰到的是：咦，这个负载均衡算法为什么导致了我的服务负载这么不均衡！</p>
<p>也就是教科书部分重点喜欢大而全的总结，实践希望我们揪住重点彻底掌握</p>
<p>教科书不负责跨界，实践需要我们跨界</p>
<p>来说一个知识上降维打击（学习）的案例</p>
<p>大多程序员对LVS的几种转发模式有点晕菜，但时不时又要涉及一下，晕菜是看着似乎懂但是没有真的懂，一用就发懵</p>
<p>实际上如果你要是理解了RFC1180，然后从包的流转，当包在LVS上被LVS修改并继续路由的时候，你要理解LVS对包做了什么修改、为什么要做这个修改、这种修改必须要求的场景（将来部署的缺陷）、作了修改后包怎么到达后端的Real Server，你就彻底理解了这些转发模式，同时优缺点也了如指掌。</p>
<p>这个时候再让你就某个应用特点来设计一个新的LVS代理转发模式就很容易了</p>
<p>RFC1180的威力 <a href="https://weibo.com/1667773473/LpXXUpLj2" target="_blank" rel="noopener">https://weibo.com/1667773473/LpXXUpLj2</a></p>
<p>用RFC 1180的逻辑来理解LVS   <a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/">https://plantegg.github.io/2019/06/20/就是要你懂负载均衡--lvs和转发模式/</a></p>
<p>大家一起切磋下如何解读性能测试数据</p>
<p>比如这个测试报告显示某个产品性能比Nginx好3倍，并且有详细的测试环境、数据比较：<a href="https://xie.infoq.cn/article/a25a30a1f190e7c6a41c4580f" target="_blank" rel="noopener">https://xie.infoq.cn/article/a25a30a1f190e7c6a41c4580f</a></p>
<p>所以问题是你仔细看完整个测试数据报告后，你觉得测试数据有问题吗？我想告诉大家的是这个数据测试不对，然后你要分析哪里不对了</p>
<p>理解超线程是掌握CPU相关知识非常重要的一个抓手、也超级实用</p>
<p>超线程(Hyper-Threading)原理</p>
<p>一个物理核还可以进一步分成几个逻辑核，来执行多个控制流程，这样可以进一步提高并行程度，这一技术就叫超线程，有时叫做 simultaneous multi-threading（SMT）。</p>
<p>超线程技术主要的出发点是，当处理器在运行一个线程，执行指令代码时，很多时候处理器并不会使用到全部的计算能力，部分计算能力就会处于空闲状态。而超线程技术就是通过多线程来进一步“压榨”处理器。<strong>pipeline进入stalled状态就可以切到其它超线程上</strong></p>
<p>举个例子，如果一个线程运行过程中，必须要等到一些数据加载到缓存中以后才能继续执行，此时 CPU 就可以切换到另一个线程，去执行其他指令，而不用去处于空闲状态，等待当前线程的数据加载完毕。<strong>通常，一个传统的处理器在线程之间切换，可能需要几万个时钟周期。而一个具有 HT 超线程技术的处理器只需要 1 个时钟周期。因此就大大减小了线程之间切换的成本，从而最大限度地让处理器满负荷运转。</strong></p>
<p>ARM芯片基本不做超线程，另外请思考为什么有了应用层的多线程切换还需要CPU层面的超线程？</p>
<p><strong>超线程(Hyper-Threading)物理实现</strong>: 在CPU内部增加寄存器等硬件设施，但是ALU、译码器等关键单元还是共享。在一个物理 CPU 核心内部，会有双份的 PC 寄存器、指令寄存器乃至条件码寄存器。超线程的目的，是在一个线程 A 的指令，在流水线里停顿的时候，让另外一个线程去执行指令。因为这个时候，CPU 的译码器和 ALU 就空出来了，那么另外一个线程 B，就可以拿来干自己需要的事情。这个线程 B 可没有对于线程 A 里面指令的关联和依赖。</p>
<p>CPU超线程设计过程中会引入5%的硬件，但是有30%的提升（经验值，场景不一样效果不一样，MySQL&#x2F;Hadoop业务经验是提升35%），这是引入超线程的理论基础。如果是一个core 4个HT的话提升会是 50%</p>
<p>这两年到处收集各种CPU，然后测试他们的性能</p>
<p>发现不同厂家CPU相同频率性能差异极大（单核）</p>
<p>在数据库场景下测试下来CPU的性能基本基本和内存延时正相关</p>
<p>谁家把延时做得低性能就好</p>
<p>如图1 core:120 490.402 表示120号core访问0号内存延时490，core:0 149.976 表示0号core访问0号内存延时149（差异巨大）</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/dataf1-1372099277050.jpg" alt="data f1"></p>
<p>20年前Intel为了搞多核，开始将两个Die封装成一块CPU售卖</p>
<p>如图1，两个Die 上共4个core，然后封装成一块CPU</p>
<p>这被大家瞧不起，说是胶水核，因为1&#x2F;2和3&#x2F;4间延时很大</p>
<p>后来Intel再也不搞胶水核了，现在一个Intel的Die能放下几十个core，延时都很低</p>
<p>不过AMD则依靠胶水核要翻身了</p>
<p>如图2，这块CPU上AMD用了8+1个Die才放下去32core，性能延时都很好</p>
<p>便宜到人人说香</p>
<p>其实这两条路线这两家都能搞，但是Intel那个路线就是太贵</p>
<p>而AMD这种搞法恰好很适合云计算（拆开售卖）</p>
<p>不信你去各家云平台看看他们的价格差距</p>
<p> CPU中的cache变迁历史</p>
<p>80486(1989), 8K的L1 cache第一次被集成在CPU中，图1</p>
<p><strong>80686</strong>(1995) ，L2被放入到CPU的Package上，但是是一个独立的Die，可以看到L2大小和一个Die差不多，图2</p>
<p>以酷睿为例，现在的CPU集成了L1&#x2F;L2&#x2F;L3等各级CACHE，<strong>CACHE面积能占到CPU的一半</strong>，图3</p>
<p>从上图可以看到L3的大小快到die的一半，L1&#x2F;L2由每个core独享，L3是所有core共享，3级CACHE总面积跟所有core差不多大了。</p>
<p>最近这些年Cache上基本也没什么大的花样了</p>
<p>折腾cache都是为了解决内存延时问题（内存墙），或者说内存的延时配不上CPU的速度了（越来越大），图4</p>
<p>之前的好几条微博写了很多网络问题、学习方法等</p>
<p>我试着把他们总结起来写了篇《程序员如何学习和构建网络知识体系》</p>
<p>核心是从程序员实用、常碰到问题出发，提炼出相关的核心知识点，然后串联起来</p>
<p>里面每一个知识点都是碰到了具体问题</p>
<p>先是去解决，解决后再分析、总结、提炼。</p>
<p>比如网络为什么不通，还有哪些原因会导致不通，这个原因在不同Linux内核版本下会有什么不一样吗？</p>
<p>以后这种问题有没有快速定位工具、手段</p>
<p>比如网络传输慢的时候，定性定量分析RT的影响、Buffer的影响、BDP如何打满等</p>
<p>这种总结性的文章一般都比较务虚，所以每一个务虚的地方我放了一个案例</p>
<p>总共写了大概10篇相关案例来反过来证明务虚的话语</p>
<p>希望对你有所帮助</p>
<p>链接地址：<a href="https://plantegg.github.io/2020/05/24/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/">https://plantegg.github.io/2020/05/24/程序员如何学习和构建网络知识体系/</a></p>
<p>程序员面试考算法（非算法岗）和考 拧魔方 差别大不大？</p>
<p>视频网站各种魔方手法总结，学一学都会</p>
<p>letcode各种算法总结也是非常完善</p>
<p>这两都能让面试官好好休息一下，有标准答案，对一下就行</p>
<p>不要抱怨招进来的人为啥水、干不了活</p>
<p>还不是letcode总结得好、八股文总结得好</p>
<p>之前的好几条微博写了很多网络问题、学习方法等<br>我试着把他们总结起来写了篇《程序员如何学习和构建网络知识体系》</p>
<p>核心是从程序员实用、常碰到问题出发，提炼出相关的核心知识点，然后串联起来<br>里面每一个知识点都是碰到了具体问题<br>先是去解决，解决后再分析、总结、提炼。</p>
<p>比如网络为什么不通，还有哪些原因会导致不通，这个原因在不同Linux内核版本下会有什么不一样吗？<br>以后这种问题有没有快速定位工具、手段</p>
<p>比如网络传输慢的时候，定性定量分析RT的影响、Buffer的影响、BDP如何打满等</p>
<p>这种总结性的文章一般都比较务虚，所以每一个务虚的地方我放了一个案例<br>总共找了大概10篇相关案例来反过来证明务虚的话语</p>
<p>希望对你有所帮助<br>链接地址：<a href="https://plantegg.github.io/2020/05/24/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/">https://plantegg.github.io/2020/05/24/程序员如何学习和构建网络知识体系/</a></p>
<p>造新词、新概念是个很有意思的事情</p>
<p>大多时候能够化腐朽为神奇，一下子让大家都通透、凝聚、共识</p>
<p>听起来还很高深、高端，这就导致很多人为了高深、高端而造新词</p>
<p>这在很多大厂非常流行，搞好了四两拨千斤</p>
<p>但很多时候也会把简单事情复杂化、搞得神神叨叨，这就是为了造词而造词</p>
<p>比如WEB3是个啥，你看各个百科的解释是有点蒙逼</p>
<p>但是如果告诉你WEB3背后的存储就是区块链，区块链就是一个大DB</p>
<p>更简单点说WEB3就是一张大Excel表格，把大家的信息都放在这张表格里</p>
<p>别的网站都能来读取，这就是WEB3描绘的网站间数据共享</p>
<p>但是WEB3太模糊了，表达不了背后的本质，明显是为了操作往WEB2上套（区块链、比特币已经没人爱上钩了）</p>
<p>长链路性能压测之瞎几把压典范</p>
<p>1-&gt;2-&gt;3-&gt;4—-&gt;N</p>
<p>1使劲加并发打压力，但是QPS非常小，1-N每个环节的工程师都说自己没有问题</p>
<p>关键是每个环节的理由是：我的CPU、网络都很闲，没有压力。我基本没见过说自己RT稳定的！</p>
<p>然后长时间扯皮、互相甩锅，在我看来这些甩锅没有一点技术含量，真的是在甩锅，这些人都没入门。</p>
<p>这里本末倒置了，你要证明自己没问题必须说加压后自己的RT没增加，而不是说CPU、网络很闲</p>
<p>这里CPU是过程，RT才是我们要的结果（不要质疑别人的CPU高，先质疑RT高–或者说RT上升快）。</p>
<p>展开下：性能优化目的是提升QPS也就是降低RT，CPU、内存、网络等等都是不关键的要素，最关键的结果就是RT</p>
<p>因为并发一定的情况下QPS和RT是反比关系</p>
<p>记住这句话：长链路性能压测先追着RT跑而不是追着CPU、内存等资源跑</p>
<p>写了这么多增删改查，你会对JDBC驱动了如指掌吗（仅限Java+MySQL技术栈）？</p>
<p>我碰到几个厉害的程序员一个增删改查下去业务代码如何与MySQL互动一清二楚，业务上有了bug、性能有了小问题</p>
<p>大概率半个小时就给你分析得明明白白</p>
<p>但是90%以上的程序员即使天天增删改查，却对JDBC驱动一直是盲人摸象，今天这里看个参数明天那里优化下、明天那里鼓捣下</p>
<p>不成系统，碰到硬扎问题还是只能瞎试、求人</p>
<p>比如什么是JDBC流式驱动，有些程序员被一个大查询把内存怼爆了，才想起来优化，然后网上抄个参数好像问题解决了，但是怎么很多情况下反而慢了，经常timeout了；</p>
<p>比如预编译优化以为加上useServerPrepStmts&#x3D;true就可以了，测试也性能好了，但他妈的上线后发现性能反而差了</p>
<p>建强的初衷是什么？肯定不是为了保护淘宝、百度、腾讯以及锅内互联网。</p>
<p>建强的事实结果导致了锅内互联网的繁荣吗？这没法证伪和证实。</p>
<p>那么现在事实上锅内互联网这么繁荣了，强能拿掉吗？</p>
<p>你可以随意使用汉字不被限流河蟹吗？这些汉字有依据不能使用吗？</p>
<p>一个体制内、媒体多年工作者(靠采访、写字为生)被全网封杀确实是灭顶之灾，封杀三年后远遁他乡，尝试过妥协解封，没有结果。</p>
<p>终于没有希望回来后，</p>
<p>可以在那些不存在的网站讲讲CCTV的一些潜规则</p>
<p>X86有个有意思的指令：pause， 调用这个指令进程啥也不干，就是休息N个时钟周期，这个时候CPU可以省电、避免切换到其它进程导致上下文切换、也可以调度给超线程。一般抢锁失败会pause一下，比如内核中到处的spin。–这是原理</p>
<p>Intel Broadwell架构（比如 E5 至强5代）及之前的CPU都是pause一次休息10个时钟周期，Skylake架构之后这个pause一次休息时钟周期从10调到了140，整整增加了14倍啊，带来的后果是灾难性的（因为很多软件是不会测试、考虑这么细致的）如图1</p>
<p>比如MySQL使用innodb_spin_wait_delay控制spin lock等待时间（底层会调用这个pause指令），跑在Broadwell CPU上等待时间时间从innodb_spin_wait_delay<em>50</em>10个时钟周期（6微秒）。如果跑在Skylake上spin一次休息innodb_spin_wait_delay<em>50</em>140个时钟周期（84微秒，我相信没有几个DBA会根据CPU型号去调整参数吧–有的话请私聊我，交个朋友），后果就是MySQL在高并发场景下TPS拉胯的厉害，如图2</p>
<p>这是我第一次感受CPU对程序的影响，后来的事前面的微博都写过了 <a href="http://t.cn/A6XHRiYl" target="_blank" rel="noopener">http://t.cn/A6XHRiYl</a> ，每一次被现实鞭打后得知耻而后勇[二哈]</p>
<p>你看程序员啥都没做，但是锅在你头上</p>
<p>图1这张图让我学习到了特别多的东西：<br>1）为了成本把坏掉了一个核四核芯片关掉2个核（红圈所示），当2核卖（i3&#x2F;i5&#x2F;i7&#x2F;i9 就是这么来的）<br>2）右下角告诉我这块CPU 芯片是177平方毫米，没概念就拿出你的大拇指看看，大概是这么大，再有人吹牛逼说他们的芯片多牛逼，先看看那块芯片的大小你就知道成本高了多少，基本主流的PC芯片都是 200以内，服务器略大点<br>3）这块芯片去掉四个核后，还有一半的面积（成本）用在了cache上，也就是现在的芯片cache成本基本和核的成本差不多了，要花掉你一半的钱在上面，他们很珍贵<br>4）PC芯片GPU占用面积（成本）也不小</p>
<p>图1 也叫Die（裸片），就是台积电加工完成后的东西，得再做封装后变成火柴盒字大小你能买到的CPU实物（如图三）</p>
<p>结合图二，这是一个封装后的AMD CPU，在云计算时代我挺看好这种模式的，AMD把一块CPU一个Die切割成了9个Die 来加工然后用胶水黏在一起卖（真他妈便宜），关键是性能还和Intel差不多，然后拿到云上本来也得切割成小的ECS售卖，所以感觉Intel真浪费！–你去各家云上看看AMD虚拟机卖得就是便宜</p>
<p>Die的大小成本到底差了多少呢？ 加工Die最关键的是良品率，Die越大良品率越低（你想想显示器大小和坏点的关系）。如图4，这是个Die大小对良品率影响的计算案例（最外面那个圆盘就是我们所说的晶圆，台积电就是把一块大晶圆加工成多个小的CPU 芯片），一般良品率超过50%就是个不错的成绩了</p>
<p>你要是反复吃透这篇博文的话基本对CPU的理解算是入门了，如果没就要把图一供起来反复看</p>
<p>软设置定制内存大小的故事<br>有一次我们有一台1T内存，CPU4个NUMA Node的物理机，但是不符合使用要求，内存太大，规定只能用512G内存。机器借过来的不能撕毁标签</p>
<p>第一次尝试，在Linux OS grub启动参数中设置 mem&#x3D;512G，起来后果然只剩下512G内存了。但是跑下来性能不够好，如图1，相当于拿掉了红框里的内存，右边的core要跑很远才能用上左边的512G内存，自然性能不好了。</p>
<p>第二次尝试，在Linux OS grub启动参数中按numa node设置每个node内存为128G（4个合起来512G，关键字 memmap），这基本是完美方案，性能也和1024G时一样好</p>
<p>不过意外发生在某次的国产CPU上，我们用方案二，另外一个团队直接拆机箱把内存，把我们打垮了</p>
<p>读中学的时候我有个同学对物理很感兴趣，天天拿着一本奥赛的书，晚自习逮到物理老师就问，开始的时候物理老师还挺有耐心的，老师不会的会回去研究下再跟这个同学交流</p>
<p>可实际上吧每次物理考试这个同学也就及格水平（班里处于中游），但是挡不住自己的热情，一学期下来物理老师看到他就躲，晚自习教室门口瞄一眼就赶紧闪，最后这同学也没考上大学，你想想物理是他最拿手的科目了只是中游水平。</p>
<p>他这些竞赛书我也拿过来翻过，题目都看不懂，只好绕着跑。</p>
<p>后来工作后认识个朋友，自学编程，一上来迷上了《计算机程序设计艺术》，三大本都买回来了，天天琢磨算法，CRUD也搞不好、计算机基础知识也不太懂，就这样自学了1年多后跑北京找工作，结果找了半年一个正经工作也没有，后来去了广州就没消息了。也怪我不应该告诉他有这书的，同样这书我也看不懂。</p>
<p>这种人大家身边也许都有，有热情但是就是不能脚踏实地，没有那么高的能力非要摸尖尖。大部分民科都是这种吧</p>
<p>它力压超线程成为冯诺依曼计算机体系下唯一的特优设计、它降低了程序运行性能但是程序员依然如痴如醉地离不开它、它究竟是如何拳打超线程脚踩cache成为计算机体系的最优设计的？它就是虚拟内存</p>
<p>一个月前的微博总是被屏蔽，周末终于知道是哪个词了（放在最后说），突然感觉挺无聊的，不想多写了</p>
<p>-——</p>
<p>高中的学校门口时候亲眼见过一个“民科”</p>
<p>一个老爷子，摆了几张大白布，上面全是各种公式，说是证明了哥德巴赫猜想（记不太清了，大概是那几个世界难题），现在想想我们学校没有数学牛人啊，这种“民科”–这也是后来形容这类人提出的名词，现在主要集中在中科院数学研究所门口了，算是找对地方了。</p>
<p>不过这种“民科”特别单纯、自娱自乐，也没啥不好，跟你玩王者荣耀、魔方、刷letcode差别不大。</p>
<p><a href="https://weibo.com/1667773473/LwlKNtfZp" target="_blank" rel="noopener">https://weibo.com/1667773473/LwlKNtfZp</a> 这条我本意不是要说民科。痴迷奥数和《计算机程序设计艺术》也不是民科，准确来说是不务实，重要的基础和工作相关的技术还没掌握好，在明确目标面前非要走“邪路”。</p>
<p>这样的例子还有很多，比如很多文章讲TCP各种拥塞算法条条是道，可是我们工作中需要用到这些吗？你TCP握手、断开还有点迷糊，就痴迷这些不恰当；</p>
<p>再比如好多书讲cache_line 的tag、组一套一套的，但是cache_line的本质、如何发现False sharing等偏实践的还没搞懂呢。</p>
<p>还有人每天都要化一两小时刷几道letcode, 你也知道用不上、面试官也知道用不上，但是大家就是痴迷拿到题一气呵成没有bug。其实都是背好了而已，真要考就先看思路、然后wei代码能表达出来就可以了。</p>
<p>其实这里讲的是取舍和重点问题。</p>
<p>说这些总的意思是优先尽量从实践出发多学天天能用上的知识，借用 jjhou 老师20年前很有名的一句：勿在浮沙筑高台(出自 程序员 杂志  @蒋涛CSDN  )</p>
<p>最后希望大家不用为了工作疲于奔命，闲暇至于为了怡情、喜欢可以多看奥数、TAOCP（高级享受）</p>
<p>-——-<br>false sharing 的中文是min敢词</p>
<p>高中的学校门口时候亲眼见过一个“民科” 一个老爷子，摆了几张大白布，上面全是各种公式，说是证明了哥德巴赫猜想（记不太清了，大概是那几个世界难题），现在想想我们学校没有数学牛人啊，这种“民科”–这也是后来形容这类人提出的名词，现在主要集中在中科院数学研究所门口了，算是找对地方了，也许是有钱有闲买票容易了。 </p>
<p>不过这种“民科”特别单纯、自娱自乐，也没啥不好，跟你玩王者荣耀、魔方、刷letcode差别不大。 <a href="http://t.cn/A6XrYUzm" target="_blank" rel="noopener">http://t.cn/A6XrYUzm</a> 这条我本意不是要说民科。痴迷奥数和《计算机程序设计艺术》也不是民科，准确来说是不务实，重要的基础和工作相关的技术还没掌握好，在明确目标面前非要走“邪路”。 这样的例子还有很多，比如很多文章讲TCP各种拥塞算法条条是道，可是我们工作中需要用到这些吗？你TCP握手、断开还有点迷糊，就痴迷这些不恰当； 再比如好多书讲cache_line 的tag、组一套一套的，但是cache_line的本质、如何发现False sharing等偏实践的还没搞懂呢。 还有人每天都要化一两小时刷几道letcode, 你也知道用不上、面试官也知道用不上，但是大家就是痴迷拿到题一气呵成没有bug。其实都是背好了而已，真要考就先看思路、然后伪代码能表达出来就可以了。 其实这里讲的是取舍和重点问题。 说这些总的意思是优先尽量从实践出发多学天天能用上的知识，借用 jjhou 老师20年前很有名的一句：勿在浮沙筑高台( 出自 程序员 杂志  @蒋涛CSDN  ) 最后希望大家不用为了工作疲于奔命，闲暇至于为了怡情、喜欢可以多看奥数、TAOCP（高级享受）</p>
<p>10年前P10大佬无招胜有招的故事</p>
<p>JBoss启动失败，没有太多错误信息，唯一有一行LogFactory.release的warning日志</p>
<p>对这个问题，如果熟悉JBoss启动流程那么很容易排查（套路熟练），如果不熟悉JBoss怎么办呢。大佬虽然对JBoss不熟但是对btrace无比熟练，所以从trace这个warning入手一步步trace出来启动流程堆栈</p>
<p>然后追踪到listenerStart，再然后trace到Exception，继续通过trace dump到Excepiton内容是因为jar加载冲突了，再加上 启动参数上增加-XX:+TraceClassLoading就能知道具体冲突的版本</p>
<p>你看一套流程下来考的是btrace无比熟练，跟我之前讲的抓包一样。抓包、strace、btrace</p>
<p>啊，P10还查问题？嗯10年前P10也要弄脏双手干活的，现在P8就不用了</p>
<p>最后讲个故事，有次别人面试我，问我TCP的close_wait 是为啥，我答偏了，在这之前我写过两篇如图2一样关于close_wait的文章</p>
<p>为什么操作系统有了多进程调度能力之后，CPU还在一个物理core上搞两个线程来分享这一个物理core呢（超线程）？</p>
<p>操作系统多进程调度有两个目的：1）让计算机拥有多任务能力；2）一个线程卡顿（比如读文件、网络）时切换到另外一个线程，高效使用CPU。目的2和超线程目标是一致的但不重叠</p>
<p>关键在于一次超线程切换只需要几个时钟周期，而一次操作系统的多线程、进程调度需要大几千个时钟周期，对CPU来说这种切换太慢了，没法充分利用CPU</p>
<p>据说4线程的CPU一直在评估设计中（90%的场景下，即使2个超线程也只跑满CPU流水线的一半能力）</p>
<p>业务代码必须插入安全团队的XSS扫描等代码，这个扫描代码每次在扫描结束的时候抛出 EOFException 然后自己catch，然后结束。用异常来控制业务流程，每次都 fillInStackTrace 然后自己悄悄吃掉，外部啥也感知不到，但是性能降低了30%。这样的同事多给我来几打</p>
<p>重新翻了公司10年前的经典案例排查过程，用10年后的姿势水平看当时的过程十分曲折。</p>
<p>现在看都是非常直白的知识点：比如JVM YGC耗时只和存活对象数有关和新生代大小无关（结果我看到10年前的工程师反复试验得到了这个结论）；比如TCP全连接队列是否爆了用 ss 看下就知道了没必要netty代码分析来去（10年前花了3天时间也搞定了）– 他们都厉害在没有知识也能解决问题<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/2018new_zhongguozan_org.png" alt="[中国赞]">，比我厉害1万倍</p>
<p>财新网等媒体眼中的著名专家 <a href="https://weibo.com/n/%E9%80%AE%E7%8D%AD%E7%A7%91%E6%8A%80" target="_blank" rel="noopener">@逮獭科技</a>  曾说过：<br>我觉得这个是时代的变化，每一代人中的佼佼者，其知识系统在下一代人看都平平；二战之后真正开挂的是全球的教育产业，几何级数膨胀受教育人口，而且，知识更迭很快，能追上前沿很不容易</p>
<p>程序员领域很卷很大一部分原因除了知识会过期还有很大一部分知识门槛变低了！我们的知识在下一代人眼里之所以平平无奇就是这些知识很快会变成八股文了，就像一个培训班训练出来的学生一样，可以解题拿高分，但是一旦出来一个新题型就嗝屁了</p>
<p>完整清晰地解决一个你所面临的新问题就像冲塔，一个人自己冲是最难的，别人冲完后告诉你攻略(八股文，就变成经验了）就容易多了，这是无招胜有招（真学霸），刷题多的都是假学霸；同样靠刷题面试牛逼的不一定是真学霸，问题出在了面试官分辨能力上。</p>
<p>信息流下几点经验分享下</p>
<p>看到一篇好文章后把整个博客都看下，挑你擅长的先看，快速确认博客内容深度以及是否适合你；</p>
<p>看到好微博也是，话痨的就算了，有些人连评论都懒得开就开始问！</p>
<p>公众号重点看看开号前面半年发的内容，一般都是干货，后面大多都是带货、为了发而发</p>
<p>不要沉迷信息流，多翻翻箱底的经典文章，他们能沉淀下来相对更有实力。我就发现新同学基本不太关心老文章，总是追求新的，你看我前一阵还在翻公司10多年前的案例</p>
<p>有疑问的先放狗搜一下再提问</p>
<p>看到一张好图片可以先搜图，然后根据图片能给你搜出来一大堆好文章（好文章配图一般也不差）</p>
<p>专门给微博用户的：不要在评论里 @**笔记 实在想，就转发 @**笔记, 不至于打扰别人</p>
<p>一个有意思的想法<br>每一代人中的佼佼者，其知识系统在下一代人看都平平。大概是因为完整清晰地解决一个你所面临的新问题就像冲塔，一个人自己冲是最难的，别人冲完后告诉你攻略(八股文，就变成经验了）就容易多了，第一个解决问题并沉淀的是牛人，让问题成为知识</p>
<p>这样让后面的普通资质的人也有了牛人的知识和“能力”，随着这种牛人沉淀下来的死知识越来越多，后面的人只需要掌握更多的死知识，但是失去了更多的单独冲塔的机会。当然每个时期的牛人还是存在的，只是牛人里面掺入的沙子越来越多了，你看互联网行业人人大佬、人人专家。</p>
<p>面试也是靠刷题、背八股文，刷题就是典型的牛人把思路方法放那里了，普通人还需要花上1&#x2F;2周来消化，消化后面试效果比牛人还牛（熟练啊），面试官也甄别不了</p>
<p>结果会怎么样呢……</p>
<p>讲一个诈骗程序员的案例</p>
<p>程序员都喜欢注册域名，如果注册域名并在公安注册后，过几年域名到期了（大概率），这个时候有专门的流氓公司</p>
<p>他们会抢注域名，然后在这个域名下放一些热门盗版电影（不涉黄），这个时候他们的另一个公司（拥有电影版权的公司）出来取证了</p>
<p>接下来就是去法院告你盗版要求赔偿，在公安那里这个域名的所有人还是你（或贵司）从法律流程上来说完美无缺，你一定会输掉官司，这个时候流氓公司就等着你和解割地赔款</p>
<p>他们有专门的团队把整个过程流程化、低成本化</p>
<p>如果你们有废弃的域名记得注销ICP备案，如果是大厂更要记得这事，大厂赔得更多</p>
<p>以前主要是分析TCP协议，HTTP 接触得少，这几天补了一把，只能说wireshark对HTTP解析做得太好了. 比如以前我说抓包发现一个请求15ms，然来这个数据wireshark帮我们解析好了，MySQL协议的解析就没那么友好</p>
<p>我常用又不多见的命令（参数）</p>
<p>用curl调试sock5代理：curl -x socks5h:&#x2F;&#x2F;localhost:8001 www.不存在的网站.com&#x2F;</p>
<p>nc走sock5转发: ProxyCommand  &#x2F;usr&#x2F;bin&#x2F;nc -X 5 -x 127.0.0.1:13659 %h %p &#x2F;&#x2F;连github时</p>
<p>wget不存在的网站：wget -Y on -e “http_proxy&#x3D;http:&#x2F;&#x2F;<strong>[HTTP_HOST]</strong>:<strong>[HTTP_PORT]</strong>“ http:&#x2F;&#x2F;不存在的网站.com&#x2F;其中：[HTTP_HOST]和[HTTP_PORT]是http proxy的ADDRESS和PORT。</p>
<p>curl指定本地端口连远程（这样抓包只抓这个端口）：curl –local-port</p>
<p>nc测试udp能否通（比如overlay网络、dns）：nc -v -u -z -w 3 1.1.1.1 53 </p>
<p>awk分组统计分析平均值：awk ‘{ sum[$1]+&#x3D;$2; count[$1]+&#x3D;1 ;} END { for (key in count) {  printf  “time&#x3D; %s  \t count&#x3D;%s   \t avg&#x3D;%.6f \n”, key,  count[key], sum[key]&#x2F;count[key] } }’</p>
<p>将最近多少天的md笔记发表到博客：find $srcDir -maxdepth 1 -type f -mtime -$1 -name “*.md” -not -name “template.md” -not -name “temp.md” -exec cp “{}” .&#x2F;source&#x2F;_posts&#x2F; ; &#x2F;&#x2F;改改可以用户备份本地最近修改的文件、配置</p>
<p>将博客上的大图压小（节省博客流量）：find img_small -size +1024k -type f -exec sips -Z 1024 {} ;</p>
<p>检查用户使用的是长、短连接（别被用户的描述坑了）：netstat -ato</p>
<p>发起ping 风暴：ping -f </p>
<p>测试网络MTU：ping -M</p>
<p>带时间戳的ping: ping -D 114.114.114.114 | awk ‘{ if(gsub(&#x2F;[|]&#x2F;, “”, $1)) $1&#x3D;strftime(“[%F %T]”, $1); print}’</p>
<p>算是小抄，大多时候都是man、放狗可以获取，还有很多下次放</p>
<p>很多知识没啥用，但是逼格高，网上讲得多</p>
<p>比如拥塞算法，那个算法能搞明白的没几个，程序员基本不需要懂，最多最多就是sysctl配置换一下。程序员要的是打满带宽、延迟低、不丢包</p>
<p>还有cache line 分组编码，我们程序员要的是cache line不发生False sharing，多给我几个Disruptor如何做、Netty如果做的案例就完美了，比如Netty里面的代码实现全错了这么多年也没啥大事，还有程序员继续在错误的代码上，继续提交仍然是错误的patch，还没合并了 <a href="https://weibo.com/1667773473/LrEKR1lIL" target="_blank" rel="noopener">https://weibo.com/1667773473/LrEKR1lIL</a></p>
<p>比如说起DNS，现在的资料主要是将服务器怎么递归解析域名，程序员一脸懵逼，你就告诉我域名解析是我配置的问题（出在本机）还是发走后服务器解析不了（可以call 运维支撑），比如本机能ping但是不能nslookup 又是怎么回事？本机配置问题、lookup流程可以给程序员多讲讲，这里程序员可以兜底，出了本机就得运维啥的来支撑了。程序员要的是这种 <a href="https://plantegg.github.io/2019/01/09/nslookup-OK-but-ping-fail/">https://plantegg.github.io/2019/01/09/nslookup-OK-but-ping-fail/</a> </p>
<p>还有刷算法题，面试造火箭入职拧螺丝，大多指的这种。现在保守估计99%的letcode算法都用不上，95%的程序员不需要写心的算法，也就是现在的工具箱里螺丝刀这么多、这么好用了，你居然要面试让程序员如何设计一个新的螺丝刀？关键是贵司也不是螺丝刀工厂啊。最后说新员工能力不行</p>
<p>我是真没想到985毕业还非要说等额本金比等额本息的利息少！</p>
<p>首先两者的利率是一样的，你还多少利息&#x3D;借钱数*利率（按月算吧，就可以去掉时间变量了）</p>
<p>这几个变量一样利息就一样，之所以等额本金给你感觉还利息少是因为你从第二个月开始欠的本金少了（不是还款方式导致的利息差异），欠的本金少了是因为你每个月还得多。</p>
<p>你借100万，每个月还1万，假设一个月这100万欠款的利息是5000，那么下个月你只欠99.5万了，下个月只需要还99.5万一个月的利息。–这是核心逻辑</p>
<p>顺便说下现在信用卡、套路贷就是用的这种方式打插边球让你以为利率低（不敢直接宣传利率是多少），比如1万块分期账单分10期，每期还1050，让你以为利息一个月 50块（按1万本金算一年利率折合6%），总利息也确实只还了 500块，但是你想想最后一个月你只欠他1000本金，但是仍然还了50块利息，也就是1000块年息600块（12*50），也就是年利率 60%，妥妥的高利贷</p>
<p>#程序员的螺丝刀# nc（netcat）</p>
<p>测试udp端口的连通性（比如dns、比如overlay服务），如图1</p>
<h2 id="nc-l-u-4789"><a href="#nc-l-u-4789" class="headerlink" title="nc -l -u 4789"></a>nc -l -u 4789</h2><h2 id="文件上传下载，下载有更方便的：python-m-SimpleHTTPServer-8080-（如果要上传呢，如果没有python呢）nc-也可以的：nc-l-p-8210-gt-demo-txt-（server上），client端上传：nc-dest-ip-8210-lt-demo-txt"><a href="#文件上传下载，下载有更方便的：python-m-SimpleHTTPServer-8080-（如果要上传呢，如果没有python呢）nc-也可以的：nc-l-p-8210-gt-demo-txt-（server上），client端上传：nc-dest-ip-8210-lt-demo-txt" class="headerlink" title="文件上传下载，下载有更方便的：python -m SimpleHTTPServer 8080 （如果要上传呢，如果没有python呢）nc 也可以的：nc -l -p 8210 &gt; demo.txt  （server上），client端上传：nc dest_ip 8210 &lt; demo.txt"></a>文件上传下载，下载有更方便的：python -m SimpleHTTPServer 8080 （如果要上传呢，如果没有python呢）<br>nc 也可以的：nc -l -p 8210 &gt; demo.txt  （server上），client端上传：nc dest_ip 8210 &lt; demo.txt</h2><p>打洞，我的ssh配置里面无数的nc转发，用ssl加密，然后nc代理</p>
<p>#程序员的螺丝刀# netstat<br>netstat -o 查看keepalive、重传<br>netstat -t 查看收包（自身）慢，还是发包走后对方慢<br>强大的丢包统计，保命的命令：netstat -s |egrep -i “drop|route|overflow|filter|retran|fails|listen”<br>tcp队列是否溢出：netstat -s | egrep “listen|LISTEN”<br>通过netstat -s来观察IPReversePathFilter 是否导致了网络不通<br>tc你说没听过，好吧，ping、netcat总归是耳熟能详了吧，你会用吗？</p>
<p>给大家一些具体的数字<br>用ab压Nginx的index.html页面数据对比（软中断在0核上, 软中断队列都设为1, nginx version: nginx&#x2F;1.21.0， 测试中所有场景Nginx 把CPU全部吃满）<br>用了两台Intel服务器，同一台E5-2682 开关NUMA对比（对比NUMA的差异），另外一台是 intel 8163，看看芯片之间能力差异<br>结论：<br>1）单物理核TPS能到82000,8163 比2682 提升了60%<br>2）开NUMA有5%的提升<br>3）超线程能提升 50% 左右的性能（开NUMA后提升了30%）<br>4）但就内存延时来比8163的内存延时其实较2682改进不大，内存时延发展一直追不上 CPU 的速度（图中蓝色线是2682、橙色是8163，灰色是8269）</p>
<p>以后别动不动就吊打Nginx，我只测试到一个Server在一堆限定的场景下比Nginx好了5%。</p>
<table>
<thead>
<tr>
<th></th>
<th>E5-2682 NUMA on</th>
<th>E5-2682  off</th>
<th>8163 off (2 ab 压)</th>
</tr>
</thead>
<tbody><tr>
<td>1号单核</td>
<td>49991 us:37% 0.96 IPC</td>
<td>45722 us:35% 0.90 IPC</td>
<td>82720 us:36% 1.27 IPC</td>
</tr>
<tr>
<td>HT(1&#x2F;33)</td>
<td>65469 us:31% 0.65 IPC</td>
<td>62734 us:37% 0.65 IPC</td>
<td>120100 us:38% 0.92 IPC</td>
</tr>
<tr>
<td>0号单核</td>
<td>29881 us:27% si:29% 0.90 IPC</td>
<td>28551 us:28% si:27% 0.88 IPC</td>
<td>65615 us:32% si:17% 1.20IPC</td>
</tr>
</tbody></table>
<p>#程序员的螺丝刀# tc（traffic control）<br>模拟丢包率、设置时延等等简直太香了。我不知道不会用的程序员是怎么搞的<br>延时设置：<br>give packets from eth0 a delay of 2ms<br>bash$ tc qdisc add dev eth0 root netem delay 2ms</p>
<p>change the delay to 300ms<br>bash$ tc qdisc change dev eth0 root netem delay 3ms</p>
<p>display eth0 delay setting<br>bash$ tc qdisc show dev eth0</p>
<p>stop the delay<br>bash$ tc qdisc del dev eth0 root</p>
<p>设置1%丢包率<br>tc qdisc add dev eth0 root netem loss 1%</p>
<p>高级版，指定ip和端口延时</p>
<p>智商过滤器：0）等额本金比等额本息更合算；1）相互宝好不好；2）如何看待中医中药，中成药、中药注射剂好不好？</p>
<p>长期有耐心放到程序员身上也一样管用，进到好的公司、碰到好的领路人、赶上各种案例都是小概率事件，但是保证自己抓住机会的能力。</p>
<p>比如身边有高人，就好好多学习，哪怕是干点累活脏活（你要认为被PUA就无救了）；</p>
<p>看到好的文章就把整个博客都翻翻；</p>
<p>碰到奇怪的问题要像平头哥一样死咬不放，多问几个为什么，搞清楚所有背后的未解之谜（每一个未解之谜都是你的一个盲点）；</p>
<p>多学点实用的，少在公司搞些花架子，本事才是自己的（有些技术文章一看就是水文，各种框架图、结构图）；</p>
<p>少把自己绑死在特定的技术上，尤其是公司自己发明的特殊轮子上。</p>
<p>最后时间才是最重要的，刚毕业急不来，长期有耐心。你看到的那些只是特别优秀–机会好、平台好、智商高……等中间的一例，大多都是跟你一样的普通人，不要过于焦虑</p>
<h2 id="鸡汤"><a href="#鸡汤" class="headerlink" title="鸡汤"></a>鸡汤</h2><p><a href="https://www.zhihu.com/question/39430220/answer/81648584" target="_blank" rel="noopener">Do More， Do Better， Do exercise</a>（<strong>口号和实践</strong>）</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/dd8b77138555d5a23563f5691a60e2dd.png" alt="image.png"></p>
<p>有个 Nginx 间歇性卡死的分析案例我追着看了三年，作者三年后也进步一更新的最根本的原因和优美的fix方法，简直太过瘾了。三年前就找到原因了，以及很多很多疑问点，剩下两三个小疑问，三年后终于也填补完美了。可惜不能分享。比如卡死的那个阶段所有 rt 都不对了容易导致分析跑偏</p>
<p>想搞个案例分析集，要求案例典型普适性强，代表基础组件基本原理等知识。分析手段尽量通用，重现容易的更好，分析过程一定要逻辑合理每个疑问都能回答清晰。有没有想要贡献案例的同学？这种案例搞清楚一个基本能横扫一个领域，比如上一条说的Nginx案例就让我这个从没用过Nginx的人学会了 惊群、epoll条件触发等之类的知识点 #拍案惊奇# 案例首先会去掉敏感信息，然后在分享过的同学之间内部共享，然后再开放。如果你们在网上看过已经发布过的案例更好，我先去学习下</p>
<p>趁着热点写下iptables+ipset的组合拳<br>如果有1万个白名单IP&#x2F;CIDR, 往iptables里写1万条规则不现实也严重影响性能，这个时候可以把1万个ip、CIDR放到一个ipset里面，然后再在iptables里添加一条规则就可以了。动态增删白名单只需要动态修改ipset就可以了，iptables规则不需要修改</p>
<p>案例：<br>#timeout 259200是集合内新增的IP有三天的寿命<br>ipset create myset hash:net timeout 259200  &#x2F;&#x2F;myset 还是空的</p>
<p>ipset add myset 100.1.2.0&#x2F;24 &#x2F;&#x2F;从set中增加ip段，也可以是一个ip，可以反复添加不同ip</p>
<p>&#x2F;&#x2F;iptables 添加规则，对myset里面的所有ip访问端口1234 放行<br>iptables -N white_rule<br>iptables -A white_rule -m set –match-set myset src -p tcp –dport 1234 -j ACCEPT </p>
<p>限制：要求对所有ip规则一样才适用</p>
<p>#程序员的螺丝刀# wget</p>
<p>wget –limit-rate&#x3D;2.5k 限制下载速度，进行测试, 挺有用的，比如你想模拟网络慢的场景下会不会出现什么问题；让 数据堆在接收窗口、发送窗口里面也很好玩；这个时候抓包看看输出的时候在干啥就更有意思了</p>
<p>用 Wget 的递归方式下载整个网站：wget –random-wait -r -p -e robots&#x3D;off -U Mozilla <a href="http://www.example.com/" target="_blank" rel="noopener">www.example.com</a></p>
<p><strong>7.0.0.0&#x2F;8，11.0.0.0&#x2F;8，21.0.0.0&#x2F;8，22.0.0.0&#x2F;8，30.0.0.0&#x2F;8</strong> 这些虽然不像192.168一样是私网地址，但是常被大家用来做内部地址，这是因为公网上只有美国国防部使用，所以不会和公网上冲突</p>
<p>全链路性能分析套路：<br>1） 先看监控，有各种鹰眼、狼眼最好；<br>2） 没有的话就要徒手上了，先要权限，只要每个节点有权限就好搞了，按着 <a href="https://www.weibo.com/1667773473/Lsb7CkVed" target="_blank" rel="noopener">https://www.weibo.com/1667773473/Lsb7CkVed</a> 这里的方法徒手撸，从客户端一直撸到最后面的数据库；<br>3）如果没有权限是最悲惨的，一般外包都没权限，但是又要干活，比如我。那么就只能用 ping 到处围着蹭蹭，不进去，谁让你是外包没有权限的，参考这三个案例 <a href="https://www.weibo.com/1667773473/LAjwUldTr" target="_blank" rel="noopener">https://www.weibo.com/1667773473/LAjwUldTr</a> 和<a href="https://www.weibo.com/1667773473/LzYwyaIB4" target="_blank" rel="noopener">https://www.weibo.com/1667773473/LzYwyaIB4</a> 下面这个是cloudflare的（高手的做法都差不多）<a href="https://www.weibo.com/1667773473/LAJ5mnp7b" target="_blank" rel="noopener">https://www.weibo.com/1667773473/LAJ5mnp7b</a><br>4）最后啥都没有肯定要有钱，出钱找我就行（案例典型可以不要钱）</p>
<p><a href="https://s.weibo.com/weibo?q=%23%E5%A5%97%E8%B7%AF+%E6%A1%88%E4%BE%8B%23" target="_blank" rel="noopener">#套路+案例#</a></p>
<p>感觉推上的技术氛围更浓厚啊，这个要超赞，图一这个问题居然讨论这么热闹。默认500个time_wait确实是四元组要唯一（6万个可用端口除以120等于500），如果是探活一个服务的话src_ip dest_ip dest_port固定了，只剩下src_port可变，反过来说探活120秒一般不会超过500次，所以应该是够的。探活一般是connect，也就是随机选择（其实不是随机，有算法的）src_port, 所以是500. 如果创建 socket 的时候做了bind也就是写死src_port了那就120秒只能有一个time_wait. 当然还可以是图三 reuse time_wait </p>
<p>贴个文章：<a href="https://plantegg.github.io/2020/11/30/%E4%B8%80%E5%8F%B0%E6%9C%BA%E5%99%A8%E4%B8%8A%E6%9C%80%E5%A4%9A%E8%83%BD%E5%88%9B%E5%BB%BA%E5%A4%9A%E5%B0%91%E4%B8%AATCP%E8%BF%9E%E6%8E%A5/">https://plantegg.github.io/2020/11/30/一台机器上最多能创建多少个TCP连接/</a></p>
<p><a href="https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf" target="_blank" rel="noopener">https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf</a> 这篇2014年的论文给了一个很牛逼的结论，通过 morsel-driver 和 numa-aware 对TPC-H性能有数量级的提升，按理这个数据很牛逼了，但是我好奇为啥没有大规模上生产呢？</p>
<p>但是文章中对 numa-aware的理解还是很赞的，一般搞数据库的理解这些算是跨行业，有点难度很正常。论文里都给了非常专业的数据和理解。</p>
<p>其实论文不应该把morsel-driver和numa-aware混到一起，最后不知道是谁的功劳</p>
<p><a href="https://plantegg.github.io/2017/01/01/top_linux_commands/">https://plantegg.github.io/2017/01/01/top_linux_commands/</a></p>
<p>#拍案惊奇# 我最喜欢的Nginx卡顿案例终于整理完毕</p>
<p>简述就是：总有耗时任务造成worker进程卡死，就是某个任务总是总用worker，导致worker没法响应新连接（用户感知连不上Nginx）、没法更新woker计时（导致nginx日志时间失真，排查麻烦）、普通正在处理的请求也变成了慢请求。</p>
<p>涉及到：TCP连接、Nginx进程模型、惊群、边缘触发和条件触发、网络buffer等，真是一个让我爱不释手的case，这几天我都不想上班只想好好把玩</p>
<p>类似的案例可以参考：<br><a href="http://t.cn/RB8hxqU" target="_blank" rel="noopener">Why does one NGINX worker take all the load?</a><br><a href="http://t.cn/RUuS8TE" target="_blank" rel="noopener">The story of one latency spike</a> </p>
<p>但比我这个案例有趣性差得太远了</p>
<p>作为极客时间的企业用户（所有课程随便看，但是我看的不算多）来给你们推荐几个课程。MySQL45讲，趣谈网络一定是值得推荐的；网络案例也可以看看(不想花钱就看我的博客网络部分，嘿嘿其实比这课要好)；芯片那个有兴趣可以看看，算是比较偏门了。</p>
<p>其实企业用户超级不值当，包年性质，过期了就不能看了，还不如买下来的，不推荐购买企业用户。</p>
<p>要想省钱3&#x2F;5个人组团，每个人买3-5门课，然后互相学习</p>
<p>还行吧，2个小时把一个系统优化性能翻了一倍，主要还是CPU、网络都比较了解，容易出成绩</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220813173113345.png" alt="image-20220813173113345" style="zoom:50%;">

<p>优化前 800MB<br>1 开numa<br>    1.2GB<br>2 使用irqbalance，自动将irq 绑定到对应的位置的numa 核心<br>    1.6GB<br>3 软件绑核，各绑定到2 个相邻的核心。将CPU 和numa 分开。<br>    2.0GB<br>4 更改网卡中断队列数。<br>   2.6GB</p>
<p>内核 TCP 协议栈 bug 导致应用卡死的 case：<a href="http://t.cn/A6SYWIsh" target="_blank" rel="noopener">http://t.cn/A6SYWIsh</a>  还得会看包，就不用走这么多弯路了，教训总结得不够[微笑]，kernel上的修复patch：github.com&#x2F;torvalds&#x2F;linux&#x2F;commit&#x2F;b617158dc096709d8600c53b6052144d12b89fab （5月引入的bug：<a href="http://t.cn/A6SYHu9W" target="_blank" rel="noopener">http://t.cn/A6SYHu9W</a> 7月修复）<br>为啥 Databricks 不直接follow kernel而是follow ubuntu，吃二手消息呢？<br>redhat对这个 bug的描述：<a href="http://t.cn/A6SYQ29h" target="_blank" rel="noopener">http://t.cn/A6SYQ29h</a><br>图三中 红色代码为了修 CVE-2019-11478 添加的，引入了这个卡死 的bug，绿色部分增加了更严格的条件又修复了卡死的 bug</p>
<p>双网卡下的 kubernetes 集群：1）控制面走外网网卡；2）flannel等overlay走内网网卡</p>
<p>实现：</p>
<p>控制面指定外网网卡ip：kubeadm init –control-plane-endpoint 192.168.0.21:6443 </p>
<p>Flannel yaml配置中指定网卡：–iface&#x3D;enp33s0f0 </p>
<p>默认路由选择外网网卡</p>
<p>比较不忍看到大多新同事刚一进来被丢到了错误的位置上，新同事还不敢拒绝。第一种是要面对新公司一大坨内部产品和术语，没有人带，这种太不人道；第二种因为错配而要面对新技术领域，大体还是能 Google 到，不过对他们压力太大，这种我一般会带一次，再多也没精力，毕竟跨了团队；几个月下来新同事肯定觉得被 PUA了</p>
<p>这次看了一篇排查分析两条TCP流互窜的问题。我两年前碰到过，一抓包就把问题KO了，但这只是老法师的经验丰富而已，再看新同学抓包、会话分析、IPVS debug等一套组合拳下来也把问题解决了，我更喜欢这种清纯、处男手法。油腻老法师反而不值得学习， #无招胜有招 </p>
<p>最近看了几个 golang 的排查分析，一个是DNS，一个是延时增大。golang 挺能整事的，自己新搞了一堆DNS 解析逻辑，结果带来一堆问题。延时增大分析那个我看老法师把golang 的pprof 使用的真流畅，废了一大波力气终于找到了延时增加是某个同步写日志等待时间太久了，难道 golang 就没有 arthas 之类的工具吗？</p>
<p>CRUD Boy 最喜欢说我的日志没问题！但是你的线程不会被调度到你怎么记时间？就连 Nginx 记录的access time都不可靠，不出问题的时候一个个贼精确，一出问题全完蛋。Nginx 经常出现worker卡了导致这个worker不去更新时间最终输出的 RT 比实际小很多</p>
<p>也有你抓了包确实慢，但是 CRUD 头铁：可能是网卡坏了、机器故障，反正我的日志没问题。给了两个选择：要不切换一台机器；要不调用方重启一下。都被我否决了，我就是不相信你的日志，最后发现是业务线程排队了，日志还没机会开始记。如果真切换机器(100%问题恢复)、业务重启(小概率问题恢复) CRUD 下次还头铁，这次被我坚决按住并全网广而告之你的日志不可靠！这是解决这个问题的最大价值</p>
<p>三年前碰到过一次MySQL线程池某个group卡顿的问题，当时调大 thread_pool_oversubscribe 就恢复了，最近又碰到了，同样是调大问题就解决了。但这次经过分析后我认为不是 thread_pool_oversubscribe 太小，而是 group 里的线程有泄漏 </p>
<p>几乎所有互联网公司对内想把员工当小偷一样的安全管控都是幌子，除了老板自我感觉良好、给员工带来麻烦外，墙都能翻还弄不了你这点安全把戏</p>
<p>比如菊花司，早年物理隔离，工作本不能连外网，要连外网请换个本本，或者买一块硬盘，物理切换</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2022/10/10/Linux BUG内核导致的 TCP连接卡死/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/10/10/Linux BUG内核导致的 TCP连接卡死/" itemprop="url">一个Linux 内核 bug 导致的 TCP连接卡死</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2022-10-10T17:30:03+08:00">
                2022-10-10
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-BUG内核导致的-TCP连接卡死"><a href="#Linux-BUG内核导致的-TCP连接卡死" class="headerlink" title="Linux BUG内核导致的 TCP连接卡死"></a>Linux BUG内核导致的 TCP连接卡死</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>客户端从 server 拖数据，偶尔会出现 TCP 连接卡死，卡死的现象就是 server 不遵循 TCP 重传逻辑，客户端不停地发 dup ack，但是服务端不响应这些dup ack仍然发一些新的包(从server抓包可以看到)，一会后服务端不再发任何新包，也不响应dup ack 来传丢掉的包，进入永久静默，最终连接闲置过久被reset，客户端抛连接异常.</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20230515162204533.png" alt="image-20230515162204533"></p>
<p>Client MySQL JDBC 协议拉取 Server 3306端口 数据，频繁出现卡死与超时，Client端Java 报错：Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server.  </p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>服务端抓包可以看到：这个 TCP 流， 17:40:40 后 3306 端口不做任何响应，进入卡死状态，在卡死前有一些重传</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1662602586968-b20b6006-884e-4c33-9938-0277c012579e.png" alt="image.png"></p>
<p>同时通过观察这些连接的实时状态：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220922092105581.png" alt="image-20220922092105581"></p>
<p>rto一直在增加，但是这个时候 server 上抓不到任何包，说明内核在做 rto 重传，但是重传包没有到达本机网卡，应该还是被内核其它环节吃掉了。</p>
<p>再观察 netstat -s 状态，重传的时候，TCPWqueueTooBig 值会增加，也就是重传-&gt;TCPWqueueTooBig-&gt;重传包未发出-&gt;循环-&gt;相当于 TCP 连接卡死、静默状态</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220922092321039.png" alt="image-20220922092321039"></p>
<p>顺着 TCPWqueueTooBig 查看<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="noopener">内核代码提交记录</a>， 红色部分是修 CVE-2019-11478 添加的代码，引入了这个 卡死 的bug，绿色部分增加了更严格的条件又修复了卡死的 bug</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1662698955965-276e9936-6ca4-4269-9fbd-ae05176bf1a6.png" alt="image.png"></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>2019-05 为了解决 <a href="https://www.secrss.com/articles/11570" target="_blank" rel="noopener">CVE-2019-11478</a> 增加了这个commit：<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="noopener">f070ef2ac66716357066b683fb0baf55f8191a2e</a>，这部分代码在发送 buffer 满的时候忽略要发的包，进入静默有包也不发</p>
<p>为了解决这个问题 2019-07-20 fix 版本：<a href="https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab" target="_blank" rel="noopener">https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab</a></p>
<p>4.19.57 是 2019-07-03 发布，完美引入了这个 bug</p>
<p>快速确认：netstat -s | grep TCPWqueueTooBig  如果不为0 就出现过 TCP 卡死，同时还可以看到 tb(待发送队列) 大于 rb（发送队列 buffer）</p>
<h2 id="重现条件"><a href="#重现条件" class="headerlink" title="重现条件"></a>重现条件</h2><p>必要条件：合并了 commit：<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="noopener">f070ef2ac66716357066b683fb0baf55f8191a2e</a> 的内核版本</p>
<p>提高重现概率的其它非必要条件：</p>
<ol>
<li>数据量大—拖数据任务、大查询；</li>
<li>有丢包—链路偏长连接，丢包概率大；</li>
<li>多个任务 —一个失败整个任务失败，客户体感强烈</li>
<li>Server 设置了小buffer，出现概率更高</li>
</ol>
<p>在这四种情况下出现概率更高。用户单个小查询SQL 睬中这个bug后一般可能就是个连接异常，重试就过去了，所以可能没有抱怨。 得这四个条件一起用户的抱怨就会凸显出来。</p>
<h2 id="用-packetdrill-复现"><a href="#用-packetdrill-复现" class="headerlink" title="用 packetdrill 复现"></a><a href="https://github.com/google/packetdrill" target="_blank" rel="noopener">用 packetdrill 复现</a></h2><p>编译 packetdrill 报找不到lib包的错误的话，到Makefile 里去掉 -static , 默认用静态link方式，本地没有pthread静态包</p>
<p><a href="https://xargin.com/packetdrill-intro/" target="_blank" rel="noopener">https://xargin.com/packetdrill-intro/</a> packetdrill介绍</p>
<p>文章末尾一堆链接里好多人重现这个bug都用到了 packetdrill </p>
<h3 id="复现的关键两点"><a href="#复现的关键两点" class="headerlink" title="复现的关键两点"></a>复现的关键两点</h3><ol>
<li>让对端重传一个大包（包的长度超过一个mss，进而触发tcp_fragment）</li>
<li>sk_wmem_queued 远大于 sk_sndbuf，即使得tcp_fragment函数的条件成立，具体如下：</li>
</ol>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1662822142241-1ce17636-546d-4203-a77c-66c74cb2521e.png" alt="img"></p>
<h3 id="复现代码"><a href="#复现代码" class="headerlink" title="复现代码"></a>复现代码</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">`gtests/net/common/defaults.sh`</span><br><span class="line">0 `echo start`</span><br><span class="line"></span><br><span class="line">// Establish a connection.</span><br><span class="line">+0.1   socket(..., SOCK_STREAM, IPPROTO_TCP) = 3</span><br><span class="line">+0 setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</span><br><span class="line">+0 setsockopt(3, SOL_SOCKET, SO_SNDBUF, [4096], 4) = 0</span><br><span class="line">+0 setsockopt(3, SOL_SOCKET, SO_RCVBUF, [8192], 4) = 0</span><br><span class="line">+0  bind(3, ..., ...) = 0</span><br><span class="line">+0  listen(3, 1) = 0</span><br><span class="line"></span><br><span class="line">+0  &lt; S 0:0(0) win 32792 &lt;mss 1460,sackOK,nop,nop,nop,wscale 7&gt;</span><br><span class="line">+0  &gt; S. 0:0(0) ack 1 &lt;...&gt;</span><br><span class="line">+.1 &lt; . 1:1(0) ack 1 win 257</span><br><span class="line">+0  accept(3, ..., ...) = 4</span><br><span class="line"></span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 write(4, ..., 3000) = 3000</span><br><span class="line">+0 &lt; . 1:1(0) ack 3001 win 257</span><br><span class="line">// wait for retransmission</span><br><span class="line">+100 `echo done`</span><br></pre></td></tr></table></figure>

<p>复现结果有问题的内核版本上 tcpdump抓包看到卡死，用ss命令展示的信息，可以看到sk_wmem_queued为w22680，远大于tb8192</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">State      Recv-Q Send-Q                 Local Address:Port                                Peer Address:Port              </span><br><span class="line">ESTAB      0      15000                192.168.169.124:8080                                   192.0.2.1:50069              </span><br><span class="line">         skmem:(r0,rb16384,t0,tb8192,f1896,w22680,o0,bl0,d0) cubic wscale:7,0 rto:37760 backoff:7 rtt:87.643/51.642 mss:1460 rcvmss:536 advmss:1460 cwnd:1 ssthresh:9 bytes_acked:3000 segs_out:14 segs_in:3 data_segs_out:14 send 133.3Kbps lastsnd:63524 lastrcv:63524 lastack:63524 pacing_rate 3.5Mbps delivery_rate 796.4Mbps app_limited busy:63524ms unacked:11 lost:11 rcv_space:7300 minrtt:0.044</span><br></pre></td></tr></table></figure>

<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>升级内核到带有2019-07-20 fix 版本：<a href="https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab" target="_blank" rel="noopener">https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab</a></p>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.secrss.com/articles/11570" target="_blank" rel="noopener">https://www.secrss.com/articles/11570</a></p>
<p><a href="https://access.redhat.com/solutions/4302501" target="_blank" rel="noopener">https://access.redhat.com/solutions/4302501</a></p>
<p><a href="https://access.redhat.com/solutions/5162381" target="_blank" rel="noopener">https://access.redhat.com/solutions/5162381</a></p>
<p>databricks 的相同案例： <a href="https://www.databricks.com/blog/2019/09/16/adventures-in-the-tcp-stack-performance-regressions-vulnerability-fixes.html" target="_blank" rel="noopener">https://www.databricks.com/blog/2019/09/16/adventures-in-the-tcp-stack-performance-regressions-vulnerability-fixes.html</a></p>
<p>6月第一个人报了这个bug：<a href="https://lore.kernel.org/netdev/CALMXkpYVRxgeqarp4gnmX7GqYh1sWOAt6UaRFqYBOaaNFfZ5sw@mail.gmail.com/" target="_blank" rel="noopener">https://lore.kernel.org/netdev/CALMXkpYVRxgeqarp4gnmX7GqYh1sWOAt6UaRFqYBOaaNFfZ5sw@mail.gmail.com/</a></p>
<blockquote>
<p>Hi Eric, I now have a packetdrill test that started failing (see below). Admittedly, a bit weird test with the SO_SNDBUF forced so low. Nevertheless, previously this test would pass, now it stalls after the write() because tcp_fragment() returns -ENOMEM. Your commit-message mentions that this could trigger when one sets SO_SNDBUF low. But, here we have a complete stall of the connection and we never recover.<br>I don’t know if we care about this, but there it is :-)</p>
</blockquote>
<p><a href="https://patches.linaro.org/project/stable/patch/20210125183204.684104321@linuxfoundation.org/" target="_blank" rel="noopener">一个 zero windows 下卡死的内核bug</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/3/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="twitter @plantegg">
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">187</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">17</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">274</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
