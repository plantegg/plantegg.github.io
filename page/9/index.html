<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css">


  <meta name="keywords" content="Hexo, NexT">





  <link rel="alternate" href="/atom.xml" title="plantegg" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1">






<meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/9/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="plantegg">
<meta name="twitter:description" content="java mysql tcp performance network docker Linux">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://plantegg.github.io/page/9/">





  <title>plantegg - java tcp mysql performance network docker Linux</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  















  
  
    
  

  

  <div class="container sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">plantegg</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-categories"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br>
            
            站点地图
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/05/23/如何在工作中学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/23/如何在工作中学习/" itemprop="url">如何在工作中学习</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-23T12:30:03+08:00">
                2018-05-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/技巧/" itemprop="url" rel="index">
                    <span itemprop="name">技巧</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何在工作中学习"><a href="#如何在工作中学习" class="headerlink" title="如何在工作中学习"></a>如何在工作中学习</h1><p>大家平时都看过很多方法论的文章，看的时候很爽觉得非常有用，但是一两周后基本还是老样子了。其中有很大一部分原因那些方法对脑力有要求、或者方法论比较空缺少落地的步骤。 下文中描述的方式方法是不需要智商也能学会的，非常具体可以复制。</p>
<blockquote>
<p>先说一件值得思考的事情：高考的时候大家都是一样的教科书，同一个教室，同样的老师辅导，时间精力基本差不多，可是最后别人考的是清华北大或者一本，而你的实力只能考个三本，为什么？ 当然这里主要是智商的影响，那么其他因素呢？智商解决的问题能不能后天用其他方式来补位一下？</p>
</blockquote>
<p>思考10秒钟再往下看</p>
<h2 id="关键问题点"><a href="#关键问题点" class="headerlink" title="关键问题点"></a>关键问题点</h2><p>解决问题的能力就是从你储蓄的知识中提取到方案，差别就是知识储存能力和运用能力的差异</p>
<h3 id="为什么你的知识积累不了？"><a href="#为什么你的知识积累不了？" class="headerlink" title="为什么你的知识积累不了？"></a>为什么你的知识积累不了？</h3><p>有些知识看过就忘、忘了再看，实际碰到问题还是联系不上这个知识，这其实是知识的积累出了问题，没有深入理解好自然就不能灵活运用，也就谈不上解决不了问题。这跟大家一起看相同的高考教科书但是高考结果不一样。问题出在了理解上，每个人的理解能力不一样（智商），绝大多数人对知识的理解要靠不断地实践（做题）来巩固。</p>
<h3 id="同样实践效果不一样？"><a href="#同样实践效果不一样？" class="headerlink" title="同样实践效果不一样？"></a>同样实践效果不一样？</h3><p>同样工作一年碰到了10个问题（或者说做了10套高考模拟试卷），但是结果不一样，那是因为在实践过程中方法不够好。或者说你对你为什么做对了、为什么做错了没有去分析，存在一定的瞎蒙成分。</p>
<p>假如碰到一个问题，身边的同事解决了，而我解决不了。那么我就去想这个问题他是怎么解决的，他看到这个问题后的逻辑和思考是怎么样的，有哪些知识指导了他这么逻辑推理，这些知识哪些我也知道但是我没有想到这么去运用推理（说明我对这个知识理解的不到位导致灵活运用缺乏）；这些知识中又有哪些是我不知道的（知识缺乏，没什么好说的快去Google什么学习下–有场景案例和目的加持，学习理解起来更快）。</p>
<p>等你把这个问题基本按照你同事掌握的知识和逻辑推理想明白后，需要再去琢磨一下他的逻辑推理解题思路中有没有不对的，有没有啰嗦的地方，有没有更直接的方式（对知识更好地运用）。</p>
<p>我相信每个问题都这么去实践的话就不会再抱怨为什么自己做不到灵活运用、举一反三，同时知识也积累下来了，实战场景下积累到的知识是不容易忘记的。</p>
<p>这就是向身边的牛人学习，同时很快超过他的办法。这就是为什么高考前你做了10套模拟题还不如其他人做一套的效果好的原因</p>
<p><strong>知识+逻辑 基本等于你的能力</strong>，知识让你知道那个东西，逻辑让你把东西和问题联系起来。碰到问题如果你连相关知识都没有就谈不上解决问题，有时候碰到问题被别人解决后你才发现有相应的知识贮备，但还不能转化成能力，那就是你只是知道那个知识点，但理解不到位、不深，也就无法实战了。</p>
<p><strong>这里的问题你可以理解成方案、架构、设计等</strong></p>
<p>逻辑可以理解为：元认知能力(思考方式、思路，像教练一样反复在大脑里追问为什么)</p>
<p>我们说能力强的人比如在读书的时候，他们读到的不仅仅是文字以及文字所阐述的道理，他们更多注意到j的是作者的“思考方式” ，作者的“思考方式”与自己的“思考方式”之间的不同，以及，若是作者的“思考方式”有可取之处的话，自己的“思考方式”要做出哪些调整？于是，一本概率论读完，大多数人就是考个试也不一定能及格，而另外的极少数人却成了科学家——因为他们改良了自己的思考方式，从此可以“像一个科学家一样思考”……</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/bg2023113016.webp" alt="img"></p>
<h3 id="系统化的知识哪里来？"><a href="#系统化的知识哪里来？" class="headerlink" title="系统化的知识哪里来？"></a>系统化的知识哪里来？</h3><p>知识之间是可以联系起来的并且像一颗大树一样自我生长，但是当你都没理解透彻，自然没法产生联系，也就不能够自我生长了。当我们讲到入门了某块的知识的时候一般是指的对关键问题点理解清晰，并且能够自我生长，也就是滚雪球一样可以滚起来了。</p>
<p>但是我们最容易陷入的就是掌握的深度、系统化（工作中碎片时间过多，学校里缺少实践）不够，所以一个知识点每次碰到花半个小时学习下来觉得掌握了，但是3个月后就又没印象了。总是感觉自己在懵懵懂懂中，或者一个领域学起来总是不得要领，根本的原因还是在于：宏观整体大图了解不够（缺乏体系，每次都是盲人摸象）；关键知识点深度不够，理解不透彻，这些关键点就是这个领域的骨架、支点、抓手。缺了抓手自然不能生长，缺了宏观大图容易误入歧途。</p>
<p>我们有时候发现自己在某个领域学起来特别快，但是换个领域就总是不得要领，问题出在了上面，即使花再多时间也是徒然。这也就是为什么学霸看两个小时的课本比你看两天效果还好，感受下来还觉得别人好聪明，是不是智商比我高啊。</p>
<p><strong>所以新进入一个领域的时候要去找他的大图和抓手。</strong></p>
<p>好的书籍或者培训总是能很轻易地把这个大图交给你，再顺便给你几个抓手，你就基本入门了，这就是培训的魅力，这种情况肯定比自学效率高多了。但是目前绝大部分的书籍和培训都做不到这点</p>
<h3 id="好的逻辑又怎么来？"><a href="#好的逻辑又怎么来？" class="headerlink" title="好的逻辑又怎么来？"></a>好的逻辑又怎么来？</h3><p><strong>实践、复盘</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/webp-5540564.jpg" alt="img"></p>
<h2 id="讲个前同事的故事"><a href="#讲个前同事的故事" class="headerlink" title="讲个前同事的故事"></a>讲个前同事的故事</h2><p>有一个前同事是5Q过来的，负责技术（所有解决不了的问题都找他），这位同学从chinaren出道，跟着王兴一块创业5Q，5Q在学校靠鸡腿打下大片市场，最后被陈一舟的校内收购（据说被收购后5Q的好多技术都走了，最后王兴硬是呆在校内网把合约上的所有钱都拿到了）。这位同学让我最佩服的解决问题的能力，好多问题其实他也不一定就擅长，但是他就是有本事通过Help、Google不停地验证尝试就把一个不熟悉的问题给解决了，这是我最羡慕的能力，在后面的职业生涯中一直不停地往这个方面尝试。</p>
<h3 id="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"><a href="#应用刚启动连接到数据库的时候比较慢，但又不是慢查询" class="headerlink" title="应用刚启动连接到数据库的时候比较慢，但又不是慢查询"></a>应用刚启动连接到数据库的时候比较慢，但又不是慢查询</h3><ol>
<li>这位同学的解决办法是通过tcpdump来分析网络通讯包，看具体卡在哪里把这个问题硬生生地给找到了。</li>
<li>如果是专业的DBA可能会通过show processlist 看具体连接在做什么，比如看到这些连接状态是 <strong>authentication</strong> 状态，然后再通过Google或者对这个状态的理解知道创建连接的时候MySQL需要反查IP、域名这里比较耗时，通过配置参数 <strong>skip-name-resolve</strong> 跳过去就好了。</li>
<li>如果是MySQL的老司机，一上来就知道 <strong>skip-name-resolve</strong> 这个参数要改改默认值。</li>
</ol>
<p>在我眼里这三种方式都解决了问题，最后一种最快但是纯靠积累和经验，换个问题也许就不灵了；第一种方式是最牛逼和通用的，只需要最少的业务知识+方法论就可以更普遍地解决各种问题。</p>
<p>我当时跟着他从sudo、ls等linux命令开始学起。当然我不会轻易去打搅他问他，每次碰到问题我尽量让他在我的电脑上来操作，解决后我<strong>再自己复盘，通过history调出他的所有操作记录，看他在我的电脑上用Google搜啥了，然后一个个去学习分析他每个动作，去想他为什么搜这个关键字，复盘完还有不懂的再到他面前跟他面对面的讨论他为什么要这么做，指导他这么做的知识和逻辑又是什么</strong>（这个动作没有任何难度吧，你照着做就是了，实际我发现绝对不会有10%的同学会去分析history的，而我则是通过history 搞到了各种黑科技 :) ）。</p>
<h2 id="场景式学习、体感的来源、面对问题学习"><a href="#场景式学习、体感的来源、面对问题学习" class="headerlink" title="场景式学习、体感的来源、面对问题学习"></a>场景式学习、体感的来源、面对问题学习</h2><p>前面提到的对知识的深入理解这有点空，如何才能做到深入理解？</p>
<p><img src="https://cdn.beekka.com/blogimg/asset/202306/bg2023062902.webp" alt="img"></p>
<p>书本知识只是基础(大部分人没能力用理论直接解决问题)，实践应用可以学到更多，如果实践发生错误(踩坑)，那就是最好的学习机会</p>
<h3 id="举个学习TCP三次握手例子"><a href="#举个学习TCP三次握手例子" class="headerlink" title="举个学习TCP三次握手例子"></a>举个学习TCP三次握手例子</h3><p>经历稍微丰富点的工程师都觉得TCP三次握手看过很多次、很多篇文章了，但是文章写得再好似乎当时理解了，但是总是过几个月就忘了或者一看就懂，过一阵子被人一问就模模糊糊了，或者多问两个为什么就答不上了，自己都觉得自己的回答是在猜或者不确定。</p>
<p>为什么会这样呢？而学其它知识就好通畅多了，我觉得这里最主要的是我们对TCP缺乏<strong>体感</strong>，比如没有几个工程师去看过TCP握手的代码，也没法想象真正的TCP握手是如何在电脑里运作的（打电话能给你一些类似的体感，但是细节覆盖面不够）。</p>
<p>如果这个时候你一边学习的时候一边再用wireshark抓包看看三次握手具体在干什么、交换了什么信息，比抽象的描述具象实在多了，你能看到握手的一来一回，并且看到一来一回带了哪些内容，这些内容又是用来做什么、为什么要带，这个时候你再去看别人讲解的理论顿时会觉得好理解多了，以后也很难忘记。</p>
<p>但是这里很多人执行能力不强，想去抓包，但是觉得要下载安装wireshark，要学习wireshark就放弃了。<strong>只看不动手当然是最舒适的，但是这个最舒适给了你在学习的假象，没有结果</strong>。</p>
<p>这是不是跟你要解决一个难题非常像，这个难题需要你去做很多事，比如下载源代码（翻不了墙，放弃）；比如要编译（还要去学习那些编译参数，放弃）；比如要搭建环境（太琐屑，放弃）。你看这中间九九八十一难你放弃了一难都取不了真经。这也是为什么同样学习、同样的问题，他能学会，他能解决，你不可以。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/167211888bc4f2a368df3d16c68e6d51.png" alt="167211888bc4f2a368df3d16c68e6d51.png"></p>
<h2 id="空洞的口号"><a href="#空洞的口号" class="headerlink" title="空洞的口号"></a>空洞的口号</h2><p>很多文章都会教大家：举一反三、灵活运用、活学活用、多做多练。但是只有这些口号是没法落地的，落地的基本原则就是前面提到的，却总是被忽视了。</p>
<p>还有些人做事情第六感很好，他自己也不一定能阐述清楚合理的逻辑，就是感觉对了，让他给你讲道理，你还真学不来。</p>
<p>我这里主要是在描述<strong>能复制的一些具体做法</strong>，少喊些放哪里都正确的口号。不要那些抽象的套路，主要是不一定适合你和能复制。</p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举一反三，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>肯定知识效率最牛逼，但是拥有这种技能的人毕竟非常少（天生的高智商吧）。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快的掌握一个新知识，非常气人。剩下的绝大部分只能拼时间+方法+总结等也能掌握一些知识</p>
<p>非常遗憾我就是工程效率型，只能羡慕那些知识效率型的学霸。但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，两者之间当然没有明显的界限，知识积累多了逻辑训练好了在别人看来你的智商就高了</p>
<h2 id="知识分两种"><a href="#知识分两种" class="headerlink" title="知识分两种"></a>知识分两种</h2><p>一种是通用知识（不是说对所有人通用，而是说在一个专业领域去到哪个公司都能通用）；另外一种是跟业务公司绑定的特定知识</p>
<p>通用知识没有任何疑问碰到后要非常饥渴地扑上去掌握他们（受益终生，这还有什么疑问吗？）。对于特定知识就要看你对业务需要掌握的深度了，肯定也是需要掌握一些的，特定知识掌握好的一般在公司里混的也会比较好</p>
<p>这篇文章我最喜欢的一条评论是：</p>
<blockquote>
<p>看完深有感触，尤其是后面的知识效率和工程效率型的区别。以前总是很中二的觉得自己看一遍就理解记住了，结果一次次失败又怀疑自己的智商是不是有问题，其实就是把自己当作知识效率型来用了。一个不太恰当的形容就是，有颗公主心却没公主命！</p>
</blockquote>
<p>我喜欢这条评论是很真实地说出来我们平时总是高估自己然后浪费了精力</p>
<h2 id="案例学习的例子"><a href="#案例学习的例子" class="headerlink" title="案例学习的例子"></a>案例学习的例子</h2><p>通过一个小问题，花上一周看源代码、做各种实验反复验证，把这里涉及到的知识全部拿下，同时把业务代码、内核配置、出问题的表征、监控指标等等都连贯起来，<strong>要么不做要么一杆到底</strong>： <a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></p>
<h2 id="进一步阅读"><a href="#进一步阅读" class="headerlink" title="进一步阅读"></a>进一步阅读</h2><p>如果喜欢本文的话，你也会喜欢我亲身经历的：<a href="https://plantegg.github.io/2022/01/01/%E4%B8%89%E4%B8%AA%E6%95%85%E4%BA%8B/">《三个故事》</a></p>
<h2 id="练习"><a href="#练习" class="headerlink" title="练习"></a>练习</h2><p>大家都知道贷款有等额本息、等额本金两种还款方式，网上到处流传等额本金划得来因为利息少，等额本息提前还贷划不来，尤其是已经还了10年了提前还贷就划不来！</p>
<p>任务：你可以先去搜索什么是等额本息、等额本金这两概念入手，然后去计算第一个月、第二个月的利息是怎么计算的(从具体到抽象)，然后再思考：</p>
<ol>
<li>无论哪种还贷方式利率是不是一样——肯定一样的，贷款利率和还贷方式无关</li>
<li>等额本息你多还了利息是因为什么？</li>
<li>提前还贷跟时间有没有关系？(换个说法：你第一个月还的利息有没有替10年后还？)</li>
</ol>
<p>结果：你一次把概念搞清楚，然后通过一个很具体的第一个月、第二个月(不行你就多迭代几个月)来强化你对当月利息是怎么产生的理论：当月所欠本金*利率 。利率固定不变就不存在划不划得来，你看没有人跟你说借100万划得来、借200万就划不来这个概念吧，只会跟你说年华5%的房贷划不来有点高，年化3%的房贷很划得来</p>
<p>进阶：你把这个概念完全理解后再去看分期付款、保险划不划得来就很容易了</p>
<p>你看所有核心知识就是每个月的利息怎么计算的这一个小学知识的概念，但是居然搞出这么多包装概念把大家搞糊涂了</p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a href="https://twitter.com/plantegg" target="_blank" rel="noopener">@plantegg</a></p>
<p>知识星球：<a href="https://t.zsxq.com/0cSFEUh2J" target="_blank" rel="noopener">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/05/07/就是要你懂TCP--通过案例来学习MSS、MTU/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/05/07/就是要你懂TCP--通过案例来学习MSS、MTU/" itemprop="url">通过案例来理解MSS、MTU等相关TCP概念</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-05-07T12:30:03+08:00">
                2018-05-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="就是要你懂TCP–通过案例来学习MSS、MTU"><a href="#就是要你懂TCP–通过案例来学习MSS、MTU" class="headerlink" title="就是要你懂TCP–通过案例来学习MSS、MTU"></a>就是要你懂TCP–通过案例来学习MSS、MTU</h1><h2 id="问题的描述"><a href="#问题的描述" class="headerlink" title="问题的描述"></a>问题的描述</h2><ul>
<li>最近要通过Docker的方式把产品部署到客户机房， 过程中需要部署一个hbase集群，hbase总是部署失败（在我们自己的环境没有问题）</li>
<li>发现hbase卡在同步文件，人工登上hbase 所在的容器中看到在hbase节点之间scp同步一些文件的时候，同样总是失败（稳定重现） </li>
<li>手工尝试scp那些文件，发现总是在传送某个文件的时候scp卡死了</li>
<li>尝试单独scp这个文件依然卡死</li>
<li>在这个容器上scp其它文件没问题(小文件)</li>
<li>换一个容器scp这个文件没问题</li>
</ul>
<h2 id="分析过程"><a href="#分析过程" class="headerlink" title="分析过程"></a>分析过程</h2><blockquote>
<p>实在很难理解为什么单单这个文件在这个容器上scp就卡死了，既然scp网络传输卡死，那么就同时在两个容器上tcpdump抓包，想看看为什么传不动了</p>
</blockquote>
<h4 id="在客户端抓包如下：（33端口是服务端的sshd端口，10-16-11-108是客户端ip）"><a href="#在客户端抓包如下：（33端口是服务端的sshd端口，10-16-11-108是客户端ip）" class="headerlink" title="在客户端抓包如下：（33端口是服务端的sshd端口，10.16.11.108是客户端ip）"></a>在客户端抓包如下：（33端口是服务端的sshd端口，10.16.11.108是客户端ip）</h4><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240506090810608.png" alt="image-20240506090810608"></p>
<h4 id="从抓包中可以得到这样一些结论："><a href="#从抓包中可以得到这样一些结论：" class="headerlink" title="从抓包中可以得到这样一些结论："></a>从抓包中可以得到这样一些结论：</h4><ul>
<li>从抓包中可以明显知道scp之所以卡死是因为丢包了，客户端一直在重传，图中绿框</li>
<li>图中篮框显示时间间隔，时间都是花在在丢包重传等待的过程</li>
<li>奇怪的问题是图中橙色框中看到的，网络这时候是联通的，客户端跟服务端在这个会话中依然有些包能顺利到达（Keep-Alive包）</li>
<li>同时注意到重传的包长是1442，包比较大了，看了一下tcp建立连接的时候MSS是1500，应该没有问题</li>
<li>查看了scp的两个容器的网卡mtu都是1500，正常</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">基本上看到这里，能想到是因为丢包导致的scp卡死，因为两个容器mtu都正常，包也小于mss，那只能是网络路由上某个环节mtu太小导致这个1442的包太大过不去，所以一直重传，看到的现状就是scp卡死了</span><br></pre></td></tr></table></figure>

<h2 id="接下来分析网络传输链路"><a href="#接下来分析网络传输链路" class="headerlink" title="接下来分析网络传输链路"></a>接下来分析网络传输链路</h2><h4 id="scp传输的时候实际路由大概是这样的"><a href="#scp传输的时候实际路由大概是这样的" class="headerlink" title="scp传输的时候实际路由大概是这样的"></a>scp传输的时候实际路由大概是这样的</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">容器A---&gt; 宿主机1 ---&gt; ……中间的路由设备 …… ---&gt; 宿主机2 ---&gt; 容器B</span><br></pre></td></tr></table></figure>

<ul>
<li>前面提过其它容器scp同一个文件到容器B没问题，所以我认为中间的路由设备没问题，问题出在两台宿主机上</li>
<li>在宿主机1上抓包发现抓不到丢失的那个长度为 1442 的包，也就是问题出在了  容器A—&gt; 宿主机1 上</li>
</ul>
<h2 id="查看宿主机1的dmesg看到了这样一些信息"><a href="#查看宿主机1的dmesg看到了这样一些信息" class="headerlink" title="查看宿主机1的dmesg看到了这样一些信息"></a>查看宿主机1的dmesg看到了这样一些信息</h2><pre><code>2016-08-08T08:15:27.125951+00:00 server kernel: openvswitch: ens2f0.627: dropped over-mtu packet: 1428 &gt; 1400
2016-08-08T08:15:27.536517+00:00 server kernel: openvswitch: ens2f0.627: dropped over-mtu packet: 1428 &gt; 1400
</code></pre>
<h2 id="验证方法"><a href="#验证方法" class="headerlink" title="验证方法"></a>验证方法</h2><blockquote>
<p>-D      Set the Don’t Fragment bit.<br>-s packetsize<br>             Specify the number of data bytes to be sent.  The default is 56, which translates into 64<br>             ICMP data bytes when combined with the 8 bytes of ICMP header data.  This option cannot be<br>             used with ping sweeps.</p>
</blockquote>
<p>ping 测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">✘ ren@mac  ~/Downloads  ping -c 1 -D -s 1500 www.baidu.com</span><br><span class="line">PING www.a.shifen.com (110.242.68.4): 1500 data bytes</span><br><span class="line">ping: sendto: Message too long</span><br><span class="line">^C</span><br><span class="line">--- www.a.shifen.com ping statistics ---</span><br><span class="line">1 packets transmitted, 0 packets received, 100.0% packet loss</span><br><span class="line"> ✘ ren@mac  ~/Downloads  ping -c 1 -D -s 1400 www.baidu.com</span><br><span class="line">PING www.a.shifen.com (110.242.68.4): 1400 data bytes</span><br><span class="line">1408 bytes from 110.242.68.4: icmp_seq=0 ttl=49 time=21.180 ms</span><br><span class="line"></span><br><span class="line">--- www.a.shifen.com ping statistics ---</span><br><span class="line">1 packets transmitted, 1 packets received, 0.0% packet loss</span><br><span class="line">round-trip min/avg/max/stddev = 21.180/21.180/21.180/0.000 ms</span><br><span class="line"> ren@mac  ~/Downloads </span><br></pre></td></tr></table></figure>

<h2 id="一些结论"><a href="#一些结论" class="headerlink" title="一些结论"></a>一些结论</h2><p> <strong>到这里问题已经很明确了 openvswitch 收到了 一个1428大小的包因为比mtu1400要大，所以扔掉了，接着查看宿主机1的网卡mtu设置果然是1400，悲催，马上修改mtu到1500，问题解决。</strong></p>
<p>正常分片是ip层来操作，路由器工作在3层，有分片能力，从容器到宿主机走的是bridge，没有进行分片，或者是因为收到这个IP包的时候里面带了 Don’t Fragment标志，路由器就不进行分片了，那为什么IP包要带这个标志呢？当然是为了有更好的性能，都经过TCP握手协商出了一个MSS，就不要再进行分片了。</p>
<p>当然这里TCP协商MSS的时候应该经过 <a href="http://en.wikipedia.org/wiki/Path_MTU_Discovery" target="_blank" rel="noopener">PMTUD（ This process is called “Path MTU discovery”.）</a> 来确认整个路由上的所有最小MTU，但是有些路由器会因为安全的原因过滤掉ICMP，导致PMTUD不可靠，所以这里的PMTUD形同虚设，比如在我们的三次握手中会协商一个MSS，这只是基于Client和Server两方的MTU来确定的，链路上如果还有比Client和Server的MTU更小的那么就会出现包超过MTU的大小，同时设置了DF标志而不再进行分片被丢掉。</p>
<p>centos或者ubuntu下：</p>
<pre><code>$cat /proc/sys/net/ipv4/tcp_mtu_probing //1 表示开启路径mtu检测
0

$sudo sysctl -a |grep -i pmtu
net.ipv4.ip_forward_use_pmtu = 0
net.ipv4.ip_no_pmtu_disc = 0 //默认似乎是没有启用PMTUD
net.ipv4.route.min_pmtu = 552
</code></pre>
<p><a href="https://medium.com/@fcamel/tcp-maximum-segment-size-%E6%98%AF%E4%BB%80%E9%BA%BC%E4%BB%A5%E5%8F%8A%E6%98%AF%E5%A6%82%E4%BD%95%E6%B1%BA%E5%AE%9A%E7%9A%84-b5fd9005702e" target="_blank" rel="noopener">IPv4规定路由器至少要能处理576bytes的包，Ethernet规定的是1500 bytes，所以一般都是假设链路上MTU不小于1500</a></p>
<p><a href="https://medium.com/@fcamel/%E7%94%A8-systemtap-%E6%89%BE%E5%87%BA-tcp-%E5%A6%82%E4%BD%95%E6%B1%BA%E5%AE%9A-mss-%E7%9A%84%E5%80%BC-4b6b7a969d04" target="_blank" rel="noopener">TCP中的MSS总是在SYN包中设置成下一站的MTU减去HeaderSize（40）。</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/23df36d95295c839722627b5d63bac48.png" alt="image.png"></p>
<p>一般终端只有收到PATH MTU 调整报文才会去调整mss报文大小，PATH MTU是封装在ICMP报文里面。所以重新在ECS上抓包，抓取数据交互报文和ICMP报文。 </p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20221125133218008.png" alt="image-20221125133218008"></p>
<p>上图可以看到当服务端(3717端口)发送1460 payload的报文的时候，中间链路上的ECS会返回一个ICMP报文，此ICMP报文作用是告诉服务端，ECS的链路MTU只有1476，当服务端收到这个ICMP报文的时候，服务端就会知道中间链路只能允许payload 1436的报文通过，自然就会缩小发送的mss大小。</p>
<p>这个ICMP包在链路上有可能会被丢掉，比如：</p>
<p>Intel网卡驱动老版本RSS使用的是vxlan外层报文, 在新版本切到了内层RSS; 用的外层RSS, 对于GRE代理访问模式没有问题; 新版本用的内层RSS, 看到的源地址是192.168.0.64, 但实际发icmp包的是gre那台ecs-ip, 所以icmp跟session按内层rss策略落不到一个核去了，所以后端服务器无法收到ICMP报文，从而无法自动调整报文MSS大小。<br>简单说, 就是gre代理回icmp的这种场景, 在内层rss版本上不支持了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>因为这是客户给的同一批宿主机默认想当然的认为他们的配置到一样，尤其是mtu这种值，只要不是故意捣乱就不应该乱修改才对，我只检查了两个容器的mtu，没看宿主机的mtu，导致诊断中走了一些弯路</li>
<li>通过这个案例对mtu&#x2F;mss等有了进一步的了解</li>
<li>从这个案例也理解了vlan模式下容器、宿主机、交换机之间的网络传输链路</li>
<li>其实抓包还发现了比1500大得多的包顺利通过，反而更小的包无法通过，这是因为网卡基本都有拆包的功能了</li>
<li>设置由<a href="https://sysctl-explorer.net/net/ipv4/ip_no_pmtu_disc/" target="_blank" rel="noopener">系统主动允许分片的参数</a> sysctl -w net.ipv4.ip_no_pmtu_disc&#x3D;1  可以解决这种问题</li>
</ul>
<h2 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h2><p>Q: 传输的包超过MTU后表现出来的症状？<br>A：卡死，比如scp的时候不动了，或者其他更复杂操作的时候不动了，卡死的状态。</p>
<p>Q: mtu 如何配置<br>ifconfig eth1 mtu 9000 up<br>vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0</p>
<p>Q： 为什么我的MTU是1500，但是抓包看到有个包2700，没有卡死？<br>A： 有些网卡有拆包的能力，具体可以Google：LSO、TSO，这样可以减轻CPU拆包的压力，节省CPU资源。</p>
<p>Q: 到哪里可以设置MSS</p>
<p>A: 网卡配置–ifconfig；ip route在路由上指定；iptables中限制</p>
<blockquote>
<p># Add rules<br>$ sudo iptables -I OUTPUT -p tcp -m tcp –tcp-flags SYN,RST SYN -j TCPMSS –set-mss 48<br># delete rules<br>$ sudo iptables -D OUTPUT -p tcp -m tcp –tcp-flags SYN,RST SYN -j TCPMSS –set-mss 48</p>
<p># show router information<br>$ route -ne<br>$ ip route show<br>192.168.11.0&#x2F;24 dev ens33 proto kernel scope link src 192.168.11.111 metric 100<br># modify route table<br>$ sudo ip route change 192.168.11.0&#x2F;24 dev ens33 proto kernel scope link src 192.168.11.111 metric 100 advmss 48</p>
</blockquote>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/04/26/如何定位上亿次调用才出现一次的Bug/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/04/26/如何定位上亿次调用才出现一次的Bug/" itemprop="url">如何定位上亿次调用才出现一次的Bug</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-26T16:30:03+08:00">
                2018-04-26
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/troubleshooting/" itemprop="url" rel="index">
                    <span itemprop="name">troubleshooting</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何定位上亿次调用才出现一次的Bug"><a href="#如何定位上亿次调用才出现一次的Bug" class="headerlink" title="如何定位上亿次调用才出现一次的Bug"></a>如何定位上亿次调用才出现一次的Bug</h1><h2 id="引文"><a href="#引文" class="headerlink" title="引文"></a>引文</h2><p>对于那种出现概率非常低，很难重现的bug有时候总是感觉有力使不上，比如<a href="https://zhuanlan.zhihu.com/p/21348220?f3fb8ead20=e041f967b1b416071a11f7702126d7a0&from=singlemessage&isappinstalled=0" target="_blank" rel="noopener">这个问题</a></p>
<p>正好最近也碰到一个极低概率下的异常，我介入前一大帮人花了几个月，OS、ECS、网络等等各个环节都被怀疑一遍但是又都没有实锤，所以把过程记录下。</p>
<h2 id="问题背景"><a href="#问题背景" class="headerlink" title="问题背景"></a>问题背景</h2><p>客户会调用我们的一个服务，正常都是client request -&gt; server response 如此反复直到client主动完成，然后断开tcp连接。但是就是在这个过程中，有极低的概率client 端抛出连接非正常断开的异常堆栈，由于这个业务比较特殊，客户无法接受这种异常，所以要求一定要解决这个问题。</p>
<p>重现麻烦，只能在客户环境，让客户把他们的测试跑起来才能一天重现1-2次，每次跟客户沟通成本很高。出现问题的精确时间点不好确定</p>
<h3 id="tcpdump-抓包所看到的问题表现"><a href="#tcpdump-抓包所看到的问题表现" class="headerlink" title="tcpdump 抓包所看到的问题表现"></a>tcpdump 抓包所看到的问题表现</h3><p>在client 和 server上一直进行tcpdump 抓包，然后压力测试不停地跑，一旦client抛了连接异常，根据时间点、端口信息在两边的抓包中分析当时的tcp会话</p>
<p>比如，通过tcpdump分析到的会话是这样的：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/ed9b5b2d81bdc58b9cf41217763939e5.png" alt="screenshot.png"></p>
<p>如上图所示，正常都是client发送request，server返回response，但是出问题的时候（截图红框）server收到了client的request，也回复了ack给client说收到请求了，但是很快server又回复了一个fin包（server主动发起四次挥手断开连接），这是不正常的。</p>
<p>到这里可以有一个明确的结论：<strong>出问题都是因为server主动发起连接断开的fin包，即使刚收到client的request请求还没有返回response</strong></p>
<h3 id="开发增加debug日志"><a href="#开发增加debug日志" class="headerlink" title="开发增加debug日志"></a>开发增加debug日志</h3><p>在server端的应用中可能会调用 socket.close 的地方都增加了日志，但是实际发生异常的时候没有任何日志输出，所以到此开发认为应用代码没有问题（毕竟没有证据–实际不能排除）</p>
<h3 id="怀疑ECS网络抖动（是个好背锅侠，什么锅都可以背）"><a href="#怀疑ECS网络抖动（是个好背锅侠，什么锅都可以背）" class="headerlink" title="怀疑ECS网络抖动（是个好背锅侠，什么锅都可以背）"></a>怀疑ECS网络抖动（是个好背锅侠，什么锅都可以背）</h3><p>申请单独的物理机资源给客户，保证没有其它应用来争抢网络和其它资源，前三天一次异常也没有发生（在ECS上一天发生1-2次），非常高兴以为找到问题了。结果第四天异常再次出现，更换物理机也只是好像偶然性地降低了发生频率而已。</p>
<h3 id="去底层挖掘tcp协议，到底什么条件下会出现主动断开连接"><a href="#去底层挖掘tcp协议，到底什么条件下会出现主动断开连接" class="headerlink" title="去底层挖掘tcp协议，到底什么条件下会出现主动断开连接"></a>去底层挖掘tcp协议，到底什么条件下会出现主动断开连接</h3><p>实际也没有什么进展</p>
<h3 id="用strace、pstack去监控-socket-close-这个事件"><a href="#用strace、pstack去监控-socket-close-这个事件" class="headerlink" title="用strace、pstack去监控 socket.close 这个事件"></a>用strace、pstack去监控 socket.close 这个事件</h3><p>但实际可能在上亿次正常的 socket.close (查询全部结束，client主动请求断开连接）才会出现一次不正常的 socket.close .量太大，还没发在这么多事件中区分那个是不正常的close</p>
<h3 id="应用被-OOM-kill"><a href="#应用被-OOM-kill" class="headerlink" title="应用被 OOM kill"></a>应用被 OOM kill</h3><p>调查过程中为了更快地重现异常，将客户端连接都改成长连接，这样应用不再去调 socket.close ，除非超时、异常之类的，这样一旦出现不正常的 socket.close 就更容易定位了。</p>
<p>实际跑了一段时间后，发现确实 tcpdump 能抓到很多 server在接收到request还没有返回response的时候主动发送 fin包来断开连接的情况，跟前面的症状是一模一样的。但是最终发现这个时候应用被杀掉了，只是说明应用被杀的情况下 server会主动去掉 socket.close关闭连接，但这只是充分条件，而不是必要条件。实际生产线上也没有被 OOM kill过。</p>
<h3 id="给力的开发同学"><a href="#给力的开发同学" class="headerlink" title="给力的开发同学"></a>给力的开发同学</h3><p>分析了这个异常后，开发简化了整个测试，实现client上跑一行PHP代码反复调用就能够让这个bug触发，这一下把整个测试重现bug的过程简化了，终于不再需要客户配合了，让问题的定位效率快了一个数量级。</p>
<p>为了快速地定位到异常的具体连接，实现脚本来自动分析tcpdump结果找到异常close的连接</p>
<p>快速在tcpdump包中找到出问题的那个stream（这个命令行要求tshark的版本为1.12及以上，默认的阿里服务器上的版本都太低，解析不了_ws.col.Info列）：</p>
<pre><code>tshark -r capture.pcap135 -T fields -e frame.number -e frame.time_epoch -e ip.addr -e tcp.port  -e tcp.stream   -e _ws.col.Info | egrep &quot;FIN|Request Quit&quot; | awk &#39;{ print $5, $6 $7 }&#39; | sort -k1n | awk &#39;{ print $1 }&#39; | uniq -c | grep -v &quot;^      3&quot; | less
</code></pre>
<p>在这一系列的工具作用下，稳定跑上一天，异常能发生3、4次，产生的日志和网络包有几百G。</p>
<p>出现问题的后，通过上面的脚本分析连接异常断开的client ip+port和时间，同时拿这三个信息到下面的异常堆栈中搜索匹配找到调用 socket.close()的堆栈。</p>
<h3 id="上Btrace-监听所有-socket-close-事件"><a href="#上Btrace-监听所有-socket-close-事件" class="headerlink" title="上Btrace 监听所有 socket.close 事件"></a>上Btrace 监听所有 socket.close 事件</h3><pre><code>    @OnMethod(clazz=&quot;+java.net.Socket&quot;, method=&quot;close&quot;)
    public static void onSocketClose(@Self Object me) {
      println(&quot;\n==== java.net.Socket#close ====&quot;);
      BTraceUtils.println(BTraceUtils.timestamp() );
      BTraceUtils.println(BTraceUtils.Time.millis() );
      println(concat(&quot;Socket closing:&quot;, str(me)));
      println(concat(&quot;thread: &quot;, str(currentThread())));
      printFields(me);
      jstack();
}
</code></pre>
<p>终于在出现异常的时候btrace抓到了异常的堆栈，在之前代码review看来不可能的逻辑里server主动关闭了连接</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/02bcccd66af82c929c4eee8c88875733.png" alt="screenshot.png"></p>
<p>图左是应用代码，图右是关闭连接的堆栈，有了这个堆栈就可以去修复问题了</p>
<p>实际上这里可能有几个问题：</p>
<ol>
<li>buffer.position 是不可能为0的；</li>
<li>即使buffer.position 等于0 也不应该直接 socket.close, 可能发送error信息给客户端更好；</li>
</ol>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>最终原因是因为NIO过程中buffer有极低的概率被两个socket重用，从而导致出现正在使用的buffer被另外一个socket拿过去并且设置了buffer.position为0，进而导致前一个socket认为数据异常赶紧close了。</li>
<li>开发简化问题的重现步骤非常关键，同时对异常进行分类分析，加快了定位效率</li>
<li>能够通过tcpdump去抓包定位到具体问题大概所在点这是比较关键的一步，同时通过btrace再去监控出问题的调用堆栈从而找到具体代码行。</li>
<li>过程看似简单，实际牵扯了一大波工程师进来，经过几个月才最终定位到出问题的代码行，确实不容易</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/03/25/iptables监控reset的连接/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/25/iptables监控reset的连接/" itemprop="url">iptables监控reset的连接信息</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-25T17:30:03+08:00">
                2018-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="iptables监控reset的连接信息"><a href="#iptables监控reset的连接信息" class="headerlink" title="iptables监控reset的连接信息"></a>iptables监控reset的连接信息</h1><h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>如果连接被reset需要记录下reset包是哪边放出来的，并记录reset连接的四元组信息</p>
<h2 id="iptables规则"><a href="#iptables规则" class="headerlink" title="iptables规则"></a>iptables规则</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Generated by iptables-save v1.4.21 on Wed Apr  1 11:39:31 2020</span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [557:88127]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [527:171711]</span><br><span class="line"># 不监听3406上的reset，日志前面添加 [drds] </span><br><span class="line">-A INPUT -p tcp -m tcp ! --sport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line"># -A INPUT -p tcp -m tcp ! --dport 3406  --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">-A OUTPUT -p tcp -m tcp ! --sport 3406 --tcp-flags RST RST -j LOG --log-prefix &quot;[drds] &quot; --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">COMMIT</span><br><span class="line"># Completed on Wed Apr  1 11:39:31 2020</span><br></pre></td></tr></table></figure>

<p>将如上配置保存在 drds_filter.conf中，设置开机启动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//注意，tee 命令的 &quot;-a&quot; 选项的作用等同于 &quot;&gt;&gt;&quot; 命令，如果去除该选项，那么 tee 命令的作用就等同于 &quot;&gt;&quot; 命令。</span><br><span class="line">//echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid //sudo强行修改写入</span><br><span class="line">echo &quot;sudo iptables-restore &lt; drds_filter.conf&quot; | sudo tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure>

<h2 id="单独记录到日志文件中"><a href="#单独记录到日志文件中" class="headerlink" title="单独记录到日志文件中"></a>单独记录到日志文件中</h2><p>默认情况下 iptables 日志记录在 dmesg中不方便查询，可以修改rsyslog.d规则将日志存到单独的文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/rsyslog.d/drds_filter_log.conf</span><br><span class="line">:msg, startswith, &quot;[drds]&quot; -/home/admin/logs/drds-tcp.log</span><br></pre></td></tr></table></figure>

<p>将 [drds] 开头的日志存到对应的文件</p>
<p>将如上配置放到： &#x2F;etc&#x2F;rsyslog.d&#x2F; 目录下， 重启 rsyslog 就生效了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /home/admin/drds-worker/install/drds_filter_log.conf /etc/rsyslog.d/drds_filter_log.conf</span><br><span class="line">sudo chown -R root:root /etc/rsyslog.d/drds_filter_log.conf</span><br><span class="line">sudo systemctl restart rsyslog</span><br></pre></td></tr></table></figure>

<h2 id="防止日志打满磁盘"><a href="#防止日志打满磁盘" class="headerlink" title="防止日志打满磁盘"></a>防止日志打满磁盘</h2><p>配置 logrotate, 保留最近30天的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#cat /etc/logrotate.d/drds</span><br><span class="line">/home/admin/logs/drds-tcp.log</span><br><span class="line">&#123;</span><br><span class="line">daily</span><br><span class="line">rotate 30</span><br><span class="line">copytruncate</span><br><span class="line">compress</span><br><span class="line">dateext</span><br><span class="line">#size 1k</span><br><span class="line">prerotate</span><br><span class="line">/usr/bin/chattr -a /home/admin/logs/drds-tcp.log</span><br><span class="line">endscript</span><br><span class="line">postrotate</span><br><span class="line">/usr/bin/chattr +a /home/admin/logs/drds-tcp.log</span><br><span class="line">endscript</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行：</span><br><span class="line">sudo /usr/sbin/logrotate --force --verbose /etc/logrotate.d/drds</span><br><span class="line">debug：</span><br><span class="line">sudo /usr/sbin/logrotate -d --verbose /etc/logrotate.d/drds</span><br><span class="line">查看日志：</span><br><span class="line">cat /var/lib/logrotate/logrotate.status</span><br></pre></td></tr></table></figure>

<p>logrotate操作的日志需要权限正常，并且上级目录权限也要对，解决方案参考：<a href="https://chasemp.github.io/2013/07/24/su-directive-logrotate/" target="_blank" rel="noopener">https://chasemp.github.io/2013/07/24/su-directive-logrotate/</a> 报错信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rotating pattern: /var/log/myapp/*.log  weekly (4 rotations)</span><br><span class="line">empty log files are rotated, old logs are removed</span><br><span class="line">considering log /var/log/myapp/default.log</span><br><span class="line"></span><br><span class="line">error: skipping &quot;/var/log/myapp/default.log&quot; because parent directory has insecure permissions</span><br><span class="line">(It&apos;s world writable or writable by group which is not &quot;root&quot;) Set &quot;su&quot; directive in </span><br><span class="line">config file to tell logrotate which user/group should be used for rotation</span><br></pre></td></tr></table></figure>

<h2 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$tail -10 logs/drds-tcp.log</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9366 SEQ=0 ACK=1747027778 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9368 SEQ=0 ACK=3840170438 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9370 SEQ=0 ACK=3892381139 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:38 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.171.173 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=38225 SEQ=0 ACK=1436910913 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br></pre></td></tr></table></figure>

<h2 id="NetFilter-Hooks"><a href="#NetFilter-Hooks" class="headerlink" title="NetFilter Hooks"></a>NetFilter Hooks</h2><p>下面几个 hook 是内核协议栈中已经定义好的：</p>
<ul>
<li><code>NF_IP_PRE_ROUTING</code>: 接收到的包进入协议栈后立即触发此 hook，在进行任何路由判断 （将包发往哪里）之前</li>
<li><code>NF_IP_LOCAL_IN</code>: 接收到的包经过路由判断，如果目的是本机，将触发此 hook</li>
<li><code>NF_IP_FORWARD</code>: 接收到的包经过路由判断，如果目的是其他机器，将触发此 hook</li>
<li><code>NF_IP_LOCAL_OUT</code>: 本机产生的准备发送的包，在进入协议栈后立即触发此 hook</li>
<li><code>NF_IP_POST_ROUTING</code>: 本机产生的准备发送的包或者转发的包，在经过路由判断之后， 将触发此 hook</li>
</ul>
<h2 id="IPTables-表和链（Tables-and-Chains）"><a href="#IPTables-表和链（Tables-and-Chains）" class="headerlink" title="IPTables 表和链（Tables and Chains）"></a>IPTables 表和链（Tables and Chains）</h2><p>下面可以看出，内置的 chain 名字和 netfilter hook 名字是一一对应的：</p>
<ul>
<li><code>PREROUTING</code>: 由 <code>NF_IP_PRE_ROUTING</code> hook 触发</li>
<li><code>INPUT</code>: 由 <code>NF_IP_LOCAL_IN</code> hook 触发</li>
<li><code>FORWARD</code>: 由 <code>NF_IP_FORWARD</code> hook 触发</li>
<li><code>OUTPUT</code>: 由 <code>NF_IP_LOCAL_OUT</code> hook 触发</li>
<li><code>POSTROUTING</code>: 由 <code>NF_IP_POST_ROUTING</code> hook 触发</li>
</ul>
<h2 id="tracing-point-监控"><a href="#tracing-point-监控" class="headerlink" title="tracing_point 监控"></a>tracing_point 监控</h2><p>对于 4.19内核的kernel，可以通过tracing point来监控重传以及reset包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"># grep tcp:tcp /sys/kernel/debug/tracing/available_events</span><br><span class="line">tcp:tcp_probe</span><br><span class="line">tcp:tcp_retransmit_synack</span><br><span class="line">tcp:tcp_rcv_space_adjust</span><br><span class="line">tcp:tcp_destroy_sock</span><br><span class="line">tcp:tcp_receive_reset</span><br><span class="line">tcp:tcp_send_reset</span><br><span class="line">tcp:tcp_retransmit_skb</span><br><span class="line"></span><br><span class="line">#开启本机发出的 reset 监控，默认输出到：/sys/kernel/debug/tracing/trace_pipe</span><br><span class="line"># echo 1 &gt; /sys/kernel/debug/tracing/events/tcp/tcp_send_reset/enable</span><br><span class="line"></span><br><span class="line">#如下是开启重传以及reset的记录，本机ip 10.0.186.140</span><br><span class="line"># cat trace_pipe</span><br><span class="line">//重传</span><br><span class="line">          &lt;idle&gt;-0     [002] ..s. 9520196.657431: tcp_retransmit_skb: sport=3306 dport=62460 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70          </span><br><span class="line"> Wisp-Root-Worke-540   [000] .... 9522308.074233: tcp_destroy_sock: sport=3306 dport=20594 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 sock_cookie=51a</span><br><span class="line"> C2 CompilerThre-543   [002] ..s. 9522308.074296: tcp_destroy_sock: sport=3306 dport=20670 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 sock_cookie=574</span><br><span class="line"> </span><br><span class="line">// 被reset</span><br><span class="line">  Wisp-Root-Worke-540   [002] ..s. 9522519.353756: tcp_receive_reset: sport=33822 dport=8080 saddr=10.0.186.140 daddr=10.0.171.193 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.193 sock_cookie=5dd</span><br><span class="line">// 主动reset  </span><br><span class="line">     DragoonAgent-28297 [002] .... 9522433.144611: tcp_send_reset: sport=8182 dport=61783 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174</span><br><span class="line">  Wisp-Root-Worke-540   [002] ..s. 9522519.353773: tcp_send_reset: sport=33822 dport=8080 saddr=10.0.186.140 daddr=10.0.171.193 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.193</span><br><span class="line">  </span><br><span class="line"> // 3306对端中断 </span><br><span class="line">              cat-28727 [000] ..s. 9524341.650740: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23262 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">          &lt;idle&gt;-0     [000] .ns. 9524397.184608: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23262 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_ESTABLISHED newstate=TCP_CLOSE</span><br><span class="line">         </span><br><span class="line">//8182 主动关闭         </span><br><span class="line">          &lt;idle&gt;-0     [000] .Ns. 9525499.045236: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">    DragoonAgent-6240  [001] .... 9525499.118092: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_ESTABLISHED newstate=TCP_FIN_WAIT1</span><br><span class="line">          &lt;idle&gt;-0     [000] ..s. 9525499.159032: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_FIN_WAIT1 newstate=TCP_FIN_WAIT2</span><br><span class="line">          &lt;idle&gt;-0     [000] .Ns. 9525499.159056: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_FIN_WAIT2 newstate=TCP_CLOSE</span><br><span class="line"></span><br><span class="line">//3306 被动关闭</span><br><span class="line">          &lt;idle&gt;-0     [002] .Ns. 9524484.864509: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">             cat-28568 [002] ..s. 9524496.913199: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_ESTABLISHED newstate=TCP_CLOSE_WAIT</span><br><span class="line"> Wisp-Root-Worke-540   [003] .... 9524496.915450: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_CLOSE_WAIT newstate=TCP_LAST_ACK</span><br><span class="line"> Wisp-Root-Worke-539   [002] .Ns. 9524496.915572: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_LAST_ACK newstate=TCP_CLOSE</span><br></pre></td></tr></table></figure>

<h2 id="iptables-打通网络"><a href="#iptables-打通网络" class="headerlink" title="iptables 打通网络"></a>iptables 打通网络</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//本机到 172.16.0.102 不通，但是和 47.100.29.16能通(阿里云弹性ip)</span><br><span class="line">iptables -t nat -A OUTPUT -d 172.16.0.102 -j DNAT --to-destination 47.100.29.16</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/" target="_blank" rel="noopener">深入理解 iptables 和 netfilter 架构</a></p>
<p><a href="http://arthurchiao.art/blog/nat-zh/" target="_blank" rel="noopener">NAT - 网络地址转换（2016）</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/03/25/iptables使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/25/iptables使用/" itemprop="url">iptables使用</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-25T17:30:03+08:00">
                2018-03-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="iptables使用"><a href="#iptables使用" class="headerlink" title="iptables使用"></a>iptables使用</h1><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211116101345648.png" alt="image-20220608093532338"></p>
<p><a href="https://stuffphilwrites.com/wp-content/uploads/2014/09/FW-IDS-iptables-Flowchart-v2019-04-30-1.png" target="_blank" rel="noopener">包流</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/FW-IDS-iptables-Flowchart-v2019-04-30-1.png" alt="img"></p>
<h2 id="iptables监控reset的连接信息"><a href="#iptables监控reset的连接信息" class="headerlink" title="iptables监控reset的连接信息"></a>iptables监控reset的连接信息</h2><p>如果连接被reset需要记录下reset包是哪边发出来的，并记录reset连接的四元组信息</p>
<h3 id="iptables规则"><a href="#iptables规则" class="headerlink" title="iptables规则"></a>iptables规则</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> Generated by iptables-save v1.4.21 on Wed Apr  1 11:39:31 2020</span></span><br><span class="line">*filter</span><br><span class="line">:INPUT ACCEPT [557:88127]</span><br><span class="line">:FORWARD ACCEPT [0:0]</span><br><span class="line">:OUTPUT ACCEPT [527:171711]</span><br><span class="line"><span class="meta">#</span><span class="bash"> 不监听3406上的reset，日志前面添加 [plantegg] </span></span><br><span class="line">-A INPUT -p tcp -m tcp ! --sport 3406  --tcp-flags RST RST -j LOG --log-prefix "[plantegg] " --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line"><span class="meta">#</span><span class="bash"> -A INPUT -p tcp -m tcp ! --dport 3406  --tcp-flags RST RST -j LOG --<span class="built_in">log</span>-prefix <span class="string">"[plantegg] "</span> --<span class="built_in">log</span>-level7 --<span class="built_in">log</span>-tcp-sequence --<span class="built_in">log</span>-tcp-options --<span class="built_in">log</span>-ip-options</span></span><br><span class="line">-A OUTPUT -p tcp -m tcp ! --sport 3406 --tcp-flags RST RST -j LOG --log-prefix "[plantegg] " --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">COMMIT</span><br><span class="line"><span class="meta">#</span><span class="bash"> Completed on Wed Apr  1 11:39:31 2020</span></span><br></pre></td></tr></table></figure>

<p>将如上配置保存在 plantegg_filter.conf中，设置开机启动:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//注意，tee 命令的 &quot;-a&quot; 选项的作用等同于 &quot;&gt;&gt;&quot; 命令，如果去除该选项，那么 tee 命令的作用就等同于 &quot;&gt;&quot; 命令。</span><br><span class="line">//echo -1 | sudo tee /proc/sys/kernel/perf_event_paranoid //sudo强行修改写入</span><br><span class="line">echo &quot;sudo iptables-restore &lt; plantegg_filter.conf&quot; | sudo tee -a /etc/rc.d/rc.local</span><br></pre></td></tr></table></figure>

<h3 id="单独记录到日志文件中"><a href="#单独记录到日志文件中" class="headerlink" title="单独记录到日志文件中"></a>单独记录到日志文件中</h3><p>默认情况下 iptables 日志记录在 dmesg中不方便查询，可以修改rsyslog.d规则将日志存到单独的文件中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /etc/rsyslog.d/plantegg_filter_log.conf</span><br><span class="line">:msg, startswith, &quot;[plantegg]&quot; -/home/admin/logs/plantegg-tcp.log</span><br></pre></td></tr></table></figure>

<p>将 [plantegg] 开头的日志存到对应的文件</p>
<p>将如上配置放到： &#x2F;etc&#x2F;rsyslog.d&#x2F; 目录下， 重启 rsyslog 就生效了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo cp /home/admin/plantegg-worker/install/plantegg_filter_log.conf /etc/rsyslog.d/plantegg_filter_log.conf</span><br><span class="line">sudo chown -R root:root /etc/rsyslog.d/plantegg_filter_log.conf</span><br><span class="line">sudo systemctl restart rsyslog</span><br></pre></td></tr></table></figure>

<h3 id="防止日志打满磁盘"><a href="#防止日志打满磁盘" class="headerlink" title="防止日志打满磁盘"></a>防止日志打满磁盘</h3><p>配置 logrotate, 保留最近30天的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">cat /etc/logrotate.d/drds</span></span><br><span class="line">/home/admin/logs/drds-tcp.log</span><br><span class="line">&#123;</span><br><span class="line">daily</span><br><span class="line">rotate 30</span><br><span class="line">copytruncate</span><br><span class="line">compress</span><br><span class="line">dateext</span><br><span class="line"><span class="meta">#</span><span class="bash">size 1k</span></span><br><span class="line">prerotate</span><br><span class="line">/usr/bin/chattr -a /home/admin/logs/drds-tcp.log</span><br><span class="line">endscript</span><br><span class="line">postrotate</span><br><span class="line">/usr/bin/chattr +a /home/admin/logs/drds-tcp.log</span><br><span class="line">endscript</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">执行：</span><br><span class="line">sudo /usr/sbin/logrotate --force --verbose /etc/logrotate.d/drds</span><br><span class="line">debug：</span><br><span class="line">sudo /usr/sbin/logrotate -d --verbose /etc/logrotate.d/drds</span><br><span class="line">查看日志：</span><br><span class="line">cat /var/lib/logrotate/logrotate.status</span><br></pre></td></tr></table></figure>

<p>logrotate操作的日志需要权限正常，并且上级目录权限也要对，解决方案参考：<a href="https://chasemp.github.io/2013/07/24/su-directive-logrotate/" target="_blank" rel="noopener">https://chasemp.github.io/2013/07/24/su-directive-logrotate/</a> 报错信息：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rotating pattern: /var/log/myapp/*.log  weekly (4 rotations)</span><br><span class="line">empty log files are rotated, old logs are removed</span><br><span class="line">considering log /var/log/myapp/default.log</span><br><span class="line"></span><br><span class="line">error: skipping &quot;/var/log/myapp/default.log&quot; because parent directory has insecure permissions</span><br><span class="line">(It&apos;s world writable or writable by group which is not &quot;root&quot;) Set &quot;su&quot; directive in </span><br><span class="line">config file to tell logrotate which user/group should be used for rotation</span><br></pre></td></tr></table></figure>

<h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$tail -10 logs/drds-tcp.log</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9366 SEQ=0 ACK=1747027778 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9368 SEQ=0 ACK=3840170438 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:36 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.175.109 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=9370 SEQ=0 ACK=3892381139 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br><span class="line">Apr 26 15:27:38 vb kernel: [drds] IN= OUT=eth0 SRC=10.0.186.75 DST=10.0.171.173 LEN=40 TOS=0x00 PREC=0x00 TTL=64 ID=0 DF PROTO=TCP SPT=8182 DPT=38225 SEQ=0 ACK=1436910913 WINDOW=0 RES=0x00 ACK RST URGP=0</span><br></pre></td></tr></table></figure>

<h2 id="tracing-point-监控"><a href="#tracing-point-监控" class="headerlink" title="tracing_point 监控"></a>tracing_point 监控</h2><p>对于 4.19内核的kernel，可以通过tracing point来监控重传以及reset包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> grep tcp:tcp /sys/kernel/debug/tracing/available_events</span></span><br><span class="line">tcp:tcp_probe</span><br><span class="line">tcp:tcp_retransmit_synack</span><br><span class="line">tcp:tcp_rcv_space_adjust</span><br><span class="line">tcp:tcp_destroy_sock</span><br><span class="line">tcp:tcp_receive_reset</span><br><span class="line">tcp:tcp_send_reset</span><br><span class="line">tcp:tcp_retransmit_skb</span><br><span class="line"></span><br><span class="line">//开启本机发出的 reset 监控，默认输出到：/sys/kernel/debug/tracing/trace_pipe</span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">echo</span> 1 &gt; /sys/kernel/debug/tracing/events/tcp/tcp_send_reset/<span class="built_in">enable</span></span></span><br><span class="line"></span><br><span class="line">//如下是开启重传以及reset的记录，本机ip 10.0.186.140</span><br><span class="line"><span class="meta">#</span><span class="bash"> cat trace_pipe</span></span><br><span class="line">//重传</span><br><span class="line">          &lt;idle&gt;-0     [002] ..s. 9520196.657431: tcp_retransmit_skb: sport=3306 dport=62460 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70          </span><br><span class="line"> Wisp-Root-Worke-540   [000] .... 9522308.074233: tcp_destroy_sock: sport=3306 dport=20594 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 sock_cookie=51a</span><br><span class="line"> C2 CompilerThre-543   [002] ..s. 9522308.074296: tcp_destroy_sock: sport=3306 dport=20670 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 sock_cookie=574</span><br><span class="line"> </span><br><span class="line">// 被reset</span><br><span class="line">  Wisp-Root-Worke-540   [002] ..s. 9522519.353756: tcp_receive_reset: sport=33822 dport=8080 saddr=10.0.186.140 daddr=10.0.171.193 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.193 sock_cookie=5dd</span><br><span class="line">// 主动reset  </span><br><span class="line">     DragoonAgent-28297 [002] .... 9522433.144611: tcp_send_reset: sport=8182 dport=61783 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174</span><br><span class="line">  Wisp-Root-Worke-540   [002] ..s. 9522519.353773: tcp_send_reset: sport=33822 dport=8080 saddr=10.0.186.140 daddr=10.0.171.193 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.193</span><br><span class="line">  </span><br><span class="line"> // 3306对端中断 </span><br><span class="line">              cat-28727 [000] ..s. 9524341.650740: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23262 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">          &lt;idle&gt;-0     [000] .ns. 9524397.184608: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23262 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_ESTABLISHED newstate=TCP_CLOSE</span><br><span class="line">         </span><br><span class="line">//8182 主动关闭         </span><br><span class="line">          &lt;idle&gt;-0     [000] .Ns. 9525499.045236: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">    DragoonAgent-6240  [001] .... 9525499.118092: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_ESTABLISHED newstate=TCP_FIN_WAIT1</span><br><span class="line">          &lt;idle&gt;-0     [000] ..s. 9525499.159032: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_FIN_WAIT1 newstate=TCP_FIN_WAIT2</span><br><span class="line">          &lt;idle&gt;-0     [000] .Ns. 9525499.159056: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=8182 dport=25448 saddr=10.0.186.140 daddr=10.0.171.174 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.171.174 oldstate=TCP_FIN_WAIT2 newstate=TCP_CLOSE</span><br><span class="line"></span><br><span class="line">//3306 被动关闭</span><br><span class="line">          &lt;idle&gt;-0     [002] .Ns. 9524484.864509: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_SYN_RECV newstate=TCP_ESTABLISHED</span><br><span class="line">             cat-28568 [002] ..s. 9524496.913199: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_ESTABLISHED newstate=TCP_CLOSE_WAIT</span><br><span class="line"> Wisp-Root-Worke-540   [003] .... 9524496.915450: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_CLOSE_WAIT newstate=TCP_LAST_ACK</span><br><span class="line"> Wisp-Root-Worke-539   [002] .Ns. 9524496.915572: inet_sock_set_state: family=AF_INET protocol=IPPROTO_TCP sport=3306 dport=23360 saddr=10.0.186.140 daddr=10.0.186.70 saddrv6=::ffff:10.0.186.140 daddrv6=::ffff:10.0.186.70 oldstate=TCP_LAST_ACK newstate=TCP_CLOSE</span><br></pre></td></tr></table></figure>

<h2 id="iptables-打通网络"><a href="#iptables-打通网络" class="headerlink" title="iptables 打通网络"></a>iptables 打通网络</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//本机到 172.16.0.102 不通，但是和 47.100.29.16能通(阿里云弹性ip)</span><br><span class="line">iptables -t nat -A OUTPUT -d 172.16.0.102 -j DNAT --to-destination 47.100.29.16</span><br></pre></td></tr></table></figure>

<h2 id="ipset-组合iptables使用"><a href="#ipset-组合iptables使用" class="headerlink" title="ipset 组合iptables使用"></a>ipset 组合iptables使用</h2><p>ipset是iptables的扩展,它允许创建匹配地址集合的规则。普通的iptables链只能单IP匹配, 进行规则匹配时，是从规则列表中从头到尾一条一条进行匹配，这像是在链表中搜索指定节点费力。ipset 提供了把这个 O(n) 的操作变成 O(1) 的方法：就是把要处理的 IP 放进一个集合，对这个集合设置一条 iptables 规则。像 iptable 一样，IP sets 是 Linux 内核提供，ipset 这个命令是对它进行操作的一个工具。<br>另外ipset的一个优势是集合可以动态的修改，即使ipset的iptables规则目前已经启动，新加的入ipset的ip也生效。</p>
<p><a href="https://www.cnblogs.com/faberbeta/p/ipset.html" target="_blank" rel="noopener">ipset</a>可以以set的形式管理大批IP以及IP段，set可以有多个，通过 ipset修改set后可以立即生效。不用再次修改iptables规则。k8s也会用ipset来管理ip集合</p>
<blockquote>
<p>ipset is an extension to iptables that allows you to create firewall rules that match entire “sets” of addresses at once. Unlike normal iptables chains, which are stored and traversed linearly, IP sets are stored in indexed data structures, making lookups very efficient, even when dealing with large sets.</p>
</blockquote>
<p>接下来用一个ip+port的白名单案例来展示他们的用法，ipset负责白名单，iptables负责拦截规则：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">  240  [2021-11-30 19:57:10] ipset list drds_whitelist_ips |grep "^127.0."</span><br><span class="line">  241  [2021-11-30 19:57:27] ipset del drds_whitelist_ips 127.0.0.1 //从set删除ip</span><br><span class="line">  248  [2021-11-30 19:58:50] ipset list drds_whitelist_ips |grep "^11.1.2"</span><br><span class="line">  249  [2021-11-30 19:59:05] ipset del drds_whitelist_ips 11.1.2.30</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">timeout 259200是集合内新增的IP有三天的寿命</span></span><br><span class="line">ipset create myset hash:net timeout 259200 </span><br><span class="line">  </span><br><span class="line">ipset list drds_whitelist_ips             //列出set中的所有ip、ip段</span><br><span class="line">ipset add drds_whitelist_ips 100.1.2.0/24 //从set中增加ip段</span><br><span class="line"></span><br><span class="line">iptables -I INPUT 1 -p tcp  -j drds_whitelist //创建新规则链drds_whitelist，所有tcp流入的包都跳转到 drds_whitelist规则</span><br><span class="line">//有了以上drds_whitelist_ips这个名单, 接下来可以在iptables规则中使用这个set了</span><br><span class="line">//在第一行增加规则：访问端口1234的tcp请求走规则 drds_whitelist</span><br><span class="line">iptables -I INPUT 1 -p tcp --dport 1234 -j drds_whitelist </span><br><span class="line"></span><br><span class="line">//规则drds_whitelist 添加如下三条</span><br><span class="line">//第一条白名单中的来源ip访问1234就ACCEPT，不再走后面的. 关键的白名单列表就取自ipset中的drds_whitelist_ips</span><br><span class="line">iptables -A drds_whitelist -m set --match-set drds_whitelist_ips src -p tcp --dport 1234 -j ACCEPT </span><br><span class="line"></span><br><span class="line">//同规则1，记录日志，走到这里说明规则1没生效，那么就是黑名单要拦截的了</span><br><span class="line">iptables -A drds_whitelist -p tcp --dport 1234 -j LOG --log-prefix '[drds_reject] ' --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line">//拦截          </span><br><span class="line">iptables -A drds_whitelist -p tcp --dport 1234 -j REJECT --reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure>

<p>经过如上操作后，可以得到iptables规则如下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">iptables -L -n --line-numbers</span></span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">drds_whitelist  tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:1234</span><br><span class="line"></span><br><span class="line">Chain drds_whitelist (1 references)</span><br><span class="line">target     prot opt source               destination</span><br><span class="line">1    ACCEPT     tcp  --  0.0.0.0/0            0.0.0.0/0           match-set drds_whitelist_ips src tcp dpt:80</span><br><span class="line">2    LOG        tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:1234 LOG flags 7 level 7 prefix `[drds_reject] ` --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options '</span><br><span class="line">3    REJECT     tcp  --  0.0.0.0/0            0.0.0.0/0           tcp dpt:1234 reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure>

<p>从以上Chain drds_whitelist中删除第三条规则</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -D drds_whitelist 3</span><br></pre></td></tr></table></figure>

<h3 id="block-ip-案例"><a href="#block-ip-案例" class="headerlink" title="block ip 案例"></a>block ip 案例</h3><p>模拟断网测试的时候可以通过iptables固定屏蔽某几个ip来实现。</p>
<p>创建ipset，存放好需要block的ip列表</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ipset create block_ips hash:net timeout 259200</span><br><span class="line">ipset add block_ips 10.176.2.245</span><br></pre></td></tr></table></figure>

<p>添加iptables过滤规则，规则中不需要列出一堆ip，只需要指定上一步创建好的ipset，以后屏蔽、放开某些ip不需要修改iptables规则了，只需要往ipset添加、删除目标ip</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">iptables -N drds_rule //创建新规则链</span><br><span class="line"></span><br><span class="line">iptables -I INPUT 1 -m set --match-set block_ips src  -p tcp  -j drds_rule  //命中就跳转到drds_rule</span><br><span class="line">//这条可有可无，记录日志，方便调试</span><br><span class="line">iptables -I drds_rule -m set --match-set block_ips src -j LOG --log-prefix '[drds_reject] ' --log-level 7 --log-tcp-sequence --log-tcp-options --log-ip-options</span><br><span class="line"></span><br><span class="line">iptables -A drds_rule -m set --match-set block_ips src -p tcp  -j REJECT --reject-with icmp-host-prohibited</span><br></pre></td></tr></table></figure>

<h2 id="iptables记录日志"><a href="#iptables记录日志" class="headerlink" title="iptables记录日志"></a>iptables记录日志</h2><p>记录每个新连接创建的时间，日志在&#x2F;var&#x2F;log&#x2F;kern或者&#x2F;var&#x2F;log&#x2F;dmesg中：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">iptables -I INPUT -m state --state NEW -j LOG --log-prefix &quot;Connection In: &quot;</span><br><span class="line">iptables -I OUTPUT -m state --state NEW -j LOG --log-prefix &quot;Connection Out: &quot;</span><br><span class="line"></span><br><span class="line">//检查包，记录invalid包到日志中</span><br><span class="line">iptables -A INPUT -m conntrack --ctstate INVALID -m limit --limit 1/sec   -j LOG --log-prefix &quot;invalid: &quot; --log-level 7</span><br></pre></td></tr></table></figure>

<p>在宿主机上执行，然后在dmesg中能看到包的传递流程。只有raw有TRACE能力，nat、filter、mangle都没有。这个方式对性能影响非常大，时延高（增加1秒左右）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iptables -t raw -A OUTPUT -p icmp -j TRACE</span><br><span class="line">iptables -t raw -A PREROUTING -p icmp -j TRACE</span><br></pre></td></tr></table></figure>

<h2 id="端口转发"><a href="#端口转发" class="headerlink" title="端口转发"></a><a href="https://www.cnblogs.com/dongzhiquan/p/11427461.html" target="_blank" rel="noopener">端口转发</a></h2><h3 id="iptables"><a href="#iptables" class="headerlink" title="iptables"></a>iptables</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -d 10.176.7.5 -p tcp --dport 8507 -j DNAT --to-destination 10.176.7.6:3307</span><br><span class="line">iptables -t nat -D PREROUTING  -p tcp --dport 18080 -j DNAT --to-destination 10.176.7.245:8080</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">将访问8022端口的进出流量转发到22端口</span></span><br><span class="line">iptables -t nat -A PREROUTING -p tcp --dport 8022 -j REDIRECT --to-ports 22 </span><br><span class="line">iptables -t nat -A PREROUTING -p tcp --dport 8507 -j REDIRECT --to-ports 3307 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">将本机的端口转发到其他机器</span></span><br><span class="line">iptables -t nat -A PREROUTING -d 192.168.172.130 -p tcp --dport 8000 -j DNAT --to-destination 192.168.172.131:80</span><br><span class="line"><span class="meta">#</span><span class="bash">将192.168.172.131:80 端口将数据返回给客户端时，将源ip改为192.168.172.130</span></span><br><span class="line">iptables -t nat -A POSTROUTING -d 192.168.172.131 -p tcp --dport 80 -j SNAT --to 192.168.172.130</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">ip 转发，做完转发后netstat能看到两条连接</span></span><br><span class="line">sudo iptables -t nat -A OUTPUT -d 100.69.170.27 -j DNAT --to-destination 127.0.0.1</span><br><span class="line"></span><br><span class="line">/sbin/iptables -t nat -I PREROUTING -d 23.27.6.15 -j DNAT --to-destination 45.61.255.176</span><br><span class="line">/sbin/iptables -t nat -I POSTROUTING -d 45.61.255.176 -j SNAT --to-source 23.27.6.15</span><br><span class="line">/sbin/iptables -t nat -I POSTROUTING -s 45.61.255.176 -j SNAT --to-source 23.27.6.15</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">清空nat表的所有链</span></span><br><span class="line">iptables -t nat -F PREROUTING</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">禁止访问某个端口</span></span><br><span class="line">iptables -A OUTPUT -p tcp --dport 31165 -j DROP</span><br></pre></td></tr></table></figure>

<p>iptables 工作图如下，进来的包走1、2；出去的包走4、5；转发的包走1、3、5</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-7027461." alt="Image"></p>
<h3 id="ncat端口转发"><a href="#ncat端口转发" class="headerlink" title="ncat端口转发"></a>ncat端口转发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">监听本机 9876 端口，将数据转发到 192.168.172.131的 80 端口</span><br><span class="line">ncat --sh-exec &quot;ncat 192.168.172.131 80&quot; -l 9876  --keep-open</span><br></pre></td></tr></table></figure>

<p>scat</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在本地监听12345端口，并将请求转发至192.168.172.131的22端口。</span><br><span class="line">socat TCP4-LISTEN:12345,reuseaddr,fork TCP4:192.168.172.131:22</span><br></pre></td></tr></table></figure>

<h3 id="iptables-屏蔽IP"><a href="#iptables-屏蔽IP" class="headerlink" title="iptables 屏蔽IP"></a>iptables 屏蔽IP</h3><p>一分钟内新建22端口连接超过 4 次，不分密码对错, 直接 block.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">iptables -A INPUT -p tcp -m tcp --dport 22 -m state --state NEW -m recent --set --name SSH --rsource</span><br><span class="line">iptables -A INPUT -p tcp -m tcp --dport 22 -m state --state NEW -m recent --update --seconds 60 --hitcount 4 --name SSH --rsource -j DROP</span><br><span class="line"></span><br><span class="line">或者 block 掉暴力破解 ssh 的 IP</span><br><span class="line">grep &quot;Failed&quot; /var/log/auth.log | \</span><br><span class="line">     awk &apos;&#123;print $(NF-3)&#125;&apos; | \</span><br><span class="line">     sort | uniq -c | sort -n | \</span><br><span class="line">     awk &apos;&#123;if ($1&gt;100) print $2&#125;&apos; | \</span><br><span class="line">     xargs -I &#123;&#125; iptables -A INPUT -s &#123;&#125; -j DROP</span><br><span class="line">     </span><br><span class="line">iptables -A INPUT  -p tcp --sport 3306 -j DROP</span><br><span class="line">iptables -A OUTPUT -p tcp --dport 3306 -j DROP</span><br></pre></td></tr></table></figure>

<p><a href="https://making.pusher.com/per-ip-rate-limiting-with-iptables/index.html" target="_blank" rel="noopener">Per-IP rate limiting with iptables</a></p>
<h2 id="iptables-扔掉指定端口的-ack-包"><a href="#iptables-扔掉指定端口的-ack-包" class="headerlink" title="iptables 扔掉指定端口的 ack 包"></a>iptables 扔掉指定端口的 ack 包</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">//扔掉发给 8000 端口的 ack 包，但是不要扔 rst包</span><br><span class="line">sudo iptables -A OUTPUT -p tcp --dport 8000 -m tcp ! --tcp-flags RST,ACK RST,ACK  -m tcp  --tcp-flags ACK ACK -j DROP</span><br></pre></td></tr></table></figure>

<p>将 ack delay 1 秒中，故意制造乱序：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tc qdisc add dev eth0 root handle 1: prio</span><br><span class="line">tc qdisc add dev eth0 parent 1:3 handle 30: netem delay 1000ms</span><br><span class="line"></span><br><span class="line">iptables -t mangle -A OUTPUT -p tcp --dport 8000 -m tcp ! --tcp-flags RST,ACK RST,ACK -m tcp --tcp-flags ACK ACK  -j MARK --set-mark 3</span><br><span class="line"></span><br><span class="line">tc filter add dev eth0 protocol ip parent 1:0 prio 3 handle 3 fw flowid 1:3</span><br></pre></td></tr></table></figure>

<p>执行效果(在 172.26.137.120.17922 端抓包)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">15:31:57.815944 IP 172.26.137.120.17922 &gt; 172.26.137.130.8000: Flags [S], seq 2408440162, win 29200, options [mss 1460,sackOK,TS val 1952321086 ecr 0,nop,wscale 7], length 0</span><br><span class="line">15:31:57.815963 IP 172.26.137.130.8000 &gt; 172.26.137.120.17922: Flags [S.], seq 1188559140, ack 2408440163, win 65160, options [mss 1460,sackOK,TS val 1076070423 ecr 1952321086,nop,wscale 7], length 0</span><br><span class="line">15:31:57.816236 IP 172.26.137.120.17922 &gt; 172.26.137.130.8000: Flags [R.], seq 1, ack 1, win 229, options [nop,nop,TS val 1952321086 ecr 1076070423], length 0  //本来是先 ack，再 RST，但因为 ack delay 了导致先抓到 RST</span><br><span class="line"></span><br><span class="line">//1 秒后 ack 发出</span><br><span class="line">15:31:58.816225 IP 172.26.137.120.17922 &gt; 172.26.137.130.8000: Flags [.], ack 1, win 229, options [nop,nop,TS val 1952321086 ecr 1076070423], length 0</span><br><span class="line">15:31:58.816264 IP 172.26.137.130.8000 &gt; 172.26.137.120.17922: Flags [R], seq 1188559141, win 0, length 0</span><br></pre></td></tr></table></figure>

<h2 id="iptables-常用参数"><a href="#iptables-常用参数" class="headerlink" title="iptables 常用参数"></a>iptables 常用参数</h2><blockquote>
<p><strong>-I</strong> : Insert rule at given rule number</p>
<p><strong>-t</strong> : Specifies the packet matching table such as nat, filter, security, mangle, and raw.</p>
<p><strong>-L</strong> : List info for specific chain (such as INPUT&#x2F;FORWARD&#x2F;OUTPUT) of given packet matching table</p>
<p><strong>–line-numbers</strong> : See firewall rules with line numbers</p>
<p><strong>-n</strong> : Do not resolve names using dns i.e. only show numeric output for IP address and port numbers.</p>
<p><strong>-v</strong> : Verbose output. This option makes the list command show the interface name, the rule options (if any), and the TOS masks</p>
</blockquote>
<h2 id="NetFilter-Hooks"><a href="#NetFilter-Hooks" class="headerlink" title="NetFilter Hooks"></a>NetFilter Hooks</h2><p>下面几个 hook 是内核协议栈中已经定义好的：</p>
<ul>
<li><code>NF_IP_PRE_ROUTING</code>: 接收到的包进入协议栈后立即触发此 hook，在进行任何路由判断 （将包发往哪里）之前</li>
<li><code>NF_IP_LOCAL_IN</code>: 接收到的包经过路由判断，如果目的是本机，将触发此 hook</li>
<li><code>NF_IP_FORWARD</code>: 接收到的包经过路由判断，如果目的是其他机器，将触发此 hook</li>
<li><code>NF_IP_LOCAL_OUT</code>: 本机产生的准备发送的包，在进入协议栈后立即触发此 hook</li>
<li><code>NF_IP_POST_ROUTING</code>: 本机产生的准备发送的包或者转发的包，在经过路由判断之后， 将触发此 hook</li>
</ul>
<h2 id="IPTables-表和链（Tables-and-Chains）"><a href="#IPTables-表和链（Tables-and-Chains）" class="headerlink" title="IPTables 表和链（Tables and Chains）"></a>IPTables 表和链（Tables and Chains）</h2><p>下面可以看出，内置的 chain 名字和 netfilter hook 名字是一一对应的：</p>
<ul>
<li><code>PREROUTING</code>: 由 <code>NF_IP_PRE_ROUTING</code> hook 触发</li>
<li><code>INPUT</code>: 由 <code>NF_IP_LOCAL_IN</code> hook 触发</li>
<li><code>FORWARD</code>: 由 <code>NF_IP_FORWARD</code> hook 触发</li>
<li><code>OUTPUT</code>: 由 <code>NF_IP_LOCAL_OUT</code> hook 触发</li>
<li><code>POSTROUTING</code>: 由 <code>NF_IP_POST_ROUTING</code> hook 触发</li>
</ul>
<p>如果没有匹配到任何规则那么执行默认规则。下面括号中的policy</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#iptables -L | grep policy</span><br><span class="line">Chain INPUT (policy ACCEPT)</span><br><span class="line">Chain FORWARD (policy ACCEPT)</span><br><span class="line">Chain OUTPUT (policy ACCEPT)</span><br></pre></td></tr></table></figure>

<p>If you would rather deny all connections and manually specify which ones you want to allow to connect, you should change the default policy of your chains to drop. Doing this would probably only be useful for servers that contain sensitive information and only ever have the same IP addresses connect to them.</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; iptables --policy INPUT DROP`</span><br><span class="line">&gt; `iptables --policy OUTPUT DROP`</span><br><span class="line">&gt; `iptables --policy FORWARD DROP</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<h2 id="iptables规则对性能的影响"><a href="#iptables规则对性能的影响" class="headerlink" title="iptables规则对性能的影响"></a>iptables规则对性能的影响</h2><p>蓝色是iptables规则数量，不过如果规则内容差不多，只是ip不一样，完全可以用ipset将他们合并到一条或者几条规则，从而提升性能</p>
<img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220521141020452.png" alt="image-20220521141020452" style="zoom:50%;">

<h2 id="iptables-丢包监控"><a href="#iptables-丢包监控" class="headerlink" title="iptables 丢包监控"></a>iptables 丢包监控</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//每 2 秒中 diff 一下因为命中 iptables 导致的 Drop 包的数量，并高亮差异(丢包数)， -v :Verbose  output</span><br><span class="line">watch -d &quot;iptables -nvL | grep DROP &quot;</span><br><span class="line"></span><br><span class="line">//输出如下：</span><br><span class="line">Chain OUTPUT (policy ACCEPT 19G packets, 3618G bytes)</span><br><span class="line"> pkts bytes target     prot opt in     out     source               destination</span><br><span class="line">   15   600 DROP       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:8000 option=!8 flags:0x04/0x04</span><br><span class="line">  160  6400 DROP       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            tcp dpt:22345 option=!8 flags:0x04/0x04</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://arthurchiao.art/blog/deep-dive-into-iptables-and-netfilter-arch-zh/" target="_blank" rel="noopener">深入理解 iptables 和 netfilter 架构</a></p>
<p><a href="http://arthurchiao.art/blog/nat-zh/" target="_blank" rel="noopener">NAT - 网络地址转换（2016）</a></p>
<p><a href="https://making.pusher.com/per-ip-rate-limiting-with-iptables/" target="_blank" rel="noopener">通过iptables 来控制每个ip的流量</a></p>
<p><a href="https://lotabout.me/2022/Horrible-Iptables-tutorials/" target="_blank" rel="noopener">iptables 实用教程</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/03/24/Linux环境变量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/24/Linux环境变量/" itemprop="url">Linux环境变量问题汇总</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-24T17:30:03+08:00">
                2018-03-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux环境变量问题汇总"><a href="#Linux环境变量问题汇总" class="headerlink" title="Linux环境变量问题汇总"></a>Linux环境变量问题汇总</h1><h3 id="测试好的脚本放到-crontab-里就报错-找不到命令"><a href="#测试好的脚本放到-crontab-里就报错-找不到命令" class="headerlink" title="测试好的脚本放到 crontab 里就报错: 找不到命令"></a>测试好的脚本放到 crontab 里就报错: 找不到命令</h3><p>写好一个脚本，测试没有问题，然后放到crontab 想要定时执行，但是总是报错，去看日志的话显示某些命令找不到，这种一般都是因为PATH环境变量变了导致的</p>
<p>自己在shell命令行下测试的时候当前环境变量就是这个用户的环境变量，可以通过命令：env 看到，脚本放到crontab 里面后一般都加了sudo 这个时候 env 变了。比如你可以在命令行下执行 env 和 sudo env 比较一下就发现他们很不一样</p>
<p>sudo有一个参数 -E （–preserver-env）就是为了解决这个问题的。</p>
<p>这个时候再比较一下 </p>
<ul>
<li>env</li>
<li>sudo env</li>
<li>sudo -E env</li>
</ul>
<p>大概就能理解这里的区别了。</p>
<p>本文后面的回复中有同学提到了：</p>
<blockquote>
<p>第一个问题，sudo -E在集团的容器中貌似是不行的，没有特别好的解，我们最后是通过在要执行的脚本中手动source “&#x2F;etc&#x2F;profile.d&#x2F;dockerenv.sh”才行</p>
</blockquote>
<p>我也特意去测试了一下官方的Docker容器，也有同样的问题，&#x2F;etc&#x2F;profile.d&#x2F;dockerenv.sh 中的脚本没有生效，然后debug看了看，主要是因为bashrc中的 . 和 source 不同导致的，不能说没有生效，而是加载 &#x2F;etc&#x2F;profile.d&#x2F;dockerenv.sh 是在一个独立的bash 进程中，加载完毕进程结束，所有加载过的变量都完成了生命周期释放了，类似我文章中的export部分提到的。我尝试把 ~&#x2F;.bashrc 中的 .  &#x2F;etc&#x2F;bashrc 改成 source &#x2F;etc&#x2F;bashrc , 同时也把 &#x2F;etc&#x2F;bashrc 中的 . 改成 source，就可以了，再次进到容器不需要任何操作就能看到所有：&#x2F;etc&#x2F;profile.d&#x2F;dockerenv.sh 中的变量了，所以我们制作镜像的时候考虑改改这里</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/crontab-7372074.png" alt="crontab"></p>
<h3 id="docker-容器中admin取不到env参数"><a href="#docker-容器中admin取不到env参数" class="headerlink" title="docker 容器中admin取不到env参数"></a>docker 容器中admin取不到env参数</h3><p>docker run的时候带入一堆参数，用root能env中能看到这些参数，admin用户也能看见这些参数，但是通过crond用admin就没法启动应用了，因为读不到这些env。</p>
<h3 id="同样一个命令ssh执行不了，-报找不到命令"><a href="#同样一个命令ssh执行不了，-报找不到命令" class="headerlink" title="同样一个命令ssh执行不了， 报找不到命令"></a>同样一个命令ssh执行不了， 报找不到命令</h3><p>比如：</p>
<p>ssh user@ip “ ip a “  报错： bash: ip: command not found</p>
<p>但是你要是先执行 ssh user@ip 连上服务器后，再执行 ip a 就可以，这里是同一个命令通过两种不同的方式使用，但是环境变量也不一样了。</p>
<p>同样想要解决这个问题的话可以先 ssh 连上服务器，再执行 which ip ; env | grep PATH  </p>
<pre><code>$ which ip
/usr/sbin/ip
$ env | grep PATH
PATH=/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
</code></pre>
<p>很明显这里因为 ip在&#x2F;usr&#x2F;sbin下，而&#x2F;usr&#x2F;sbin又在PATH变量中，所以可以找到。</p>
<p>那么接下来我们看看 </p>
<pre><code>$ssh user@ip &quot;env | grep PATH&quot;
PATH=/usr/local/bin:/usr/bin
</code></pre>
<p>很明显这里的PATH比上面的PATH短了一截，&#x2F;usr&#x2F;sbin也没有在里面，所以&#x2F;usr&#x2F;sbin 下的ip命令自然也找不到了，这里虽然都是同一个用户，但是他们的环境变量还不一样，有点出乎我的意料之外。</p>
<p>主要原因是我们的shell 分为login shell 和 no-login shell , 先ssh 登陆上去再执行命令就是一个login shell，Linux要为这个终端分配资源。</p>
<p>而下面的直接在ssh 里面放执行命令实际上就不需要login，所以这是一个no-login shell.</p>
<h4 id="login-shell-和-no-login-shell又有什么区别呢？"><a href="#login-shell-和-no-login-shell又有什么区别呢？" class="headerlink" title="login shell 和 no-login shell又有什么区别呢？"></a>login shell 和 no-login shell又有什么区别呢？</h4><ul>
<li>login shell加载环境变量的顺序是：① &#x2F;etc&#x2F;profile ② ~&#x2F;.bash_profile ③ ~&#x2F;.bashrc ④ &#x2F;etc&#x2F;bashrc </li>
<li>而non-login shell加载环境变量的顺序是： ① ~&#x2F;.bashrc ② &#x2F;etc&#x2F;bashrc</li>
</ul>
<p>也就是nog-login少了前面两步，我们先看后面两步。</p>
<p>下面是一个 .bashrc 的内容：</p>
<pre><code>$ cat .bashrc 
# .bashrc

# Source global definitions
if [ -f /etc/bashrc ]; then
    . /etc/bashrc
fi
</code></pre>
<p>基本没有什么内容，它主要是去加载 &#x2F;etc&#x2F;bashrc  而他里面也没有看到sbin相关的东西</p>
<p>那我们再看non-login少的两步： ① &#x2F;etc&#x2F;profile ② ~&#x2F;.bash_profile </p>
<p>cat &#x2F;etc&#x2F;profile :<br>    if [ “$EUID” &#x3D; “0” ]; then<br>        pathmunge &#x2F;usr&#x2F;sbin<br>        pathmunge &#x2F;usr&#x2F;local&#x2F;sbin<br>    else<br>        pathmunge &#x2F;usr&#x2F;local&#x2F;sbin after<br>        pathmunge &#x2F;usr&#x2F;sbin after<br>    fi</p>
<p>这几行代码就是把 &#x2F;usr&#x2F;sbin 添加到 PATH 变量中，正是他们的区别决定了这里的环境变量不一样。</p>
<p><strong>用一张图来表述他们的结构，箭头代表加载顺序，红框代表不同的shell的初始入口</strong>：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/ae3095f063dede80a8c1ee79ec25685c.png" alt="image.png"></p>
<p>像 ansible 这种自动化工具，或者我们自己写的自动化脚本，底层通过ssh这种non-login的方式来执行的话，那么都有可能碰到这个问题，如何修复呢？</p>
<p>在 &#x2F;etc&#x2F;profile.d&#x2F; 下创建一个文件：&#x2F;etc&#x2F;profile.d&#x2F;my_bashenv.sh 内容如下：</p>
<pre><code>$cat /etc/profile.d/my_bashenv.sh 

pathmunge () {
if ! echo $PATH | /bin/egrep -q &quot;(^|:)$1($|:)&quot; ; then
   if [ &quot;$2&quot; = &quot;after&quot; ] ; then
  PATH=$PATH:$1
   else
  PATH=$1:$PATH
   fi
fi
}
 
pathmunge /sbin
pathmunge /usr/sbin
pathmunge /usr/local/sbin
pathmunge /usr/local/bin
pathmunge /usr/X11R6/bin after
 
unset pathmunge

complete -cf sudo
 
    alias chgrp=&#39;chgrp --preserve-root&#39;
    alias chown=&#39;chown --preserve-root&#39;
    alias chmod=&#39;chmod --preserve-root&#39;
    alias rm=&#39;rm -i --preserve-root&#39;
    
HISTTIMEFORMAT=&#39;[%F %T] &#39;
HISTSIZE=1000
export EDITOR=vim    
export PS1=&#39;\n\e[1;37m[\e[m\e[1;32m\u\e[m\e[1;33m@\e[m\e[1;35m\H\e[m \e[4m`pwd`\e[m\e[1;37m]\e[m\e[1;36m\e[m\n\$&#39;
</code></pre>
<p> 通过前面我们可以看到 &#x2F;etc&#x2F;bashrc 总是会去加载 &#x2F;etc&#x2F;profile.d&#x2F; 下的所有 *.sh 文件，同时我们还可以在这个文件中修改我们喜欢的 shell 配色方案和环境变量等等 </p>
<p><a href="https://www.gnu.org/software/bash/manual/html_node/Bash-Startup-Files.html" target="_blank" rel="noopener">脚本前增加如下一行是好习惯</a></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash --login</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220505213833017.png" alt="image-20220505213833017"></p>
<h3 id="BASH"><a href="#BASH" class="headerlink" title="BASH"></a>BASH</h3><p>1、交互式的登录shell （bash –il xxx.sh）<br>载入的信息：<br>&#x2F;etc&#x2F;profile<br>~&#x2F;.bash_profile（ -&gt;  ~&#x2F;.bashrc  -&gt;  &#x2F;etc&#x2F;bashrc）<br>~&#x2F;.bash_login<br>~&#x2F;.profile</p>
<p>2、非交互式的登录shell （bash –l xxx.sh）<br>载入的信息：<br>&#x2F;etc&#x2F;profile<br>~&#x2F;.bash_profile （ -&gt;  ~&#x2F;.bashrc  -&gt;  &#x2F;etc&#x2F;bashrc）<br>~&#x2F;.bash_login<br>~&#x2F;.profile<br>$BASH_ENV</p>
<p>3、交互式的非登录shell （bash –i xxx.sh）<br>载入的信息：<br>~&#x2F;.bashrc （ -&gt;  &#x2F;etc&#x2F;bashrc）</p>
<p>4、非交互式的非登录shell （bash xxx.sh）<br>载入的信息：<br>$BASH_ENV</p>
<h3 id="SH"><a href="#SH" class="headerlink" title="SH"></a>SH</h3><p>1、交互式的登录shell<br>载入的信息：<br>&#x2F;etc&#x2F;profile<br>~&#x2F;.profile</p>
<p>2、非交互式的登录shell<br>载入的信息：<br>&#x2F;etc&#x2F;profile<br>~&#x2F;.profile</p>
<p>3、交互式的非登录shell<br>载入的信息：<br>$ENV</p>
<h4 id="练习验证一下bash、sh和login、non-login"><a href="#练习验证一下bash、sh和login、non-login" class="headerlink" title="练习验证一下bash、sh和login、non-login"></a>练习验证一下bash、sh和login、non-login</h4><ul>
<li>sudo ll 或者 sudo cd 是不是都报找不到命令</li>
<li>先sudo bash 然后执行 ll或者cd就可以了</li>
<li>先sudo sh   然后执行 ll或者cd还是报找不到命令</li>
<li>sudo env | grep PATH 然后 sudo bash 后再执行 env | grep PATH 看到的PATH环境变量不一样了</li>
</ul>
<p><strong>找不到ll、cd命令不是因为login&#x2F;non-login而是因为这两个命令是bash内部定义的，所以sh找不到，通过type -a cd 可以看到一个命令到底是哪里来的</strong></p>
<p>4、非交互式的非登录shell<br>载入的信息：<br>nothing</p>
<h2 id="history-为什么没有输出"><a href="#history-为什么没有输出" class="headerlink" title="history 为什么没有输出"></a>history 为什么没有输出</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#cat test.sh</span><br><span class="line">echo $$</span><br><span class="line">pwd</span><br><span class="line">echo &quot;abc&quot;</span><br><span class="line">history | tail -5</span><br></pre></td></tr></table></figure>

<p>执行如上测试文件，为什么pwd 的内容有输出，但是第五行的 history 确是空的？效果如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#bash test.sh  //为什么这样 history 没有输出</span><br><span class="line">31147</span><br><span class="line">/root</span><br><span class="line">abc</span><br><span class="line"></span><br><span class="line">#./test.sh       //这样 history 有输出</span><br><span class="line">31151</span><br><span class="line">/root</span><br><span class="line">abc</span><br><span class="line"> 8596  17/04/24 16:02:35 strace -p 30643</span><br><span class="line"> 8597  17/04/24 16:15:18 cat test.sh</span><br><span class="line"> 8598  17/04/24 16:15:22 vi test.sh</span><br><span class="line"> 8599  17/04/24 16:15:44 bash test.sh</span><br><span class="line"> 8600  17/04/24 16:15:48 ./test.sh</span><br><span class="line"></span><br><span class="line">#source ./test.sh  //这样 history 有输出</span><br><span class="line">25179</span><br><span class="line">/root</span><br><span class="line">abc</span><br><span class="line"> 8589  17/04/24 16:15:18 cat test.sh</span><br><span class="line"> 8590  17/04/24 16:15:22 vi test.sh</span><br><span class="line"> 8591  17/04/24 16:15:44 bash test.sh</span><br><span class="line"> 8592  17/04/24 16:15:48 ./test.sh</span><br><span class="line"> 8593  17/04/24 16:15:56 source ./test.sh</span><br></pre></td></tr></table></figure>

<p>history 是 bash 的内部命令，在非交互环境里默认关闭，可以通过 set -o history 开启。bash test.sh 执行的时候是新启动了一个非交互的bash，然后读入test.sh再执行，在这个新的bash 执行环境下 history 默认是关闭的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#echo &quot;set -o | grep history&quot; | bash</span><br><span class="line">history        	off</span><br><span class="line"></span><br><span class="line">#set -o |grep history</span><br><span class="line">history        	on</span><br></pre></td></tr></table></figure>

<p>如果上面的脚本修改一下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#cat test.sh</span><br><span class="line">set -o history #打开history 功能</span><br><span class="line">echo $$</span><br><span class="line">pwd</span><br><span class="line">echo &quot;abc&quot;</span><br><span class="line">history | tail -5</span><br></pre></td></tr></table></figure>

<p>然后即使用 bash .&#x2F;test.sh 也能看到history 的输出了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#bash test.sh</span><br><span class="line">31463</span><br><span class="line">/root</span><br><span class="line">abc</span><br><span class="line">    1  17/04/24 16:24:25 echo $$</span><br><span class="line">    2  17/04/24 16:24:25 echo &quot;abc&quot;</span><br><span class="line">    3  17/04/24 16:24:25 history | tail -5</span><br></pre></td></tr></table></figure>

<h3 id="export命令的作用"><a href="#export命令的作用" class="headerlink" title="export命令的作用"></a>export命令的作用</h3><p>Linux 中export是一种命令工具通过export命令把shell变量中包含的用户变量导入给子程序.<strong>默认情况下子程序仅会继承父程序的环境变量</strong>，子程序不会继承父程序的自定义变量，所以需要export让父程序中的<strong>自定义变量</strong>变成环境变量，然后子程序就能继承过来了。</p>
<p>我们来看一个例子， 有一个变量，名字 abc 内容123 如果没有export ，那么通过bash创建一个新的shell（新shell是之前bash的子程序），在新的shell里面就没有abc这个变量， export之后在新的 shell 里面才可以看到这个变量，但是退出重新login后（产生了一个新的bash，只会加载env）abc变量都不在了</p>
<pre><code>$echo $abc
$abc=&quot;123&quot;
$echo $abc
123
$bash
$echo $abc

$exit
exit

$export abc

$echo $abc
123

$bash

$echo $abc
123
</code></pre>
<h2 id="一些常见问题"><a href="#一些常见问题" class="headerlink" title="一些常见问题"></a>一些常见问题</h2><h3 id="执行好好地shell-脚本换台服务器就：source-not-found"><a href="#执行好好地shell-脚本换台服务器就：source-not-found" class="headerlink" title="执行好好地shell 脚本换台服务器就：source: not found"></a>执行好好地shell 脚本换台服务器就：source: not found</h3><p>source 是bash的一个内建命令（所以你找不到一个&#x2F;bin&#x2F;source 这样的可执行文件），也就是他是bash自带的，如果我们执行脚本是这样： sh shell.sh 而shell.sh中用到了source命令的话就会报 source: not found</p>
<p>这是因为bash 和 sh是两个东西，sh是 POSIX shell，你可以把它看成是一个兼容某个规范的shell，而bash是 Bourne-Again shell script， bash是 POSIX shell的扩展，就是bash支持所有符合POSIX shell的规范，但是反过来就不一定了，而这里的 source 恰好就是 bash内建的，不符合 POSIX shell的规范（<strong>POSIX shell 中用 . 来代替source</strong>)</p>
<blockquote>
<p><a href="https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Bourne-Shell-Builtins" target="_blank" rel="noopener">. (a period)</a></p>
<p>. filename [arguments]</p>
<p>Read and execute commands from the filename argument in the current shell context. If filename does not contain a slash, the <code>PATH</code> variable is used to find filename. When Bash is not in POSIX mode, the current directory is searched if filename is not found in <code>$PATH</code>. If any arguments are supplied, they become the positional parameters when filename is executed. Otherwise the positional parameters are unchanged. If the -T option is enabled, <code>source</code> inherits any trap on <code>DEBUG</code>; if it is not, any <code>DEBUG</code> trap string is saved and restored around the call to <code>source</code>, and <code>source</code> unsets the <code>DEBUG</code> trap while it executes. If -T is not set, and the sourced file changes the <code>DEBUG</code> trap, the new value is retained when <code>source</code> completes. The return status is the exit status of the last command executed, or zero if no commands are executed. If filename is not found, or cannot be read, the return status is non-zero. This builtin is equivalent to <code>source</code>.</p>
</blockquote>
<h3 id="在centos执行好好的脚本放到Ubuntu上就不行了，报语法错误"><a href="#在centos执行好好的脚本放到Ubuntu上就不行了，报语法错误" class="headerlink" title="在centos执行好好的脚本放到Ubuntu上就不行了，报语法错误"></a>在centos执行好好的脚本放到Ubuntu上就不行了，报语法错误</h3><p>同上，如果到ubuntu上用 bash shell.sh是可以的，但是sh shell.sh就报语法错误，但是在centos上执行：sh或者bash shell.sh 都可以通过。 在centos上执行 ls -lh &#x2F;usr&#x2F;bin&#x2F;sh 可以看到 &#x2F;usr&#x2F;bin&#x2F;sh link到了 &#x2F;usr&#x2F;bin&#x2F;bash 也就是sh等同于bash，所以都可以通过不足为奇。 </p>
<p>但是在ubuntu上执行 ls -lh &#x2F;usr&#x2F;bin&#x2F;sh 可以看到 &#x2F;usr&#x2F;bin&#x2F;sh link到了 <strong>&#x2F;usr&#x2F;bin&#x2F;dash</strong> ， 这就是为什么ubuntu上会报错</p>
<h3 id="source-shell-sh-和-bash-shell-sh以及-x2F-shell-sh的区别"><a href="#source-shell-sh-和-bash-shell-sh以及-x2F-shell-sh的区别" class="headerlink" title="source shell.sh 和 bash shell.sh以及 .&#x2F;shell.sh的区别"></a>source shell.sh 和 bash shell.sh以及 .&#x2F;shell.sh的区别</h3><p>source shell.sh就在本shell中展开执行<br>bash shell.sh表示在本shell启动一个子程序（bash），在子程序中执行 shell.sh (shell.sh中产生的一些环境变量就没法带回父shell进程了)， 只需要有读 shell.sh 权限就可以执行<br>.&#x2F;shell.sh 跟bash shell.sh类似，但是必须要求shell.sh有rx权限，然后根据shell.sh前面的 #! 后面的指示来确定用bash还是sh </p>
<pre><code>$cat test.sh 
echo $$

$echo $$
2299

$source test.sh 
2299

$bash test.sh 
4037

$./test.sh 
4040
</code></pre>
<p>如上实例，只有source的时候进程ID和bash进程ID一样，其它方式都创建了一个新的bash进程，所以ID也变了。</p>
<p>bash test.sh 产生一个新的bash，但是这个新的bash中不会加载 .bashrc 需要加载的话必须 bash -l test.sh.</p>
<h3 id="通过ssh-执行命令（命令前有sudo）的时候报错：sudo-sorry-you-must-have-a-tty-to-run-sudo"><a href="#通过ssh-执行命令（命令前有sudo）的时候报错：sudo-sorry-you-must-have-a-tty-to-run-sudo" class="headerlink" title="通过ssh 执行命令（命令前有sudo）的时候报错：sudo: sorry, you must have a tty to run sudo"></a>通过ssh 执行命令（命令前有sudo）的时候报错：sudo: sorry, you must have a tty to run sudo</h3><p>这是因为 &#x2F;etc&#x2F;sudoers (Linux控制sudo行为、权限的配置文件）中指定了 requiretty（<a href="https://www.shell-tips.com/2014/09/08/sudo-sorry-you-must-have-a-tty-to-run-sudo/" target="_blank" rel="noopener">Redhat、Fedora默认行为</a>），但是 通过ssh远程执行命令是没有tty的（不需要交互）。<br>解决办法可以试试 ssh -t or -tt (强制分配tty）或者先修改 &#x2F;etc&#x2F;sudoers把 requiretty 删掉或者改成 !requiretty</p>
<h3 id="cp-命令即使使用了-f-force参数，overwrite的时候还是弹出交互信息，必须手工输入Y、yes等"><a href="#cp-命令即使使用了-f-force参数，overwrite的时候还是弹出交互信息，必须手工输入Y、yes等" class="headerlink" title="cp 命令即使使用了 -f force参数，overwrite的时候还是弹出交互信息，必须手工输入Y、yes等"></a>cp 命令即使使用了 -f force参数，overwrite的时候还是弹出交互信息，必须手工输入Y、yes等</h3><p>Google搜索一下别人给出的方案是这样 echo yes | cp -rf xxx yyy 算是笨办法，但是没有找到这里为什么-f 不管用。<br>type -a cp 先确认一下 cp到底是个什么东西：</p>
<pre><code>    #type -a cp
    cp is aliased to `cp -i&#39;
    cp is /usr/bin/cp
</code></pre>
<p>这下算是有点清楚了，原来默认cp 都是-i了（-i, –interactive prompt before overwrite (overrides a previous -n option)），看起来就是默认情况下为了保护我们的目录不经意间被修改了。所以真的确认要overwrite的话直接用 &#x2F;usr&#x2F;bin&#x2F;cp -f 就不需要每次yes确认了</p>
<h3 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h3><p>sudo docker logs swarm-agent-master &gt;master.log 2&gt;&amp;1 输出重定向<a href="http://www.kissyu.org/2016/12/25/shell%E4%B8%AD%3E%20:dev:null%202%20%3E%20&1%E6%98%AF%E4%BB%80%E4%B9%88%E9%AC%BC%EF%BC%9F/" target="_blank" rel="noopener">http://www.kissyu.org/2016/12/25/shell%E4%B8%AD%3E%20:dev:null%202%20%3E%20&amp;1%E6%98%AF%E4%BB%80%E4%B9%88%E9%AC%BC%EF%BC%9F/</a></p>
<pre><code>&gt;/dev/null 2&gt;&amp;1 标准输出丢弃 错误输出丢弃
2&gt;&amp;1 &gt;/dev/null 标准输出丢弃 错误输出屏幕
</code></pre>
<p><a href="http://kodango.com/bash-one-liners-explained-part-three" target="_blank" rel="noopener">http://kodango.com/bash-one-liners-explained-part-three</a></p>
<h3 id="umask"><a href="#umask" class="headerlink" title="umask"></a>umask</h3><p>创建文件的默认权限是 666 文件夹是777 但是都要跟 umask做运算（按位减法） 一般umask是002<br>所以创建出来文件最终是664，文件夹是775，如果umask 是027的话最终文件是 640 文件夹是750<br>『尽量不要以数字相加减啦！』你应该要这样想(-rw-rw- rw-) – (——–wx)&#x3D;-rw-rw-r–这样就对啦！不要用十进制的数字喔！够能力的话，用二进制来算，不晓得的话，用 rwx 来算喔！</p>
<h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><pre><code>echo $-   // himBH 
</code></pre>
<p>“$-” 中含有“i”代表“交互式shell”<br>“$0”的显示结果为“-bash”，bash前面多个“-”，代表“登录shell”.<br>没有“i“和“-”的，是“非交互式的非登录shell”</p>
<p>set +o histexpand （！ 是history展开符号， histexpand 可以打开或者关闭这个展开符）<br>alias 之后，想要用原来的命令：+alias  （命令前加)</p>
<p>bash程序执行，当“$0”是“sh”的时候，则要求下面的代码遵循一定的规范，当不符合规范的语法存在时，则会报错，所以可以这样理解，“sh”并不是一个程序，而是一种标准（POSIX），这种标准，在一定程度上（具体区别见下面的“Things bash has that sh does not”）保证了脚本的跨系统性（跨UNIX系统）</p>
<p>Linux 分 shell变量(set)，用户变量(env)， shell变量包含用户变量，export是一种命令工具，是显式那些通过export命令把shell变量中包含的用户变量导入给用户变量的那些变量.</p>
<p>set -euxo pipefail &#x2F;&#x2F;-u unset -e 异常退出  <a href="http://www.ruanyifeng.com/blog/2017/11/bash-set.html" target="_blank" rel="noopener">http://www.ruanyifeng.com/blog/2017/11/bash-set.html</a></p>
<h3 id="引号"><a href="#引号" class="headerlink" title="引号"></a>引号</h3><p>shell 中：单引号的处理是比较简单的，被单引号包括的所有字符都保留原有的意思，例如’$a’不会被展开, ‘<code>cmd</code>‘也不会执行命令；而双引号，则相对比较松，在双引号中，以下几个字符 $, `, \ 依然有其特殊的含义，比如$可以用于变量展开, 反引号`可以执行命令，反斜杠\可以用于转义。但是，在双引号包围的字符串里，反斜杠的转义也是有限的，它只能转义$, &#96;, “, \或者newline（回车）这几个字符，后面如果跟着的不是这几个字符，只不会被黑底，反斜杠会被保留  <a href="http://kodango.com/simple-bash-programming-skills-2" target="_blank" rel="noopener">http://kodango.com/simple-bash-programming-skills-2</a></p>
<h3 id="su-和-su-的区别"><a href="#su-和-su-的区别" class="headerlink" title="su 和 su - 的区别"></a>su 和 su - 的区别</h3><p>su命令和su -命令最大的本质区别就是：前者只是切换了root身份，但Shell环境仍然是普通用户的Shell；而后者连用户和Shell环境一起切换成root身份了。只有切换了Shell环境才不会出现PATH环境变量错误。su切换成root用户以后，pwd一下，发现工作目录仍然是普通用户的工作目录；而用su -命令切换以后，工作目录变成root的工作目录了。用echo $PATH命令看一下su和su -以后的环境变量有何不同。以此类推，要从当前用户切换到其它用户也一样，应该使用su -命令。</p>
<p>比如：<br>   su admin 会重新加载 ~&#x2F;.bashrc ，但是不会切换到admin 的home目录。<br>   但是 su - admin 不会重新加载 ~&#x2F;.bashrc ，但是会切换admin的home目录。</p>
<p>The su command is used to become another user during a login session. Invoked without a username, su defaults to becoming the superuser. The optional argument - may be used to provide an environment similar to what the user would expect had the user logged in directly.</p>
<h3 id="后台任务执行"><a href="#后台任务执行" class="headerlink" title="后台任务执行"></a>后台任务执行</h3><p>将任务放到后台，断开ssh后还能运行：<br>“ctrl-Z”将当前任务挂起（实际是发送 SIGTSTP 信号），父进程ssh退出时会给所有子进程发送 SIGHUP；</p>
<p>jobs -l 查看所有job</p>
<p>“disown -h %序号” 让该任务忽略SIGHUP信号（不会因为掉线而终止执行），序号为 Jobs -l 看到的顺序号；<br>“bg”让该任务在后台恢复运行。</p>
<h2 id="shell-调试与参数"><a href="#shell-调试与参数" class="headerlink" title="shell 调试与参数"></a>shell 调试与参数</h2><p>为了方便 Debug，有时在启动 Bash 的时候，可以加上启动参数。</p>
<ul>
<li><code>-n</code>：不运行脚本，只检查是否有语法错误。</li>
<li><code>-v</code>：输出每一行语句运行结果前，会先输出该行语句。</li>
<li><code>-x</code>：每一个命令处理完以后，先输出该命令，再进行下一个命令的处理。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bash -n scriptname</span><br><span class="line">$ bash -v scriptname</span><br><span class="line">$ bash -x scriptname</span><br></pre></td></tr></table></figure>

<h2 id="shell-数值运算"><a href="#shell-数值运算" class="headerlink" title="shell 数值运算"></a>shell 数值运算</h2><p>bash中数值运算要这样 $(( $a+$b )) &#x2F;&#x2F; declare -i 才是定义一个整型变量</p>
<ul>
<li>在中括号 [] 内的每个组件都需要有空白键来分隔；</li>
<li>在中括号内的变量，最好都以双引号括号起来；</li>
<li>在中括号内的常数，最好都以单或双引号括号起来。</li>
</ul>
<p>在bash中为变量赋值的语法是<code>foo=bar</code>，访问变量中存储的数值，其语法为 <code>$foo</code>。 需要注意的是，<code>foo = bar</code> （使用空格隔开）是不能正确工作的，因为解释器会调用程序<code>foo</code> 并将 <code>=</code> 和 <code>bar</code>作为参数。 总的来说，在shell脚本中使用空格会起到分割参数的作用，有时候可能会造成混淆，请务必多加检查。</p>
<h2 id="其它-1"><a href="#其它-1" class="headerlink" title="其它"></a>其它</h2><ul>
<li>系统合法的 shell 均写在 &#x2F;etc&#x2F;shells 文件中；</li>
<li>用户默认登陆取得的 shell 记录于 &#x2F;etc&#x2F;passwd 的最后一个字段；</li>
<li>type 可以用来找到运行命令为何种类型，亦可用于与 which 相同的功能 [<strong>type -a</strong>]；</li>
<li>变量主要有环境变量与自定义变量，或称为全局变量与局部变量</li>
<li>使用 env 与 export 可观察环境变量，其中 export 可以将自定义变量转成环境变量；</li>
<li>set 可以观察目前 bash 环境下的所有变量；</li>
<li>stty -a</li>
<li><strong>$? 亦为变量，是前一个命令运行完毕后的回传值</strong>。在 Linux 回传值为 0 代表运行成功；</li>
<li>bash 的配置文件主要分为 login shell 与 non-login shell。login shell 主要读取 &#x2F;etc&#x2F;profile 与 ~&#x2F;.bash_profile， non-login shell 则仅读取 ~&#x2F;.bashrc</li>
</ul>
<p>在bash中进行比较时，尽量使用双方括号 <code>[[ ]]</code> 而不是单方括号 <code>[ ]</code>，<a href="http://mywiki.wooledge.org/BashFAQ/031" target="_blank" rel="noopener">这样会降低犯错的几率</a>，尽管这样并不能兼容 <code>sh</code></p>
<h3 id="type"><a href="#type" class="headerlink" title="type"></a>type</h3><p><strong>执行顺序(type -a ls 可以查看到顺序)：</strong></p>
<ol>
<li>以相对&#x2F;绝对路径运行命令，例如『 &#x2F;bin&#x2F;ls 』或『 .&#x2F;ls 』；</li>
<li>由 alias 找到该命令来运行；</li>
<li>由 bash 内建的 (builtin) 命令来运行；</li>
<li>透过 $PATH 这个变量的顺序搜寻到的第一个命令来运行。</li>
</ol>
<p><a href="https://tldr.sh/" target="_blank" rel="noopener">tldr 可以用来查询命令的常用语法</a>，比man简短些，偏case型</p>
<h2 id="参考文章："><a href="#参考文章：" class="headerlink" title="参考文章："></a>参考文章：</h2><p><a href="https://blog.csdn.net/u010871982/article/details/78525367" target="_blank" rel="noopener">关于ansible远程执行的环境变量问题</a></p>
<p><a href="http://bbs.chinaunix.net/thread-1068678-1-1.html" target="_blank" rel="noopener">Bash和Sh的区别</a></p>
<p><a href="http://kodango.com/what-is-interactive-and-login-shell" target="_blank" rel="noopener">什么是交互式登录 Shell what-is-interactive-and-login-shell</a></p>
<p><a href="http://kodango.com/explain-shell-default-options" target="_blank" rel="noopener">Shell 默认选项 himBH 的解释</a></p>
<p><a href="http://kodango.com/useful-documents-about-shell" target="_blank" rel="noopener">useful-documents-about-shell</a></p>
<p><a href="http://coolnull.com/4432.html" target="_blank" rel="noopener">linux cp实现强制覆盖</a></p>
<p><a href="https://wangdoc.com/bash/startup.html" target="_blank" rel="noopener">https://wangdoc.com/bash/startup.html</a></p>
<p><a href="https://cjting.me/2020/12/10/tiny-x64-helloworld/" target="_blank" rel="noopener">编写一个最小的 64 位 Hello World</a></p>
<p><a href="https://missing-semester-cn.github.io/" target="_blank" rel="noopener">计算机教育中缺失的一课</a></p>
<p> <a href="https://foxwho.com/article/184" target="_blank" rel="noopener">macOS设置环境变量path&#x2F;paths的完全总结</a> 很详细</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/03/14/如何设置git Proxy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/03/14/如何设置git Proxy/" itemprop="url">Git HTTP Proxy and SSH Proxy</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-14T10:30:03+08:00">
                2018-03-14
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/SSH/" itemprop="url" rel="index">
                    <span itemprop="name">SSH</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何设置git-Proxy"><a href="#如何设置git-Proxy" class="headerlink" title="如何设置git Proxy"></a>如何设置git Proxy</h1><h2 id="git-http-proxy"><a href="#git-http-proxy" class="headerlink" title="git http proxy"></a>git http proxy</h2><blockquote>
<p>首先你要有一个socks5代理服务器，从 github.com 拉代码的话海外的代理速度才快，可以用阿里郎的网络加速，也可以自己配置shadowsocks这样的代理。</p>
<p>Windows 阿里郎会在本地生成socks5代理：127.0.0.1:13658</p>
</blockquote>
<p>下面的例子假设你的socks5代理是： 127.0.0.1:13658</p>
<h3 id="配置git-http-proxy"><a href="#配置git-http-proxy" class="headerlink" title="配置git http proxy"></a>配置git http proxy</h3><pre><code>git config --global http.proxy socks5h://127.0.0.1:13658 //或者 socks5://127.0.0.1:13658
</code></pre>
<p>上面的命令实际上是修改了 .gitconfig：</p>
<pre><code>$cat ~/.gitconfig   
[http]
    proxy = socks5h://127.0.0.1:13658
</code></pre>
<p>现在git的http代理就配置好了， git clone <a href="https://github.com/torvalds/linux.git" target="_blank" rel="noopener">https://github.com/torvalds/linux.git</a> 速度会快到你流泪（取决于你的代理速度），我这里是从每秒10K到了3M 。</p>
<p>注意：</p>
<ul>
<li>http.proxy就可以了，不需要配置https.proxy</li>
<li>这个http代理仅仅针对 git clone <strong>https:&#x2F;&#x2F;</strong> 的方式生效</li>
<li>socks5 本地解析域名；socks5h 将域名也发到远程代理来解析(推荐使用，比如 github.com 在 2024 走 socks5 都无法拉取)</li>
</ul>
<h2 id="配置git-ssh-proxy"><a href="#配置git-ssh-proxy" class="headerlink" title="配置git ssh proxy"></a>配置git ssh proxy</h2><p>如果想要 git clone **git@**github.com:torvalds&#x2F;linux.git 也要快起来的话 需要配置 ssh proxy</p>
<blockquote>
<p>这里要求你有一台海外的服务器，能ssh登陆，做好免密码，假设这台服务器的IP是：2.2.2.2</p>
</blockquote>
<p>修改（如果没有就创建这个文件）~&#x2F;.ssh&#x2F;config, 内容如下：</p>
<pre><code>$cat ~/.ssh/config 
host github.com
#LogLevel DEBUG3
ProxyCommand ssh -l root 2.2.2.2 exec /usr/bin/nc %h %p
</code></pre>
<p>然后 git clone <a href="mailto:&#x67;&#x69;&#116;&#64;&#x67;&#105;&#116;&#104;&#117;&#x62;&#46;&#99;&#111;&#109;" target="_blank" rel="noopener">&#x67;&#x69;&#116;&#64;&#x67;&#105;&#116;&#104;&#117;&#x62;&#46;&#99;&#111;&#109;</a>:torvalds&#x2F;linux.git 也能飞起来了</p>
<p>需要注意你的代理服务器2.2.2.2上nc有没有安装，没有的话yum装上，装上后再检查一下安装的位置，对应配置中的 &#x2F;usr&#x2F;bin&#x2F;nc<br>写这些主要是从Google上搜索到的一些文章，http的倒还是靠谱，但是ssh的就有点乱，还要在本地安装东西，对nc版本有要求之类的，于是就折腾了一下，上面的方式都是靠谱的。</p>
<p>整个原理还是穿墙术。 可以参考 ：<a href="https://www.atatech.org/articles/76026" target="_blank" rel="noopener">SSH 高级用法和技巧大全</a>  </p>
<h3 id="配置git-走socks"><a href="#配置git-走socks" class="headerlink" title="配置git 走socks"></a><a href="https://superuser.com/questions/454210/how-can-i-use-ssh-with-a-socks-5-proxy" target="_blank" rel="noopener">配置git 走socks</a></h3><p>如果没有海外服务器，但是本地已经有了socks5 服务那么也可以直接走socks5来proxy所有git 流量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/config</span><br><span class="line">host github.com</span><br><span class="line">ProxyCommand  /usr/bin/nc -X 5 -x 127.0.0.1:12368 %h %p  //走本地socks5端口来转发代理流量</span><br><span class="line">#ProxyCommand ssh -l root jump exec /usr/bin/nc %h %p    //这个是走 jump</span><br></pre></td></tr></table></figure>

<p>nc 代理参数-X proxy_version 指定 nc 请求时使用代理服务的协议</p>
<ul>
<li><code>proxy_version</code> 为 <code>4</code> : 表示使用的代理为 SOCKS4 代理</li>
<li><code>proxy_version</code> 为 <code>5</code> : 表示使用的代理为 SOCKS5 代理</li>
<li><code>proxy_version</code> 为 <code>connect</code> : 表示使用的代理为 HTTPS 代理</li>
<li>如果不指定协议, 则默认使用的代理为 SOCKS5 代理</li>
</ul>
<blockquote>
<p><strong>-X</strong> <em>proxy_version</em><br>Requests that <strong>nc</strong> should use the specified protocol when talking to the proxy server. Supported protocols are ‘’4’’ (SOCKS v.4), ‘’5’’ (SOCKS v.5) and ‘’connect’’ (HTTPS proxy). If the protocol is not specified, SOCKS version 5 is used.</p>
</blockquote>
<h2 id="我的拉起代理自动脚本"><a href="#我的拉起代理自动脚本" class="headerlink" title="我的拉起代理自动脚本"></a>我的拉起代理自动脚本</h2><p>下面的脚本总共拉起了三个socks5代理，端口13657-13659，其中13659是阿里郎网络加速的代理<br>最后还启动了一个8123的http 代理（有些场景只支持http代理）</p>
<p>macOS：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">listPort=`/usr/sbin/netstat -ant |grep &quot;127.0.0.1.13658&quot; |grep LISTEN`</span><br><span class="line">if [[ &quot;$listPort&quot; != tcp4* ]]; then</span><br><span class="line">    #sh ~/ssh-jump.sh</span><br><span class="line">    nohup ssh -qTfnN -D 13658 root@jump vmstat 10  &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    echo &quot;start socks5 on port 13658&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">listPort=`/usr/sbin/netstat -ant |grep &quot;127.0.0.1.13657&quot; |grep LISTEN`</span><br><span class="line">if [[ &quot;$listPort&quot; != tcp4* ]]; then</span><br><span class="line">    nohup ssh -qTfnN -D 13657 azureuser@yu2 vmstat 10  &gt;/dev/null 2&gt;&amp;1</span><br><span class="line">    echo &quot;start socks5 on port 13657&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">listPort=`/usr/sbin/netstat -ant |grep &quot;127.0.0.1.13659&quot; |grep LISTEN`</span><br><span class="line">#if [ &quot;$listPort&quot; != &quot;tcp4       0      0  127.0.0.1.13659        *.*                    LISTEN     &quot; ]; then</span><br><span class="line">if [[ &quot;$listPort&quot; != tcp4* ]]; then</span><br><span class="line">    Applications/AliLang.app/Contents/Resources/AliMgr/AliMgrSockAgent -bd 参数1 -wd 工号 -td 参数2 &gt;~/jump.log 2&gt;&amp;1</span><br><span class="line">    echo &quot;start listPort $listPort&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">listPort=`/usr/sbin/netstat -ant |grep &quot;127.0.0.1.8123 &quot; |grep LISTEN`</span><br><span class="line">if [[ &quot;$listPort&quot; != tcp4* ]]; then</span><br><span class="line">    polipo socksParentProxy=127.0.0.1:13659 1&gt;~/jump.log 2&gt;1&amp;</span><br><span class="line">    echo &quot;start polipo http proxy at 8123&quot;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#分别测试http和socks5代理能工作</span><br><span class="line">#curl --proxy http://127.0.0.1:8123 https://www.google.com</span><br><span class="line">#curl -x socks5h://localhost:13657 http://www.google.com/</span><br></pre></td></tr></table></figure>


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/02/25/Docker常见问题/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/25/Docker常见问题/" itemprop="url">Docker 常见问题</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-25T17:30:03+08:00">
                2018-02-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Docker-常见问题"><a href="#Docker-常见问题" class="headerlink" title="Docker 常见问题"></a>Docker 常见问题</h1><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><p>docker daemon启动的时候如果报 socket错误，是因为daemon启动参数配置了： -H fd:&#x2F;&#x2F;  ，但是 docker.socket是disable状态，启动daemon依赖socket，但是systemctl又拉不起来docker.socket，因为被disable了，先  sudo systemctl enable docker.socket 就可以了。</p>
<p>如果docker.socket service被mask后比disable更粗暴，mask后手工都不能拉起来了，但是disable后还可以手工拉起，然后再拉起docker service。 这是需要先 systemctl unmask </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$sudo systemctl restart docker.socket</span><br><span class="line">Failed to restart docker.socket: Unit docker.socket is masked.</span><br></pre></td></tr></table></figure>

<p>另外 docker.socket 启动依赖环境的要有 docker group这个组，可以添加： groupadd docker</p>
<h2 id="failed-to-start-docker-service-unit-not-found-rhel-7-7"><a href="#failed-to-start-docker-service-unit-not-found-rhel-7-7" class="headerlink" title="failed to start docker.service unit not found. rhel 7.7"></a>failed to start docker.service unit not found. rhel 7.7</h2><p>systemctl list-unit-files |grep docker.service 可以看到docker.service 是存在并enable了</p>
<p>实际是redhat 7.7的yum仓库所带的docker启动参数变了， 如果手工启动的话也会报找不到docker-runc 手工:</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; ln -s /usr/libexec/docker/docker-runc-current /usr/bin/docker-runc</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p><a href="https://access.redhat.com/solutions/2876431" target="_blank" rel="noopener">https://access.redhat.com/solutions/2876431</a>  <a href="https://stackoverflow.com/questions/42754779/docker-runc-not-installed-on-system" target="_blank" rel="noopener">https://stackoverflow.com/questions/42754779/docker-runc-not-installed-on-system</a></p>
<p>yum安装docker会在 &#x2F;etc&#x2F;sysconfig 下放一些配置参数(docker.service 环境变量)</p>
<h3 id="Docker-启动报错：-Error-starting-daemon：-Error-initializing-network-controller：-list-bridge-addresses-failed：-no-available-network"><a href="#Docker-启动报错：-Error-starting-daemon：-Error-initializing-network-controller：-list-bridge-addresses-failed：-no-available-network" class="headerlink" title="Docker 启动报错： Error starting daemon： Error initializing network controller： list bridge addresses failed： no available network"></a><a href="http://blog.joylau.cn/2019/04/08/Docker-Start-Error/" target="_blank" rel="noopener">Docker 启动报错： Error starting daemon： Error initializing network controller： list bridge addresses failed： no available network</a></h3><p>这是因为daemon启动的时候缺少docker0网桥，导致启动失败，手工添加：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link add docker0 type bridge</span><br><span class="line">ip addr add dev docker0 172.30.0.0/24</span><br></pre></td></tr></table></figure>

<p>启动成功后即使手工删除docker0，然后再次启动也会成功，这次会自动创建docker0 172.30.0.0&#x2F;16 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#systemctl status docker -l</span><br><span class="line">● docker.service - Docker Application Container Engine</span><br><span class="line">   Loaded: loaded (/etc/systemd/system/docker.service; enabled; vendor preset: disabled)</span><br><span class="line">   Active: failed (Result: exit-code) since Fri 2021-01-22 17:21:45 CST; 2min 12s ago</span><br><span class="line">     Docs: http://docs.docker.io</span><br><span class="line">  Process: 68318 ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT (code=exited, status=0/SUCCESS)</span><br><span class="line">  Process: 68317 ExecStart=/opt/kube/bin/dockerd (code=exited, status=1/FAILURE)</span><br><span class="line"> Main PID: 68317 (code=exited, status=1/FAILURE)</span><br><span class="line"></span><br><span class="line">Jan 22 17:21:43 l57f12112.sqa.nu8 dockerd[68317]: time=&quot;2021-01-22T17:21:43.991179104+08:00&quot; level=warning msg=&quot;failed to load plugin io.containerd.snapshotter.v1.aufs&quot; error=&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">Jan 22 17:21:43 l57f12112.sqa.nu8 dockerd[68317]: time=&quot;2021-01-22T17:21:43.991371956+08:00&quot; level=warning msg=&quot;could not use snapshotter btrfs in metadata plugin&quot; error=&quot;path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.btrfs must be a btrfs filesystem to be used with the btrfs snapshotter&quot;</span><br><span class="line">Jan 22 17:21:43 l57f12112.sqa.nu8 dockerd[68317]: time=&quot;2021-01-22T17:21:43.991381620+08:00&quot; level=warning msg=&quot;could not use snapshotter aufs in metadata plugin&quot; error=&quot;modprobe aufs failed: &quot;modprobe: FATAL: Module aufs not found.\n&quot;: exit status 1&quot;</span><br><span class="line">Jan 22 17:21:43 l57f12112.sqa.nu8 dockerd[68317]: time=&quot;2021-01-22T17:21:43.991388991+08:00&quot; level=warning msg=&quot;could not use snapshotter zfs in metadata plugin&quot; error=&quot;path /var/lib/docker/containerd/daemon/io.containerd.snapshotter.v1.zfs must be a zfs filesystem to be used with the zfs snapshotter: skip plugin&quot;</span><br><span class="line">Jan 22 17:21:44 l57f12112.sqa.nu8 systemd[1]: Stopping Docker Application Container Engine...</span><br><span class="line">Jan 22 17:21:45 l57f12112.sqa.nu8 dockerd[68317]: failed to start daemon: Error initializing network controller: list bridge addresses failed: PredefinedLocalScopeDefaultNetworks List: [172.17.0.0/16 172.18.0.0/16 172.19.0.0/16 172.20.0.0/16 172.21.0.0/16 172.22.0.0/16 172.23.0.0/16 172.24.0.0/16 172.25.0.0/16 172.26.0.0/16 172.27.0.0/16 172.28.0.0/16 172.29.0.0/16 172.30.0.0/16 172.31.0.0/16 192.168.0.0/20 192.168.16.0/20 192.168.32.0/20 192.168.48.0/20 192.168.64.0/20 192.168.80.0/20 192.168.96.0/20 192.168.112.0/20 192.168.128.0/20 192.168.144.0/20 192.168.160.0/20 192.168.176.0/20 192.168.192.0/20 192.168.208.0/20 192.168.224.0/20 192.168.240.0/20]: no available network</span><br><span class="line">Jan 22 17:21:45 l57f12112.sqa.nu8 systemd[1]: docker.service: main process exited, code=exited, status=1/FAILURE</span><br><span class="line">Jan 22 17:21:45 l57f12112.sqa.nu8 systemd[1]: Stopped Docker Application Container Engine.</span><br><span class="line">Jan 22 17:21:45 l57f12112.sqa.nu8 systemd[1]: Unit docker.service entered failed state.</span><br><span class="line">Jan 22 17:21:45 l57f12112.sqa.nu8 systemd[1]: docker.service failed.</span><br></pre></td></tr></table></figure>

<p>参考：<a href="https://github.com/docker/for-linux/issues/123" target="_blank" rel="noopener">https://github.com/docker/for-linux/issues/123</a>  </p>
<p>或者这样解决：<a href="https://stackoverflow.com/questions/39617387/docker-daemon-cant-initialize-network-controller" target="_blank" rel="noopener">https://stackoverflow.com/questions/39617387/docker-daemon-cant-initialize-network-controller</a></p>
<p>This was related to the machine having several network cards (can also happen in machines with VPN)</p>
<p>The solution was to start manually docker like this:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/usr/bin/docker daemon --debug --bip=192.168.y.x/24</span><br></pre></td></tr></table></figure>

<p>where the 192.168.y.x is the MAIN machine IP and &#x2F;24 that ip netmask. Docker will use this network range for building the bridge and firewall riles. The –debug is not really needed, but might help if something else fails.</p>
<p>After starting once, you can kill the docker and start as usual. AFAIK, docker have created a cache config for that –bip and should work now without it. Of course, if you clean the docker cache, you may need to do this again. </p>
<p>本机网络信息默认保存在：&#x2F;var&#x2F;lib&#x2F;docker&#x2F;network&#x2F;files&#x2F;local-kv.db  想要清理bridge网络的话，不能直接 docker network rm bridge 因为bridge是预创建的受保护不能直接删除，可以删掉：&#x2F;var&#x2F;lib&#x2F;docker&#x2F;network&#x2F;files&#x2F;local-kv.db 并且同时删掉 docker0 然后重启dockerd就可以了</p>
<h3 id="alios下容器里面ping不通docker0"><a href="#alios下容器里面ping不通docker0" class="headerlink" title="alios下容器里面ping不通docker0"></a>alios下容器里面ping不通docker0</h3><p>alios上跑docker，然后启动容器，发现容器里面ping不通docker0, 手工重新brctl addbr docker0 , 然后把虚拟网卡加进去就可以了。应该是系统哪里bug了. </p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2ba8bc014d93ad4b6e77c889a024772f.png" alt="image.png"></p>
<p>非常神奇的是不通的时候如果在宿主机上对docker0抓包就瞬间通了，停掉抓包就不通</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/dbc4dac5a9a0289b58952375c5759b15.gif" alt="docker0-tcpdump.gif"></p>
<p>猜测是 alios 的bug</p>
<h2 id="systemctl-start-docker"><a href="#systemctl-start-docker" class="headerlink" title="systemctl start docker"></a>systemctl start docker</h2><p>Failed to start docker.service: Unit not found.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">UNIT LOAD PATH</span><br><span class="line">          Unit files are loaded from a set of paths determined during </span><br><span class="line">          compilation, described in the two tables below. Unit files found </span><br><span class="line">          in directories listed earlier override files with the same name </span><br><span class="line">          in directories lower in the list.</span><br><span class="line"></span><br><span class="line">           Table 1.  Load path when running in system mode (--system).</span><br><span class="line">           ┌────────────────────────┬─────────────────────────────┐</span><br><span class="line">           │Path                    │ Description                 │</span><br><span class="line">           ├────────────────────────┼─────────────────────────────┤</span><br><span class="line">           │/etc/systemd/system     │ Local configuration         │</span><br><span class="line">           ├────────────────────────┼─────────────────────────────┤</span><br><span class="line">           │/run/systemd/system     │ Runtime units               │</span><br><span class="line">           ├────────────────────────┼─────────────────────────────┤</span><br><span class="line">           │/usr/lib/systemd/system │ Units of installed packages │</span><br><span class="line">           └────────────────────────┴─────────────────────────────┘</span><br></pre></td></tr></table></figure>

<p><a href="https://askubuntu.com/questions/1014480/how-do-i-add-bin-to-path-for-a-systemd-service" target="_blank" rel="noopener">systemd 设置path环境变量，可以设置</a>：</p>
<blockquote>
<p>[Service]<br>Type&#x3D;notify<br>Environment&#x3D;PATH&#x3D;&#x2F;opt&#x2F;kube&#x2F;bin:&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;sbin:&#x2F;usr&#x2F;local&#x2F;bin:&#x2F;usr&#x2F;sbin:&#x2F;usr&#x2F;bin:&#x2F;usr&#x2F;X11R6&#x2F;bin:&#x2F;opt&#x2F;satools:&#x2F;root&#x2F;bin</p>
</blockquote>
<h2 id="容器没有systemctl"><a href="#容器没有systemctl" class="headerlink" title="容器没有systemctl"></a>容器没有systemctl</h2><p><strong>Failed to get D-Bus connection: Operation not permitted: systemd容器中默认无法启动，需要启动容器的时候</strong> </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -itd --privileged --name=ren drds_base:centos init //init 必须要或者systemd</span><br></pre></td></tr></table></figure>

<p>1号进程需要是systemd(init 是systemd的link)，才可以使用systemctl，推荐用这个来解决：<a href="https://github.com/gdraheim/docker-systemctl-replacement" target="_blank" rel="noopener">https://github.com/gdraheim/docker-systemctl-replacement</a></p>
<p>systemd是用来取代init的，之前init管理所有进程启动，是串行的，耗时久，也不管最终状态，systemd主要是串行并监控进程状态能反复重启。</p>
<p><strong>新版本init link向了systemd</strong></p>
<h2 id="busybox-x2F-Alpine-x2F-Scratch"><a href="#busybox-x2F-Alpine-x2F-Scratch" class="headerlink" title="busybox&#x2F;Alpine&#x2F;Scratch"></a>busybox&#x2F;Alpine&#x2F;Scratch</h2><p>busybox集成了常用的linux工具(nc&#x2F;telnet&#x2F;cat……），保持精细，方便一张软盘能装下。</p>
<p>Alpine一个精简版的Linux 发行版，更小更安全，用的musl libc而不是glibc</p>
<p>scratch一个空的框架，什么也没有</p>
<h2 id="找不到shell"><a href="#找不到shell" class="headerlink" title="找不到shell"></a>找不到shell</h2><p>Dockerfile 中(<a href="https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html)%EF%BC%9A" target="_blank" rel="noopener">https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html)：</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CMD ./hello OR RUN 等同于 /bin/sh -c &quot;./hello&quot;, 需要shell，</span><br><span class="line">改用：</span><br><span class="line">CMD [&quot;./hello&quot;] 等同于 ./hello 不需要shell</span><br></pre></td></tr></table></figure>

<h2 id="entrypoint-VS-cmd"><a href="#entrypoint-VS-cmd" class="headerlink" title="entrypoint VS cmd"></a>entrypoint VS cmd</h2><p>dockerfile中：CMD 可以是命令、也可以是参数，如果是参数， 把它传递给：ENTRYPOINT</p>
<p>在写Dockerfile时, ENTRYPOINT或者CMD命令会自动覆盖之前的ENTRYPOINT或者CMD命令</p>
<p>从参数中传入的ENTRYPOINT或者CMD命令会自动覆盖Dockerfile中的ENTRYPOINT或者CMD命令</p>
<h2 id="copy-VS-add"><a href="#copy-VS-add" class="headerlink" title="copy VS add"></a>copy VS add</h2><p><strong>COPY</strong>指令和<strong>ADD</strong>指令的唯一区别在于是否支持从远程URL获取资源。 <strong>COPY</strong>指令只能从执行<strong>docker</strong> build所在的主机上读取资源并复制到镜像中。 而<strong>ADD</strong>指令还支持通过URL从远程服务器读取资源并复制到镜像中。 </p>
<p>满足同等功能的情况下，推荐使用<strong>COPY</strong>指令。ADD指令更擅长读取本地tar文件并解压缩</p>
<h2 id="Digest-VS-Image-ID"><a href="#Digest-VS-Image-ID" class="headerlink" title="Digest VS Image ID"></a>Digest VS Image ID</h2><p>pull镜像的时候，将docker digest带上，即使黑客使用手段将某一个digest对应的内容强行修改了，docker也能check出来，因为docker会在pull下镜像的时候，只要根据image的内容计算sha256</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images --digests</span><br></pre></td></tr></table></figure>

<ul>
<li>The “digest” is a hash of the manifest, introduced in Docker registry v2.</li>
<li>The image ID is a hash of the local image JSON configuration. 就是inspect 看到的 RepoDigests</li>
</ul>
<h2 id="容器中抓包和调试-–-nsenter"><a href="#容器中抓包和调试-–-nsenter" class="headerlink" title="容器中抓包和调试 – nsenter"></a>容器中抓包和调试 – nsenter</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">获取pid：docker inspect -f &#123;&#123;.State.Pid&#125;&#125; c8f874efea06</span><br><span class="line"></span><br><span class="line">进入namespace：nsenter --target 17277 --net --pid –mount</span><br><span class="line"></span><br><span class="line">//只进入network namespace，这样看到的文件还是宿主机的，能直接用tcpdump，但是看到的网卡是容器的</span><br><span class="line">nsenter --target 17277 --net </span><br><span class="line"></span><br><span class="line">// ip netns 获取容器网络信息</span><br><span class="line"> 1022  [2021-04-14 15:53:06] docker inspect -f &apos;&#123;&#123;.State.Pid&#125;&#125;&apos; ab4e471edf50   //获取容器进程id</span><br><span class="line"> 1023  [2021-04-14 15:53:30] ls /proc/79828/ns/net</span><br><span class="line"> 1024  [2021-04-14 15:53:57] ln -sfT /proc/79828/ns/net /var/run/netns/ab4e471edf50 //link 以便ip netns List能访问</span><br><span class="line"> </span><br><span class="line">// 宿主机上查看容器ip</span><br><span class="line"> 1026  [2021-04-14 15:54:11] ip netns list</span><br><span class="line"> 1028  [2021-04-14 15:55:19] ip netns exec ab4e471edf50 ifconfig</span><br><span class="line"> </span><br><span class="line"> //nsenter调试网络</span><br><span class="line"> Get the pause container&apos;s sandboxkey: </span><br><span class="line">root@worker01:~# docker inspect k8s_POD_ubuntu-5846f86795-bcbqv_default_ea44489d-3dd4-11e8-bb37-02ecc586c8d5_0 | grep SandboxKey</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/82ec9e32d486&quot;,</span><br><span class="line">root@worker01:~#</span><br><span class="line">Now, using nsenter you can see the container&apos;s information.</span><br><span class="line">root@worker01:~# nsenter --net=/var/run/docker/netns/82ec9e32d486 ip addr show</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">3: eth0@if7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default</span><br><span class="line">   link/ether 0a:58:0a:f4:01:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">   inet 10.244.1.2/24 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">Identify the peer_ifindex, and finally you can see the veth pair endpoint in root namespace.</span><br><span class="line">root@worker01:~# nsenter --net=/var/run/docker/netns/82ec9e32d486 ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">     peer_ifindex: 7</span><br><span class="line">root@worker01:~#</span><br><span class="line">root@worker01:~# ip -d link show | grep &apos;7: veth&apos;</span><br><span class="line">7: veth5e43ca47@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default</span><br><span class="line">root@worker01:~#</span><br></pre></td></tr></table></figure>

<p>nsenter相当于在setns的示例程序之上做了一层封装，使我们无需指定命名空间的文件描述符，而是指定进程号即可，<a href="https://medium.com/@anilkreddyr/kubernetes-with-flannel-understanding-the-networking-part-2-78b53e5364c7" target="_blank" rel="noopener">详细case</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#docker inspect cb7b05d82153 | grep -i SandboxKey   //根据 pause 容器id找network namespace</span><br><span class="line">            &quot;SandboxKey&quot;: &quot;/var/run/docker/netns/d6b2ef3cf886&quot;,</span><br><span class="line"></span><br><span class="line">[root@hygon252 19:00 /root]</span><br><span class="line">#nsenter --net=/var/run/docker/netns/d6b2ef3cf886 ip addr show</span><br><span class="line">3: eth0@if496: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default  //496对应宿主机上的veth编号</span><br><span class="line">    link/ether 1e:95:dd:d9:88:bd brd ff:ff:ff:ff:ff:ff link-netnsid 0</span><br><span class="line">    inet 192.168.3.22/24 brd 192.168.3.255 scope global eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">#nsenter --net=/var/run/docker/netns/d6b2ef3cf886 ethtool -S eth0</span><br><span class="line">NIC statistics:</span><br><span class="line">     peer_ifindex: 496</span><br><span class="line">     </span><br><span class="line">#ip -d -4 addr show cni0</span><br><span class="line">475: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 8e:34:ba:e2:a4:c6 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q bridge_id 8000.8e:34:ba:e2:a4:c6 designated_root 8000.8e:34:ba:e2:a4:c6 root_port 0 root_path_cost 0 topology_change 0 topology_change_detected 0 hello_timer    0.00 tcn_timer    0.00 topology_change_timer    0.00 gc_timer   43.31 vlan_default_pvid 1 vlan_stats_enabled 0 group_fwd_mask 0 group_address 01:80:c2:00:00:00 mcast_snooping 1 mcast_router 1 mcast_query_use_ifaddr 0 mcast_querier 0 mcast_hash_elasticity 4 mcast_hash_max 512 mcast_last_member_count 2 mcast_startup_query_count 2 mcast_last_member_interval 100 mcast_membership_interval 26000 mcast_querier_interval 25500 mcast_query_interval 12500 mcast_query_response_interval 1000 mcast_startup_query_interval 3124 mcast_stats_enabled 0 mcast_igmp_version 2 mcast_mld_version 1 nf_call_iptables 0 nf_call_ip6tables 0 nf_call_arptables 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.1/24 brd 192.168.3.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<h2 id="创建虚拟网卡"><a href="#创建虚拟网卡" class="headerlink" title="创建虚拟网卡"></a>创建虚拟网卡</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">To make this interface you&apos;d first need to make sure that you have the dummy kernel module loaded. You can do this like so:</span><br><span class="line">$ sudo lsmod | grep dummy</span><br><span class="line">$ sudo modprobe dummy</span><br><span class="line">$ sudo lsmod | grep dummy</span><br><span class="line">dummy                  12960  0 </span><br><span class="line">With the driver now loaded you can create what ever dummy network interfaces you like:</span><br><span class="line"></span><br><span class="line">$ sudo ip link add eth10 type dummy</span><br></pre></td></tr></table></figure>

<h2 id="修改网卡名字"><a href="#修改网卡名字" class="headerlink" title="修改网卡名字"></a>修改网卡名字</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ip link set ens33 down</span><br><span class="line">ip link set ens33 name eth0</span><br><span class="line">ip link set eth0 up</span><br><span class="line"></span><br><span class="line">mv /etc/sysconfig/network-scripts/ifcfg-&#123;ens33,eth0&#125;</span><br><span class="line">sed -ire &quot;s/NAME=\&quot;ens33\&quot;/NAME=\&quot;eth0\&quot;/&quot; /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">sed -ire &quot;s/DEVICE=\&quot;ens33\&quot;/DEVICE=\&quot;eth0\&quot;/&quot; /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">MAC=$(cat /sys/class/net/eth0/address)</span><br><span class="line">echo -n &apos;HWADDR=&quot;&apos;$MAC\&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-eth0</span><br></pre></td></tr></table></figure>

<h2 id="OS版本"><a href="#OS版本" class="headerlink" title="OS版本"></a>OS版本</h2><p><strong>搞Docker就得上el7， 6的性能太差了</strong> Docker 对 Linux 内核版本的最低要求是3.10，如果内核版本低于 3.10 会缺少一些运行 Docker 容器的功能。这些比较旧的内核，在一定条件下会导致数据丢失和频繁恐慌错误。</p>
<h2 id="清理mount文件"><a href="#清理mount文件" class="headerlink" title="清理mount文件"></a>清理mount文件</h2><p>删除 &#x2F;var&#x2F;lib&#x2F;docker 目录如果报busy，一般是进程在使用中，可以fuser查看哪个进程在用，然后杀掉进程；另外就是目录mount删不掉问题，可以 mount | awk ‘{ print $3 }’ |grep overlay2| xargs umount 批量删除</p>
<h2 id="No-space-left-on-device"><a href="#No-space-left-on-device" class="headerlink" title="No space left on device"></a><a href="https://www.manjusaka.blog/posts/2023/01/07/special-case-no-space-left-on-device/" target="_blank" rel="noopener">No space left on device</a></h2><p><strong>OSError: [Errno 28] No space left on device</strong>：</p>
<p>​	大部分时候不是真的磁盘没有空间了还有可能是inode不够了(df -ih 查看inode使用率)</p>
<p>​	尝试用 fallocate 来测试创建文件是否成功</p>
<p>​	尝试fdisk-l &#x2F; tune2fs -l 来确认分区和文件系统的正确性</p>
<p>​	fallocate 创建一个文件名很长的文件失败(也就是原始报错的文件名)，同时fallocate 创建一个短文件名的文件成功</p>
<p>​	dmesg 查看系统报错信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[13155344.231942] EXT4-fs warning (device sdd): ext4_dx_add_entry:2461: Directory (ino: 3145729) index full, reach max htree level :2</span><br><span class="line">[13155344.231944] EXT4-fs warning (device sdd): ext4_dx_add_entry:2465: Large directory feature is not enabled on this filesystem</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1673940401316-88667b79-1ba2-43fe-9f92-9e51affc49b0.png" alt="img">	</p>
<p>看起来是小文件太多撑爆了ext4的BTree索引，通过 tune2fs -l &#x2F;dev&#x2F;nvme1n1p1 验证下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#tune2fs -l /dev/nvme1n1p1 |grep Filesystem</span><br><span class="line">Filesystem volume name:   /flash2</span><br><span class="line">Filesystem revision #:    1 (dynamic)</span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent 64bit flex_bg sparse_super large_file huge_file uninit_bg dir_nlink extra_isize</span><br><span class="line">Filesystem flags:         signed_directory_hash</span><br><span class="line">Filesystem state:         clean</span><br><span class="line">Filesystem OS type:       Linux</span><br><span class="line">Filesystem created:       Fri Mar  6 17:08:36 2020</span><br></pre></td></tr></table></figure>

<p>​	执行 <code>tune2fs -O large_dir </code> &#x2F;dev&#x2F;nvme1n1p1 打开 large_dir 选项</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tune2fs -l /dev/nvme1n1p1 |grep -i large</span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery extent flex_bg large_dir sparse_super large_file huge_file uninit_bg dir_nlink extra_isize</span><br></pre></td></tr></table></figure>

<p>如上所示，开启后Filesystem features 多了 large_dir，<a href="https://git.kernel.org/pub/scm/linux/kernel/git/tytso/ext4.git/commit/?h=dev&id=88a399955a97fe58ddb2a46ca5d988caedac731b" target="_blank" rel="noopener">不过4.13以上内核才支持这个功能</a></p>
<h2 id="CPU-资源分配"><a href="#CPU-资源分配" class="headerlink" title="CPU 资源分配"></a>CPU 资源分配</h2><p>对于cpu的限制，Kubernetes采用cfs quota来限制进程在单位时间内可用的时间片。当独享和共享实例在同一台node节点上的时候，一旦实例的工作负载增加，可能会导致独享实例工作负载在不同的cpu核心上来回切换，影响独享实例的性能。所以，为了不影响独享实例的性能，我们希望在同一个node上，独享实例和共享实例的cpu能够分开绑定，互不影响。</p>
<p>内核的默认cpu.shares是1024，也可以通过 cpu.cfs_quota_us &#x2F; cpu.cfs_period_us去控制容器规格(除后的结果就是核数)</p>
<p>cpu.shares 多层级限制后上层有更高的优先级，可能会经常看到 CPU 多核之间不均匀的现象，部分核总是跑不满之类的。  cpu.shares 是用来调配争抢用，比如离线、在线混部可以通过 cpu.shares 多给在线业务</p>
<p>给容器限制16core的quota：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker update --cpu-quota=1600000 --cpu-period=100000 c1 c2</span><br></pre></td></tr></table></figure>

<h2 id="sock"><a href="#sock" class="headerlink" title="sock"></a>sock</h2><p>docker有两个sock，一个是dockershim.sock，一个是docker.sock。dockershim.sock是由实现了CRI接口的一个插件提供的，主要把k8s请求转换成docker请求，最终docker还是要 通过docker.sock来管理容器。</p>
<blockquote>
<p>kubelet —CRI—-&gt; docker-shim(kubelet内置的CRI-plugin) –&gt; docker</p>
</blockquote>
<h2 id="docker-image-api"><a href="#docker-image-api" class="headerlink" title="docker image api"></a>docker image api</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">获取所有镜像名字： GET /v2/_catalog   </span><br><span class="line">curl registry:5000/v2/_catalog</span><br><span class="line"></span><br><span class="line">获取某个镜像的tag： GET /v2/&lt;name&gt;/tags/list  </span><br><span class="line">curl registry:5000/v2/drds/corona-server/tags/list</span><br></pre></td></tr></table></figure>

<h3 id="从registry中删除镜像"><a href="#从registry中删除镜像" class="headerlink" title="从registry中删除镜像"></a>从registry中删除镜像</h3><p>默认registry仓库不支持删除镜像，修改registry配置来支持删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#cat config.yml</span><br><span class="line">version: 0.1</span><br><span class="line">log:</span><br><span class="line">  fields:</span><br><span class="line">    service: registry</span><br><span class="line">storage:</span><br><span class="line">  delete: //增加如下两行，默认是false，不能删除</span><br><span class="line">    enabled: true</span><br><span class="line">  cache:</span><br><span class="line">    blobdescriptor: inmemory</span><br><span class="line">  filesystem:</span><br><span class="line">    rootdirectory: /var/lib/registry</span><br><span class="line">http:</span><br><span class="line">  addr: :5000</span><br><span class="line">  headers:</span><br><span class="line">    X-Content-Type-Options: [nosniff]</span><br><span class="line">health:</span><br><span class="line">  storagedriver:</span><br><span class="line">    enabled: true</span><br><span class="line">    interval: 10s</span><br><span class="line">    threshold: 3</span><br><span class="line">    </span><br><span class="line">#docker cp ./config.yml registry:/etc/docker/registry/config.yml    </span><br><span class="line">#docker restart registry</span><br></pre></td></tr></table></figure>

<p>然后通过API来查询要删除镜像的id：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//查询要删除镜像的tag</span><br><span class="line">curl registry:5000/v2/drds/corona-server/tags/list</span><br><span class="line">//根据tag查找Etag</span><br><span class="line">curl -v registry:5000/v2/drds/corona-server/manifests/2.0.0_3012622_20220214_4ca91d96-arm64 -H &apos;Accept: application/vnd.docker.distribution.manifest.v2+json&apos;</span><br><span class="line">//根据前一步返回的Etag来删除对应的tag</span><br><span class="line">curl -X  DELETE registry:5000/v2/drds/corona-server/manifests/sha256:207ec19c1df6a3fa494d41a1a8b5332b969a010f0d4d980e39f153b1eaca2fe2 -v</span><br><span class="line"></span><br><span class="line">//执行垃圾回收</span><br><span class="line">docker exec -it registry bin/registry garbage-collect /etc/docker/registry/config.yml</span><br></pre></td></tr></table></figure>

<h2 id="检查是否restart能支持只重启deamon，容器还能正常运行"><a href="#检查是否restart能支持只重启deamon，容器还能正常运行" class="headerlink" title="检查是否restart能支持只重启deamon，容器还能正常运行"></a>检查是否restart能支持只重启deamon，容器还能正常运行</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$sudo docker info | grep Restore</span><br><span class="line">Live Restore Enabled: true</span><br></pre></td></tr></table></figure>

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html" target="_blank" rel="noopener">https://www.ardanlabs.com/blog/2020/02/docker-images-part1-reducing-image-size.html</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/02/25/Linux LVM使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/25/Linux LVM使用/" itemprop="url">Linux LVM使用</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-25T17:30:03+08:00">
                2018-02-25
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-LVM使用"><a href="#Linux-LVM使用" class="headerlink" title="Linux LVM使用"></a>Linux LVM使用</h1><p>LVM是 Logical Volume Manager（逻辑<a href="https://baike.baidu.com/item/%E5%8D%B7%E7%AE%A1%E7%90%86" target="_blank" rel="noopener">卷管理</a>）的简写, 用来解决磁盘分区大小动态分配。LVM不是软RAID（Redundant Array of Independent Disks）。</p>
<p><strong>从一块硬盘到能使用LV文件系统的步骤：</strong></p>
<p>​     <strong>硬盘—-分区(fdisk)—-PV(pvcreate)—-VG(vgcreate)—-LV(lvcreate)—-格式化(mkfs.ext4 LV为ext文件系统)—-挂载</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/949069-20200416104045527-1858978940.png" alt="img"></p>
<p>LVM磁盘管理方式</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20220725100705140.png" alt="image-20220725100705140"></p>
<p><strong>lvreduce 缩小LV</strong></p>
<p><strong>先卸载—&gt;然后减小逻辑边界—-&gt;最后减小物理边界—&gt;在检测文件系统  &#x3D;&#x3D;谨慎用&#x3D;&#x3D;</strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[aliyun@uos15 15:07 /dev/disk/by-label]</span><br><span class="line">$sudo e2label /dev/nvme0n1p1 polaru01  //给磁盘打标签</span><br><span class="line"></span><br><span class="line">[aliyun@uos15 15:07 /dev/disk/by-label]</span><br><span class="line">$lsblk  -f</span><br><span class="line">NAME        FSTYPE LABEL     UUID                                 FSAVAIL FSUSE% MOUNTPOINT</span><br><span class="line">sda                                                                              </span><br><span class="line">├─sda1      vfat   EFI       D0E3-79A8                               299M     0% /boot/efi</span><br><span class="line">├─sda2      ext4   Boot      f204c992-fb20-40e1-bf58-b11c994ee698    1.3G     6% /boot</span><br><span class="line">├─sda3      ext4   Roota     dbc68010-8c36-40bf-b794-271e59ff5727   14.8G    61% /</span><br><span class="line">├─sda4      ext4   Rootb     73fe0ac6-ff6b-46cc-a609-c574be026e8f                </span><br><span class="line">├─sda5      ext4   _dde_data 798fce56-fc82-4f59-bcaa-d2ed5c48da8d   42.1G    54% /data</span><br><span class="line">├─sda6      ext4   Backup    267dc7a8-1659-4ccc-b7dc-5f2cd80f4e4e    3.7G    57% /recovery</span><br><span class="line">└─sda7      swap   SWAP      7a5632dc-bc7b-410e-9a50-07140f20cd13                [SWAP]</span><br><span class="line">nvme0n1                                                                          </span><br><span class="line">└─nvme0n1p1 ext4   polaru01  762a5700-8cf1-454a-b385-536b9f63c25d  413.4G    54% /u01</span><br><span class="line">nvme1n1     xfs    u02       8ddf19c4-fe71-4428-b2aa-e45acf08050c                </span><br><span class="line">nvme2n1     xfs    u03       2b8625b4-c67d-4f1e-bed6-88814adfd6cc                </span><br><span class="line">nvme3n1     ext4   u01       cda85750-c4f7-402e-a874-79cb5244d4e1</span><br></pre></td></tr></table></figure>

<h2 id="LVM-创建、扩容"><a href="#LVM-创建、扩容" class="headerlink" title="LVM 创建、扩容"></a>LVM 创建、扩容</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">sudo vgcreate vg1 /dev/nvme0n1 /dev/nvme1n1 //两块物理磁盘上创建vg1</span><br><span class="line">如果报错：</span><br><span class="line">  Can&apos;t open /dev/nvme1n1 exclusively.  Mounted filesystem?</span><br><span class="line">  Can&apos;t open /dev/nvme0n1 exclusively.  Mounted filesystem?</span><br><span class="line">是说/dev/nvme0n1已经mounted了，需要先umount</span><br><span class="line"></span><br><span class="line">vgdisplay </span><br><span class="line">sudo lvcreate -L 5T -n u03 vg1  //在虚拟volume-group vg1上创建一个5T大小的分区or: sudo lvcreate -l 100%free -n u03 vg1</span><br><span class="line">sudo mkfs.ext4 /dev/vg1/u03   </span><br><span class="line">sudo mkdir /lvm</span><br><span class="line">sudo fdisk -l</span><br><span class="line">sudo umount /lvm</span><br><span class="line">sudo lvresize -L 5.8T /dev/vg1/u03 //lv 扩容，但此时还未生效；缩容的话风险较大，且需要先 umount</span><br><span class="line">sudo e2fsck -f /dev/vg1/u03 </span><br><span class="line">sudo resize2fs /dev/vg1/u03 //触发生效</span><br><span class="line">sudo mount /dev/vg1/u03 /lvm</span><br><span class="line">cd /lvm/</span><br><span class="line">lvdisplay </span><br><span class="line">sudo vgdisplay vg1</span><br><span class="line">lsblk -l</span><br><span class="line">lsblk </span><br><span class="line">sudo vgextend vg1 /dev/nvme3n1  //vg 扩容, 增加一块磁盘到vg1</span><br><span class="line">ls /u01</span><br><span class="line">sudo vgdisplay </span><br><span class="line">sudo fdisk  -l</span><br><span class="line">sudo pvdisplay </span><br><span class="line">sudo lvcreate -L 1T -n lv2 vg1  //从vg1中再分配一块1T大小的磁盘</span><br><span class="line">sudo lvdisplay </span><br><span class="line">sudo mkfs.ext4 /dev/vg1/lv2 </span><br><span class="line">mkdir /lv2</span><br><span class="line">ls /</span><br><span class="line">sudo mkdir /lv2</span><br><span class="line">sudo mount /dev/vg1/lv2 /lv2</span><br><span class="line">df -lh</span><br><span class="line"></span><br><span class="line">//手工创建lvm</span><br><span class="line"> 1281  18/05/22 11:04:22 ls -l /dev/|grep -v ^l|awk &apos;&#123;print $NF&#125;&apos;|grep -E &quot;^nvme[7-9]&#123;1,2&#125;n1$|^df[a-z]$|^os[a-z]$&quot;</span><br><span class="line"> 1282  18/05/22 11:05:06 vgcreate -s 32 vgbig /dev/nvme7n1 /dev/nvme8n1 /dev/nvme9n1</span><br><span class="line"> 1283  18/05/22 11:05:50 vgcreate -s 32 vgbig /dev/nvme7n1 /dev/nvme8n1 /dev/nvme9n1</span><br><span class="line"> 1287  18/05/22 11:07:59 lvcreate -A y -I 128K -l 100%FREE  -i 3 -n big vgbig</span><br><span class="line"> 1288  18/05/22 11:08:02 df -h</span><br><span class="line"> 1289  18/05/22 11:08:21 lvdisplay</span><br><span class="line"> 1290  18/05/22 11:08:34 df -lh</span><br><span class="line"> 1291  18/05/22 11:08:42 df -h</span><br><span class="line"> 1292  18/05/22 11:09:05 mkfs.ext4 /dev/vgbig/big -m 0 -O extent,uninit_bg -E lazy_itable_init=1 -q -L big -J size=4000</span><br><span class="line"> 1298  18/05/22 11:10:28 mkdir -p /big</span><br><span class="line"> 1301  18/05/22 11:12:11 mount /dev/vgbig/big /big</span><br></pre></td></tr></table></figure>

<h2 id="创建LVM"><a href="#创建LVM" class="headerlink" title="创建LVM"></a>创建LVM</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">function create_polarx_lvm_V62()&#123;</span><br><span class="line">    vgremove vgpolarx</span><br><span class="line"></span><br><span class="line">    #sed -i "97 a\    types = ['nvme', 252]" /etc/lvm/lvm.conf</span><br><span class="line">    parted -s /dev/nvme0n1 rm 1</span><br><span class="line">    parted -s /dev/nvme1n1 rm 1</span><br><span class="line">    parted -s /dev/nvme2n1 rm 1</span><br><span class="line">    parted -s /dev/nvme3n1 rm 1</span><br><span class="line">    dd if=/dev/zero of=/dev/nvme0n1  count=10000 bs=512</span><br><span class="line">    dd if=/dev/zero of=/dev/nvme1n1  count=10000 bs=512</span><br><span class="line">    dd if=/dev/zero of=/dev/nvme2n1  count=10000 bs=512</span><br><span class="line">    dd if=/dev/zero of=/dev/nvme3n1  count=10000 bs=512</span><br><span class="line"></span><br><span class="line">    #lvmdiskscan</span><br><span class="line">    vgcreate -s 32 vgpolarx /dev/nvme0n1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1</span><br><span class="line">    lvcreate -A y -I 16K -l 100%FREE  -i 4 -n polarx vgpolarx</span><br><span class="line">    mkfs.ext4 /dev/vgpolarx/polarx -m 0 -O extent,uninit_bg -E lazy_itable_init=1 -q -L polarx -J size=4000</span><br><span class="line">    sed  -i  "/polarx/d" /etc/fstab</span><br><span class="line">    mkdir -p /polarx</span><br><span class="line">    echo "LABEL=polarx /polarx     ext4        defaults,noatime,data=writeback,nodiratime,nodelalloc,barrier=0    0 0" &gt;&gt; /etc/fstab</span><br><span class="line">    mount -a</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">create_polarx_lvm_V62</span><br></pre></td></tr></table></figure>

<p>-I 64K 值条带粒度，默认64K，mysql pagesize 16K，所以最好16K</p>
<p>默认创建的是 linear，一次只用一块盘，不能累加多快盘的iops能力：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#lvcreate -h</span><br><span class="line">  lvcreate - Create a logical volume</span><br><span class="line"></span><br><span class="line">  Create a linear LV.</span><br><span class="line">  lvcreate -L|--size Size[m|UNIT] VG</span><br><span class="line">	[ -l|--extents Number[PERCENT] ]</span><br><span class="line">	[    --type linear ]</span><br><span class="line">	[ COMMON_OPTIONS ]</span><br><span class="line">	[ PV ... ]</span><br><span class="line"></span><br><span class="line">  Create a striped LV (infers --type striped).</span><br><span class="line">  lvcreate -i|--stripes Number -L|--size Size[m|UNIT] VG</span><br><span class="line">	[ -l|--extents Number[PERCENT] ]</span><br><span class="line">	[ -I|--stripesize Size[k|UNIT] ]</span><br><span class="line">	[ COMMON_OPTIONS ]</span><br><span class="line">	[ PV ... ]</span><br><span class="line"></span><br><span class="line">  Create a raid1 or mirror LV (infers --type raid1|mirror).</span><br><span class="line">  lvcreate -m|--mirrors Number -L|--size Size[m|UNIT] VG</span><br><span class="line">	[ -l|--extents Number[PERCENT] ]</span><br><span class="line">	[ -R|--regionsize Size[m|UNIT] ]</span><br><span class="line">	[    --mirrorlog core|disk ]</span><br><span class="line">	[    --minrecoveryrate Size[k|UNIT] ]</span><br><span class="line">	[    --maxrecoveryrate Size[k|UNIT</span><br></pre></td></tr></table></figure>

<h2 id="remount"><a href="#remount" class="headerlink" title="remount"></a>remount</h2><p>正常使用中的文件系统是不能被umount的，如果需要修改mount参数的话可以考虑用mount 的 -o remount 参数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@ky3 ~]# mount -o lazytime,remount /polarx/  //增加lazytime参数</span><br><span class="line">[root@ky3 ~]# mount -t ext4</span><br><span class="line">/dev/mapper/vgpolarx-polarx on /polarx type ext4 (rw,noatime,nodiratime,lazytime,nodelalloc,nobarrier,stripe=128,data=writeback)</span><br><span class="line">[root@ky3 ~]# mount -o rw,remount /polarx/  //去掉刚加的lazytime 参数</span><br><span class="line">[root@ky3 ~]# mount -t ext4</span><br><span class="line">/dev/mapper/vgpolarx-polarx on /polarx type ext4 (rw,noatime,nodiratime,nodelalloc,nobarrier,stripe=128,data=writeback)</span><br></pre></td></tr></table></figure>

<p>remount 时要特别小心，会大量回收 slab 等导致sys CPU 100% 打挂整机，remount会导致slab回收等，请谨慎执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[2023-10-26 15:04:49][kernel][info]EXT4-fs (dm-0): re-mounted. Opts: lazytime,data=writeback,nodelalloc,barrier=0,nolazytime</span><br><span class="line">[2023-10-26 15:04:49][kernel][info]EXT4-fs (dm-1): re-mounted. Opts: lazytime,data=writeback,nodelalloc,barrier=0,nolazytime</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]Modules linked in: ip_tables tcp_diag inet_diag venice_reduce_print(OE) bianque_driver(OE) 8021q garp mrp bridge stp llc ip6_tables tcp_rds_rt_j(OE) tcp_rt_base(OE) slb_vctk(OE) slb_vtoa(OE) hookers slb_ctk_proxy(OE) slb_ctk_session(OE) slb_ctk_debugfs(OE) loop nf_conntrack fuse btrfs zlib_deflate raid6_pq xor vfat msdos fat xfs libcrc32c ext3 jbd dm_mod khotfix_D902467(OE) kpatch_D537536(OE) kpatch_D793896(OE) kpatch_D608634(OE) kpatch_D629788(OE) kpatch_D820113(OE) kpatch_D723518(OE) kpatch_D616841(OE) kpatch_D602147(OE) kpatch_D523456(OE) kpatch_D559221(OE) ipflt(OE) kpatch_D656712(OE) kpatch_D753272(OE) kpatch_D813404(OE) i40e kpatch_D543129(OE) kpatch_D645707(OE) kpatch(OE) rpcrdma(OE) xprtrdma(OE) ib_isert(OE) ib_iser(OE) ib_srpt(OE) ib_srp(OE) ib_ipoib(OE) ib_addr(OE) ib_sa(OE)</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]ib_mad(OE) bonding iTCO_wdt iTCO_vendor_support intel_powerclamp coretemp intel_rapl kvm_intel kvm crc32_pclmul ghash_clmulni_intel aesni_intel lrw gf128mul glue_helper ablk_helper cryptd ipmi_devintf pcspkr sg lpc_ich mfd_core i2c_i801 shpchp wmi ipmi_si ipmi_msghandler acpi_pad acpi_power_meter binfmt_misc aocblk(OE) mlx5_ib(OE) ext4 mbcache jbd2 crc32c_intel ast(OE) syscopyarea mlx5_core(OE) sysfillrect sysimgblt ptp i2c_algo_bit pps_core drm_kms_helper aocnvm(OE) vxlan ttm aocmgr(OE) ip6_udp_tunnel udp_tunnel drm i2c_core sd_mod crc_t10dif crct10dif_generic crct10dif_pclmul crct10dif_common ahci libahci libata rdma_ucm(OE) rdma_cm(OE) iw_cm(OE) ib_umad(OE) ib_ucm(OE) ib_uverbs(OE) ib_cm(OE) ib_core(OE) mlx_compat(OE) [last unloaded: ip_tables]</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]CPU: 85 PID: 105195 Comm: mount Tainted: G        W  OE K------------   3.10.0-327.ali2017.alios7.x86_64 #1</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]Hardware name: Foxconn AliServer-Thor-04-12U-v2/Thunder2.0 2U, BIOS 1.0.PL.FC.P.026.05 03/04/2020</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]task: ffff8898016c5b70 ti: ffff88b2b5094000 task.ti: ffff88b2b5094000</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]RIP: 0010:[&lt;ffffffff81656502&gt;]  [&lt;ffffffff81656502&gt;] _raw_spin_lock+0x12/0x50</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]RSP: 0018:ffff88b2b5097d98  EFLAGS: 00000202</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]RAX: 0000000000160016 RBX: ffffffff81657696 RCX: 007d44c33c3e3c3e</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]RDX: 007d44c23c3e3c3e RSI: 00000000007d44c3 RDI: ffff88b0247b67d8</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]RBP: ffff88b2b5097d98 R08: 0000000000000000 R09: 0000000000000007</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]R10: ffff88b0247a7bc0 R11: 0000000000000000 R12: ffffffff81657696</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]R13: ffff88b2b5097d80 R14: ffffffff81657696 R15: ffff88b2b5097d78</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]FS:  00007ff7d3f4f880(0000) GS:ffff88bd6a340000(0000) knlGS:0000000000000000</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]CR2: 00007fff5286b000 CR3: 0000008177750000 CR4: 00000000003607e0</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]Call Trace:</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff812085df&gt;] shrink_dentry_list+0x4f/0x480</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff81208a9c&gt;] shrink_dcache_sb+0x8c/0xd0</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff811f3a7c&gt;] do_remount_sb+0x4c/0x1a0</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff81212519&gt;] do_mount+0x6a9/0xa40</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff8117830e&gt;] ? __get_free_pages+0xe/0x50</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff81212946&gt;] SyS_mount+0x96/0xf0</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning][&lt;ffffffff816600fd&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[2023-10-26 15:05:16][kernel][warning]Code: f6 47 02 01 74 e5 0f 1f 00 e8 a6 17 ff ff eb db 66 0f 1f 84 00 00 00 00 00 0f 1f 44 00 00 55 48 89 e5 b8 00 00 02 00 f0 0f c1 07 &lt;89&gt; c2 c1 ea 10 66 39 c2 75 02 5d c3 83 e2 fe 0f b7 f2 b8 00 80</span><br><span class="line">[2023-10-26 15:05:44][kernel][emerg]BUG: soft lockup - CPU#85 stuck for 23s! [mount:105195]</span><br></pre></td></tr></table></figure>

<h2 id="复杂版创建LVM"><a href="#复杂版创建LVM" class="headerlink" title="复杂版创建LVM"></a>复杂版创建LVM</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">function disk_part()&#123;</span><br><span class="line">    set -e</span><br><span class="line">    if [ $# -le 1 ]</span><br><span class="line">    then</span><br><span class="line">        echo "disk_part argument error"</span><br><span class="line">        exit -1</span><br><span class="line">    fi</span><br><span class="line">    action=$1</span><br><span class="line">    disk_device_list=(`echo $*`)</span><br><span class="line"></span><br><span class="line">    echo $disk_device_list</span><br><span class="line">    unset disk_device_list[0]</span><br><span class="line"></span><br><span class="line">    echo $action</span><br><span class="line">    echo $&#123;disk_device_list[*]&#125;</span><br><span class="line">    len=`echo $&#123;#disk_device_list[@]&#125;`</span><br><span class="line">    echo "start remove origin partition  "</span><br><span class="line">    for dev in  $&#123;disk_device_list[@]&#125;</span><br><span class="line">    do</span><br><span class="line">        #echo $&#123;dev&#125;</span><br><span class="line">        `parted -s $&#123;dev&#125; rm 1` || true</span><br><span class="line">        dd if=/dev/zero of=$&#123;dev&#125;  count=100000 bs=512</span><br><span class="line">    done</span><br><span class="line"><span class="meta">#</span>替换98行，插入的话r改成a</span><br><span class="line">    sed -i "98 r\    types = ['aliflash' , 252 , 'nvme' ,252 , 'venice', 252 , 'aocblk', 252]" /etc/lvm/lvm.conf</span><br><span class="line">    sed  -i  "/flash/d" /etc/fstab</span><br><span class="line"></span><br><span class="line">    if [ x$&#123;1&#125; == x"split" ]</span><br><span class="line">    then</span><br><span class="line">        echo "split disk "</span><br><span class="line">        #lvmdiskscan</span><br><span class="line">        echo $&#123;disk_device_list&#125;</span><br><span class="line">        #vgcreate -s 32 vgpolarx /dev/nvme0n1 /dev/nvme2n1</span><br><span class="line">        vgcreate -s 32 vgpolarx $&#123;disk_device_list[*]&#125;</span><br><span class="line">        #stripesize 16K 和MySQL pagesize适配</span><br><span class="line">        #lvcreate -A y -I 16K -l 100%FREE  -i 2 -n polarx vgpolarx</span><br><span class="line">    		lvcreate -A y -I 16K -l 100%FREE  -i $&#123;#disk_device_list[@]&#125; -n polarx vgpolarx</span><br><span class="line">        #lvcreate -A y -I 128K -l 75%VG  -i $&#123;len&#125; -n volume1 vgpolarx</span><br><span class="line">        #lvcreate -A y -I 128K -l 100%FREE  -i $&#123;len&#125; -n volume2 vgpolarx</span><br><span class="line">        mkfs.ext4 /dev/vgpolarx/polarx -m 0 -O extent,uninit_bg -E lazy_itable_init=1 -q -L polarx -J size=4000</span><br><span class="line">        sed  -i  "/polarx/d" /etc/fstab</span><br><span class="line">        mkdir -p /polarx</span><br><span class="line">    		opt="defaults,noatime,data=writeback,nodiratime,nodelalloc,barrier=0"</span><br><span class="line">        echo "LABEL=polarx /polarx     ext4        $&#123;opt&#125;    0 0" &gt;&gt; /etc/fstab</span><br><span class="line">        mount -a</span><br><span class="line">    else</span><br><span class="line">        echo "unkonw action "</span><br><span class="line">    fi</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">function format_nvme_mysql()&#123;</span><br><span class="line"></span><br><span class="line">    if [ `df |grep flash|wc -l` -eq $1  ]</span><br><span class="line">    then</span><br><span class="line">        echo "check success"</span><br><span class="line">        echo "start umount partition "</span><br><span class="line">        parttion_list=`df |grep flash|awk -F ' ' '&#123;print $1&#125;'`</span><br><span class="line">        for partition in $&#123;parttion_list[@]&#125;</span><br><span class="line">        do</span><br><span class="line">            echo $partition</span><br><span class="line">            umount $partition</span><br><span class="line">        done</span><br><span class="line">    else</span><br><span class="line">        echo "check host fail"</span><br><span class="line">        exit -1</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    disk_device_list=(`ls -l /dev/|grep -v ^l|awk '&#123;print $NF&#125;'|grep -E "^nvme[0-9]&#123;1,2&#125;n1$|^df[a-z]$|^os[a-z]$"`)</span><br><span class="line">    full_disk_device_list=()</span><br><span class="line">    for i in $&#123;!disk_device_list[@]&#125;</span><br><span class="line">    do</span><br><span class="line">        echo $&#123;i&#125;</span><br><span class="line">        full_disk_device_list[$&#123;i&#125;]=/dev/$&#123;disk_device_list[$&#123;i&#125;]&#125;</span><br><span class="line">    done</span><br><span class="line">    echo $&#123;full_disk_device_list[@]&#125;</span><br><span class="line">    disk_part split $&#123;full_disk_device_list[@]&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">if [ ! -d "/polarx" ]; then</span><br><span class="line">    umount /dev/vgpolarx/polarx</span><br><span class="line">    vgremove -f vgpolarx</span><br><span class="line">    dmsetup --force --retry --deferred remove vgpolarx-polarx</span><br><span class="line">    format_nvme_mysql $1</span><br><span class="line">else</span><br><span class="line">   echo "the lvm exists."</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<p>LVM性能还没有做到多盘并行，也就是性能和单盘差不多，盘数多读写性能也一样</p>
<p>查看 lvcreate 使用的参数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#lvs -o +lv_full_name,devices,stripe_size,stripes</span><br><span class="line">  LV   VG   Attr       LSize Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert LV       Devices           Stripe #Str</span><br><span class="line">  drds vg1  -wi-ao---- 5.37t                                                     vg1/drds /dev/nvme0n1p1(0)     0     1</span><br><span class="line">  drds vg1  -wi-ao---- 5.37t                                                     vg1/drds /dev/nvme2n1p1(0)     0     1</span><br><span class="line">  </span><br><span class="line"># lvs -v --segments</span><br><span class="line">  LV     VG       Attr       Start SSize  #Str Type    Stripe  Chunk</span><br><span class="line">  polarx vgpolarx -wi-ao----    0  11.64t    4 striped 128.00k    0 </span><br><span class="line">  </span><br><span class="line"># lvdisplay -m</span><br><span class="line">  --- Logical volume ---</span><br><span class="line">  LV Path                /dev/vgpolarx/polarx</span><br><span class="line">  LV Name                polarx</span><br><span class="line">  VG Name                vgpolarx</span><br><span class="line">  LV UUID                Wszlwf-SCjv-Txkw-9B1t-p82Z-C0Zl-oJopor</span><br><span class="line">  LV Write Access        read/write</span><br><span class="line">  LV Creation host, time ky4, 2022-08-18 15:53:29 +0800</span><br><span class="line">  LV Status              available</span><br><span class="line">  # open                 1</span><br><span class="line">  LV Size                11.64 TiB</span><br><span class="line">  Current LE             381544</span><br><span class="line">  Segments               1</span><br><span class="line">  Allocation             inherit</span><br><span class="line">  Read ahead sectors     auto</span><br><span class="line">  - currently set to     2048</span><br><span class="line">  Block device           254:0</span><br><span class="line"></span><br><span class="line">  --- Segments ---</span><br><span class="line">  Logical extents 0 to 381543:</span><br><span class="line">    Type		striped</span><br><span class="line">    Stripes		4</span><br><span class="line">    Stripe size		128.00 KiB</span><br><span class="line">    Stripe 0:</span><br><span class="line">      Physical volume	/dev/nvme1n1</span><br><span class="line">      Physical extents	0 to 95385</span><br><span class="line">    Stripe 1:</span><br><span class="line">      Physical volume	/dev/nvme3n1</span><br><span class="line">      Physical extents	0 to 95385</span><br><span class="line">    Stripe 2:</span><br><span class="line">      Physical volume	/dev/nvme2n1</span><br><span class="line">      Physical extents	0 to 95385</span><br><span class="line">    Stripe 3:</span><br><span class="line">      Physical volume	/dev/nvme0n1</span><br><span class="line">      Physical extents	0 to 95385</span><br></pre></td></tr></table></figure>

<p>&#x3D;&#x3D;要特别注意 stripes 表示多快盘一起用，iops能力累加，但是默认 stripes 是1，也就是只用1块盘，也就是linear&#x3D;&#x3D;</p>
<h2 id="安装LVM"><a href="#安装LVM" class="headerlink" title="安装LVM"></a>安装LVM</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo yum install lvm2 -y</span><br></pre></td></tr></table></figure>

<h2 id="dmsetup查看LVM"><a href="#dmsetup查看LVM" class="headerlink" title="dmsetup查看LVM"></a>dmsetup查看LVM</h2><p>管理工具dmsetup是 Device mapper in the kernel 中的一个</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dmsetup ls</span><br><span class="line">dmsetup info /dev/dm-0</span><br></pre></td></tr></table></figure>

<h2 id="reboot-失败"><a href="#reboot-失败" class="headerlink" title="reboot 失败"></a>reboot 失败</h2><p>在麒麟下OS reboot的时候可能因为<code>mount: /polarx: 找不到 LABEL=/polarx.</code> 导致OS无法启动，可以进入紧急模式，然后注释掉 &#x2F;etc&#x2F;fstab 中的polarx 行，再reboot</p>
<p>这是因为LVM的label、uuid丢失了，导致挂载失败。</p>
<p>查看设备的label</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsblk -o name,mountpoint,label,size,uuid  or lsblk -f</span><br></pre></td></tr></table></figure>

<p>修复：</p>
<p>紧急模式下修改 &#x2F;etc&#x2F;fstab 去掉有问题的挂载; 修改标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#blkid   //查询uuid、label</span><br><span class="line">/dev/mapper/klas-root: UUID=&quot;c4793d67-867e-4f14-be87-f6713aa7fa36&quot; BLOCK_SIZE=&quot;512&quot; TYPE=&quot;xfs&quot;</span><br><span class="line">/dev/sda2: UUID=&quot;8DCEc5-b4P7-fW0y-mYwR-5YTH-Yf81-rH1CO8&quot; TYPE=&quot;LVM2_member&quot; PARTUUID=&quot;4ffd9bfa-02&quot;</span><br><span class="line">/dev/nvme0n1: UUID=&quot;nJAHxP-d15V-Fvmq-rxa3-GKJg-TCqe-gD1A2Z&quot; TYPE=&quot;LVM2_member&quot;</span><br><span class="line">/dev/sda1: UUID=&quot;29f59517-91c6-4b3c-bd22-0a47c800d7f4&quot; BLOCK_SIZE=&quot;512&quot; TYPE=&quot;xfs&quot; PARTUUID=&quot;4ffd9bfa-01&quot;</span><br><span class="line">/dev/mapper/vgpolarx-polarx: LABEL=&quot;polarx&quot; UUID=&quot;025a3ac5-d38a-42f1-80b6-563a55cba12a&quot; BLOCK_SIZE=&quot;4096&quot; TYPE=&quot;ext4&quot;</span><br><span class="line"></span><br><span class="line">e2label /dev/mapper/vgpolarx-polarx polarx</span><br></pre></td></tr></table></figure>

<p>比如，下图右边的是启动失败的</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20211228185144635.png" alt="image-20211228185144635"></p>
<h2 id="软RAID"><a href="#软RAID" class="headerlink" title="软RAID"></a><a href="https://xiaoz.co/2020/04/28/array-with-mdadm/" target="_blank" rel="noopener">软RAID</a></h2><blockquote>
<p>mdadm(multiple devices admin)是一个非常有用的管理软raid的工具，可以用它来创建、管理、监控raid设备，当用mdadm来创建磁盘阵列时，可以使用整块独立的磁盘(如&#x2F;dev&#x2F;sdb,&#x2F;dev&#x2F;sdc)，也可以使用特定的分区(&#x2F;dev&#x2F;sdb1,&#x2F;dev&#x2F;sdc1)</p>
</blockquote>
<p>mdadm使用手册</p>
<blockquote>
<p>mdadm –create device –level&#x3D;Y –raid-devices&#x3D;Z devices<br>    -C | –create &#x2F;dev&#x2F;mdn<br>    -l | –level  0|1|4|5<br>    -n | –raid-devices device [..]<br>    -x | –spare-devices device [..]</p>
</blockquote>
<p><a href="https://www.cxyzjd.com/article/weixin_51486343/113114906" target="_blank" rel="noopener">创建</a> -l 0表示raid0， -l 10表示raid10</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mdadm -C /dev/md0 -a yes -l 0 -n2 /dev/nvme&#123;6,7&#125;n1  //raid0</span><br><span class="line">mdadm -D /dev/md0</span><br><span class="line">mkfs.ext4 /dev/md0</span><br><span class="line">mkdir /md0</span><br><span class="line">mount /dev/md0 /md0</span><br><span class="line"></span><br><span class="line">//条带</span><br><span class="line">mdadm --create --verbose /dev/md0 --level=linear --raid-devices=2 /dev/sdb /dev/sdc</span><br><span class="line">检查</span><br><span class="line">mdadm -E /dev/nvme[0-5]n1</span><br></pre></td></tr></table></figure>

<p>删除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">umount /md0 </span><br><span class="line">mdadm -S /dev/md0</span><br></pre></td></tr></table></figure>

<p>监控raid</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/mdstat</span><br><span class="line">Personalities : [raid0] [raid6] [raid5] [raid4]</span><br><span class="line">md6 : active raid6 nvme3n1[3] nvme2n1[2] nvme1n1[1] nvme0n1[0]</span><br><span class="line">      7501211648 blocks super 1.2 level 6, 512k chunk, algorithm 2 [4/4] [UUUU]</span><br><span class="line">      [=&gt;...................]  resync =  7.4% (280712064/3750605824) finish=388.4min speed=148887K/sec</span><br><span class="line">      bitmap: 28/28 pages [112KB], 65536KB chunk //raid6一直在异步刷数据</span><br><span class="line"></span><br><span class="line">md0 : active raid0 nvme7n1[3] nvme6n1[2] nvme4n1[0] nvme5n1[1]</span><br><span class="line">      15002423296 blocks super 1.2 512k chunks</span><br></pre></td></tr></table></figure>

<p>控制刷盘速度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#sysctl -a |grep raid</span><br><span class="line">dev.raid.speed_limit_max = 0</span><br><span class="line">dev.raid.speed_limit_min = 0</span><br></pre></td></tr></table></figure>

<h2 id="nvme-cli"><a href="#nvme-cli" class="headerlink" title="nvme-cli"></a>nvme-cli</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nvme id-ns /dev/nvme1n1 -H</span><br><span class="line">for i in `seq 0 1 2`; do nvme format --lbaf=3 /dev/nvme$&#123;i&#125;n1 ; done  //格式化，选择不同的扇区大小，默认512，可选4K</span><br><span class="line"></span><br><span class="line">fuser -km /data/</span><br></pre></td></tr></table></figure>

<h2 id="raid硬件卡"><a href="#raid硬件卡" class="headerlink" title="raid硬件卡"></a>raid硬件卡</h2><p><a href="http://aijishu.com/a/1060000000225602" target="_blank" rel="noopener">raid卡外观</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/bV6Ra.png" alt="image.png"></p>
<h2 id="mount-参数对性能的影响"><a href="#mount-参数对性能的影响" class="headerlink" title="mount 参数对性能的影响"></a>mount 参数对性能的影响</h2><p>推荐mount参数：defaults,noatime,data&#x3D;writeback,nodiratime,nodelalloc,barrier&#x3D;0  这些和 default 0 0 的参数差别不大，但是如果加了<a href="https://yuque.antfin-inc.com/mysql/release_notes/rds_oncall_find_inode_nowait?singleDoc#" target="_blank" rel="noopener">lazytime 会在某些场景下性能很差</a></p>
<p>比如在mysql filesort 场景下就可能触发 find_inode_nowait 热点，MySQL filesort 过程中，对文件的操作时序是 create,open,unlink,write,read,close; 而文件系统的 lazytime 选项，在发现 inode 进行修改了之后，会对同一个 inode table 中的 inode 进行修改，导致 file_inode_nowait 函数中，spin lock 的热点。</p>
<p>所以mount时注意不要有 lazytime </p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/format,png.png" alt="img"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1593311985695-a9a135ad-fac5-47d2-945f-9826c3739cbc.png" alt="img"></p>
<p>如果一个SQL 要创建大量临时表，而 &#x2F;tmp&#x2F; 挂在参数有lazytime的话也会导致同样的问题，如图堆栈：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1593577258724-57d2a896-48c5-4ed7-890e-322d0533ae40.png" alt="img"></p>
<p>对应的内核代码：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231027170529214.png" alt="image-20231027170529214"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231027170707041.png" alt="image-20231027170707041"></p>
<p>另外一个应用，也经常因为find_inode_nowait 热点把CPU 爆掉：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1698206577587-29d21289-4251-4297-a08b-f82d77332289.png" alt="img"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20231027165103176.png" alt="image-20231027165103176"></p>
<p>lazytime 的问题可以通过代码复现：</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;fcntl.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/stat.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sys/types.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unistd.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"pthread.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdlib.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;atomic&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="keyword">typedef</span> <span class="keyword">unsigned</span> <span class="keyword">long</span> <span class="keyword">long</span> ulonglong;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *FILE_PREFIX = <span class="string">"stress"</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">const</span> <span class="keyword">char</span> *FILE_DIR = <span class="string">"/flash4/tmp/"</span>;</span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::atomic&lt;ulonglong&gt; <span class="title">f_num</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">constexpr</span> <span class="keyword">size_t</span> THREAD_NUM = <span class="number">128</span>;</span><br><span class="line"><span class="keyword">static</span> <span class="keyword">constexpr</span> ulonglong LOOP = <span class="number">1000000000000</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">file_op</span><span class="params">(<span class="keyword">const</span> <span class="keyword">char</span> *file_name)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">int</span> f;</span><br><span class="line"> <span class="keyword">char</span> content[<span class="number">1024</span>];</span><br><span class="line"> content[<span class="number">0</span>] = <span class="string">'a'</span>;</span><br><span class="line"> content[<span class="number">500</span>] = <span class="string">'b'</span>;</span><br><span class="line"> content[<span class="number">1023</span>] = <span class="string">'c'</span>;</span><br><span class="line"> f = open(file_name, O_RDWR | O_CREAT);</span><br><span class="line"> unlink(file_name);</span><br><span class="line"> <span class="keyword">for</span> (ulonglong i = <span class="number">0</span>; i &lt; <span class="number">1024</span> * <span class="number">16</span>; i++) &#123;</span><br><span class="line">   write(f, content, <span class="number">1024</span>);</span><br><span class="line"> &#125;</span><br><span class="line"> close(f);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> *<span class="title">handle</span><span class="params">(<span class="keyword">void</span> *data)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">char</span> file[<span class="number">1024</span>];</span><br><span class="line"> ulonglong current_id;</span><br><span class="line"> <span class="keyword">for</span> (ulonglong i = <span class="number">0</span>; i &lt; LOOP; i++) &#123;</span><br><span class="line">   current_id = f_num++;</span><br><span class="line">   <span class="built_in">snprintf</span>(file, <span class="number">1024</span>, <span class="string">"%s%s_%d.txt"</span>, FILE_DIR, FILE_PREFIX, current_id);</span><br><span class="line">   file_op(file);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span>** args)</span> </span>&#123;</span><br><span class="line"> <span class="keyword">for</span> (<span class="built_in">std</span>::<span class="keyword">size_t</span> i = <span class="number">0</span>; i &lt; THREAD_NUM; i++) &#123;</span><br><span class="line">   <span class="keyword">pthread_t</span> tid;</span><br><span class="line">   <span class="keyword">int</span> ret = pthread_create(&amp;tid, <span class="literal">NULL</span>, handle, <span class="literal">NULL</span>);</span><br><span class="line"> &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>主动；工具、生产效率；面向故障、事件</p>
<h2 id="LVM-异常修复"><a href="#LVM-异常修复" class="headerlink" title="LVM 异常修复"></a>LVM 异常修复</h2><p>文件系统损坏，是导致系统启动失败比较常见的原因。文件系统损坏，比较常见的原因是分区丢失和文件系统需要手工修复。</p>
<p>1、分区表丢失，只要重新创建分区表即可。因为分区表信息只涉及变更磁盘上第一个扇区指定位置的内容。所以只要确认有分区情况，在分区表丢失的情况下，重做分区是不会损坏磁盘上的数据的。但是分区起始位置和尺寸需要正确 。起始位置确定后，使用fdisk重新分区即可。所以，问题的关键是如何确定分区的开始位置。</p>
<h4 id="确定分区起始位置："><a href="#确定分区起始位置：" class="headerlink" title="确定分区起始位置："></a>确定分区起始位置：</h4><p>MBR(Master Boot Record)是指磁盘第一块扇区上的一种数据结构，512字节，磁盘分区数据是MBR的一部分，可以使用通过dd if&#x3D;&#x2F;dev&#x2F;vdc bs&#x3D;512 count&#x3D;1 | hexdump -C 以16进制列出扇区0的裸数据：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240118103305675.png" alt="image-20240118103305675"></p>
<p>可以看出磁盘的分区类型ID、分区起始扇区和分区包含扇区数量，通过这几个数值可以确定分区位置。后面LVM可以通过LABELONE计算出起始位置。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="https://www.tecmint.com/manage-and-create-lvm-parition-using-vgcreate-lvcreate-and-lvextend/" target="_blank" rel="noopener">https://www.tecmint.com/manage-and-create-lvm-parition-using-vgcreate-lvcreate-and-lvextend/</a></p>
<p><a href="https://www.thegeekdiary.com/lvm-error-cant-open-devsdx-exclusively-mounted-filesystem/" target="_blank" rel="noopener">pvcreate error : Can’t open &#x2F;dev&#x2F;sdx exclusively. Mounted filesystem?</a></p>
<p>软RAID配置方法<a href="https://halysl.github.io/2020/06/09/%E8%BD%AFraid%E9%85%8D%E7%BD%AE/" target="_blank" rel="noopener">参考这里</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/02/24/Linux LVS配置/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/02/24/Linux LVS配置/" itemprop="url">Linux LVS 配置</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-02-24T17:30:03+08:00">
                2018-02-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Linux-LVS-配置"><a href="#Linux-LVS-配置" class="headerlink" title="Linux LVS 配置"></a>Linux LVS 配置</h1><h2 id="NAT"><a href="#NAT" class="headerlink" title="NAT"></a>NAT</h2><ul>
<li><p>Enable IP forwarding. This can be done by adding the following to</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure></li>
</ul>
<p>then</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 172.26.137.117:9376 -s rr //创建了一个rr lvs</span><br><span class="line">// -m 表示nat模式，不加的话默认是route模式</span><br><span class="line">ipvsadm -a -t 172.26.137.117:9376 -r 172.20.22.195:9376 -m //往lvs中添加一个RS</span><br><span class="line">ipvsadm -ln</span><br><span class="line">ipvsadm -a -t 172.26.137.117:9376 -r 172.20.22.196:9376 -m //往lvs中添加另外一个RS</span><br><span class="line">ipvsadm -ln</span><br><span class="line"></span><br><span class="line">//删除realserver</span><br><span class="line">ipvsadm -d -t 100.81.131.221:18507 -r 100.81.131.237:8507 -m</span><br><span class="line"></span><br><span class="line">//连接状态查看</span><br><span class="line"><span class="meta">#</span>ipvsadm -L -n --connection</span><br><span class="line">IPVS connection entries</span><br><span class="line">pro expire state       source             virtual            destination</span><br><span class="line">TCP 15:00  ESTABLISHED 127.0.0.1:40630    127.0.0.1:3001     127.0.0.1:3306</span><br><span class="line">TCP 14:59  ESTABLISHED 127.0.0.1:40596    127.0.0.1:3001     127.0.0.1:3306</span><br><span class="line">TCP 14:59  ESTABLISHED 127.0.0.1:40614    127.0.0.1:3001     127.0.0.1:3307</span><br><span class="line">TCP 15:00  ESTABLISHED 127.0.0.1:40598    127.0.0.1:3001     127.0.0.1:3307</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>流量统计</span><br><span class="line">ipvsadm -L -n --stats -t 192.168.1.10:28080 //-t service-address</span><br><span class="line">Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span><br><span class="line"><span class="meta">  -&gt;</span> RemoteAddress:Port</span><br><span class="line">TCP  192.168.1.10:28080              39835    1030M  863494K     150G     203G</span><br><span class="line"><span class="meta">  -&gt;</span> 172.20.62.78:3306                 774 46173852 38899725    6575M    9250M</span><br><span class="line"><span class="meta">  -&gt;</span> 172.20.78.79:3306                 781 45106566 37997254    6421M    9038M</span><br><span class="line"><span class="meta">  -&gt;</span> 172.20.81.80:3306                 783 45531236 38387112    6479M    9128M</span><br><span class="line">  </span><br><span class="line"><span class="meta">#</span>清空统计数据</span><br><span class="line"><span class="meta">#</span>ipvsadm --zero</span><br><span class="line"><span class="meta">#</span>列出所有连接信息</span><br><span class="line"><span class="meta">#</span>/sbin/ipvsadm -L -n --connection</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line"><span class="meta">  -&gt;</span> RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  11.197.140.20:18089 wlc</span><br><span class="line"><span class="meta">  -&gt;</span> 11.197.140.20:28089          Masq    1      0          0</span><br><span class="line"><span class="meta">  -&gt;</span> 11.197.141.110:28089         Masq    1      0          0</span><br></pre></td></tr></table></figure>

<h2 id="ipvsadm常用参数"><a href="#ipvsadm常用参数" class="headerlink" title="ipvsadm常用参数"></a>ipvsadm常用参数</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">添加虚拟服务器</span><br><span class="line">    语法:ipvsadm -A [-t|u|f]  [vip_addr:port]  [-s:指定算法]</span><br><span class="line">    -A:添加</span><br><span class="line">    -t:TCP协议</span><br><span class="line">    -u:UDP协议</span><br><span class="line">    -f:防火墙标记</span><br><span class="line">    -D:删除虚拟服务器记录</span><br><span class="line">    -E:修改虚拟服务器记录</span><br><span class="line">    -C:清空所有记录</span><br><span class="line">    -L:查看</span><br><span class="line">添加后端RealServer</span><br><span class="line">    语法:ipvsadm -a [-t|u|f] [vip_addr:port] [-r ip_addr] [-g|i|m] [-w 指定权重]</span><br><span class="line">    -a:添加</span><br><span class="line">    -t:TCP协议</span><br><span class="line">    -u:UDP协议</span><br><span class="line">    -f:防火墙标记</span><br><span class="line">    -r:指定后端realserver的IP</span><br><span class="line">    -g:DR模式</span><br><span class="line">    -i:TUN模式</span><br><span class="line">    -m:NAT模式</span><br><span class="line">    -w:指定权重</span><br><span class="line">    -d:删除realserver记录</span><br><span class="line">    -e:修改realserver记录</span><br><span class="line">    -l:查看</span><br><span class="line">通用:</span><br><span class="line">    ipvsadm -ln:查看规则</span><br><span class="line">    service ipvsadm save:保存规则</span><br></pre></td></tr></table></figure>

<h3 id="查看连接对应的RS-ip和端口"><a href="#查看连接对应的RS-ip和端口" class="headerlink" title="查看连接对应的RS ip和端口"></a>查看连接对应的RS ip和端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> ipvsadm -Lcn |grep "10.68.128.202:1406"</span><br><span class="line">TCP 15:01  ESTABLISHED 10.68.128.202:1406 10.68.128.202:3306 172.20.188.72:3306</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> ipvsadm -Lcn | head -10</span><br><span class="line">IPVS connection entries</span><br><span class="line">pro expire state       source             virtual            destination</span><br><span class="line">TCP 15:01  ESTABLISHED 10.68.128.202:1390 10.68.128.202:3306 172.20.185.132:3306</span><br><span class="line">TCP 15:01  ESTABLISHED 10.68.128.202:1222 10.68.128.202:3306 172.20.165.202:3306</span><br><span class="line">TCP 15:01  ESTABLISHED 10.68.128.202:1252 10.68.128.202:3306 172.20.222.65:3306</span><br><span class="line">TCP 15:01  ESTABLISHED 10.68.128.202:1328 10.68.128.202:3306 172.20.149.68:3306</span><br><span class="line"></span><br><span class="line">ipvsadm -Lcn</span><br><span class="line">IPVS connection entries</span><br><span class="line">pro expire state       source             virtual            destination</span><br><span class="line">TCP 00:57  NONE        110.184.96.173:0   122.225.32.142:80  122.225.32.136:80</span><br><span class="line">TCP 01:57  FIN_WAIT    110.184.96.173:54568 122.225.32.142:80  122.225.32.136:80</span><br></pre></td></tr></table></figure>

<p>当一个client访问vip的时候，ipvs或记录一条状态为NONE的信息，expire初始值是persistence_timeout的值，然后根据时钟主键变小，在以下记录存在期间，同一client ip连接上来，都会被分配到同一个后端。</p>
<p>FIN_WAIT的值就是tcp tcpfin udp的超时时间，当NONE的值为0时，如果FIN_WAIT还存在，那么NONE的值会从新变成60秒，再减少，直到FIN_WAIT消失以后，NONE才会消失，只要NONE存在，同一client的访问，都会分配到统一real server。</p>
<h2 id="通过keepalived来检测RealServer的状态"><a href="#通过keepalived来检测RealServer的状态" class="headerlink" title="通过keepalived来检测RealServer的状态"></a>通过keepalived来检测RealServer的状态</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> cat /etc/keepalived/keepalived.conf</span><br><span class="line">global_defs &#123;</span><br><span class="line">   notification_email &#123;</span><br><span class="line">   &#125;</span><br><span class="line">   router_id LVS_DEVEL</span><br><span class="line">   vrrp_skip_check_adv_addr</span><br><span class="line">   vrrp_strict</span><br><span class="line">   vrrp_garp_interval 0</span><br><span class="line">   vrrp_gna_interval 0</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta">#</span>添加虚拟服务器</span><br><span class="line"><span class="meta">#</span>相当于 ipvsadm -A -t 172.26.137.117:9376 -s wrr </span><br><span class="line">virtual_server 172.26.137.117 9376 &#123;</span><br><span class="line">    delay_loop 3             #服务健康检查周期,单位是秒</span><br><span class="line">    lb_algo wrr                 #调度算法</span><br><span class="line">    lb_kind NAT                 #模式 </span><br><span class="line"><span class="meta">#</span>   persistence_timeout 50   #会话保持时间,单位是秒</span><br><span class="line">    protocol TCP             #TCP协议转发</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>添加后端realserver</span><br><span class="line"><span class="meta">#</span>相当于 ipvsadm -a -t 172.26.137.117:9376 -r 172.20.56.148:9376 -w 1</span><br><span class="line">    real_server 172.20.56.148 9376 &#123;</span><br><span class="line">        weight 1</span><br><span class="line">        TCP_CHECK &#123;               # 通过TcpCheck判断RealServer的健康状态</span><br><span class="line">            connect_timeout 2     # 连接超时时间</span><br><span class="line">            nb_get_retry 3        # 重连次数</span><br><span class="line">            delay_before_retry 1  # 重连时间间隔</span><br><span class="line">            connect_port 9376     # 检测端口</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    real_server 172.20.248.147 9376 &#123;</span><br><span class="line">        weight 1</span><br><span class="line">        HTTP_GET &#123;</span><br><span class="line">            url &#123; </span><br><span class="line">              path /</span><br><span class="line">	          status_code 200</span><br><span class="line">            &#125;</span><br><span class="line">            connect_timeout 3</span><br><span class="line">            nb_get_retry 3</span><br><span class="line">            delay_before_retry 3</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>修改keepalived配置后只需要执行reload即可生效</p>
<blockquote>
<p>systemctl reload keepalived</p>
</blockquote>
<h2 id="timeout"><a href="#timeout" class="headerlink" title="timeout"></a>timeout</h2><p>LVS的持续时间有2个</p>
<ol>
<li>把同一个cip发来请求到同一台RS的持久超时时间。（-p persistent）</li>
<li>一个链接创建后空闲时的超时时间，这个超时时间分为3种。<ul>
<li>tcp的空闲超时时间。</li>
<li>lvs收到客户端tcp fin的超时时间</li>
<li>udp的超时时间</li>
</ul>
</li>
</ol>
<p>连接空闲超时时间的设置如下:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@poc117 ~]# ipvsadm -L --timeout</span><br><span class="line">Timeout (tcp tcpfin udp): 900 120 300</span><br><span class="line">[root@poc117 ~]# ipvsadm --set 1 2 1</span><br><span class="line">[root@poc117 ~]# ipvsadm -L --timeout</span><br><span class="line">Timeout (tcp tcpfin udp): 1 2 1</span><br><span class="line"></span><br><span class="line">ipvsadm -Lcn //查看</span><br></pre></td></tr></table></figure>

<h3 id="persistence-timeout"><a href="#persistence-timeout" class="headerlink" title="persistence_timeout"></a>persistence_timeout</h3><p>用于保证同一ip client的所有连接在timeout时间以内都发往同一个RS，比如ftp 21port listen认证、20 port传输数据，那么希望同一个client的两个连接都在同一个RS上。</p>
<p>persistence_timeout 会导致负载不均衡，timeout时间越大负载不均衡越严重。大多场景下基本没什么意义</p>
<p>PCC用来实现把某个用户的所有访问在超时时间内定向到同一台REALSERVER，这种方式在实际中不常用</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -A -t 192.168.0.1:0 -s wlc -p 600(单位是s)     //port为0表示所有端口</span><br><span class="line">ipvsadm -a -t 192.168.0.1:0 -r 192.168.1.2 -w 4 -g</span><br><span class="line">ipvsadm -a -t 192.168.0.1:0 -r 192.168.1.3 -w 2 -g</span><br></pre></td></tr></table></figure>

<p>此时测试一下会发现通过HTTP访问VIP和通过SSH登录VIP的时候都被定向到了同一台REALSERVER上面了</p>
<h2 id="lvs-管理"><a href="#lvs-管理" class="headerlink" title="lvs 管理"></a>lvs 管理</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">257  [2021-09-13 22:11:26] lscpu</span><br><span class="line">258  [2021-09-13 22:11:34] dmidecode | grep Ser</span><br><span class="line">259  [2021-09-13 22:11:53] dmidecode | grep FT</span><br><span class="line">260  [2021-09-13 22:11:58] dmidecode | grep 2500</span><br><span class="line">261  [2021-09-13 22:12:03] dmidecode</span><br><span class="line">262  [2021-09-13 22:12:27] lscpu</span><br><span class="line">263  [2021-09-13 22:12:37] ipvsadm  -ln</span><br><span class="line">264  [2021-09-13 22:12:59] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537</span><br><span class="line">265  [2021-09-13 22:14:37] base_admin --help</span><br><span class="line">266  [2021-09-13 22:14:44] base_admin --cpu-usage</span><br><span class="line">267  [2021-09-13 22:14:56] ip link</span><br><span class="line">268  [2021-09-13 22:16:04] base_admin --cpu-usage</span><br><span class="line">269  [2021-09-13 22:16:28] cat /usr/local/etc/nf-var-config</span><br><span class="line">270  [2021-09-13 22:16:43] base_admin --cpu-usage</span><br><span class="line">271  [2021-09-13 22:17:35] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537</span><br><span class="line">272  [2021-09-13 22:18:17] base_admin --cpu-usage</span><br><span class="line">273  [2021-09-13 22:22:02] ls</span><br><span class="line">274  [2021-09-13 22:22:06] ps -aux</span><br><span class="line">275  [2021-09-13 22:22:17] tsar --help</span><br><span class="line">276  [2021-09-13 22:22:24] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537</span><br><span class="line">277  [2021-09-13 22:22:31] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stat</span><br><span class="line">278  [2021-09-13 22:22:33] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats</span><br><span class="line">279  [2021-09-13 22:23:10] tsar --lvs -li1 -D|awk '&#123;print $1,"  ",($6)*8.0&#125;'</span><br><span class="line">280  [2021-09-13 22:24:29] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats</span><br><span class="line">281  [2021-09-13 22:25:26] tsar --lvs -li1 -D</span><br><span class="line">282  [2021-09-13 22:25:46] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537</span><br><span class="line">283  [2021-09-13 22:26:37] appctl -cas | grep conns</span><br><span class="line">284  [2021-09-13 22:31:16] ipvsadm  -ln</span><br><span class="line">286  [2021-09-13 22:31:43] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats</span><br><span class="line">292  [2021-09-13 22:38:16] rpm -qa | grep slb</span><br><span class="line">293  [2021-09-13 22:42:30] appctl -cas | grep conns</span><br><span class="line">294  [2021-09-13 22:43:03] base_admin --cpu-usage</span><br><span class="line">295  [2021-09-13 22:45:42] tsar --lvs -li1 -D|awk '&#123;print $1,"  ",($6)*8.0&#125;'</span><br><span class="line">296  [2021-09-13 22:57:20] base_admin --cpu-usage</span><br><span class="line">297  [2021-09-13 22:58:16] tsar --lvs -li1 -D|awk '&#123;print $1,"  ",($6)*8.0&#125;'</span><br><span class="line">298  [2021-09-13 22:59:38] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats</span><br><span class="line">299  [2021-09-13 23:00:16] appctl -a | grep conn</span><br><span class="line">300  [2021-09-13 23:00:24] base_admin --cpu-usage</span><br><span class="line">301  [2021-09-13 23:00:50] appctl -cas | grep conns</span><br><span class="line">302  [2021-09-13 23:01:15] base_admin --cpu-usage</span><br><span class="line">303  [2021-09-13 23:01:21] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats</span><br><span class="line">304  [2021-09-13 23:02:09] appctl -cas | grep conns</span><br><span class="line">305  [2021-09-13 23:03:12] base_admin --cpu-usage</span><br><span class="line">306  [2021-09-13 23:04:43] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats | head -3</span><br><span class="line">307  [2021-09-13 23:05:38] base_admin --cpu-usage</span><br><span class="line">308  [2021-09-13 23:06:10] tsar --lvs -li1 -D|awk '&#123;print $1,"  ",($6)*8.0&#125;'</span><br><span class="line">309  [2021-09-13 23:06:39] base_admin --cpu-usage</span><br><span class="line">310  [2021-09-13 23:15:59] appctl -a | grep conn_limit_enable</span><br><span class="line">311  [2021-09-13 23:15:59] appctl -a | grep cps_limit_enable</span><br><span class="line">312  [2021-09-13 23:15:59] appctl -a | grep inbps_limit_enable</span><br><span class="line">313  [2021-09-13 23:15:59] appctl -a | grep outbps_limit_enable</span><br><span class="line">314  [2021-09-13 23:17:13] appctl -w conn_limit_enable=0</span><br><span class="line">315  [2021-09-13 23:17:13] appctl -w cps_limit_enable=0</span><br><span class="line">316  [2021-09-13 23:17:13] appctl -w inbps_limit_enable=0</span><br><span class="line">317  [2021-09-13 23:17:13] appctl -w outbps_limit_enable=0</span><br><span class="line">318  [2021-09-13 23:17:43] appctl -cas | grep conn</span><br><span class="line">319  [2021-09-13 23:17:44] appctl -cas | grep conns</span><br><span class="line">320  [2021-09-13 23:19:30] last=0;while true;do pre=`ipvsadm -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats| grep TCP|awk '&#123;print $4&#125;'`;let cut=pre-last;echo $cut;last=$pre;sleep 1;done</span><br><span class="line">321  [2021-09-13 23:19:56] ipvsadm -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats| grep TCP|awk '&#123;print $4&#125;'</span><br><span class="line">322  [2021-09-13 23:20:01] last=0;while true;do pre=`ipvsadm -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats| grep TCP|awk '&#123;print $4&#125;'`;let cut=pre-last;echo $cut;last=$pre;sleep 1;done</span><br><span class="line">323  [2021-09-13 23:20:55] base_admin --cpu-usage</span><br><span class="line">324  [2021-09-13 23:22:05] ipvsadm  -lnvt 166.100.129.249:3306 --in-vid 1560537</span><br><span class="line">325  [2021-09-13 23:22:05] ipvsadm  -lnvt 166.100.128.219:3306 --in-vid 1560537</span><br><span class="line">326  [2021-09-13 23:22:05] ipvsadm  -lnvt 166.100.129.40:80 --in-vid 1560537</span><br><span class="line">327  [2021-09-13 23:24:22] base_admin --cpu-usage</span><br><span class="line">328  [2021-09-13 23:24:29] last=0;while true;do pre=`ipvsadm -lnvt 166.100.128.234:3306 --in-vid 1560537 --stats| grep TCP|awk '&#123;print $4&#125;'`;let cut=pre-last;echo $cut;last=$pre;sleep 1;done</span><br><span class="line">329  [2021-09-13 23:24:50] ipvsadm  -lnvt 166.100.129.249:3306 --in-vid 1560537</span><br><span class="line">332  [2021-09-13 23:25:38] ipvsadm  -lnvt 166.100.128.234:3306 --in-vid 1560537 —stats</span><br><span class="line">333  [2021-09-13 23:25:57] ipvsadm  -lnvt 166.100.129.249:3306 --in-vid 1560537 --stats</span><br><span class="line">334  [2021-09-13 23:25:58] ipvsadm  -lnvt 166.100.128.219:3306 --in-vid 1560537 --stats</span><br><span class="line">335  [2021-09-13 23:25:58] ipvsadm  -lnvt 166.100.129.40:80 --in-vid 1560537 --stats</span><br><span class="line">336  [2021-09-13 23:26:45] last=0;while true;do pre=`ipvsadm -lnvt 166.100.129.40:80 --in-vid 1560537 --stats| grep TCP|awk '&#123;print $4&#125;'`;let cut=pre-last;echo $cut;last=$pre;sleep 1;done</span><br></pre></td></tr></table></figure>

<h2 id="LVS-工作原理"><a href="#LVS-工作原理" class="headerlink" title="LVS 工作原理"></a>LVS 工作原理</h2><p>1.当客户端的请求到达负载均衡器的内核空间时，首先会到达PREROUTING链。 </p>
<p>2.当内核发现请求数据包的目的地址是本机时，将数据包送往INPUT链。 </p>
<p>3.LVS由用户空间的ipvsadm和内核空间的IPVS组成，ipvsadm用来定义规则，IPVS利用ipvsadm定义的规则工作，IPVS工作在INPUT链上,当数据包到达INPUT链时，首先会被IPVS检查，如果数据包里面的目的地址及端口没有在规则里面，那么这条数据包将被放行至用户空间。 </p>
<p>4.如果数据包里面的目的地址及端口在规则里面，那么这条数据报文将被修改目的地址为事先定义好的后端服务器，并送往POSTROUTING链。 </p>
<p>5.最后经由POSTROUTING链发往后端服务器。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/08cb9d37f580b03f37fcace92e21d2e3.png" alt="image.png"></p>
<h2 id="netfilter-原理"><a href="#netfilter-原理" class="headerlink" title="netfilter 原理"></a>netfilter 原理</h2><p>Netfilter 由多个表(table)组成，每个表又由多个链(chain)组成(此处可以脑补二维数组的矩阵了)，链是存放过滤规则的“容器”，里面可以存放一个或多个iptables命令设置的过滤规则。目前的表有4个：<code>raw table</code>, <code>mangle table</code>, <code>nat table</code>, <code>filter table</code>。Netfilter 默认的链有：<code>INPUT</code>, <code>OUTPUT</code>, <code>FORWARD</code>, <code>PREROUTING</code>, <code>POSTROUTING</code>，根据<code>表</code>的不同功能需求，不同的表下面会有不同的链，链与表的关系可用下图直观表示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1039cdda7040f20582f36a6a560e4e2e.png" alt="image.png"></p>
<h2 id="OSPF-LVS"><a href="#OSPF-LVS" class="headerlink" title="OSPF + LVS"></a>OSPF + LVS</h2><p>OSPF：Open Shortest Path First 开放最短路径优先，SPF算法也被称为Dijkstra算法，这是因为最短路径优先算法SPF是由荷兰计算机科学家狄克斯特拉于1959年提出的。</p>
<p>通过OSPF来替换keepalived，解决两个LVS节点的高可用，以及流量负载问题。keepalived两个节点只能是master-slave模式，而OSPF两个节点都是master，同时都有流量</p>
<p><img src="https://bbsmax.ikafan.com/static/L3Byb3h5L2h0dHAvczMuNTFjdG8uY29tL3d5ZnMwMi9NMDEvMjMvRkUvd0tpb20xTktBSnpqN2JNS0FBRTRQTzI1LVh3ODY2LmpwZw==.jpg" alt="img"></p>
<p>这个架构与LVS+keepalived 最明显的区别在于，两台Director都是Master 状态，而不是Master-Backup，如此一来，两台Director 地位就平等了。剩下的问题，就是看如何在这两台Director 间实现负载均衡了。这里会涉及路由器领域的一个概念：等价多路径</p>
<h3 id="ECMP（等价多路径）"><a href="#ECMP（等价多路径）" class="headerlink" title="ECMP（等价多路径）"></a><strong>ECMP（等价多路径）</strong></h3><p>ECMP（Equal-CostMultipathRouting）等价多路径，存在多条不同链路到达同一目的地址的网络环境中，如果使用传统的路由技术，发往该目的地址的数据包只能利用其中的一条链路，其它链路处于备份状态或无效状态，并且在动态路由环境下相互的切换需要一定时间，而等值多路径路由协议可以在该网络环境下<strong>同时</strong>使用多条链路，不仅增加了传输带宽，并且可以无时延无丢包地备份失效链路的数据传输。</p>
<p>ECMP最大的特点是实现了等值情况下，多路径负载均衡和链路备份的目的，在静态路由和OSPF中基本上都支持ECMP功能。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://www.ultramonkey.org/papers/lvs_tutorial/html/" target="_blank" rel="noopener">http://www.ultramonkey.org/papers/lvs_tutorial/html/</a></p>
<p><a href="https://www.jianshu.com/p/d4222ce9b032" target="_blank" rel="noopener">https://www.jianshu.com/p/d4222ce9b032</a></p>
<p><a href="https://www.cnblogs.com/zhangxingeng/p/10595058.html" target="_blank" rel="noopener">https://www.cnblogs.com/zhangxingeng/p/10595058.html</a></p>
<p><a href="http://xstarcd.github.io/wiki/sysadmin/lvs_persistence.html" target="_blank" rel="noopener">lvs持久性工作原理和配置</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/01/23/10+倍性能提升全过程/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/23/10+倍性能提升全过程/" itemprop="url">10+倍性能提升全过程</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-23T17:30:03+08:00">
                2018-01-23
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/performance/" itemprop="url" rel="index">
                    <span itemprop="name">performance</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"><a href="#10-倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程" class="headerlink" title="10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程"></a>10+倍性能提升全过程–优酷账号绑定淘宝账号的TPS从500到5400的优化历程</h1><h2 id="背景说明"><a href="#背景说明" class="headerlink" title="背景说明"></a>背景说明</h2><blockquote>
<p>2016年的双11在淘宝上买买买的时候，天猫和优酷土豆一起做了联合促销，在天猫双11当天购物满XXX元就赠送优酷会员，这个过程需要用户在优酷侧绑定淘宝账号(登录优酷、提供淘宝账号，优酷调用淘宝API实现两个账号绑定）和赠送会员并让会员权益生效(看收费影片、免广告等等）</p>
<p>这里涉及到优酷的两个部门：Passport(在上海，负责登录、绑定账号，下文中的优化过程主要是Passport部分）；会员(在北京，负责赠送会员，保证权益生效）</p>
</blockquote>
<blockquote>
<p>在双11活动之前，Passport的绑定账号功能一直在运行，只是没有碰到过大促销带来的挑战</p>
</blockquote>
<hr>
<p>整个过程分为两大块：</p>
<ol>
<li>整个系统级别，包括网络和依赖服务的性能等，多从整个系统视角分析问题；</li>
<li>但服务器内部的优化过程，将CPU从si&#x2F;sy围赶us，然后在us从代码级别一举全歼。</li>
</ol>
<p>系统级别都是最容易被忽视但是成效最明显的，代码层面都是很细致的力气活。</p>
<p>整个过程都是在对业务和架构不是非常了解的情况下做出的。</p>
<h2 id="会员部分的架构改造"><a href="#会员部分的架构改造" class="headerlink" title="会员部分的架构改造"></a>会员部分的架构改造</h2><ul>
<li>接入中间件DRDS，让优酷的数据库支持拆分，分解MySQL压力</li>
<li>接入中间件vipserver来支持负载均衡</li>
<li>接入集团DRC来保障数据的高可用</li>
<li>对业务进行改造支持Amazon的全链路压测</li>
</ul>
<h2 id="主要的压测过程"><a href="#主要的压测过程" class="headerlink" title="主要的压测过程"></a>主要的压测过程</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/6b24a854d91aba4dcdbd4f0155683d93.png" alt="screenshot.png"></p>
<p><strong>上图是压测过程中主要的阶段中问题和改进,主要的问题和优化过程如下：</strong></p>
<pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS)
- 生产环境snat单核导致的网络延时增大             (优化后生产环境能达到测试环境的3000TPS)
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等)          (优化后：4200-&gt;5400TPS)
</code></pre>
<p><strong>优化过程中碰到的比如淘宝api调用次数限流等一些业务原因就不列出来了</strong></p>
<hr>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>由于用户进来后先要登录并且绑定账号，实际压力先到Passport部分，在这个过程中最开始单机TPS只能到500，经过N轮优化后基本能达到5400 TPS，下面主要是阐述这个优化过程</p>
<h2 id="Passport部分的压力"><a href="#Passport部分的压力" class="headerlink" title="Passport部分的压力"></a>Passport部分的压力</h2><h3 id="Passport-核心服务分两个："><a href="#Passport-核心服务分两个：" class="headerlink" title="Passport 核心服务分两个："></a>Passport 核心服务分两个：</h3><ul>
<li>Login              主要处理登录请求</li>
<li>userservice    处理登录后的业务逻辑，比如将优酷账号和淘宝账号绑定</li>
</ul>
<p>为了更好地利用资源每台物理加上部署三个docker 容器，跑在不同的端口上(8081、8082、8083），通过bridge网络来互相通讯</p>
<h3 id="Passport机器大致结构"><a href="#Passport机器大致结构" class="headerlink" title="Passport机器大致结构"></a>Passport机器大致结构</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/b509b30218dd22e03149985cf5e15f8e.png" alt="screenshot.png"></p>
<!--这里的500 TPS到5400 TPS是指登录和将优酷账号和淘宝账号绑定的TPS，也是促销活动主要的瓶颈-->

<h3 id="userservice服务网络相关的各种问题"><a href="#userservice服务网络相关的各种问题" class="headerlink" title="userservice服务网络相关的各种问题"></a>userservice服务网络相关的各种问题</h3><hr>
<h4 id="太多SocketConnect异常-如上图）"><a href="#太多SocketConnect异常-如上图）" class="headerlink" title="太多SocketConnect异常(如上图）"></a>太多SocketConnect异常(如上图）</h4><p>在userservice机器上通过netstat也能看到大量的SYN_SENT状态，如下图：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/99bf952b880f17243953da790ff0e710.png" alt="image.png"></p>
<h4 id="因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"><a href="#因为docker-bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上" class="headerlink" title="因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上"></a>因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上</h4><p>这时SocketConnect异常不再出现<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/6ed62fd6b50ad2785e5b57687d95ad6e.png" alt="image.png"></p>
<h4 id="从新梳理一下网络流程"><a href="#从新梳理一下网络流程" class="headerlink" title="从新梳理一下网络流程"></a>从新梳理一下网络流程</h4><p>docker(bridge)—-短连接—&gt;访问淘宝API(淘宝open api只能短连接访问），性能差，cpu都花在si上； </p>
<p>如果 docker(bridge)—-长连接到宿主机的某个代理上(比如haproxy）—–短连接—&gt;访问淘宝API， 性能就能好一点。问题可能是短连接放大了Docker bridge网络的性能损耗</p>
<h4 id="当时看到的cpu-si非常高，截图如下："><a href="#当时看到的cpu-si非常高，截图如下：" class="headerlink" title="当时看到的cpu si非常高，截图如下："></a>当时看到的cpu si非常高，截图如下：</h4><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/4c1eff0f925f59977e2557acff5cf03b.png" alt="image.png"></p>
<p>去掉Docker后，性能有所提升，继续通过perf top看到内核态寻找可用的Local Port消耗了比较多的CPU，gif动态截图如下(可以点击看高清大图）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/fff502ca73e3112e585560ffe4a4dbf1.gif" alt="perf-top-netLocalPort-issue.gif"></p>
<p><strong>注意图中ipv6_rcv_saddr_equal和inet_csk_get_port 总共占了30%的CPU</strong> (系统态的CPU使用率高意味着共享资源有竞争或者I&#x2F;O设备之间有大量的交互。)</p>
<p><strong>一般来说一台机器默认配置的可用 Local Port 3万多个，如果是短连接的话，一个连接释放后默认需要60秒回收，30000&#x2F;60 &#x3D;500 这是大概的理论TPS值【这里只考虑连同一个server IP:port 的时候】</strong></p>
<p>这500的tps算是一个老中医的经验。不过有些系统调整过Local Port取值范围，比如从1024到65534，那么这个tps上限就是1000附近。</p>
<p>同时观察这个时候CPU的主要花在sy上，最理想肯定是希望CPU主要用在us上，截图如下：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<p><strong>规则：性能优化要先把CPU从SI、SY上的消耗赶到US上去(通过架构、系统配置）；然后提升 US CPU的效率(代码级别的优化）</strong></p>
<p>sy占用了30-50%的CPU，这太不科学了，同时通过 netstat 分析连接状态，确实看到很多TIME_WAIT：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2ae2cb8b0cb324b68ca22c48c019e029.png" alt="localportissue-time-wait.png"></p>
<p><strong>cpu要花在us上，这部分才是我们代码吃掉的</strong></p>
<p><em><strong>于是让PE修改了tcp相关参数：降低 tcp_max_tw_buckets和开启tcp_tw_reuse，这个时候TPS能从1000提升到3000</strong></em></p>
<p>鼓掌，赶紧休息，迎接双11啊</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/91353fb9c88116be3ff109e3528a4651.png" alt="image.png"></p>
<h2 id="测试环境优化到3000-TPS后上线继续压测"><a href="#测试环境优化到3000-TPS后上线继续压测" class="headerlink" title="测试环境优化到3000 TPS后上线继续压测"></a>测试环境优化到3000 TPS后上线继续压测</h2><p><strong>居然性能又回到了500，太沮丧了</strong>，其实最开始账号绑定慢，Passport这边就怀疑taobao api是不是在大压力下不稳定，一般都是认为自己没问题，有问题的一定是对方。我不觉得这有什么问题，要是知道自己有什么问题不早就优化掉了，但是这里缺乏证据支撑，也就是如果你觉得自己没有问题或者问题在对方，一定要拿出证据来(有证据那么大家可以就证据来讨论，而不是互相苍白地推诿）。</p>
<p>这个时候Passport更加理直气壮啊，好不容易在测试环境优化到3000，怎么一调taobao api就掉到500呢，这么点压力你们就扛不住啊。 但是taobao api那边给出调用数据都是1ms以内就返回了(alimonitor监控图表–拿证据说话）。</p>
<p>看到alimonitor给出的api响应时间图表后，我开始怀疑从优酷的机器到淘宝的机器中间链路上有瓶颈，但是需要设计方案来证明这个问题在链路上，要不各个环节都会认为自己没有问题的，问题就会卡死。但是当时Passport的开发也只能拿到Login和Userservice这两组机器的权限，中间的负载均衡、交换机都没有权限接触到。</p>
<p>在没有证据的情况下，肯定机房、PE配合你排查的欲望基本是没有的(被坑过很多回啊，你说我的问题，结果几天配合排查下来发现还是你程序的问题，凭什么我要每次都陪你玩？），所以我要给出证明问题出现在网络链路上，然后拿着这个证据跟网络的同学一起排查。</p>
<p>讲到这里我禁不住要插一句，在出现问题的时候，都认为自己没有问题这是正常反应，毕竟程序是看不见的，好多意料之外逻辑考虑不周全也是常见的，出现问题按照自己的逻辑自查的时候还是没有跳出之前的逻辑所以发现不了问题。但是好的程序员在问题的前面会尝试用各种手段去证明问题在哪里，而不是复读机一样我的逻辑是这样的，不可能出问题的。即使目的是证明问题在对方，只要能给出明确的证据都是负责任的，拿着证据才能理直气壮地说自己没有问题和干净地甩锅。</p>
<p><strong>在尝试过tcpdump抓包、ping等各种手段分析后，设计了场景证明问题在中间链路上。</strong></p>
<h3 id="设计如下三个场景证明问题在中间链路上："><a href="#设计如下三个场景证明问题在中间链路上：" class="headerlink" title="设计如下三个场景证明问题在中间链路上："></a>设计如下三个场景证明问题在中间链路上：</h3><ol>
<li>压测的时候在userservice ping 依赖服务的机器；</li>
<li>将一台userservice机器从负载均衡上拿下来(没有压力），ping 依赖服务的机器；</li>
<li>从公网上非我们机房的机器 ping 依赖服务的机器；</li>
</ol>
<p>这个时候奇怪的事情发现了，压力一上来<strong>场景1、2</strong>的两台机器ping淘宝的rt都从30ms上升到100-150ms，<strong>场景1</strong> 的rt上升可以理解，但是<strong>场景2</strong>的rt上升不应该，同时<strong>场景3</strong>中ping淘宝在压力测试的情况下rt一直很稳定(说明压力下淘宝的机器没有问题），到此确认问题在优酷到淘宝机房的链路上有瓶颈，而且问题在优酷机房出口扛不住这么大的压力。于是从上海Passport的团队找到北京Passport的PE团队，确认在优酷调用taobao api的出口上使用了snat，PE到snat机器上看到snat只能使用单核，而且对应的核早就100%的CPU了，因为之前一直没有这么大的压力所以这个问题一直存在只是没有被发现。</p>
<p><strong>于是PE去掉snat，再压的话 TPS稳定在3000左右</strong></p>
<hr>
<h2 id="到这里结束了吗？-从3000到5400TPS"><a href="#到这里结束了吗？-从3000到5400TPS" class="headerlink" title="到这里结束了吗？ 从3000到5400TPS"></a>到这里结束了吗？ 从3000到5400TPS</h2><p>优化到3000TPS的整个过程没有修改业务代码，只是通过修改系统配置、结构非常有效地把TPS提升了6倍，对于优化来说这个过程是最轻松，性价比也是非常高的。实际到这个时候也临近双11封网了，最终通过计算(机器数量*单机TPS）完全可以抗住双11的压力，所以最终双11运行的版本就是这样的。 但是有工匠精神的工程师是不会轻易放过这么好的优化场景和环境的(基线、机器、代码、工具都具备配套好了）</p>
<p><strong>优化完环境问题后，3000TPS能把CPU US跑上去，于是再对业务代码进行优化也是可行的了</strong>。</p>
<h3 id="进一步挖掘代码中的优化空间"><a href="#进一步挖掘代码中的优化空间" class="headerlink" title="进一步挖掘代码中的优化空间"></a>进一步挖掘代码中的优化空间</h3><p>双11前的这段封网其实是比较无聊的，于是和Passport的开发同学们一起挖掘代码中的可以优化的部分。这个过程中使用到的主要工具是这三个：火焰图、perf、perf-map-java。相关链接：<a href="http://www.brendangregg.com/perf.html" target="_blank" rel="noopener">http://www.brendangregg.com/perf.html</a> ; <a href="https://github.com/jrudolph/perf-map-agent" target="_blank" rel="noopener">https://github.com/jrudolph/perf-map-agent</a></p>
<h3 id="通过Perf发现的一个SpringMVC-的性能问题"><a href="#通过Perf发现的一个SpringMVC-的性能问题" class="headerlink" title="通过Perf发现的一个SpringMVC 的性能问题"></a>通过Perf发现的一个SpringMVC 的性能问题</h3><p>这个问题具体参考我之前发表的优化文章。 主要是通过火焰图发现spring mapping path消耗了过多CPU的性能问题，CPU热点都在methodMapping相关部分，于是修改代码去掉spring中的methodMapping解析后性能提升了40%，TPS能从3000提升到4200.</p>
<h3 id="著名的fillInStackTrace导致的性能问题"><a href="#著名的fillInStackTrace导致的性能问题" class="headerlink" title="著名的fillInStackTrace导致的性能问题"></a>著名的fillInStackTrace导致的性能问题</h3><p>代码中的第二个问题是我们程序中很多异常(fillInStackTrace），实际业务上没有这么多错误，应该是一些不重要的异常，不会影响结果，但是异常频率很高，对这种我们可以找到触发的地方，catch住，然后不要抛出去(也就是别触发fillInStackTrace)，打印一行error日志就行，这块也能省出10%的CPU，对应到TPS也有几百的提升。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/36ef4b16c3c400abf6eb7e6b0fbb2f58.png" alt="screenshot.png"></p>
<p>部分触发fillInStackTrace的场景和具体代码行(点击看高清大图）：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/7eb2cbb4afc2c7d7007c35304c95342a.png" alt="screenshot.png"></p>
<p>对应的火焰图(点击看高清大图）：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/894bd736dd03060e89e3fa49cc98ae5e.png" alt="screenshot.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2bb7395a2cc6833c9c7587b38402a301.png" alt="screenshot.png"></p>
<h3 id="解析useragent-代码部分的性能问题"><a href="#解析useragent-代码部分的性能问题" class="headerlink" title="解析useragent 代码部分的性能问题"></a>解析useragent 代码部分的性能问题</h3><p>整个useragent调用堆栈和cpu占用情况，做了个汇总(useragent不启用TPS能从4700提升到5400）<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/8a4a97cb74724b8baa3b90072a1914e0.png" alt="screenshot.png"></p>
<p>实际火焰图中比较分散：<br><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/afacc681a9550cd087838c2383be54c8.png" alt="screenshot.png"></p>
<p><strong>最终通过对代码的优化勉勉强强将TPS从3000提升到了5400(太不容易了，改代码过程太辛苦，不如改配置来得快）</strong></p>
<p>优化代码后压测tps可以跑到5400，截图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/38bb043c85c7b50007609484c7bf5698.png" alt="image.png"></p>
<h2 id="最后再次总结整个压测过程的问题和优化历程"><a href="#最后再次总结整个压测过程的问题和优化历程" class="headerlink" title="最后再次总结整个压测过程的问题和优化历程"></a>最后再次总结整个压测过程的问题和优化历程</h2><pre><code>- docker bridge网络性能问题和网络中断si不均衡    (优化后：500-&gt;1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS）
- 生产环境snat单核导致的网络延时增大             (优化后能达到测试环境的3000TPS）
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000-&gt;4200TPS)
- 其他业务代码的优化(比如异常、agent等）         (优化后：4200-&gt;5400TPS)
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2be2799d1eef982d77e5c0a5c896a0e9.png" alt="image.png"></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2018/01/01/通过tcpdump对Unix Socket 进行抓包解析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/01/01/通过tcpdump对Unix Socket 进行抓包解析/" itemprop="url">通过tcpdump对Unix Domain Socket 进行抓包解析</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-01T16:30:03+08:00">
                2018-01-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/tcpdump/" itemprop="url" rel="index">
                    <span itemprop="name">tcpdump</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="通过tcpdump对Unix-domain-Socket-进行抓包解析"><a href="#通过tcpdump对Unix-domain-Socket-进行抓包解析" class="headerlink" title="通过tcpdump对Unix domain Socket 进行抓包解析"></a>通过tcpdump对Unix domain Socket 进行抓包解析</h1><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>大多时候我们可以通过tcpdump对网络抓包分析请求、响应数据来排查问题。但是如果程序是通过Unix Domain Socket方式来访问的那么tcpdump就看不到Unix Domain Socket里面具体流淌的内容了，本文希望找到一种方法达到如同抓包查看网卡内容一样来抓包查看Unix Domain Socket上具体的请求、响应数据。</p>
<h2 id="socat工具"><a href="#socat工具" class="headerlink" title="socat工具"></a>socat工具</h2><p>类似nc，但是是个超级增强版的nc，<a href="https://payloads.online/tools/socat" target="_blank" rel="noopener">主要用作两个独立数据通道之间的双向数据传输的继电器（或者说代理）</a></p>
<p>基本原理，通过socat在Unix-Socket和TCP&#x2F;UDP port之间建立一个代理，然后对代理上的端口进行抓包。</p>
<p>以下案例通过对 docker.sock 抓包来分析方案。大多时候我们都可以通过curl 来将http post请求发送到docker deamon所监听的端口，这些请求和响应都可以通过tcpdump抓包分析得到。但是我们通过 docker ps &#x2F; docker run 将命令发给本地 docker-deamon的时候就是将请求翻译成 http请求发给了 docker.sock, 这个时候如果需要排查问题就没法用tcpdump来分析http内容了。</p>
<h2 id="通过socat-启动一个tcp端口来代理Unix-Domain-Socket"><a href="#通过socat-启动一个tcp端口来代理Unix-Domain-Socket" class="headerlink" title="通过socat 启动一个tcp端口来代理Unix Domain Socket"></a>通过socat 启动一个tcp端口来代理Unix Domain Socket</h2><p>启动本地8080端口，将docker.sock映射到8080端口,8080收到的东西都会转给docker.sock，docker.sock收到的东西都通过抓8080的包看到,但是要求应用访问8080而不是docker.sock。</p>
<pre><code>socat -d -d TCP-LISTEN:8080,fork,bind=127.0.0.1 UNIX:/var/run/docker.sock
</code></pre>
<p><strong>缺点：需要修改客户端的访问方式</strong></p>
<pre><code>sudo curl --unix-socket /var/run/docker.sock http://localhost/images/json
</code></pre>
<p>上面的访问方式对8080抓包还是抓不到，因为绕过了我们的代理。</p>
<p>只能通过如下方式访问8080端口，然后请求通过socat代理转发给docker.sock，整个结果跟访问–unix-socket是一样的，这个时候通过8080端口抓包能看到–unix-socket的工作数据</p>
<pre><code>sudo curl http://localhost:8080/images/json
</code></pre>
<h2 id="通过socat启动另外一个Unix-Domain-Socket代理，但是不是tcpdump抓包"><a href="#通过socat启动另外一个Unix-Domain-Socket代理，但是不是tcpdump抓包" class="headerlink" title="通过socat启动另外一个Unix Domain Socket代理，但是不是tcpdump抓包"></a>通过socat启动另外一个Unix Domain Socket代理，但是不是tcpdump抓包</h2><pre><code>sudo mv /var/run/docker.sock /var/run/docker.sock.original
sudo socat -t100 -d -x -v UNIX-LISTEN:/var/run/docker.sock,mode=777,reuseaddr,fork UNIX-CONNECT:/var/run/docker.sock.original
</code></pre>
<p>优点：客户端访问方式不变，还是直接访问–unix-socket<br>缺点：输出的数据不如tcpdump方便，也就不能用wireshark来分析了</p>
<p>本质也还是socat代理，只是不是用的一个tcp端口来代理了，而是通过一个unix-socet代理了另外一个unix-socket，直接在代理上输出所有收发的数据</p>
<h2 id="完美的办法，客户端不用改访问方式，tcpdump也能抓到数据"><a href="#完美的办法，客户端不用改访问方式，tcpdump也能抓到数据" class="headerlink" title="完美的办法，客户端不用改访问方式，tcpdump也能抓到数据"></a>完美的办法，客户端不用改访问方式，tcpdump也能抓到数据</h2><pre><code>sudo mv /var/run/docker.sock /var/run/docker.sock.original
sudo socat TCP-LISTEN:8089,reuseaddr,fork UNIX-CONNECT:/var/run/docker.sock.original
sudo socat UNIX-LISTEN:/var/run/docker.sock,fork TCP-CONNECT:127.0.0.1:8089
</code></pre>
<p>然后客户端还是直接访问–unix-socket	<br>    sudo curl –unix-socket &#x2F;var&#x2F;run&#x2F;docker.sock <a href="http://localhost/images/json" target="_blank" rel="noopener">http://localhost/images/json</a></p>
<p>这个时候通过tcpdump在8089端口上就能抓到数据了</p>
<pre><code>sudo tcpdump -i lo -netvv port 8089
</code></pre>
<p>实际是结合前面两种方法，做了两次代理，先将socket映射到8089端口上，然后再将8089端口映射到一个新的socket上，最后client访问这个新的socket。</p>
<p>实际流程如下： client -&gt; 新socket -&gt; 8089 -&gt; 原来的socket  这个时候对8089可以任意抓包了</p>
<p>参考来源：<a href="https://mivehind.net/2018/04/20/sniffing-unix-domain-sockets/" target="_blank" rel="noopener">https://mivehind.net/2018/04/20/sniffing-unix-domain-sockets/</a>	</p>
<h2 id="一些socat的其它用法"><a href="#一些socat的其它用法" class="headerlink" title="一些socat的其它用法"></a>一些socat的其它用法</h2><p> 把监听在远程主机12.34.56.78上的mysql服务Unix Domain Socket映射到本地的&#x2F;var&#x2F;run&#x2F;mysqld.temp.sock, 这样就可以用mysql -S &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.sock来访问远程主机的mysql服务了。</p>
<pre><code>socat &quot;UNIX-LISTEN:/var/run/mysqld.temp.sock,reuseaddr,fork&quot; EXEC:&quot;ssh root@12.34.56.78 socat STDIO UNIX-CONNECT:/var/run/mysqld/mysqld.sock&quot;
</code></pre>
<p> 还可以用下面的命令把12.34.56.78上的mysql映射到本地的5500端口，然后使用mysql -p 5500命令访问。</p>
<pre><code>socat TCP-LISTEN:5500 EXEC:&#39;ssh root@12.34.56.78 &quot;socat STDIO UNIX-CONNECT:/var/run/mysqld/mysqld.sock&quot;&#39;
</code></pre>
<p> 把12.34.56.78的udp 161端口映射到本地的1611端口</p>
<pre><code>socat udp-listen:1611 system:&#39;ssh root@12.34.56.78 &quot;socat stdio udp-connect:remotetarget:161&quot;&#39;	
</code></pre>
<p> 通过socat启动server，带有各种参数，比nc更灵活</p>
<pre><code>Server: socat -dd tcp-listen:2000,keepalive,keepidle=10,keepcnt=2,reuseaddr,keepintvl=1 -
Client: socat -dd - tcp:localhost:2000,keepalive,keepidle=10,keepcnt=2,keepintvl=1

Drop Connection (Unplug Cable, Shut down Link(WiFi/Interface)): sudo iptables -A INPUT -p tcp --dport 2000 -j DROP
</code></pre>
<p>启动本地8080端口，将docker.sock映射到8080端口(docker.sock收到的东西都通过抓8080的包看到)。 8080收到的东西都会转给docker.sock</p>
<pre><code>socat -d -d TCP-LISTEN:8080,fork,bind=99.13.252.208 UNIX:/var/run/docker.sock
</code></pre>
<h3 id="用socat远程Unix-Domain-Socket映射"><a href="#用socat远程Unix-Domain-Socket映射" class="headerlink" title="用socat远程Unix Domain Socket映射"></a>用socat远程Unix Domain Socket映射</h3><p>除了将我们本地服务通过端口映射提供给其它人访问，我们还可以通过端口转发玩一些更high的。比如下面这条命令，它把监听在远程主机12.34.56.78上的mysql服务Unix Domain Socket映射到本地的&#x2F;var&#x2F;run&#x2F;mysqld.temp.sock，这样，小明就可以用mysql -S &#x2F;var&#x2F;run&#x2F;mysqld&#x2F;mysqld.temp.sock来访问远程主机的mysql服务了。</p>
<pre><code>socat &quot;UNIX-LISTEN:/var/run/mysqld.temp.sock,reuseaddr,fork&quot; EXEC:&quot;ssh root@12.34.56.78 socat STDIO UNIX-CONNECT\:/var/run/mysqld/mysqld.sock&quot;
</code></pre>
<p>当然，小明如果不喜欢本地Unix Domain Socket，他还可以用下面的命令把12.34.56.78上的mysql映射到本地的5500端口，然后使用mysql -p 5500命令访问。</p>
<pre><code>socat TCP-LISTEN:5500 EXEC:&#39;ssh root@12.34.56.78 &quot;socat STDIO UNIX-CONNECT:/var/run/mysqld/mysqld.sock&quot;&#39;

# 把监听在远程主机12.34.56.78上的mysql服务Unix Domain Socket映射到本地的/var/run/mysqld.temp.sock, 这样就可以用mysql -S /var/run/mysqld/mysqld.sock来访问远程主机的mysql服务了。
socat &quot;UNIX-LISTEN:/var/run/mysqld.temp.sock,reuseaddr,fork&quot; EXEC:&quot;ssh root@12.34.56.78 socat STDIO UNIX-CONNECT:/var/run/mysqld/mysqld.sock&quot;
# 还可以用下面的命令把12.34.56.78上的mysql映射到本地
# 的5500端口，然后使用mysql -p 5500命令访问。
socat TCP-LISTEN:5500 EXEC:&#39;ssh root@12.34.56.78 &quot;socat STDIO UNIX-CONNECT:/var/run/mysqld/mysqld.sock&quot;&#39;
# 把12.34.56.78的udp 161端口映射到本地的1611端口：
socat udp-listen:1611 system:&#39;ssh root@12.34.56.78 &quot;socat stdio udp-connect:remotetarget:161&quot;&#39;
</code></pre>
<h2 id="socat启动网络服务"><a href="#socat启动网络服务" class="headerlink" title="socat启动网络服务"></a>socat启动网络服务</h2><p>在一个窗口中启动 <code>socat</code> 作为服务端，监听在 1000 端口：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> start a TCP listener at port 1000, and <span class="built_in">echo</span> back the received data</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> sudo socat TCP4-LISTEN:1000,fork <span class="built_in">exec</span>:cat</span></span><br></pre></td></tr></table></figure>

<p>另一个窗口用 <code>nc</code> 作为客户端来访问服务端，建立 socket：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> connect to the <span class="built_in">local</span> TCP listener at port 1000</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> nc localhost 1000</span></span><br></pre></td></tr></table></figure>

<h2 id="curl-7-57版本可以直接访问-–unix-socket"><a href="#curl-7-57版本可以直接访问-–unix-socket" class="headerlink" title="curl 7.57版本可以直接访问 –unix-socket"></a>curl 7.57版本可以直接访问 –unix-socket</h2><p>7.57之后的版本才支持curl –unix-socket，大大方便了我们的测试</p>
<pre><code>//Leave 测试断开一个网络
curl -H &quot;Content-Type: application/json&quot; -X POST -d &#39;{&quot;NetworkID&quot;:&quot;47866b0071e3df7e8053b9c8e499986dfe5c9c4947012db2d963c66ca971ed4b&quot;,&quot;EndpointID&quot;:&quot;3d716436e629701d3ce8650e7a85c133b0ff536aed173c624e4f62a381656862&quot;}&#39; --unix-socket /run/docker/plugins/vlan.sock http://localhost/NetworkDriver.Leave

//取镜像列表
sudo curl --unix-socket /var/run/docker.sock http://localhost/images/json

curl 11.239.155.97:2376/debug/pprof/goroutine?debug=2
echo -e &quot;GET /debug/pprof/goroutine?debug=2 HTTP/1.1\r\n&quot; | sudo nc -U /run/docker/plugins/vlan.sock
echo -e &quot;GET /debug/pprof/goroutine?debug=2 HTTP/1.1\r\n&quot; | sudo nc -U /var/run/docker.sock
//升级curl到7.57后支持 --unix-socket
sudo curl --unix-socket /var/run/docker.sock http://localh卡路里ost/images/json
sudo curl --unix-socket /run/docker/plugins/vlan.sock http://localhost/NetworkDriver.GetCapabilities
//Leave
curl -H &quot;Content-Type: application/json&quot; -X POST -d &#39;{&quot;NetworkID&quot;:&quot;47866b0071e3df7e8053b9c8e499986dfe5c9c4947012db2d963c66ca971ed4b&quot;,&quot;EndpointID&quot;:&quot;3d716436e629701d3ce8650e7a85c133b0ff536aed173c624e4f62a381656862&quot;}&#39; --unix-socket /run/docker/plugins/vlan.sock http://localhost/NetworkDriver.Leave

sudo curl --no-buffer -XGET --unix-socket /var/run/docker.sock http://localhost/events
</code></pre>
<h2 id="Unix-Domain-Socket工作原理"><a href="#Unix-Domain-Socket工作原理" class="headerlink" title="Unix Domain Socket工作原理"></a><a href="https://mp.weixin.qq.com/s/fHzKYlW0WMhP2jxh2H_59A" target="_blank" rel="noopener">Unix Domain Socket工作原理</a></h2><p>接收connect 请求的时候，会申请一个新 socket 给 server 端将来使用，和自己的 socket 建立好连接关系以后，就放到服务器正在监听的 socket 的接收队列中。这个时候，服务器端通过 accept 就能获取到和客户端配好对的新 socket 了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-0054201." alt="Image"></p>
<p>主要的连接操作都是在这个函数中完成的。和我们平常所见的 TCP 连接建立过程，这个连接过程简直是太简单了。没有三次握手，也没有全连接队列、半连接队列，更没有啥超时重传。</p>
<p>直接就是将两个 socket 结构体中的指针互相指向对方就行了。就是 unix_peer(newsk) &#x3D; sk 和 unix_peer(sk) &#x3D; newsk 这两句。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/unix/af_unix.c</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">int</span> <span class="title">unix_stream_connect</span><span class="params">(struct socket *sock, struct sockaddr *uaddr,</span></span></span><br><span class="line"><span class="function"><span class="params">          <span class="keyword">int</span> addr_len, <span class="keyword">int</span> flags)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_un</span> *<span class="title">sunaddr</span> = (<span class="title">struct</span> <span class="title">sockaddr_un</span> *)<span class="title">uaddr</span>;</span></span><br><span class="line"></span><br><span class="line"> <span class="comment">// 1. 为服务器侧申请一个新的 socket 对象</span></span><br><span class="line"> newsk = unix_create1(sock_net(sk), <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 2. 申请一个 skb，并关联上 newsk</span></span><br><span class="line"> skb = sock_wmalloc(newsk, <span class="number">1</span>, <span class="number">0</span>, GFP_KERNEL);</span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 3. 建立两个 sock 对象之间的连接</span></span><br><span class="line"> unix_peer(newsk) = sk;</span><br><span class="line"> newsk-&gt;sk_state  = TCP_ESTABLISHED;</span><br><span class="line"> newsk-&gt;sk_type  = sk-&gt;sk_type;</span><br><span class="line"> ...</span><br><span class="line"> sk-&gt;sk_state = TCP_ESTABLISHED;</span><br><span class="line"> unix_peer(sk) = newsk;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 4. 把连接中的一头（新 socket）放到服务器接收队列中</span></span><br><span class="line"> __skb_queue_tail(&amp;other-&gt;sk_receive_queue, skb);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: net/unix/af_unix.c</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> unix_peer(sk) (unix_sk(sk)-&gt;peer)</span></span><br></pre></td></tr></table></figure>

<p>收发包过程和复杂的 TCP 发送接收过程相比，这里的发送逻辑简单简单到令人发指。申请一块内存（skb），把数据拷贝进去。根据 socket 对象找到另一端，<strong>直接把 skb 给放到对端的接收队列里了</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/640-20211221105837677" alt="Image"></p>
<p>Unix Domain Socket和127.0.0.1通信相比，如果包的大小是1K以内，那么性能会有一倍以上的提升，包变大后性能的提升相对会小一些。</p>
<h2 id="tcpdump原理"><a href="#tcpdump原理" class="headerlink" title="tcpdump原理"></a>tcpdump原理</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/0923eefc85c1bf87f47591222532f1f2.png" alt="image.png"></p>
<p>tcpdump 抓包使用的是 libpcap 这种机制。它的大致原理是：在收发包时，如果该包符合 tcpdump 设置的规则（BPF filter），那么该网络包就会被拷贝一份到 tcpdump 的内核缓冲区，然后以 PACKET_MMAP 的方式将这部分内存映射到 tcpdump 用户空间，解析后就会把这些内容给输出了。</p>
<p>通过上图你也可以看到，在收包的时候，如果网络包已经被网卡丢弃了，那么 tcpdump 是抓不到它的；在发包的时候，如果网络包在协议栈里被丢弃了，比如因为发送缓冲区满而被丢弃，tcpdump 同样抓不到它。我们可以将 tcpdump 的能力范围简单地总结为：网卡以内的问题可以交给 tcpdump 来处理；对于网卡以外（包括网卡上）的问题，tcpdump 可能就捉襟见肘了。这个时候，你需要在对端也使用 tcpdump 来抓包。</p>
<h3 id="tcpdump-技巧"><a href="#tcpdump-技巧" class="headerlink" title="tcpdump 技巧"></a>tcpdump 技巧</h3><blockquote>
<p>tcpdump -B&#x2F;**–buffer-size&#x3D;<strong>*buffer_size:*Set the operating system capture buffer size to <em>buffer_size</em>, in units of KiB (1024 bytes). tcpdump 丢包，造成这种丢包的原因是由于libcap抓到包后，tcpdump上层没有及时的取出，导致libcap缓冲区溢出，从而覆盖了未处理包，此处即显示为**dropped by kernel</strong>，注意，这里的kernel并不是说是被linux内核抛弃的，而是被tcpdump的内核，即 libcap 抛弃掉的</p>
</blockquote>
<h3 id="获取接口设备列表"><a href="#获取接口设备列表" class="headerlink" title="获取接口设备列表"></a>获取接口设备列表</h3><p>tcpdump的<code>-D</code>获取接口设备列表。看到此列表后，可以决定要在哪个接口上捕获流量。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">#tcpdump -D</span><br><span class="line">1.eth0</span><br><span class="line">2.bond0</span><br><span class="line">3.docker0</span><br><span class="line">4.nflog (Linux netfilter log (NFLOG) interface)</span><br><span class="line">5.nfqueue (Linux netfilter queue (NFQUEUE) interface)</span><br><span class="line">6.eth1</span><br><span class="line">7.usbmon1 (USB bus number 1)</span><br><span class="line">8.usbmon2 (USB bus number 2)</span><br><span class="line">9.veth6f2ee76</span><br><span class="line">10.veth8cb61c2</span><br><span class="line">11.veth9d9d363</span><br><span class="line">12.veth16c25ac</span><br><span class="line">13.veth190f0fc</span><br><span class="line">14.veth07103d7</span><br><span class="line">15.veth09119c0</span><br><span class="line">16.veth9770e1a</span><br><span class="line">17.any (Pseudo-device that captures on all interfaces)</span><br><span class="line">18.lo [Loopback]</span><br><span class="line"></span><br><span class="line"># tcpdump -X //解析内容</span><br></pre></td></tr></table></figure>

<h2 id="TCP-疑难问题的轻量级分析手段：TCP-Tracepoints"><a href="#TCP-疑难问题的轻量级分析手段：TCP-Tracepoints" class="headerlink" title="TCP 疑难问题的轻量级分析手段：TCP Tracepoints"></a>TCP 疑难问题的轻量级分析手段：TCP Tracepoints</h2><p>Tracepoint 是我分析问题常用的手段之一，在遇到一些疑难问题时，我通常都会把一些相关的 Tracepoint 打开，把 Tracepoint 输出的内容保存起来，然后再在线下环境中分析。通常，我会写一些 Python 脚本来分析这些内容，毕竟 Python 在数据分析上还是很方便的。</p>
<p>对于 TCP 的相关问题，我也习惯使用这些 TCP Tracepoints 来分析问题。要想使用这些 Tracepoints，你的内核版本需要为 <strong>4.16</strong> 及以上。这些常用的 TCP Tracepoints 路径位于 &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;events&#x2F;tcp&#x2F; 和 &#x2F;sys&#x2F;kernel&#x2F;debug&#x2F;tracing&#x2F;events&#x2F;sock&#x2F;，它们的作用如下表所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/32f29686127beb5a3279e630259903ae.png" alt="image.png"></p>
<h2 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h2><p><a href="https://mivehind.net/2018/04/20/sniffing-unix-domain-sockets/" target="_blank" rel="noopener">https://mivehind.net/2018/04/20/sniffing-unix-domain-sockets/</a>	</p>
<p><a href="https://superuser.com/questions/484671/can-i-monitor-a-local-unix-domain-socket-like-tcpdump" target="_blank" rel="noopener">https://superuser.com/questions/484671/can-i-monitor-a-local-unix-domain-socket-like-tcpdump</a></p>
<p><a href="https://payloads.online/tools/socat" target="_blank" rel="noopener">https://payloads.online/tools/socat</a></p>
<p><a href="https://gaia.cs.umass.edu/kurose_ross/wireshark.php" target="_blank" rel="noopener">计算机网络</a>（Computer Networking: A Top-Down Approach）</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/12/15/从知识到能力，你到底欠缺了什么/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/15/从知识到能力，你到底欠缺了什么/" itemprop="url">从知识到能力，你到底欠缺了什么</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-15T17:30:03+08:00">
                2017-12-15
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="从知识到能力，你到底欠缺了什么"><a href="#从知识到能力，你到底欠缺了什么" class="headerlink" title="从知识到能力，你到底欠缺了什么"></a>从知识到能力，你到底欠缺了什么</h1><h2 id="写在最前面的"><a href="#写在最前面的" class="headerlink" title="写在最前面的"></a>写在最前面的</h2><p>前面推送过文章<a href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA" target="_blank" rel="noopener">《毕业3年，为何技术能力相差越来越大？》</a> 有些同学觉得还是不知道如何落地，那么本文希望借助一个程序员经常遇到的一个问题：网络为什么不通？ 来具体展开实践一下怎么将书本上的死知识真正变成我们解决问题的能力。</p>
<h2 id="大学学到的基本概念"><a href="#大学学到的基本概念" class="headerlink" title="大学学到的基本概念"></a>大学学到的基本概念</h2><p>我相信你脑子里关于网络基础知识的概念都在下面这张图上，但是有点乱，都认识，又都模模糊糊，更谈不上将他们转化成生产力，用来解决实际问题了。这就是因为知识没有贯通、没有实践、没有组织。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/d37028807148f8ec630512bdd4223335.png" alt="image.png"></p>
<p>上图中知识点的作用在<a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="noopener">RFC1180</a>中讲的无比的通熟易懂了，看第一遍你也许就看懂了，但是一个月后又忘记了。其实这些东西我们在大学也学了，但是还是忘了（能够理解，缺少实操环境和条件），或者碰到问题才发现之前即使觉得看懂了的东西实际没懂</p>
<p><strong>所以接下来我们将示范书本知识到实践的贯通，希望把网络概念之间的联系通过实践来组织起来</strong></p>
<h2 id="还是从一个网络不通的问题入手"><a href="#还是从一个网络不通的问题入手" class="headerlink" title="还是从一个网络不通的问题入手"></a>还是从一个网络不通的问题入手</h2><p>最近的环境碰到一个网络ping不通的问题，当时的网络链路是（大概是这样，略有简化）：</p>
<pre><code>容器1-&gt;容器1所在物理机1-&gt;交换机-&gt;物理机2
</code></pre>
<h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><ul>
<li>从容器1 ping 物理机2 不通；</li>
<li>从物理机1上的容器2 ping物理机2 通；</li>
<li>同时发现即使是通的，有的容器 ping物理机1只需要0.1ms，有的容器需要200ms以上（都在同一个物理机上），不合理</li>
<li>所有容器 ping 其它外网IP（比如百度）反而是通的</li>
</ul>
<p>这个问题扯了一周才解决是因为容器的网络是我们自己配置的，交换机我们没有权限接触，由客户配置。出问题的时候都会觉得自己没问题对方有问题，另外就是对网络基本知识认识不够所以都觉得自己没问题而不去找证据。</p>
<p>这个问题的答案在大家看完本文的基础知识后会总结出来。</p>
<blockquote>
<p>解决这个问题前大家先想想，假如有个面试题是：输入 ping IP 后敲回车，然后发生了什么？</p>
</blockquote>
<h2 id="复习一下大学课本中的知识点"><a href="#复习一下大学课本中的知识点" class="headerlink" title="复习一下大学课本中的知识点"></a>复习一下大学课本中的知识点</h2><p>要解决一个问题你首先要有基础知识，知识欠缺你的逻辑再好、思路再清晰、智商再高，也不一定有效。</p>
<h3 id="route-路由表"><a href="#route-路由表" class="headerlink" title="route 路由表"></a>route 路由表</h3><pre><code>$route -n
Kernel IP routing table
Destination Gateway Genmask Flags Metric RefUse Iface
0.0.0.0     1.1.15.254   0.0.0.0 UG0  00 eth0
1.0.0.0     1.1.15.254   255.0.0.0   UG0  00 eth0
1.1.0.0     0.0.0.0 255.255.240.0   U 0  00 eth0
11.0.0.0    1.1.15.254   255.0.0.0   UG0  00 eth0
30.0.0.0    1.1.15.254   255.0.0.0   UG0  00 eth0
100.64.0.0  1.1.15.254   255.192.0.0 UG0  00 eth0
169.254.0.0 0.0.0.0 255.255.0.0 U 1002   00 eth0
172.16.0.0  1.1.15.254   255.240.0.0 UG0  00 eth0
172.17.0.0  0.0.0.0 255.255.0.0 U 0  00 docker0
192.168.0.0 1.1.15.254   255.255.0.0 UG0  00 eth0
</code></pre>
<p>假如你现在在这台机器上ping 172.17.0.2 根据上面的route表得出 172.17.0.2这个IP符合下面这条路由：</p>
<pre><code>172.17.0.0  0.0.0.0 255.255.0.0 U 0  00 docker0
</code></pre>
<p>这条路由规则，那么ping 包会从docker0这张网卡发出去。</p>
<p>但是如果是ping 1.1.4.4 根据路由规则应该走eth0这张网卡而不是docker0了。</p>
<p>接下来就要判断目标IP是否在同一个子网了</p>
<h2 id="ifconfig"><a href="#ifconfig" class="headerlink" title="ifconfig"></a>ifconfig</h2><p>首先来看看这台机器的网卡情况：</p>
<pre><code>$ifconfig
docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt;  mtu 1500
    inet 172.17.42.1  netmask 255.255.0.0  broadcast 0.0.0.0
    ether 02:42:49:a7:dc:ba  txqueuelen 0  (Ethernet)
    RX packets 461259  bytes 126800808 (120.9 MiB)
    RX errors 0  dropped 0  overruns 0  frame 0
    TX packets 462820  bytes 103470899 (98.6 MiB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
    inet 1.1.3.33  netmask 255.255.240.0  broadcast 10.125.15.255
    ether 00:16:3e:00:02:67  txqueuelen 1000  (Ethernet)
    RX packets 280918095  bytes 89102074868 (82.9 GiB)
    RX errors 0  dropped 0  overruns 0  frame 0
    TX packets 333504217  bytes 96311277198 (89.6 GiB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

lo: flags=73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536
    inet 127.0.0.1  netmask 255.0.0.0
    loop  txqueuelen 0  (Local Loopback)
    RX packets 1077128597  bytes 104915529133 (97.7 GiB)
    RX errors 0  dropped 0  overruns 0  frame 0
    TX packets 1077128597  bytes 104915529133 (97.7 GiB)
    TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
</code></pre>
<p>这里有三个网卡和三个IP，三个子网掩码（netmask)，根据目标路由走哪张网卡，得到这个网卡的子网掩码，来计算目标IP是否在这个子网内。</p>
<h2 id="arp协议"><a href="#arp协议" class="headerlink" title="arp协议"></a>arp协议</h2><p>网络包在物理层传输的时候依赖的mac 地址而不是上面的IP地址，也就是根据mac地址来决定把包发到哪里去。 </p>
<p>arp协议就是查询某个IP地址的mac地址是多少，由于这种对应关系一般不太变化，所以每个os都有一份arp缓存（一般15分钟过期），也可以手工清理，下面是arp缓存的内容：</p>
<pre><code>$arp -a
e010010011202.bja.tbsite.net (1.1.11.202) at 00:16:3e:01:c2:00 [ether] on eth0
? (1.1.15.254) at 0c:da:41:6e:23:00 [ether] on eth0
v125004187.bja.tbsite.net (1.1.4.187) at 00:16:3e:01:cb:00 [ether] on eth0
e010010001224.bja.tbsite.net (1.1.1.224) at 00:16:3e:01:64:00 [ether] on eth0
v125009121.bja.tbsite.net (1.1.9.121) at 00:16:3e:01:b8:ff [ether] on eth0
e010010009114.bja.tbsite.net (1.1.9.114) at 00:16:3e:01:7c:00 [ether] on eth0
v125012028.bja.tbsite.net (1.1.12.28) at 00:16:3e:00:fb:ff [ether] on eth0
e010010005234.bja.tbsite.net (1.1.5.234) at 00:16:3e:01:ee:00 [ether] on eth0
</code></pre>
<h2 id="进入正题，回车后发生什么"><a href="#进入正题，回车后发生什么" class="headerlink" title="进入正题，回车后发生什么"></a>进入正题，回车后发生什么</h2><p>有了上面的基础知识打底，我们来思考一下 ping IP 到底发生了什么。</p>
<p>首先 OS 的协议栈需要把ping命令封成一个icmp包，要填上包头（包括src-IP、mac地址），那么OS先根据目标IP和本机的route规则计算使用哪个interface(网卡），确定了路由也就基本上知道发送包的src-ip和src-mac了。每条路由规则基本都包含目标IP范围、网关、MAC地址、网卡这样几个基本元素。</p>
<h3 id="如果目标IP和本机使用的IP在同一子网"><a href="#如果目标IP和本机使用的IP在同一子网" class="headerlink" title="如果目标IP和本机使用的IP在同一子网"></a>如果目标IP和本机使用的IP在同一子网</h3><p>如果目标IP和本机IP是同一个子网（根据本机ifconfig上的每个网卡的netmask来判断是否是同一个子网–知识点：子网掩码的作用），并且本机arp缓存没有这条IP对应的mac记录，那么给整个子网的所有机器广播发送一个 arp查询</p>
<p>比如我ping 1.1.3.42，然后tcpdump抓包首先看到的是一个arp请求：</p>
<pre><code>$sudo tcpdump -i eth0  arp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes
16:22:01.792501 ARP, Request who-has e010010003042.bja.tbsite.net tell e010125003033.bja, length 28
16:22:01.792566 ARP, Reply e010010003042.bja.tbsite.net is-at 00:16:3e:01:8d:ff (oui Unknown), length 28
</code></pre>
<p>上面就是本机发送广播消息，1.1.3.42的mac地址是多少？很快1.1.3.42回复了自己的mac地址。<br>收到这个回复后，先缓存起来，下个ping包就不需要再次发arp广播了。<br>然后将这个mac地址填写到ping包的包头的目标Mac（icmp包），然后发出这个icmp request包，按照mac地址，正确到达目标机器，然后对方正确回复icmp reply【对方回复也要查路由规则，arp查发送方的mac，这样回包才能正确路由回来，略过】。</p>
<p>来看一次完整的ping 1.1.3.43，tcpdump抓包结果：</p>
<pre><code>$sudo tcpdump -i eth0  arp or icmp
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on eth0, link-type EN10MB (Ethernet), capture size 65535 bytes
16:25:15.195401 ARP, Request who-has e010010003043.bja.tbsite.net tell e010010003033.bja, length 28
16:25:15.195459 ARP, Reply e010010003043.bja.tbsite.net is-at 00:16:3e:01:0c:ff (oui Unknown), length 28
16:25:15.211505 IP e010010003033.bja &gt; e010010003043.bja.tbsite.net: ICMP echo request, id 27990, seq 1, length 64
16:25:15.212056 IP e010010003043.bja.tbsite.net &gt; e010010003033.bja: ICMP echo reply, id 27990, seq 1, length 64
</code></pre>
<p>我换了个IP地址，接着再ping同一个IP地址，arp有缓存了就看不到arp广播查询过程了。</p>
<h3 id="如果目标IP不是同一个子网"><a href="#如果目标IP不是同一个子网" class="headerlink" title="如果目标IP不是同一个子网"></a>如果目标IP不是同一个子网</h3><p>arp只是同一子网广播查询，如果目标IP不是同一子网的话就要经过本IP网关进行转发(知识点–网关的作用)，如果本机没有缓存网关mac（一般肯定缓存了），那么先发送一次arp查询网关的mac，然后流程跟上面一样，只是这个icmp包发到网关上去了（mac地址填写的是网关的mac）</p>
<p>从本机1.1.3.33 ping 11.239.161.60的过程，因为不是同一子网按照路由规则匹配，根据route表应该走1.1.15.254这个网关，如下截图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/85e8fc6b2614aed26bc3a6d70050bf36.png" alt="image.png"></p>
<p>首先是目标IP 11.239.161.60 符合最上面红框中的路由规则，又不是同一子网，所以查找路由规则中的网关1.1.15.254的Mac地址，arp cache中有，于是将 0c:da:41:6e:23:00 填入包头，那么这个icmp request包就发到1.1.15.254上了，虽然包头的mac是 0c:da:41:6e:23:00，但是IP还是 11.239.161.60.</p>
<p>看看目标IP 11.239.161.60 的真正mac信息（跟ping包包头的Mac是不同的）：</p>
<pre><code>eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500
    inet 11.239.161.60  netmask 255.255.252.0  broadcast 11.239.163.255
    ether 00:16:3e:00:04:c4  txqueuelen 1000  (Ethernet)
</code></pre>
<p>这个包根据Mac地址路由到了网关上</p>
<h3 id="网关接下来怎么办"><a href="#网关接下来怎么办" class="headerlink" title="网关接下来怎么办"></a>网关接下来怎么办</h3><p><strong>为了简化问题，假设两个网关直连</strong></p>
<p>网关收到这个包后（因为mac地址是它的），打开一看IP地址是 11.239.161.60，不是自己的，于是继续查自己的route和arp缓存，发现11.239.161.60这个IP的网关是11.239.163.247，于是把包的目的mac地址改成11.239.163.247的mac继续发出去。</p>
<p>11.239.163.247这个网关收到包后，一看 11.239.161.60是自己同一子网的IP，于是该arp广播找mac就广播，cache有就拿cache的，然后这个包才最终到达目的11.239.161.60上。</p>
<p>整个过程中目标mac地址每一跳都在变，IP地址不变，每经过一次MAC变化可以简单理解成一跳。</p>
<p>实际上可能要经过多个网关多次跳跃才能真正到达目标机器</p>
<h3 id="目标机器收到这个icmp包后的回复过程一样，略过。"><a href="#目标机器收到这个icmp包后的回复过程一样，略过。" class="headerlink" title="目标机器收到这个icmp包后的回复过程一样，略过。"></a>目标机器收到这个icmp包后的回复过程一样，略过。</h3><h3 id="arp广播风暴和arp欺骗"><a href="#arp广播风暴和arp欺骗" class="headerlink" title="arp广播风暴和arp欺骗"></a>arp广播风暴和arp欺骗</h3><p>广播风暴：如果一个子网非常大，机器非常多，每次arp查询都是广播的话，也容易因为N*N的问题导致广播风暴。</p>
<p>arp欺骗：同样如果一个子网中的某台机器冒充网关或者其他机器，当收到arp广播查询的时候总是把自己的mac冒充目标机器的mac发给你，然后你的包先走到他，再转发给真正的网关或者目标机器，所以在里面动点什么手脚，看看你发送的内容都还是很容易的。</p>
<h2 id="讲完基础知识再来看开篇问题的答案"><a href="#讲完基础知识再来看开篇问题的答案" class="headerlink" title="讲完基础知识再来看开篇问题的答案"></a>讲完基础知识再来看开篇问题的答案</h2><p>读完上面的基础知识相信现在我们已经能够回答 ping IP 后发生了什么，这些已经足够解决99%程序员日常网络中的网络为什么不通的问题了。但是前面问题比这个要稍微复杂一点，不过还是依靠这些基础知识就能解决的–这是基础知识的威力。</p>
<h3 id="现场网络同学所做的一些其它测试："><a href="#现场网络同学所做的一些其它测试：" class="headerlink" title="现场网络同学所做的一些其它测试："></a>现场网络同学所做的一些其它测试：</h3><ol>
<li>怀疑不通的IP所使用的mac地址冲突，在交换机上清理了交换机的arp缓存，没有帮助，还是不通；</li>
<li>新拿出一台物理机配置上不通的容器的IP，这是通的，所以负责网络的同学坚持是容器网络的配置导致了问题。</li>
</ol>
<p>对于1能通，我认为这个测试不严格，新物理机所用的mac不一样，并且所接的交换机口也不一样，影响了测试结果。</p>
<h3 id="祭出万能手段–抓包"><a href="#祭出万能手段–抓包" class="headerlink" title="祭出万能手段–抓包"></a>祭出万能手段–抓包</h3><p><strong>抓包在网络问题中是万能的，但是第一次容易被tcpdump抓包命令的众多参数吓晕，不去操作你永远上不了手，差距也就拉开了，你看差距有时候只是你对一条命令的执行</strong></p>
<p>在物理机2上抓包：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/cab05a87298fc4b6ff6152b2ff4c061b.png" alt="image.png"></p>
<pre><code>tcpdump: listening on em1, link-type EN10MB (Ethernet), capture size 65535 bytes
f4:0f:1b:ae:15:fb &gt; 18:66:da:f0:15:90, ethertype 802.1Q (0x8100), length 102: vlan 134, p 0, ethertype IPv4, (tos 0x0, ttl 63, id 5654, offset 0, flags [DF], proto ICMP (1), length 84)
10.159.43.162 &gt; 10.159.43.1: ICMP echo request, id 6285, seq 1, length 64
18:66:da:f0:15:90 &gt; 00:00:0c:9f:f0:86, ethertype 802.1Q (0x8100), length 102: vlan 134, p 0, ethertype IPv4, (tos 0x0, ttl 64, id 21395, offset 0, flags [none], proto ICMP (1), length 84)
10.159.43.1 &gt; 10.159.43.162: ICMP echo reply, id 6285, seq 1, length 64
</code></pre>
<p>这个抓包能看到核心证据，ping包有到达物理机2，同时物理机2也正确回复了（mac、ip都对）</p>
<p>同时在物理机1上抓包（抓包截图略掉）只能看到ping包出去，回包没有到物理机1（所以回包肯定不会回到容器里了）</p>
<p><strong>到这里问题的核心在交换机没有正确把物理机2的回包送到物理机1上面。</strong></p>
<p>同时观察到的不正常延时都在网关那一跳：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/7a6acf5f4897118e511e7165059b33c5.png" alt="image.png"></p>
<h3 id="最终的原因"><a href="#最终的原因" class="headerlink" title="最终的原因"></a>最终的原因</h3><p>最后在交换机上分析包没正确发到物理机1上的原因跟客户交换机使用了HSRP（热备份路由器协议，就是多个交换机HA高可用，也就是同一子网可以有多个网关的IP），停掉HSRP后所有IP容器都能通了，并且前面的某些容器延时也恢复正常了。</p>
<p><strong>通俗点说就是HSRP把回包拐跑了，有些回包拐跑了又送回来了（延时200ms那些）</strong></p>
<p>至于HSRP为什么会这么做，要厂家出来解释了。这里关键在于能让客户认同问题出现在交换机上还是前面的抓包证据充分，无可辩驳。实际中我们都习惯不给证据就说：我的程序没问题，就是你的问题。这样表述没有一点意义，我们是要拿着证据这么说，对方也好就着证据来反驳，这叫优雅地甩锅。</p>
<h2 id="网络到底通不通是个复杂的问题吗？"><a href="#网络到底通不通是个复杂的问题吗？" class="headerlink" title="网络到底通不通是个复杂的问题吗？"></a>网络到底通不通是个复杂的问题吗？</h2><p>讲这个过程的核心目的是除了真正的网络不通，有些是服务不可用了也怪网络。很多现场的同学根本讲不清自己的服务（比如80端口上的tomcat服务）还在不在，网络通不通，是网络不通呢还是服务出了问题。一看到SocketTimeoutException 就想把网络同学抓过来羞辱两句：网络不通了，网络抖动导致我的程序异常了（网络抖动是个万能的扛包侠）。</p>
<p>实际这里涉及到四个节点（以两个网关直连为例），srcIP -&gt; src网关 -&gt; dest网关 -&gt; destIP。如果ping不通(也有特殊的防火墙限制ping包不让过的），那么在这四段中分段ping（二分查找程序员应该最熟悉了）。 比如前面的例子就是网关没有把包转发回来</p>
<p>抓包看ping包有没有出去，对方抓包看有没有收到，收到后有没有回复。</p>
<p>ping自己网关能不能通，ping对方网关能不能通。</p>
<h2 id="接下来说点跟程序员日常相关的"><a href="#接下来说点跟程序员日常相关的" class="headerlink" title="接下来说点跟程序员日常相关的"></a>接下来说点跟程序员日常相关的</h2><h3 id="如果网络能ping通，服务无法访问"><a href="#如果网络能ping通，服务无法访问" class="headerlink" title="如果网络能ping通，服务无法访问"></a>如果网络能ping通，服务无法访问</h3><p>那么尝试telnet IP port 看看你的服务是否还在监听端口，在的话再看看服务进程是否能正常响应新的请求。有时候是进程死掉了，端口也没人监听了；有时候是进程还在但是假死了，所以端口也不响应新的请求了；<a href="https://mp.weixin.qq.com/s/yH3PzGEFopbpA-jw4MythQ" target="_blank" rel="noopener">还有的是TCP连接队列满了不能响应新的连接</a></p>
<p>如果端口还在也是正常的话，telnet应该是好的：</p>
<pre><code>$telnet 1.1.161.60 2376
Trying 1.1.161.60...
Connected to 1.1.161.60.
Escape character is &#39;^]&#39;.
^C
Connection closed by foreign host.
</code></pre>
<p>假如我故意换成一个不存在的端口，目标机器上的OS直接就拒绝了这个连接（抓包的话一般是看到reset标识）：</p>
<pre><code>$telnet 1.1.161.60 2379
Trying 1.1.161.60...
telnet: connect to address 11.239.161.60: Connection refused
</code></pre>
<h3 id="一个SocketTimeoutException，程序员首先怀疑网络丢包的Case"><a href="#一个SocketTimeoutException，程序员首先怀疑网络丢包的Case" class="headerlink" title="一个SocketTimeoutException，程序员首先怀疑网络丢包的Case"></a>一个SocketTimeoutException，程序员首先怀疑网络丢包的Case</h3><p>当时的反馈应用代码抛SocketTimeoutException，怀疑网络问题：</p>
<ol>
<li>业务应用连接Server 偶尔会出现超时异常；</li>
<li>有很多这样的异常日志：[Server  SocketTimeoutException]</li>
</ol>
<p>检查一下当时的网络状态非常好，出问题时间段的网卡的流量信息也非常正常：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/81199130d4b2d5cf441944d9e11cc5fd.png" alt="image.png"></p>
<p>上图是通过sar监控到的9号 v24d9e0f23d40 这个网卡的流量，看起来也是正常，流量没有出现明显的波动</p>
<p>为了监控网络到底有没有问题，接着在出问题的两个容器上各启动一个http server，然后在对方每1秒钟互相发一次发http get请求访问这个http server，基本认识告诉我们如果网络丢包、卡顿严重，那么我这个http server的监控日志时间戳也会跳跃，如果应用是因为网络出现异常那么我启动的http服务也会出现异常–宁愿写个工具都不背锅（主要是背了锅也不一定能解决掉问题）。</p>
<p>从实际监控来看，应用出现异常的时候我的http服务是正常的（写了脚本判断日志的连续性）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/c9d5ff245eac6a023acd7148b7676b4f.png" alt="image.png"></p>
<p>这也强有力地证明了网络没问题，所以写业务代码的同学一门心思集中火力查看应用的问题。后来的实际调查发现是应用假死掉了（内部线程太多，卡死了），服务端口不响应请求了。</p>
<p>如果基础知识缺乏一点那么甩过来的这个锅网络是扛不动的，同时也阻碍了问题的真正发现。</p>
<h3 id="TCP建连接过程跟前面ping一样，只是把ping的icmp协议换成TCP协议，也是要先根据route，然后arp"><a href="#TCP建连接过程跟前面ping一样，只是把ping的icmp协议换成TCP协议，也是要先根据route，然后arp" class="headerlink" title="TCP建连接过程跟前面ping一样，只是把ping的icmp协议换成TCP协议，也是要先根据route，然后arp"></a>TCP建连接过程跟前面ping一样，只是把ping的icmp协议换成TCP协议，也是要先根据route，然后arp</h3><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><strong>网络丢包，卡顿，抖动很容易做扛包侠，只有找到真正的原因解决问题才会更快，要不在错误的方向上怎么发力都不对。准确的方向要靠好的基础知识和正确的逻辑以及证据来支撑，而不是猜测</strong></p>
<ul>
<li>基础知识是决定你能否干到退休的关键因素；</li>
<li>有了基础知识不代表你能真正转化成生产力；</li>
<li>越是基础，越是几十年不变的基础越是重要；</li>
<li>知识到灵活运用要靠实践，同时才能把知识之间的联系建立起来；</li>
<li>简而言之缺的是融会贯通和运用；</li>
<li>做一个有礼有节的甩包侠；</li>
<li>在别人不给证据愚昧甩包的情况下你的机会就来了。</li>
</ul>
<h2 id="留几个小问题"><a href="#留几个小问题" class="headerlink" title="留几个小问题"></a>留几个小问题</h2><ol>
<li>server回复client的时候是如何确定回复包中的src-ip和dest-mac的？一定是请求包中的dest-ip当成src-ip吗？</li>
<li>上面问题中如果是TCP或者UDP协议，他们回复包中的src-ip和dest-mac获取会不一样吗？</li>
<li>既然局域网中都是依赖Mac地址来定位，那么IP的作用又是什么呢？</li>
</ol>
<hr>
<h3 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h3><p><a href="https://tools.ietf.org/html/rfc1180" target="_blank" rel="noopener">https://tools.ietf.org/html/rfc1180</a></p>
<p>《计算机基础》</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/12/07/如何追踪网络流量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/12/07/如何追踪网络流量/" itemprop="url">如何追踪网络流量</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-12-07T17:30:03+08:00">
                2017-12-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="如何追踪网络流量"><a href="#如何追踪网络流量" class="headerlink" title="如何追踪网络流量"></a>如何追踪网络流量</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>某些场景下通过监控发现了流量比较大，不太合理，需要知道这些流量都是哪些进程访问哪些服务触发的</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ol>
<li>定位流量是由哪个进程触发的</li>
<li>定位流量主要是访问哪些ip导致的</li>
<li>定位具体的端口有较大的流量</li>
</ol>
<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><p>nethogs&#x2F;iftop&#x2F;tcptrack</p>
<h2 id="定位进程"><a href="#定位进程" class="headerlink" title="定位进程"></a>定位进程</h2><pre><code>sudo nethogs 
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/e89eaf27aa98a6192d109ee22f9c0da8.png" alt="image.png"></p>
<p>从上图可以看到总的流量，以及每个进程的流量大小。这里可以确认流量主要是3820的java进程消耗的</p>
<h2 id="定位ip"><a href="#定位ip" class="headerlink" title="定位ip"></a>定位ip</h2><pre><code>sudo iftop -p -n -B
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/1f03abbebbfc173b5af3163d017fd901.png" alt="image.png"></p>
<p>通过上图可以看到流量主要是消耗在 10.0.48.1的ip上</p>
<h2 id="定位端口"><a href="#定位端口" class="headerlink" title="定位端口"></a>定位端口</h2><p>10.0.48.1 有可能是一个mapping ip，需要进一步查看具体</p>
<pre><code>sudo tcptrack -r 5 -i eth0  //然后输入小写s，按流量排序
sudo tcptrack -r 5 -i eth0 host 10.0.48.1 //filter 语法和tcpdump一样
</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/07f0fceb6af4c4387832561b630c00b3.png" alt="image.png"></p>
<p>可以看到4355&#x2F;4356端口上流量相对较大</p>
<h2 id="软件安装"><a href="#软件安装" class="headerlink" title="软件安装"></a>软件安装</h2><p>后续发布新镜像都会带上这三个软件的rpm安装包<br>目前可以手动下载这三个rpm安装包：</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/10/31/磁盘爆掉的几种情况/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/10/31/磁盘爆掉的几种情况/" itemprop="url">磁盘爆掉的几种情况</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-10-31T12:30:03+08:00">
                2017-10-31
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Linux/" itemprop="url" rel="index">
                    <span itemprop="name">Linux</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Docker宿主机磁盘爆掉的几种情况"><a href="#Docker宿主机磁盘爆掉的几种情况" class="headerlink" title="Docker宿主机磁盘爆掉的几种情况"></a>Docker宿主机磁盘爆掉的几种情况</h1><h2 id="磁盘爆掉的几种情况"><a href="#磁盘爆掉的几种情况" class="headerlink" title="磁盘爆掉的几种情况"></a>磁盘爆掉的几种情况</h2><ol>
<li>系统磁盘没有空间，解决办法：删掉 &#x2F;var&#x2F;log&#x2F; 下边的带日期的日志，清空 &#x2F;var&#x2F;log&#x2F;messages 内容</li>
<li>容器使用的大磁盘但是仍然空间不够，有三个地方会使用大量的磁盘<ul>
<li>容器内部日志非常大，处理办法见方法一</li>
<li>容器内部产生非常多或者非常大的文件，但是这个文件的位置又通过volume 挂载到了物理机上，处理办法见方法二</li>
<li>对特别老的部署环境，还有可能是容器的系统日志没有限制大小，处理办法见方法三</li>
</ul>
</li>
</ol>
<h2 id="现场的同学按如下方法依次检查"><a href="#现场的同学按如下方法依次检查" class="headerlink" title="现场的同学按如下方法依次检查"></a>现场的同学按如下方法依次检查</h2><h3 id="方法零：-检查系统根目录下每个文件夹的大小"><a href="#方法零：-检查系统根目录下每个文件夹的大小" class="headerlink" title="方法零： 检查系统根目录下每个文件夹的大小"></a>方法零： 检查系统根目录下每个文件夹的大小</h3><p><code>sudo du / -lh --max-depth=1 --exclude=overlay --exclude=proc</code></p>
<p>看看除了容器之外有没有其它目录使用磁盘特别大，如果有那么一层层进去通过du命令来查看，比如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#sudo du / -lh --max-depth=1 --exclude=overlay --exclude=proc</span><br><span class="line">16K	/dev</span><br><span class="line">16K	/lost+found</span><br><span class="line">4.0K	/media</span><br><span class="line">17G	/home</span><br><span class="line">136M	/boot</span><br><span class="line">832K	/run</span><br><span class="line">1.9G	/usr</span><br><span class="line">75M	/tmp</span><br><span class="line">12K	/log</span><br><span class="line">8.5G	/var</span><br><span class="line">4.0K	/srv</span><br><span class="line">0	/proc</span><br><span class="line">22M	/etc</span><br><span class="line">84G	/root</span><br><span class="line">4.0K	/mnt</span><br><span class="line">508M	/opt</span><br><span class="line">0	/sys</span><br><span class="line">112G	/</span><br></pre></td></tr></table></figure>

<p>那么这个案例中应该查看 &#x2F;root下为什么用掉了84G（总共用了112G）， 先 cd &#x2F;root 然后执行： sudo du . -lh –max-depth&#x3D;1 –exclude&#x3D;overlay 进一步查看 &#x2F;root 目录下每个文件夹的大小</p>
<p><strong>如果方法零没找到占用特别大的磁盘文件，那么一般来说是容器日志占用太多的磁盘空间，请看方法一</strong></p>
<h3 id="方法一：-容器内部日志非常大（请确保先按方法零检查过了）"><a href="#方法一：-容器内部日志非常大（请确保先按方法零检查过了）" class="headerlink" title="方法一： 容器内部日志非常大（请确保先按方法零检查过了）"></a>方法一： 容器内部日志非常大（请确保先按方法零检查过了）</h3><p>在磁盘不够的物理机上执行如下脚本：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker ps -a -q &gt;containers.list</span><br><span class="line"></span><br><span class="line">sudo cat containers.list | xargs sudo docker inspect $1 | grep merged | awk -F \&quot; &apos;&#123; print $4 &#125;&apos; | sed &apos;s/\/merged//g&apos; | xargs sudo du  --max-depth=0 $1 &gt;containers.size </span><br><span class="line"></span><br><span class="line">sudo paste containers.list containers.size | awk &apos;&#123; print $1, $2 &#125;&apos;  | sort -nk2 &gt;real_size.log</span><br><span class="line"></span><br><span class="line">sudo tail -10 real_size.log  | awk &apos;BEGIN &#123;print &quot;\tcontainer     size\tunit&quot;&#125; &#123; print NR&quot;:\t&quot; $0&quot;\t kB&quot; &#125;&apos;</span><br></pre></td></tr></table></figure>

<h5 id="执行完后会输出如下格式："><a href="#执行完后会输出如下格式：" class="headerlink" title="执行完后会输出如下格式："></a>执行完后会输出如下格式：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">container     size	unit</span><br><span class="line">1:	22690f16822f 3769980	 kb</span><br><span class="line">2:	82b4ae98eeed 4869324	 kb</span><br><span class="line">3:	572a1b7c8ef6 10370404	 kb</span><br><span class="line">4:	9f9250d98df6 10566776	 kb</span><br><span class="line">5:	7fab70481929 13745648	 kb</span><br><span class="line">6:	4a14b58e3732 29873504	 kb</span><br><span class="line">7:	8a01418b6df2 30432068	 kb</span><br><span class="line">8:	83dc85caaa5c 31010960	 kb</span><br><span class="line">9:	433e51df88b1 35647052	 kb</span><br><span class="line">10:	4b42818a8148 61962416	 kb</span><br></pre></td></tr></table></figure>

<p>第二列是容器id，第三列是磁盘大小，第四列是单位， 占用最大的排在最后面</p>
<h5 id="然后进到容器后通过-du-x2F-–max-depth-x3D-2-快速发现大文件"><a href="#然后进到容器后通过-du-x2F-–max-depth-x3D-2-快速发现大文件" class="headerlink" title="然后进到容器后通过 du &#x2F; –max-depth&#x3D;2 快速发现大文件"></a>然后进到容器后通过 du &#x2F; –max-depth&#x3D;2 快速发现大文件</h5><h3 id="方法二：-容器使用的volume使用过大"><a href="#方法二：-容器使用的volume使用过大" class="headerlink" title="方法二： 容器使用的volume使用过大"></a>方法二： 容器使用的volume使用过大</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$sudo du -l /data/lib/docker/defaultVolumes --max-depth=1 | sort -rn</span><br><span class="line">456012884	/data/lib/docker/defaultVolumes</span><br><span class="line">42608332	/data/lib/docker/defaultVolumes/task_3477_g0_ark-metadb_miniDBPaaS-MetaDB_1</span><br><span class="line">32322220	/data/lib/docker/defaultVolumes/task_3477_g0_dbpaas-metadb_dbpaas_1</span><br><span class="line">27461120	/data/lib/docker/defaultVolumes/task_3001_g0_ark-metadb_miniDBPaaS-MetaDB_1</span><br><span class="line">27319360	/data/lib/docker/defaultVolumes/task_36000_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27313836	/data/lib/docker/defaultVolumes/task_3600_g0_dbpaas-metadb_minidbpaas</span><br><span class="line">27278692	/data/lib/docker/defaultVolumes/task_3604_g0_ark-metadb_miniDBPaaS-MetaDB_1</span><br><span class="line">27277004	/data/lib/docker/defaultVolumes/task_3603_g0_ark-metadb_miniDBPaaS-MetaDB_1</span><br><span class="line">27275736	/data/lib/docker/defaultVolumes/task_3542_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27271428	/data/lib/docker/defaultVolumes/task_3597_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27270840	/data/lib/docker/defaultVolumes/task_3603_g0_dbpaas-metadb_minidbpaas_1</span><br><span class="line">27270492	/data/lib/docker/defaultVolumes/task_3603_g0_dbpaas-metadb_minidbpaas</span><br><span class="line">27270468	/data/lib/docker/defaultVolumes/task_3600_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27270252	/data/lib/docker/defaultVolumes/task_3535_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27270244	/data/lib/docker/defaultVolumes/task_3538_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">27270244	/data/lib/docker/defaultVolumes/task_3536_g0_ark-metadb_miniDBPaaS-MetaDB</span><br><span class="line">25312404	/data/lib/docker/defaultVolumes/task_3477_g0_dncs-server_middleware-dncs_2</span><br></pre></td></tr></table></figure>

<p>&#x2F;data&#x2F;lib&#x2F;docker&#x2F;defaultVolumes 参数是默认设置的volume存放的目录（一般是docker的存储路径下 –graph&#x3D;&#x2F;data&#x2F;lib&#x2F;docker) ，第一列是大小，后面是容器名</p>
<p>volume路径在物理机上也有可能是 &#x2F;var&#x2F;lib&#x2F;docker 或者 &#x2F;mw&#x2F;mvdocker&#x2F; 之类的路径下，这个要依据安装参数来确定，可以用如下命令来找到这个路径：</p>
<p><code>sudo systemctl status docker -l | grep --color graph</code></p>
<p>结果如下，红色参数后面的路径就是docker 安装目录，到里面去找带volume的字眼：</p>
<p><img src="/images/oss/1558521949392-d1ab9886-9f08-4ebf-bfdb-5283461ed9de.png#align=left&display=inline&height=165&originHeight=165&originWidth=930&size=0&status=done&width=930"></p>
<p>找到 volume很大的文件件后同样可以进到这个文件夹中执行如下命令快速发现大文件：</p>
<p><code>du . --max-depth=2</code></p>
<h3 id="方法三-容器的系统日志没有限制大小"><a href="#方法三-容器的系统日志没有限制大小" class="headerlink" title="方法三 容器的系统日志没有限制大小"></a>方法三 容器的系统日志没有限制大小</h3><p>这种情况只针对2017年上半年之前的部署环境，后面部署的环境默认都控制了这些日志不会超过150M</p>
<p>按照方法二的描述先找到docker 安装目录，cd 进去，然后 ：</p>
<p><code>du ./containers --max-depth=2</code></p>
<p>就很快找到那个大json格式的日志文件了,然后执行清空这个大文件的内容：</p>
<p><code>echo &#39;&#39; | sudo tee 大文件名</code></p>
<h3 id="一些其他可能占用空间的地方"><a href="#一些其他可能占用空间的地方" class="headerlink" title="一些其他可能占用空间的地方"></a>一些其他可能占用空间的地方</h3><ul>
<li>机器上镜像太多，可以删掉一些没用的： sudo docker images -q | xargs sudo docker rmi</li>
<li>机器上残留的volume太多，删：sudo docker volume ls -q | xargs sudo docker volume rm</li>
<li>物理文件被删了，但是还有进程占用这个文件句柄，导致文件对应的磁盘空间没有释放，检查： lsof |　grep deleted  如果这个文件非常大的话，只能通过重启这个进程来真正释放磁盘空间</li>
</ul>
<h3 id="OverlayFS（overlay）的镜像分层与共享"><a href="#OverlayFS（overlay）的镜像分层与共享" class="headerlink" title="OverlayFS（overlay）的镜像分层与共享"></a><a href="https://hhbbz.github.io/2018/03/28/Docker%E5%AE%B9%E5%99%A8%E5%8D%A0%E7%94%A8%E7%A3%81%E7%9B%98%E5%86%85%E5%AD%98%E8%BF%87%E5%A4%A7%E7%9A%84%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" target="_blank" rel="noopener">OverlayFS（overlay）的镜像分层与共享</a></h3><p>OverlayFS使用两个目录，把一个目录置放于另一个之上，并且对外提供单个统一的视角。这两个目录通常被称作层，这个分层的技术被称作union mount。术语上，下层的目录叫做lowerdir，上层的叫做upperdir。对外展示的统一视图称作merged。 　　</p>
<p>如下图所示，Overlay在主机上用到2个目录，这2个目录被看成是overlay的层。 upperdir为容器层、lowerdir为镜像层使用联合挂载技术将它们挂载在同一目录(merged)下，提供统一视图。</p>
<p><img src="/images/951413iMgBlog/overlay_constructs.jpg" alt="图片"></p>
<p>注意镜像层和容器层是如何处理相同的文件的：容器层（upperdir）的文件是显性的，会隐藏镜像层（lowerdir）相同文件的存在。容器映射（merged）显示出统一的视图。 　　overlay驱动只能工作在两层之上。也就是说多层镜像不能用多层OverlayFS实现。替代的，每个镜像层在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay中用自己的目录来实现，使用硬链接这种有效利用空间的方法，来引用底层分享的数据。注意：Docker1.10之后，镜像层ID和&#x2F;var&#x2F;lib&#x2F;docker中的目录名不再一一对应。 　　创建一个容器，overlay驱动联合镜像层和一个新目录给容器。镜像顶层是overlay中的只读lowerdir，容器的新目录是可写的upperdir。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/08/28/netstat 等网络工具/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/28/netstat 等网络工具/" itemprop="url">netstat timer keepalive RTO explain</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-28T10:30:03+08:00">
                2017-08-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/network/" itemprop="url" rel="index">
                    <span itemprop="name">network</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="netstat-等网络工具"><a href="#netstat-等网络工具" class="headerlink" title="netstat 等网络工具"></a>netstat 等网络工具</h1><h2 id="netstat-和重传–-timer"><a href="#netstat-和重传–-timer" class="headerlink" title="netstat 和重传– timer"></a>netstat 和重传– timer</h2><p>经常碰到一些断网环境下需要做快速切换，那么断网后需要多久tcp才能感知到这个断网，并断开连接触发上层的重连（一般会连向新的server）</p>
<p>netstat -st命令中，tcp: 部分取自&#x2F;proc&#x2F;net&#x2F;snmp，而TCPExt部分取自&#x2F;proc&#x2F;net&#x2F;netstat，该文件对TCP记录了更多的统计。sysstat包也会采集&#x2F;proc&#x2F;net&#x2F;snmp</p>
<h3 id="keepalive"><a href="#keepalive" class="headerlink" title="keepalive"></a>keepalive</h3><p>from: <a href="https://superuser.com/questions/240456/how-to-interpret-the-output-of-netstat-o-netstat-timers" target="_blank" rel="noopener">https://superuser.com/questions/240456/how-to-interpret-the-output-of-netstat-o-netstat-timers</a></p>
<p>The timer column has two fields (from your o&#x2F;p above):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">keepalive     (6176.47/0/0)  </span><br><span class="line">&lt;1st field&gt;   &lt;2nd field&gt;</span><br></pre></td></tr></table></figure>

<p>The 1st field can have values:<br>keepalive - when the keepalive timer is ON for the socket  </p>
<p>on - when the retransmission timer is ON for the socket  </p>
<p>off - none of the above is ON</p>
<p>on - #表示是重发（retransmission）的时间计时</p>
<p>off - #表示没有时间计时</p>
<p>timewait - #表示等待（timewait）时间计时</p>
<p><a href="http://vzkernel.blogspot.com/2012/09/description-of-netstat-timers.html" target="_blank" rel="noopener">Probe zerowindow</a></p>
<p>keepalive 是指在连接闲置状态发送心跳包来检测连接是否还有效（比如对方掉电后肯定就无效了，tcp得靠这个keepalive来感知）。如果有流量在传输过程中对方掉电后会不停地 retransmission ，这个时候看到的就是 on，然后重传间隔和次数跟keepalive参数无关，只和 net.ipv4.tcp_retries1、net.ipv4.tcp_retries2相关了。</p>
<p>keepalive 状态下的连接：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/f1a219a2bd99690fd3ed391bf5ab65cb.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#netstat -anto |grep -E &quot;:3048|Q&quot; | awk &apos;&#123; if($3&gt;0) print $0 &#125;&apos;</span><br><span class="line">Proto Recv-Q Send-Q Local Address     Foreign Address         State       Timer</span><br><span class="line">tcp        0  22207 1.2.3.134:3048        4.3.44.45:40100     ESTABLISHED probe (0.05/0/0)</span><br><span class="line">tcp        0  56960 1.2.3.134:3048        4.3.7.8:40057       ESTABLISHED probe (0.06/0/0)</span><br><span class="line">tcp        0  59808 1.2.3.134:3048        4.3.9.10:40085      ESTABLISHED on (0.21/0/0)</span><br><span class="line">tcp        0  64080 1.2.3.134:3048        4.3.7.8:40055       ESTABLISHED on (0.19/0/0)</span><br><span class="line">tcp        0   4788 1.2.3.134:3048        4.3.0.1:40075       ESTABLISHED probe (0.01/0/0)</span><br><span class="line">tcp        0  44144 1.2.3.134:3048        4.3.7.8:40049       ESTABLISHED probe (0.05/0/0)</span><br><span class="line">tcp        0  52688 1.2.3.134:3048        4.3.34.35:40068     ESTABLISHED probe (0.06/0/0)</span><br><span class="line">tcp        0   4788 1.2.3.134:3048        4.3.4.5:40073       ESTABLISHED probe (0.18/0/0)</span><br><span class="line">tcp        0  31894 1.2.3.134:3048        4.3.17.18:40072     ESTABLISHED probe (0.17/0/0)</span><br><span class="line">tcp        0  44144 1.2.3.134:3048        4.3.12.13:40099     ESTABLISHED probe (0.02/0/0)</span><br></pre></td></tr></table></figure>

<p>The 2nd field has THREE subfields:</p>
<p>(6176.47&#x2F;0&#x2F;0) -&gt; (a&#x2F;b&#x2F;c)<br>a&#x3D;timer value (a&#x3D;keepalive timer, when 1st field&#x3D;“keepalive”; a&#x3D;retransmission timer, when 1st field&#x3D;“on”)  </p>
<p>b&#x3D;number of retransmissions that have occurred  </p>
<p>c&#x3D;number of keepalive probes that have been sent</p>
<blockquote>
<p>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_keepalive_time<br>当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时。</p>
<p>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_keepalive_intvl<br>当探测没有确认时，重新发送探测的频度。缺省是75秒。</p>
<p>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_keepalive_probes<br>在认定连接失效之前，发送多少个TCP的keepalive探测包。缺省值是9。这个值乘以tcp_keepalive_intvl之后决定了，一个连接发送了keepalive之后可以有多少时间没有回应</p>
</blockquote>
<p>For example, I had two sockets opened between a client &amp; a server (not loopback). The keepalive setting are:</p>
<p>KEEPALIVE_IDLETIME   30<br>KEEPALIVE_NUMPROBES   4<br>KEEPALIVE_INTVL      10  </p>
<p>And I did a shutdown of the client machine, so at …SHED on (2.47&#x2F;254&#x2F;2)  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tcp        0    210 192.0.0.1:36483             192.0.68.1:43881            ESTABLISHED on (1.39/254/2)  </span><br><span class="line">tcp        0    210 192.0.0.1:36483             192.0.68.1:43881            ESTABLISHED on (0.31/254/2)  </span><br><span class="line">tcp        0    210 192.0.0.1:36483             192.0.68.1:43881            ESTABLISHED on (2.19/255/2)  </span><br><span class="line">tcp        0    210 192.0.0.1:36483             192.0.68.1:43881            ESTABLISHED on (1.12/255/2)</span><br></pre></td></tr></table></figure>

<p>As you can see, in this case things are a little different. When the client went down, my server started sending keepalive messages, but while it was still sending those keepalives, my server tried to send a message to the client. Since the client had gone down, the server couldn’t get any ACK from the client, so the TCP retransmission started and the server tried to send the data again, each time incrementing the retransmit count (2nd field) when the retransmission timer (1st field) expired.</p>
<p>Hope this explains the netstat –timer option well.</p>
<h2 id="RTO-重传"><a href="#RTO-重传" class="headerlink" title="RTO 重传"></a><a href="https://pracucci.com/linux-tcp-rto-min-max-and-tcp-retries2.html" target="_blank" rel="noopener">RTO</a> 重传</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#define TCP_RTO_MAX ((unsigned)(120*HZ)) //HZ 通常为1秒 </span><br><span class="line">#define TCP_RTO_MIN ((unsigned)(HZ/5))</span><br></pre></td></tr></table></figure>

<p>Linux 2.6+ uses HZ of 1000ms, so <code>TCP_RTO_MIN</code> is ~200 ms and <code>TCP_RTO_MAX</code> is ~120 seconds. Given a default value of <code>tcp_retries</code> set to <code>15</code>, it means that <strong>it takes 924.6 seconds</strong> before a broken network link is notified to the upper layer (ie. application), since the connection is detected as broken when the last (15th) retry expires.</p>
<p><img src="https://pracucci.com/assets/2018-04-27-linux-tcp-rto-retries2-b71ad2ef586126c2ad4180543f78d8b0a4bf66925fb88d69889f04c4b7aedeaa.png" alt="2018-04-27-linux-tcp-rto-retries2.png"></p>
<p>The <code>tcp_retries2</code> sysctl can be <strong>tuned</strong> via <code>/proc/sys/net/ipv4/tcp_retries2</code> or the sysctl <code>net.ipv4.tcp_retries2</code>.</p>
<h3 id="查看重传状态"><a href="#查看重传状态" class="headerlink" title="查看重传状态"></a>查看重传状态</h3><p>重传状态的连接：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/88c5df7d5709e5c8b264ee0deacda0a2.png" alt="image.png"></p>
<p>前两个 syn_sent 状态明显是 9031端口不work了，握手不上。</p>
<p>最后 established 状态的连接, 是22端口给53795发了136字节的数据但是没有收到ack，所以在倒计时准备重传中。</p>
<blockquote>
<p><strong>net.ipv4.tcp_retries1 &#x3D; 3</strong></p>
</blockquote>
<p>放弃回应一个TCP <strong>连接请求前</strong>﹐需要进行多少次重试。RFC 规定最低的数值是3﹐这也是默认值﹐根据RTO的值大约在3秒 - 8分钟之间。(注意:这个值同时还决定进入的syn连接)</p>
<p><strong>(第二种解释：它表示的是TCP传输失败时不检测路由表的最大的重试次数，当超过了这个值，我们就需要检测路由表了)</strong></p>
<p>从kernel代码可以看到，一旦重传超过阈值tcp_retries1，主要的动作就是更新路由缓存。<br>用以避免由于路由选路变化带来的问题。<strong>这个时候tcp连接没有关闭</strong></p>
<blockquote>
<p> <strong>net.ipv4.tcp_retries2 &#x3D; 15</strong></p>
</blockquote>
<p>**在丢弃激活(已建立通讯状况)**的TCP连接之前﹐需要进行多少次重试。默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒).(这个值根据目前的网络设置,可以适当地改小,我的网络内修改为了5)</p>
<p><strong>(第二种解释：表示重试最大次数，只不过这个值一般要比上面的值大。和上面那个不同的是，当重试次数超过这个值，我们就必须关闭连接了)</strong></p>
<p>from：Documentation&#x2F;networking&#x2F;ip-sysctl.txt</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tcp_retries1 - INTEGER</span><br><span class="line">    This value influences the time, after which TCP decides, that</span><br><span class="line">    something is wrong due to unacknowledged RTO retransmissions,</span><br><span class="line">    and reports this suspicion to the network layer.</span><br><span class="line">    See tcp_retries2 for more details.</span><br><span class="line"></span><br><span class="line">    RFC 1122 recommends at least 3 retransmissions, which is the</span><br><span class="line">    default.</span><br><span class="line"></span><br><span class="line">tcp_retries2 - INTEGER</span><br><span class="line">    This value influences the timeout of an alive TCP connection,</span><br><span class="line">    when RTO retransmissions remain unacknowledged.</span><br><span class="line">    Given a value of N, a hypothetical TCP connection following</span><br><span class="line">    exponential backoff with an initial RTO of TCP_RTO_MIN would</span><br><span class="line">    retransmit N times before killing the connection at the (N+1)th RTO.</span><br><span class="line"></span><br><span class="line">    The default value of 15 yields a hypothetical timeout of 924.6</span><br><span class="line">    seconds and is a lower bound for the effective timeout.</span><br><span class="line">    TCP will effectively time out at the first RTO which exceeds the</span><br><span class="line">    hypothetical timeout.</span><br><span class="line"></span><br><span class="line">    RFC 1122 recommends at least 100 seconds for the timeout,</span><br><span class="line">    which corresponds to a value of at least 8.</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/1571230725657-b2b7ea40-06bc-41fb-a374-daa8de1f857d.png" alt="img"></p>
<h3 id="retries限制的重传次数吗"><a href="#retries限制的重传次数吗" class="headerlink" title="retries限制的重传次数吗"></a>retries限制的重传次数吗</h3><p>咋一看文档，很容易想到retries的数字就是限定的重传的次数，甚至源码中对于retries常量注释中都写着”This is how many retries it does…”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#define TCP_RETR1       3   /*</span><br><span class="line">                             * This is how many retries it does before it</span><br><span class="line">                             * tries to figure out if the gateway is</span><br><span class="line">                             * down. Minimal RFC value is 3; it corresponds</span><br><span class="line">                             * to ~3sec-8min depending on RTO.</span><br><span class="line">                             */</span><br><span class="line"></span><br><span class="line">#define TCP_RETR2       15  /*</span><br><span class="line">                             * This should take at least</span><br><span class="line">                             * 90 minutes to time out.</span><br><span class="line">                             * RFC1122 says that the limit is 100 sec.</span><br><span class="line">                             * 15 is ~13-30min depending on RTO.</span><br><span class="line">                             */</span><br></pre></td></tr></table></figure>

<p>那就就来看看retransmits_timed_out的具体实现，看看到底是不是限制的重传次数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">/* This function calculates a &quot;timeout&quot; which is equivalent to the timeout of a</span><br><span class="line"> * TCP connection after &quot;boundary&quot; unsuccessful, exponentially backed-off</span><br><span class="line"> * retransmissions with an initial RTO of TCP_RTO_MIN or TCP_TIMEOUT_INIT if</span><br><span class="line"> * syn_set flag is set.</span><br><span class="line"> */</span><br><span class="line">static bool retransmits_timed_out(struct sock *sk,</span><br><span class="line">                              unsigned int boundary,</span><br><span class="line">                              unsigned int timeout,</span><br><span class="line">                              bool syn_set)</span><br><span class="line">&#123;</span><br><span class="line">    unsigned int linear_backoff_thresh, start_ts;</span><br><span class="line">    // 如果是在三次握手阶段，syn_set为真</span><br><span class="line">    unsigned int rto_base = syn_set ? TCP_TIMEOUT_INIT : TCP_RTO_MIN;</span><br><span class="line"></span><br><span class="line">    if (!inet_csk(sk)-&gt;icsk_retransmits)</span><br><span class="line">            return false;</span><br><span class="line"></span><br><span class="line">    // retrans_stamp记录的是数据包第一次发送的时间，在tcp_retransmit_skb()中设置</span><br><span class="line">    if (unlikely(!tcp_sk(sk)-&gt;retrans_stamp))</span><br><span class="line">            start_ts = TCP_SKB_CB(tcp_write_queue_head(sk))-&gt;when;</span><br><span class="line">    else</span><br><span class="line">            start_ts = tcp_sk(sk)-&gt;retrans_stamp;</span><br><span class="line"></span><br><span class="line">    // 如果用户态未指定timeout，则算一个出来</span><br><span class="line">    if (likely(timeout == 0)) &#123;</span><br><span class="line">            /* 下面的计算过程，其实就是算一下如果以rto_base为第一次重传间隔，</span><br><span class="line">             * 重传boundary次需要多长时间</span><br><span class="line">             */</span><br><span class="line">            linear_backoff_thresh = ilog2(TCP_RTO_MAX/rto_base);</span><br><span class="line"></span><br><span class="line">            if (boundary &lt;= linear_backoff_thresh)</span><br><span class="line">                    timeout = ((2 &lt;&lt; boundary) - 1) * rto_base;</span><br><span class="line">            else</span><br><span class="line">                    timeout = ((2 &lt;&lt; linear_backoff_thresh) - 1) * rto_base +</span><br><span class="line">                            (boundary - linear_backoff_thresh) * TCP_RTO_MAX;</span><br><span class="line">    &#125;</span><br><span class="line">    // 如果数据包第一次发送的时间距离现在的时间间隔，超过了timeout值，则认为重传超于阈值了</span><br><span class="line">    return (tcp_time_stamp - start_ts) &gt;= timeout;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从以上的代码分析可以看到，真正起到限制重传次数的并不是真正的重传次数。<br>而是以tcp_retries1或tcp_retries2为boundary，以rto_base(如TCP_RTO_MIN 200ms)为初始RTO，计算得到一个timeout值出来。如果重传间隔超过这个timeout，则认为超过了阈值。<br>上面这段话太绕了，下面举两个个例子来说明</p>
<blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt; 以判断是否放弃TCP流为例，如果tcp_retries2=15，那么计算得到的timeout=924600ms。</span><br><span class="line">&gt; </span><br><span class="line">&gt; 1. 如果RTT比较小，那么RTO初始值就约等于下限200ms</span><br><span class="line">&gt;    由于timeout总时长是924600ms，表现出来的现象刚好就是重传了15次，超过了timeout值，从而放弃TCP流</span><br><span class="line">&gt; </span><br><span class="line">&gt; 2. 如果RTT较大，比如RTO初始值计算得到的是1000ms</span><br><span class="line">&gt;    那么根本不需要重传15次，重传总间隔就会超过924600ms。</span><br><span class="line">&gt;    比如我测试的一个RTT=400ms的情况，当tcp_retries2=10时，仅重传了3次就放弃了TCP流</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>一些重传的其它问题</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt; effective timeout指的是什么？  </span><br><span class="line">&lt;&lt; 就是retransmits_timed_out计算得到的timeout值</span><br><span class="line"></span><br><span class="line">&gt;&gt; 924.6s是怎么算出来的？</span><br><span class="line">&lt;&lt; 924.6s = (( 2 &lt;&lt; 9) -1) * 200ms + (15 - 9) * 120s</span><br><span class="line"></span><br><span class="line">&gt;&gt; 为什么924.6s是lower bound？</span><br><span class="line">&lt;&lt; 重传总间隔必须大于timeout值，即 (tcp_time_stamp - start_ts) &gt;= timeout</span><br><span class="line"></span><br><span class="line">&gt;&gt; 那RTO超时的间隔到底是不是源码注释的&quot;15 is ~13-30min depending on RTO.&quot;呢？  </span><br><span class="line">&lt;&lt; 显然不是! 虽然924.6s(15min)是一个lower bound，但是它同时也是一个upper bound!</span><br><span class="line">   怎么理解？举例说明  </span><br><span class="line">        1. 如果某个RTO值导致，在已经重传了14次后，总重传间隔开销是924s</span><br><span class="line">        那么它还需要重传第15次，即使离924.6s只差0.6s。这就是发挥了lower bound的作用</span><br><span class="line">        2. 如果某个RTO值导致，在重传了10次后，总重传间隔开销是924s</span><br><span class="line">        重传第11次后，第12次超时触发时计算得到的总间隔变为1044s，超过924.6s</span><br><span class="line">        那么此时会放弃第12次重传，这就是924.6s发挥了upper bound的作用</span><br><span class="line">   总的来说，在Linux3.10中，如果tcp_retres2设置为15。总重传超时周期应该在如下范围内</span><br><span class="line">        [924.6s, 1044.6s)</span><br></pre></td></tr></table></figure>

<h3 id="RTO重传案例"><a href="#RTO重传案例" class="headerlink" title="RTO重传案例"></a>RTO重传案例</h3><p>我们来看如下这个51432端口向9627端口上传过程，十分缓慢，重传包间隔基本是122秒，速度肯定没法快</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/4f1e9afd7ccd8ac6d69cf08c60ce8b84.png" alt="image.png"></p>
<p>上图中垂直方向基本都是发出3-5个包，然后休息120秒，继续发3-5个 包，速度肯定慢，下图可以看到具体的包：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/79ea2c2c5473d61cd1e780944ab0d0c5.png" alt="image.png"></p>
<p>来看下到9627的RTT，基本稳定在245秒或者122秒，这RTT也实在太大了。可以看到：</p>
<ol>
<li><p>网络质量很不好，丢包有点多；</p>
</li>
<li><p>rtt高得离谱，导致rto计算出来120秒了，所以一旦丢包就卡120秒以上。</p>
</li>
</ol>
<p>下图是RTT图</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/932b5124b7d445ee82c82a5f65a98321.png" alt="image.png"></p>
<p>两个原因一叠加，就出现了奇慢无比.</p>
<p>正常情况下RTO是从200ms开始翻倍，实际上OS层面限制了最小RTO 200ms、最大RTO 120秒，由于RTT都超过120秒了，计算所得的RTO必定也大于120秒，所以最终就是我们看到的一上来第一个RTO不是常见的200ms，直接干到了120秒。</p>
<h2 id="netstat-s"><a href="#netstat-s" class="headerlink" title="netstat -s"></a>netstat -s</h2><p>netstat -s统计，有两个和timestamp stamp reject相关的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">netstat -st | grep stamp | grep reject</span><br><span class="line">18 passive connections rejected because of time stamp</span><br><span class="line">1453 packets rejects in established connections because of timestamp</span><br></pre></td></tr></table></figure>

<p>丢包统计：</p>
<blockquote>
<p>netstat -s |egrep -i “drop|route|overflow|filter|retran|fails|listen”</p>
<p>nstat -z -t 1 | egrep -i “drop|route|overflow|filter|retran|fails|listen”</p>
</blockquote>
<p>netstat -st命令中，Tcp: 部分取自&#x2F;proc&#x2F;net&#x2F;snmp，而TCPExt部分取自&#x2F;proc&#x2F;net&#x2F;netstat，该文件对TCP记录了更多的统计。sysstat包也会采集&#x2F;proc&#x2F;net&#x2F;snmp</p>
<h2 id="nc-测试"><a href="#nc-测试" class="headerlink" title="nc 测试"></a>nc 测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nc -v -u -z -w 3 10.101.0.1 53 //测试server 的53端口上的udp服务能否通</span><br></pre></td></tr></table></figure>

<h3 id="nc-6-5-快速fin"><a href="#nc-6-5-快速fin" class="headerlink" title="nc 6.5 快速fin"></a>nc 6.5 快速fin</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1607660605575-1305739f-1621-4a01-89ad-0f81eef94922.png" alt="img"></p>
<p> nc -i 3 10.97.170.11 3306 -w 4 -p 1234</p>
<p>-i 3 表示握手成功后 等三秒钟nc退出（发fin）</p>
<p>nc 6.5 握手后立即发fin断开连接，导致可能收不到Greeting，换成7.5或者mysql client就OK了</p>
<p>也就是用nc 6.5来验证mysql 服务是否正常可能会碰到nc自己断的太快，实际mysql还是正常的，从而像是mysql没有回复Greeting，从而产生误判。</p>
<p>nc 7.5的抓包，明显可以看到nc在发fin前会先等3秒钟：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/1607660937618-d66c4074-9aa2-44cb-8054-f7d3680d1181.png" alt="img"></p>
<h2 id="ping"><a href="#ping" class="headerlink" title="ping"></a>ping</h2><p>sudo ping -f ip 大批量的icmp包</p>
<p>ping -D  带时间戳 或者：ping -i 5 google.com | xargs -L 1 -I ‘{}’ date ‘+%Y-%m-%d %H:%M:%S: {}’  或者 ping <a href="http://www.google.fr/" target="_blank" rel="noopener">www.google.fr</a> | while read pong; do echo “$(date): $pong”; done</p>
<p>ping -O  不通的时候输出：no answer yet for icmp_seq&#x3D;xxx </p>
<p>或者-D + awk</p>
<blockquote>
<p>ping -D 114.114.114.114 | awk ‘{ if(gsub(&#x2F;[|]&#x2F;, “”, $1)) $1&#x3D;strftime(“[%F %T]”, $1); print}’</p>
</blockquote>
<p>Linux 下直接增加如下函数：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ping.ts()&#123;</span><br><span class="line">    if [ -t 1 ]; then</span><br><span class="line">        ping -D &quot;$@&quot; | awk &apos;&#123; if(gsub(/\[|\]/, &quot;&quot;, $1)) $1=strftime(&quot;[\033[34m%F %T\033[0m]&quot;, $1); print; fflush()&#125;&apos;</span><br><span class="line">    else</span><br><span class="line">        ping -D &quot;$@&quot; | awk &apos;&#123; if(gsub(/\[|\]/, &quot;&quot;, $1)) $1=strftime(&quot;[%F %T]&quot;, $1); print; fflush()&#125;&apos;</span><br><span class="line">    fi  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="mtr"><a href="#mtr" class="headerlink" title="mtr"></a>mtr</h2><p>若需要将mtr的结果提供给第三方，建议可以使用-rc参数，r代表不使用交互界面，而是在最后给出一个探测结果报告；c参数指定需要作几次探测（一般建议是至少200个包，可以配合-i参数减少包间隔来加快得到结果的时间）。</p>
<h2 id="traceroute"><a href="#traceroute" class="headerlink" title="traceroute"></a>traceroute</h2><p>和mtr不同的是，traceroute默认使用UDP作为四层协议，下层还是依靠IP头的TTL来控制中间的节点返回ICMP差错报文，来获得中间节点的IP和延时。唯一的区别是，在达到目标节点时，若是ICMP协议，目标大概率是会回复ICMP reply；如果是UDP协议，按照RFC协议规定，系统是要回复ICMP 端口不可达的差错报文，虽然三大平台Windows&#x2F;macOS&#x2F;Linux都实现了这个行为，但出于某些原因，这个包可能还是会在链路上被丢弃，导致路由跟踪的结果无法显示出最后一跳。所以建议在一般的情况下，traceroute命令可以加上-I参数，让程序使用ICMP协议来发送探测数据包。</p>
<h2 id="dstat"><a href="#dstat" class="headerlink" title="dstat"></a>dstat</h2><p><a href="https://www.huaweicloud.com/articles/9fc282e450af6f9b2878008a9e938d4d.html" target="_blank" rel="noopener">dstat 监控</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20210425082343156.png" alt="image-20210425082343156"></p>
<p>dstat -cdgilmnrsy –aio –fs –lock –raw</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a href="http://perthcharles.github.io/2015/09/07/wiki-tcp-retries/" target="_blank" rel="noopener">http://perthcharles.github.io/2015/09/07/wiki-tcp-retries/</a></p>
<p><a href="ttps://github.com/huigher/tcpping2" target="_blank" rel="noopener">tcpping2</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/06/07/就是要你懂TCP--半连接队列和全连接队列/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/07/就是要你懂TCP--半连接队列和全连接队列/" itemprop="url">就是要你懂TCP--半连接队列和全连接队列</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-07T17:30:03+08:00">
                2017-06-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="关于TCP-半连接队列和全连接队列"><a href="#关于TCP-半连接队列和全连接队列" class="headerlink" title="关于TCP 半连接队列和全连接队列"></a>关于TCP 半连接队列和全连接队列</h1><blockquote>
<p>最近碰到一个client端连接异常问题，然后定位分析发现是因为全连接队列满了导致的。查阅各种资料文章和通过一系列的实验对TCP连接队列有了更深入的理解</p>
<p>查资料过程中发现没有文章把这两个队列以及怎么观察他们的指标说清楚，希望通过这篇文章能说清楚:</p>
<ol>
<li>这两个队列是干什么用的；</li>
</ol>
<p>2）怎么设置和观察他们的最大值；</p>
<p>3）怎么查看这两个队列当前使用到了多少；</p>
<p>4）一旦溢出的后果和现象是什么</p>
</blockquote>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><pre><code>场景：JAVA的client和server，使用socket通信。server使用NIO。

1.间歇性的出现client向server建立连接三次握手已经完成，但server的selector没有响应到这连接。
2.出问题的时间点，会同时有很多连接出现这个问题。
3.selector没有销毁重建，一直用的都是一个。
4.程序刚启动的时候必会出现一些，之后会间歇性出现。
</code></pre>
<h2 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h2><h3 id="正常TCP建连接三次握手过程："><a href="#正常TCP建连接三次握手过程：" class="headerlink" title="正常TCP建连接三次握手过程："></a>正常TCP建连接三次握手过程：</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/159a331ff8cdd4b8994dfe6a209d035f.png" alt="image.png"></p>
<ul>
<li>第一步：client 发送 syn 到server 发起握手；</li>
<li>第二步：server 收到 syn后回复syn+ack给client；</li>
<li>第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的56911端口的连接已经是established）</li>
</ul>
<p>从问题的描述来看，有点像TCP建连接的时候全连接队列（accept队列，后面具体讲）满了，尤其是症状2、4. 为了证明是这个原因，马上通过 netstat -s | egrep “listen” 去看队列的溢出统计数据：</p>
<pre><code>667399 times the listen queue of a socket overflowed
</code></pre>
<p>反复看了几次之后发现这个overflowed 一直在增加，那么可以明确的是server上全连接队列一定溢出了</p>
<p>接着查看溢出后，OS怎么处理：</p>
<pre><code># cat /proc/sys/net/ipv4/tcp_abort_on_overflow
0
</code></pre>
<p><strong>tcp_abort_on_overflow 为0表示如果三次握手第三步的时候全连接队列满了那么 server 扔掉 client 发过来的ack（在server端认为连接还没建立起来）</strong></p>
<p>为了证明客户端应用代码的异常跟全连接队列满有关系，我先把tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）。</p>
<p>接着测试，这时在客户端异常中可以看到很多connection reset by peer的错误，<strong>到此证明客户端错误是这个原因导致的（逻辑严谨、快速证明问题的关键点所在）</strong>。</p>
<p>于是开发同学翻看java 源代码发现socket 默认的backlog（这个值控制全连接队列的大小，后面再详述）是50，于是改大重新跑，经过12个小时以上的压测，这个错误一次都没出现了，同时观察到 overflowed 也不再增加了。</p>
<p>到此问题解决，<strong>简单来说TCP三次握手后有个accept队列，进到这个队列才能从Listen变成accept，默认backlog 值是50，很容易就满了</strong>。满了之后握手第三步的时候server就忽略了client发过来的ack包（隔一段时间server重发握手第二步的syn+ack包给client），如果这个连接一直排不上队就异常了。</p>
<blockquote>
<p>但是不能只是满足问题的解决，而是要去复盘解决过程，中间涉及到了哪些知识点是我所缺失或者理解不到位的；这个问题除了上面的异常信息表现出来之外，还有没有更明确地指征来查看和确认这个问题。</p>
</blockquote>
<h2 id="深入理解TCP握手过程中建连接的流程和队列"><a href="#深入理解TCP握手过程中建连接的流程和队列" class="headerlink" title="深入理解TCP握手过程中建连接的流程和队列"></a>深入理解TCP握手过程中建连接的流程和队列</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/bcf463efeb677d5749d8d7571274ee79.png" alt="image.png"></p>
<p>如上图所示，这里有两个队列：syns queue(半连接队列）；accept queue（全连接队列）</p>
<p>三次握手中，在第一步server收到client的syn后，把这个连接信息放到半连接队列中，同时回复syn+ack给client（第二步）；</p>
<pre><code>题外话，比如syn floods 攻击就是针对半连接队列的，攻击方不停地建连接，但是建连接的时候只做第一步，第二步中攻击方收到server的syn+ack后故意扔掉什么也不做，导致server上这个队列满其它正常请求无法进来
</code></pre>
<p>第三步的时候server收到client的ack，如果这时全连接队列没满，那么从半连接队列拿出这个连接的信息放入到全连接队列中，同时将连接状态从 SYN_RECV 改成 ESTABLISHED 状态，否则按tcp_abort_on_overflow指示的执行。</p>
<p>这时如果全连接队列满了并且tcp_abort_on_overflow是0的话，server会扔掉三次握手中第三步收到的ack（假装没有收到一样），过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，就很容易异常了。其实这个时候client认为连接已经建立了，可以发数据或者可以断开，而实际server上连接还没建立好（还没能力）。</p>
<p>在我们的os中retry 第二步的默认次数是2（centos默认是5次）：</p>
<pre><code>net.ipv4.tcp_synack_retries = 2
</code></pre>
<h2 id="如果TCP连接队列溢出，有哪些指标可以看呢？"><a href="#如果TCP连接队列溢出，有哪些指标可以看呢？" class="headerlink" title="如果TCP连接队列溢出，有哪些指标可以看呢？"></a>如果TCP连接队列溢出，有哪些指标可以看呢？</h2><p>上述解决过程有点绕，听起来蒙，那么下次再出现类似问题有什么更快更明确的手段来确认这个问题呢？</p>
<p>（<em>通过具体的、可见的东西来强化我们对知识点的理解和吸收</em>）</p>
<h3 id="netstat-s"><a href="#netstat-s" class="headerlink" title="netstat -s"></a>netstat -s</h3><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 
667399 times the listen queue of a socket overflowed  //全连接队列溢出
//以下两行是一个意思， netstat 版本不同导致显示不同，新版本显示为 dropped
667399 SYNs to LISTEN sockets ignored                 //含全连接/半连接队列溢出+PAWSPassive 等
905080 SYNs to LISTEN sockets dropped                 //含全连接/半连接队列溢出+PAWSPassive 等
 
//和本文无关的一些其它指标 
919614 passive connections rejected because of time stamp //tcp_recycle 丢 syn 包，对应/proc/net/netstat 中 PAWSPassive
TCPTimeWaitOverflow: 65000                                //tcp_max_tw_buckets 溢出
</code></pre>
<p>比如上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了</p>
<p>ignored 和 dropped：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240807112156872.png" alt="image-20240807112156872"></p>
<p>这些指标都是从 &#x2F;proc&#x2F;net&#x2F;netstat 中采集，含义可以参考 <a href="https://github.com/ecki/net-tools.git" target="_blank" rel="noopener">net-tool 工具(netstat 命令来源)中的源码</a>：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240807112356787.png" alt="image-20240807112356787"></p>
<h3 id="ss-命令"><a href="#ss-命令" class="headerlink" title="ss 命令"></a>ss 命令</h3><pre><code>[root@server ~]# ss -lnt
Recv-Q Send-Q Local Address:Port  Peer Address:Port 
0        50               *:3306             *:* 
</code></pre>
<p><strong>上面看到的第二列Send-Q 值是50，表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少</strong></p>
<p><strong>全连接队列的大小取决于：min(backlog, somaxconn) . backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数</strong></p>
<p>《Unix Network Programming》中关于backlog的描述</p>
<blockquote>
<p>The backlog argument to the listen function has historically specified the maximum value for the sum of both queues.</p>
<p>There has never been a formal definition of what the backlog means. The 4.2BSD man page says that it “defines the maximum length the queue of pending connections may grow to.” Many man pages and even the POSIX specification copy this definition verbatim, but this definition does not say whether a pending connection is one in the SYN_RCVD state, one in the ESTABLISHED state that has not yet been accepted, or either. The historical definition in this bullet is the Berkeley implementation, dating back to 4.2BSD, and copied by many others.</p>
</blockquote>
<p>关于 <a href="https://github.com/torvalds/linux/commit/19f92a030ca6d772ab44b22ee6a01378a8cb32d4" target="_blank" rel="noopener">somaxconn 终于在2019年将默认值从128调整到了2048</a>, 这个调整合并到了kernel 5.17中</p>
<blockquote>
<p>SOMAXCONN is &#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn default value.</p>
<p>It has been defined as 128 more than 20 years ago.</p>
<p>Since it caps the listen() backlog values, the very small value has<br>caused numerous problems over the years, and many people had<br>to raise it on their hosts after beeing hit by problems.</p>
<p>Google has been using 1024 for at least 15 years, and we increased<br>this to 4096 after TCP listener rework has been completed, more than<br>4 years ago. We got no complain of this change breaking any<br>legacy application.</p>
<p>Many applications indeed setup a TCP listener with listen(fd, -1);<br>meaning they let the system select the backlog.</p>
<p>Raising SOMAXCONN lowers chance of the port being unavailable under<br>even small SYNFLOOD attack, and reduces possibilities of side channel<br>vulnerabilities.</p>
</blockquote>
<p>这个时候可以跟我们的代码建立联系了，比如Java创建ServerSocket的时候会让你传入backlog的值：</p>
<pre><code>ServerSocket()
    Creates an unbound server socket.
ServerSocket(int port)
    Creates a server socket, bound to the specified port.
ServerSocket(int port, int backlog)
    Creates a server socket and binds it to the specified local port number, with the specified backlog.
ServerSocket(int port, int backlog, InetAddress bindAddr)
    Create a server with the specified port, listen backlog, and local IP address to bind to.
</code></pre>
<p>（来自JDK帮助文档：<a href="https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html%EF%BC%89" target="_blank" rel="noopener">https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）</a></p>
<p><strong>半连接队列的大小取决于：max(64,  &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_max_syn_backlog)。 <a href="https://developer.aliyun.com/article/804896" target="_blank" rel="noopener">不同版本的os会有些差异</a></strong></p>
<blockquote>
<p>我们写代码的时候从来没有想过这个backlog或者说大多时候就没给他值（那么默认就是50），直接忽视了他，首先这是一个知识点的忙点；其次也许哪天你在哪篇文章中看到了这个参数，当时有点印象，但是过一阵子就忘了，这是知识之间没有建立连接，不是体系化的。但是如果你跟我一样首先经历了这个问题的痛苦，然后在压力和痛苦的驱动自己去找为什么，同时能够把为什么从代码层推理理解到OS层，那么这个知识点你才算是比较好地掌握了，也会成为你的知识体系在TCP或者性能方面成长自我生长的一个有力抓手</p>
</blockquote>
<h4 id="netstat-命令"><a href="#netstat-命令" class="headerlink" title="netstat 命令"></a>netstat 命令</h4><p>netstat跟ss命令一样也能看到Send-Q、Recv-Q这些状态信息，不过如果这个连接不是<strong>Listen状态</strong>的话，Recv-Q就是指收到的数据还在缓存中，还没被进程读取，这个值就是还没被进程读取的 bytes；而 Send 则是发送队列中没有被远程主机确认的 bytes 数</p>
<pre><code>$netstat -tn  
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address   Foreign Address State  
tcp    0  0 server:8182  client-1:15260 SYN_RECV   
tcp    0 28 server:22    client-1:51708  ESTABLISHED
tcp    0  0 server:2376  client-1:60269 ESTABLISHED
</code></pre>
<p><strong>netstat -tn 看到的 Recv-Q 跟全连接半连接中的Queue没有关系，这里特意拿出来说一下是因为容易跟 ss -lnt 的 Recv-Q 搞混淆</strong>  </p>
<p>所以ss看到的 Send-Q、Recv-Q是目前全连接队列使用情况和最大设置<br>netstat看到的 Send-Q、Recv-Q，如果这个连接是Established状态的话就是发出的bytes并且没有ack的包、和os接收到的bytes还没交给应用</p>
<p>我们看到的 Recv-Q、Send-Q获取源代码如下（ net&#x2F;ipv4&#x2F;tcp_diag.c ）：   </p>
<pre><code>static void tcp_diag_get_info(struct sock *sk, struct inet_diag_msg *r,
  void *_info)
{
    const struct tcp_sock *tp = tcp_sk(sk);
    struct tcp_info *info = _info;
    
    if (sk-&gt;sk_state == TCP_LISTEN) {  //LISTEN状态下的 Recv-Q、Send-Q
        r-&gt;idiag_rqueue = sk-&gt;sk_ack_backlog;
        r-&gt;idiag_wqueue = sk-&gt;sk_max_ack_backlog; //Send-Q 最大backlog
    } else {						   //其它状态下的 Recv-Q、Send-Q
        r-&gt;idiag_rqueue = max_t(int, tp-&gt;rcv_nxt - tp-&gt;copied_seq, 0);
        r-&gt;idiag_wqueue = tp-&gt;write_seq - tp-&gt;snd_una;
    }
    if (info != NULL)
        tcp_get_info(sk, info);
}
</code></pre>
<p>比如如下netstat -t 看到的Recv-Q有大量数据堆积，那么一般是CPU处理不过来导致的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/77ed9ba81f70f7940546f0a22dabf010.png" alt="image.png"></p>
<h4 id="netstat看到的listen状态的Recv-Q-x2F-Send-Q"><a href="#netstat看到的listen状态的Recv-Q-x2F-Send-Q" class="headerlink" title="netstat看到的listen状态的Recv-Q&#x2F;Send-Q"></a>netstat看到的listen状态的Recv-Q&#x2F;Send-Q</h4><p>netstat 看到的listen状态下的Recv-Q&#x2F;Send-Q意义跟 ss -lnt看到的完全不一样。上面的 netstat 对非listen的描述没问题，但是listen状态似乎Send-Q这个值总是0，这要去看netstat的代码了，实际上Listen状态它不是一个连接，所以肯定统计不到流量，netstat似乎只是针对连接的统计</p>
<p>从网上找了两个Case，server的8765端口故意不去读取对方发过来的2000字节，所看到的是：</p>
<pre><code>$ netstat -ano | grep 8765  
tcp0  0 0.0.0.0:87650.0.0.0:*   LISTEN  off (0.00/0/0)  
tcp 2000  0 10.100.70.140:8765  10.100.70.139:43634 ESTABLISHED off (0.00/0/0)
</code></pre>
<p>第二个Case，8000端口的半连接满了（129），但是这个时候Send-Q还是看到的0</p>
<pre><code>$ netstat -ntap | grep 8000 
tcp      129      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      1526/XXXXX- 
tcp        0      0 9.11.6.36:8000          9.11.6.37:48306         SYN_RECV    - 
tcp        0      0 9.11.6.36:8000          9.11.6.34:44936         SYN_RECV    - 
tcp      365      0 9.11.6.36:8000          9.11.6.37:58446         CLOSE_WAIT  -  
</code></pre>
<h2 id="案列：如果TCP连接队列溢出，抓包是什么现象呢？"><a href="#案列：如果TCP连接队列溢出，抓包是什么现象呢？" class="headerlink" title="案列：如果TCP连接队列溢出，抓包是什么现象呢？"></a>案列：如果TCP连接队列溢出，抓包是什么现象呢？</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/c0849615ae52531887ce6b0313d7d2d1.png" alt="image.png"></p>
<p>如上图server端8989端口的服务全连接队列已经满了（设置最大5，已经6了，通过后面步骤的ss -lnt可以验证）， 所以 server尝试过一会假装继续三次握手的第二步，跟client说我们继续谈恋爱吧。可是这个时候client比较性急，忙着分手了，server觉得都没恋上那什么分手啊。所以接下来两边自说自话也就是都不停滴重传</p>
<h3 id="通过ss和netstat所观察到的状态"><a href="#通过ss和netstat所观察到的状态" class="headerlink" title="通过ss和netstat所观察到的状态"></a>通过ss和netstat所观察到的状态</h3><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/ec25ccb6cce8f554b7ef6927f05bd530.png" alt="image.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2fbdd05162e9fd51e803682b8a18cc51.png" alt="image.png"></p>
<p><a href="/2019/08/31/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP%E9%98%9F%E5%88%97--%E9%80%9A%E8%BF%87%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B%E6%9D%A5%E5%B1%95%E7%A4%BA%E9%97%AE%E9%A2%98/">另外一个案例，虽然最终的锅不是TCP全连接队列太小，但是也能从重传、队列溢出找到根因</a></p>
<h2 id="实践验证一下上面的理解"><a href="#实践验证一下上面的理解" class="headerlink" title="实践验证一下上面的理解"></a>实践验证一下上面的理解</h2><p>上面是通过一些具体的工具、指标来认识全连接队列，接下来结合文章开始的问题来具体验证一下 </p>
<p>把java中backlog改成10（越小越容易溢出），继续跑压力，这个时候client又开始报异常了，然后在server上通过 ss 命令观察到：</p>
<pre><code>Fri May  5 13:50:23 CST 2017
Recv-Q Send-QLocal Address:Port  Peer Address:Port
11         10         *:3306               *:*
</code></pre>
<p>按照前面的理解，这个时候我们能看到3306这个端口上的服务全连接队列最大是10，但是现在有11个在队列中和等待进队列的，肯定有一个连接进不去队列要overflow掉，同时也确实能看到overflow的值在不断地增大。</p>
<p><strong>能够进入全连接队列的 Socket 最大数量始终比配置的全连接队列最大长度 + 1</strong>，结合内核代码，发现<strong>内核在判断全连接队列是否满的情况下，使用的是 &gt; 而非 &gt;&#x3D;</strong> </p>
<blockquote>
<p>Linux下发SIGSTOP信号发给用户态进程，就可以让进程stop不再accept，模拟accept溢出的效果</p>
<p>kill -19 pid 即可； kill -18 pid 恢复暂停进程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; #define SIGKILL     9    /* Kill, unblockable (POSIX). */</span><br><span class="line">&gt; #define SIGCONT     18   /* Continue (POSIX).  */</span><br><span class="line">&gt; #define SIGSTOP     19   /* Stop, unblockable (POSIX). */</span><br><span class="line">&gt;</span><br></pre></td></tr></table></figure>
</blockquote>
<p>tsar监控accept队列的溢出</p>
<blockquote>
<p>tsar –tcpx -s&#x3D;lisove -li 1</p>
</blockquote>
<h3 id="Tomcat和Nginx中的Accept队列参数"><a href="#Tomcat和Nginx中的Accept队列参数" class="headerlink" title="Tomcat和Nginx中的Accept队列参数"></a>Tomcat和Nginx中的Accept队列参数</h3><p>Tomcat 默认短连接，backlog（Tomcat里面的术语是Accept count）Ali-tomcat默认是200, Apache Tomcat默认100. </p>
<pre><code>#ss -lnt
Recv-Q Send-Q   Local Address:Port Peer Address:Port
0       100                 *:8080            *:*
</code></pre>
<p>Nginx默认是511</p>
<pre><code>$sudo ss -lnt
State  Recv-Q Send-Q Local Address:PortPeer Address:Port
LISTEN    0     511              *:8085           *:*
LISTEN    0     511              *:8085           *:*
</code></pre>
<p>因为Nginx是多进程模式，所以看到了多个8085，也就是多个进程都监听同一个端口以尽量避免上下文切换来提升性能   </p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/01dc036aca4b445ed86e3e295bf245b8.png" alt="image.png"></p>
<h2 id="进一步思考-client-fooling-问题"><a href="#进一步思考-client-fooling-问题" class="headerlink" title="进一步思考 client fooling 问题"></a>进一步思考 client fooling 问题</h2><p>如果client走完第三步在client看来连接已经建立好了，但是server上的对应的连接有可能因为accept queue满了而仍然是syn_recv状态，这个时候如果client发数据给server，server会怎么处理呢？（有同学说会reset，还是实践看看）</p>
<p>先来看一个例子：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/9179e08ac24ce3d53e74b92dbd044906.png" alt="image.png"></p>
<p>如上图，图中3号包是三次握手中的第三步，client发送ack给server，这个时候在client看来握手完成，然后4号包中client发送了一个长度为238的包给server，因为在这个时候client认为连接建立成功，但是server上这个连接实际没有ready，所以server没有回复，一段时间后client认为丢包了然后重传这238个字节的包，等到server reset了该连接（或者client一直重传这238字节到超时，client主动发fin包断开该连接，如下图）</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/3f5f1eeb0646a3af8afd6bbff2a9ea0b.png" alt="image.png"></p>
<p>这个问题也叫client fooling，可以看这个patch在4.10后修复了：<a href="https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071" target="_blank" rel="noopener">https://github.com/torvalds/linux/commit/5ea8ea2cb7f1d0db15762c9b0bb9e7330425a071</a> ，修复的逻辑就是，如果全连接队列满了就不再回复syn+ack了，免得client误认为这个连接建立起来了，这样client端收不到syn+ack就只能重发syn。</p>
<p><strong>从上面的实际抓包来看不是reset，而是server忽略这些包，然后client重传，一定次数后client认为异常，然后断开连接。</strong></p>
<p>如果这个连接已经放入了全连接队列但是应用没有accept（比如应用卡住了），那么这个时候client发过来的包是不会被扔掉，OS会先收下放到接收buffer中，知道buffer满了再扔掉新进来的。</p>
<h2 id="过程中发现的一个奇怪问题"><a href="#过程中发现的一个奇怪问题" class="headerlink" title="过程中发现的一个奇怪问题"></a>过程中发现的一个奇怪问题</h2><pre><code>[root@server ~]# date; netstat -s | egrep &quot;listen|LISTEN&quot; 
Fri May  5 15:39:58 CST 2017
1641685 times the listen queue of a socket overflowed  # 全连接队列溢出
1641685 SYNs to LISTEN sockets ignored                 # 半连接队列溢出

[root@server ~]# date; netstat -s | egrep &quot;listen|LISTEN&quot; 
Fri May  5 15:39:59 CST 2017
1641906 times the listen queue of a socket overflowed
1641906 SYNs to LISTEN sockets ignored
</code></pre>
<p>如上所示：<br>overflowed 和 ignored 总是一样多，并且都是同步增加，overflowed 表示全连接队列溢出次数，SYNs to LISTEN socket ignored 表示半连接队列溢出等等指标的次数，没这么巧吧。</p>
<p>翻看内核源代码（<a href="https://github.com/torvalds/linux/blob/v6.13-rc6/net/ipv4/tcp_ipv4.c#L1849" target="_blank" rel="noopener">https://github.com/torvalds/linux/blob/v6.13-rc6/net/ipv4/tcp_ipv4.c#L1849</a> ）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/a5616904df3a505572d99d557b534db2.png" alt="image.png"></p>
<p>可以看到overflow的时候一定会drop++（socket ignored），也就是drop一定大于等于overflow。</p>
<p>同时我也查看了另外几台server的这两个值来证明drop一定大于等于overflow：</p>
<pre><code>server1
150 SYNs to LISTEN sockets dropped

server2
193 SYNs to LISTEN sockets dropped

server3
16329 times the listen queue of a socket overflowed
16422 SYNs to LISTEN sockets dropped

server4
20 times the listen queue of a socket overflowed
51 SYNs to LISTEN sockets dropped

server5
984932 times the listen queue of a socket overflowed
988003 SYNs to LISTEN sockets dropped
</code></pre>
<p>总结：SYNs to LISTEN sockets dropped(ListenDrops)表示：全连接&#x2F;半连接队列溢出<strong>以及PAWSPassive</strong> 等造成的 SYN 丢包</p>
<blockquote>
<p>ListenDrops 表示： SYNs to LISTEN sockets dropped</p>
</blockquote>
<h2 id="那么全连接队列满了会影响半连接队列吗？"><a href="#那么全连接队列满了会影响半连接队列吗？" class="headerlink" title="那么全连接队列满了会影响半连接队列吗？"></a>那么全连接队列满了会影响半连接队列吗？</h2><p>来看三次握手第一步的源代码（<a href="http://elixir.free-electrons.com/linux/v2.6.33/source/net/ipv4/tcp_ipv4.c#L1249%EF%BC%89%EF%BC%9A" target="_blank" rel="noopener">http://elixir.free-electrons.com/linux/v2.6.33/source/net/ipv4/tcp_ipv4.c#L1249）：</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/0c6bbb5d4a10f40c8b3c4ba6cab82292.png" alt="image.png"></p>
<p>TCP 三次握手第一步的时候如果全连接队列满了会影响第一步drop 半连接的发生，流程的如下：</p>
<pre><code>tcp_v4_do_rcv-&gt;tcp_rcv_state_process-&gt;tcp_v4_conn_request
//如果accept backlog队列已满，且未超时的request socket的数量大于1，则丢弃当前请求  
  if(sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_yong(sk)&gt;1)
      goto drop;
</code></pre>
<h2 id="半连接队列的长度"><a href="#半连接队列的长度" class="headerlink" title="半连接队列的长度"></a>半连接队列的长度</h2><p><strong>半连接队列的长度由三个参数指定：</strong></p>
<ul>
<li><strong>调用</strong> <strong>listen</strong> <strong>时，传入的 backlog</strong></li>
<li><strong>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;somaxconn</strong> <strong>默认值为 128</strong></li>
<li><em>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_max_syn_backlog</em>* <strong>默认值为 1024</strong></li>
</ul>
<p>假设 listen 传入的 backlog &#x3D; 128，其他配置采用默认值，来计算下半连接队列的最大长度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">backlog = min(somaxconn, backlog) = min(128, 128) = 128</span><br><span class="line">nr_table_entries = backlog = 128</span><br><span class="line">nr_table_entries = min(backlog, sysctl_max_syn_backlog) = min(128, 1024) = 128</span><br><span class="line">nr_table_entries = max(nr_table_entries, 8) = max(128, 8) = 128</span><br><span class="line">nr_table_entries = roundup_pow_of_two(nr_table_entries + 1) = 256</span><br><span class="line">max_qlen_log = max(3, log2(nr_table_entries)) = max(3, 8) = 8</span><br><span class="line">max_queue_length = 2^max_qlen_log = 2^8 = 256</span><br></pre></td></tr></table></figure>

<p>可以得到半队列大小是 256，以上计算方法：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">backlog = min(somaxconn, backlog)</span><br><span class="line">nr_table_entries = backlog</span><br><span class="line">nr_table_entries = min(backlog, sysctl_max_syn_backlog)</span><br><span class="line">nr_table_entries = max(nr_table_entries, 8)</span><br><span class="line">// roundup_pow_of_two: 将参数向上取整到最小的 2^n，注意这里存在一个 +1</span><br><span class="line">nr_table_entries = roundup_pow_of_two(nr_table_entries + 1)</span><br><span class="line">max_qlen_log = max(3, log2(nr_table_entries))</span><br><span class="line">max_queue_length = 2^max_qlen_log</span><br></pre></td></tr></table></figure>

<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/5f63b8e0-952c-47a2-8179-48793034f86b.png"></p>
<p>没开启tcp_syncookies的话，到tcp_max_syn_backlog 75%水位就开始drop syn包了</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Linux内核就引入半连接队列（用于存放收到SYN，但还没收到ACK的连接）和全连接队列（用于存放已经完成3次握手，但是<strong>应用层代码还没有完成 accept() 的连接</strong>）两个概念，用于存放在握手中的连接。</p>
<p>全连接队列、半连接队列溢出这种问题很容易被忽视，但是又很关键，特别是对于一些短连接应用（比如Nginx、PHP，当然他们也是支持长连接的）更容易爆发。 一旦溢出，从cpu、线程状态看起来都比较正常，但是压力上不去，在client看来rt也比较高（rt&#x3D;网络+排队+真正服务时间），但是从server日志记录的真正服务时间来看rt又很短。</p>
<p>另外就是jdk、netty等一些框架默认backlog比较小，可能有些情况下导致性能上不去，比如 @毕玄 碰到的这个 <a href="https://www.atatech.org/articles/12919" target="_blank" rel="noopener">《netty新建连接并发数很小的case》 </a><br>都是类似原因</p>
<p>希望通过本文能够帮大家理解TCP连接过程中的半连接队列和全连接队列的概念、原理和作用，更关键的是有哪些指标可以明确看到这些问题。</p>
<p>另外每个具体问题都是最好学习的机会，光看书理解肯定是不够深刻的，请珍惜每个具体问题，碰到后能够把来龙去脉弄清楚。</p>
<h2 id="为什么-netstat-看到的-listen-状态的-SEND-Q-总是0"><a href="#为什么-netstat-看到的-listen-状态的-SEND-Q-总是0" class="headerlink" title="为什么 netstat 看到的 listen 状态的 SEND-Q 总是0"></a>为什么 netstat 看到的 listen 状态的 SEND-Q 总是0</h2><p><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e7073830cc8b52ef3df7dd150e4dac7706e0e104" target="_blank" rel="noopener">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=e7073830cc8b52ef3df7dd150e4dac7706e0e104</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#netstat -ntap | grep 8000</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State</span><br><span class="line">tcp      129      0 0.0.0.0:8000            0.0.0.0:*               LISTEN      1526/XXXXX- //第三列总是0</span><br><span class="line">tcp        0      0 9.11.6.36:8000          9.11.6.37:48306         SYN_RECV    -</span><br><span class="line">tcp        0      0 9.11.6.36:8000          9.11.6.34:44936         SYN_RECV    -</span><br><span class="line">tcp      365      0 9.11.6.36:8000          9.11.6.37:58446         CLOSE_WAIT  -</span><br></pre></td></tr></table></figure>

<p>如下图代码，最开始 2 边一起支持了 LISTEN socket 显示 accept 队列当前长度，后来右边支持显示最大长度时，左边没有加。netstat 是读取的 &#x2F;proc&#x2F;net&#x2F;tcp，然后 ss 走了 diag 接口去拿的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/image-20240506134119413.png" alt="image-20240506134119413"></p>
<hr>
<p>也就是改了 tcp_diag_get_info ，但是忘了改 get_tcp4_sock，但是写 man netstat 自己也没验证过就以讹传讹</p>
<p>d31d2480d9840f0d88739941bed0654d5b581dcb ?</p>
<h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>星球同学最详细的实践(代码、抓包等等，共13篇，含<a href="https://github.com/xiaodongQ/prog-playground/tree/main/network" target="_blank" rel="noopener">演示代码</a>) <a href="https://xiaodongq.github.io/2024/05/30/tcp_syn_queue/" target="_blank" rel="noopener">https://xiaodongq.github.io/2024/05/30/tcp_syn_queue/</a>  <a href="https://xiaodongq.github.io/2024/06/26/libbpf-trace-tcp_connect/" target="_blank" rel="noopener">https://xiaodongq.github.io/2024/06/26/libbpf-trace-tcp_connect/</a></p>
<p>X推友实验：<a href="https://wgzhao.github.io/notes/troubleshooting/deep-in-tcp-connect/" target="_blank" rel="noopener">https://wgzhao.github.io/notes/troubleshooting/deep-in-tcp-connect/</a></p>
<p>张师傅：<a href="https://juejin.cn/post/6844904071367753736" target="_blank" rel="noopener">https://juejin.cn/post/6844904071367753736</a></p>
<p>详细的实验以及分析，附Go 实验代码：<a href="https://www.51cto.com/article/687595.html" target="_blank" rel="noopener">https://www.51cto.com/article/687595.html</a> </p>
<p><a href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html" target="_blank" rel="noopener">http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html</a></p>
<p><a href="http://www.cnblogs.com/zengkefu/p/5606696.html" target="_blank" rel="noopener">http://www.cnblogs.com/zengkefu/p/5606696.html</a></p>
<p><a href="http://www.cnxct.com/something-about-phpfpm-s-backlog/" target="_blank" rel="noopener">http://www.cnxct.com/something-about-phpfpm-s-backlog/</a></p>
<p><a href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</a></p>
<p><a href="http://jin-yang.github.io/blog/network-synack-queue.html#" target="_blank" rel="noopener">http://jin-yang.github.io/blog/network-synack-queue.html#</a></p>
<p><a href="http://blog.chinaunix.net/uid-20662820-id-4154399.html" target="_blank" rel="noopener">http://blog.chinaunix.net/uid-20662820-id-4154399.html</a></p>
<p><a href="https://www.atatech.org/articles/12919" target="_blank" rel="noopener">https://www.atatech.org/articles/12919</a></p>
<p><a href="https://blog.cloudflare.com/syn-packet-handling-in-the-wild/" target="_blank" rel="noopener">https://blog.cloudflare.com/syn-packet-handling-in-the-wild/</a></p>
<p><a href="https://ops.tips/blog/how-linux-tcp-introspection/" target="_blank" rel="noopener">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a href="https://www.cnblogs.com/xiaolincoding/p/12995358.html" target="_blank" rel="noopener">https://www.cnblogs.com/xiaolincoding/p/12995358.html</a></p>
<p><a href="https://developer.aliyun.com/article/804896" target="_blank" rel="noopener">从一次线上问题说起，详解 TCP 半连接队列、全连接队列–详细的实验验证各种溢出</a></p>
<p><a href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ" target="_blank" rel="noopener">案例三：诡异的幽灵连接，全连接队列满后4.10内核不再回复syn+ack, 但是3.10会回syn+ack</a></p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">commit <span class="number">5</span>ea8ea2cb7f1d0db15762c9b0bb9e7330425a071</span><br><span class="line">Author: Eric Dumazet &lt;edumazet@google.com&gt;</span><br><span class="line">Date:   Thu Oct <span class="number">27</span> <span class="number">00</span>:<span class="number">27</span>:<span class="number">57</span> <span class="number">2016</span></span><br><span class="line"></span><br><span class="line"> tcp/dccp: drop SYN packets <span class="keyword">if</span> accept <span class="built_in">queue</span> is full</span><br><span class="line"></span><br><span class="line"> Per listen(fd, backlog) rules, there is really no point accepting a SYN,</span><br><span class="line"> sending a SYNACK, <span class="keyword">and</span> dropping the following ACK packet <span class="keyword">if</span> accept <span class="built_in">queue</span></span><br><span class="line"> is full, because application is <span class="keyword">not</span> draining accept <span class="built_in">queue</span> fast enough.</span><br><span class="line"></span><br><span class="line"> This behavior is fooling TCP clients that believe they established a</span><br><span class="line"> flow, <span class="keyword">while</span> there is nothing at server side. They might then send about</span><br><span class="line"> <span class="number">10</span> MSS (<span class="keyword">if</span> <span class="keyword">using</span> IW10) that will be dropped anyway <span class="keyword">while</span> server is under</span><br><span class="line"> stress.</span><br><span class="line"></span><br><span class="line">   -       <span class="comment">/* Accept backlog is full. If we have already queued enough</span></span><br><span class="line"><span class="comment">   -        * of warm entries in syn queue, drop request. It is better than</span></span><br><span class="line"><span class="comment">   -        * clogging syn queue with openreqs with exponentially increasing</span></span><br><span class="line"><span class="comment">   -        * timeout.</span></span><br><span class="line"><span class="comment">   -        */</span></span><br><span class="line">   -       <span class="keyword">if</span> (sk_acceptq_is_full(sk) &amp;&amp; inet_csk_reqsk_queue_young(sk) &gt; <span class="number">1</span>) &#123;</span><br><span class="line">   +       <span class="keyword">if</span> (sk_acceptq_is_full(sk)) &#123;</span><br><span class="line">                   NET_INC_STATS(sock_net(sk), LINUX_MIB_LISTENOVERFLOWS);</span><br><span class="line">                   <span class="keyword">goto</span> drop;</span><br><span class="line">           &#125;</span><br></pre></td></tr></table></figure>

<p>Client 不断地 connect 建新连接：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line">#include&lt;stdlib.h&gt;</span><br><span class="line">#include&lt;string.h&gt;</span><br><span class="line">#include&lt;errno.h&gt;</span><br><span class="line">#include&lt;sys/types.h&gt;</span><br><span class="line">#include&lt;sys/socket.h&gt;</span><br><span class="line">#include&lt;netinet/in.h&gt;</span><br><span class="line">#include&lt;arpa/inet.h&gt;</span><br><span class="line">#include&lt;unistd.h&gt;</span><br><span class="line">#define MAXLINE 4096</span><br><span class="line"></span><br><span class="line">int main(int argc, char** argv)</span><br><span class="line">&#123;</span><br><span class="line">    int sockfd, n;</span><br><span class="line">    char recvline[4096], sendline[4096];</span><br><span class="line">    struct sockaddr_in servaddr;</span><br><span class="line"></span><br><span class="line">    memset(&amp;servaddr, 0, sizeof(servaddr));</span><br><span class="line">    servaddr.sin_family = AF_INET;</span><br><span class="line">    servaddr.sin_port = htons(6666);</span><br><span class="line">    inet_pton(AF_INET, &quot;127.0.0.1&quot;, &amp;servaddr.sin_addr);</span><br><span class="line"></span><br><span class="line">    for (n = 0; n &lt; 100; n++)</span><br><span class="line">    &#123;</span><br><span class="line">        if( (sockfd = socket(AF_INET, SOCK_STREAM, 0)) &lt; 0)&#123;</span><br><span class="line">            printf(&quot;create socket error: %s(errno: %d)\n&quot;, strerror(errno),errno);</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        //客户端不停的向服务端发起新连接，成功之后继续发，没成功会阻塞在这里        //--------------</span><br><span class="line">        if(connect(sockfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) &lt; 0)</span><br><span class="line">        &#123;</span><br><span class="line">            printf(&quot;connect error: %s(errno: %d)\n&quot;,strerror(errno),errno);</span><br><span class="line">            return 0;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        printf(&quot;connected to server: %d\n&quot;, n);</span><br><span class="line">        close(sockfd);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>server 故意不accept：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#include&lt;stdio.h&gt;</span><br><span class="line">#include&lt;stdlib.h&gt;</span><br><span class="line">#include&lt;string.h&gt;</span><br><span class="line">#include&lt;errno.h&gt;</span><br><span class="line">#include&lt;sys/types.h&gt;</span><br><span class="line">#include&lt;sys/socket.h&gt;</span><br><span class="line">#include&lt;netinet/in.h&gt;</span><br><span class="line">#include&lt;unistd.h&gt;</span><br><span class="line"></span><br><span class="line">#define MAXLINE 4096</span><br><span class="line"></span><br><span class="line">int main(int argc, char* argv[])</span><br><span class="line">&#123;</span><br><span class="line">    int listenfd, connfd;</span><br><span class="line">    struct sockaddr_in servaddr;</span><br><span class="line">    char buff[4096];</span><br><span class="line">    int  n;</span><br><span class="line"></span><br><span class="line">    if( (listenfd = socket(AF_INET, SOCK_STREAM, 0)) == -1 )</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;create socket error: %s(errno: %d)\n&quot;, strerror(errno), errno);</span><br><span class="line">        return 0;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    memset(&amp;servaddr, 0, sizeof(servaddr));</span><br><span class="line">    servaddr.sin_family = AF_INET;</span><br><span class="line">    servaddr.sin_addr.s_addr = htonl(INADDR_ANY);</span><br><span class="line">    servaddr.sin_port = htons(6666);</span><br><span class="line"></span><br><span class="line">    if(bind(listenfd, (struct sockaddr*)&amp;servaddr, sizeof(servaddr)) == -1)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;bind socket error: %s(errno: %d)\n&quot;, strerror(errno), errno);</span><br><span class="line">        return 1;</span><br><span class="line">    &#125;</span><br><span class="line">    //全连接队列设置为10    //--------------</span><br><span class="line">    if(listen(listenfd, 10) == -1)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;listen socket error: %s(errno: %d)\n&quot;, strerror(errno), errno);</span><br><span class="line">        return 2;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    if( (connfd = accept(listenfd, (struct sockaddr*)NULL, NULL)) == -1)</span><br><span class="line">    &#123;</span><br><span class="line">        printf(&quot;accept socket error: %s(errno: %d)&quot;, strerror(errno), errno);</span><br><span class="line">        return 3;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    printf(&quot;accepet a socket\n&quot;);</span><br><span class="line">    </span><br><span class="line">    //服务端仅accept一次，之后就不再accept，此时全连接队列会被堆满    //----------------------------------</span><br><span class="line">    sleep(1000);</span><br><span class="line">        </span><br><span class="line">    close(connfd);</span><br><span class="line">    close(listenfd);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/06/02/就是要你懂TCP--连接和握手/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/02/就是要你懂TCP--连接和握手/" itemprop="url">就是要你懂TCP--握手和挥手</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-02T17:30:03+08:00">
                2017-06-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="就是要你懂TCP–握手和挥手"><a href="#就是要你懂TCP–握手和挥手" class="headerlink" title="就是要你懂TCP–握手和挥手"></a>就是要你懂TCP–握手和挥手</h1><blockquote>
<p>看过太多tcp相关文章，但是看完总是不过瘾，似懂非懂，反复考虑过后，我觉得是那些文章太过理论，看起来没有体感，所以吸收不了。</p>
<p>希望这篇文章能做到言简意赅，帮助大家透过案例来理解原理</p>
</blockquote>
<h2 id="tcp的特点"><a href="#tcp的特点" class="headerlink" title="tcp的特点"></a>tcp的特点</h2><p>这个大家基本都能说几句，面试的时候候选人也肯定会告诉你这些：</p>
<ul>
<li>三次握手</li>
<li>四次挥手</li>
<li>可靠连接</li>
<li>丢包重传</li>
<li>速度自我调整</li>
</ul>
<p>但是我只希望大家记住一个核心的：<strong>tcp是可靠传输协议，它的所有特点都为这个可靠传输服务</strong>。</p>
<h3 id="那么tcp是怎么样来保障可靠传输呢？"><a href="#那么tcp是怎么样来保障可靠传输呢？" class="headerlink" title="那么tcp是怎么样来保障可靠传输呢？"></a>那么tcp是怎么样来保障可靠传输呢？</h3><p>tcp在传输过程中都有一个ack，接收方通过ack告诉发送方收到那些包了。这样发送方能知道有没有丢包，进而确定重传</p>
<h3 id="tcp建连接的三次握手"><a href="#tcp建连接的三次握手" class="headerlink" title="tcp建连接的三次握手"></a>tcp建连接的三次握手</h3><p>来看一个java代码连接数据库的三次握手过程</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/6d66dadecb72e11e3e5ab765c6c3ea2e.png" alt="image.png"></p>
<p>三个红框表示建立连接的三次握手：</p>
<ul>
<li>第一步：client 发送 syn 到server 发起握手；</li>
<li>第二步：server 收到 syn后回复syn+ack给client；</li>
<li>第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的48287端口的连接已经是established）</li>
</ul>
<p>握手的核心目的是告知对方seq（绿框是client的初始seq，蓝色框是server 的初始seq），对方回复ack（收到的seq+包的大小），这样发送端就知道有没有丢包了</p>
<p>握手的次要目的是告知和协商一些信息，图中黄框。</p>
<ul>
<li>MSS–最大传输包</li>
<li>SACK_PERM–是否支持Selective ack(用户优化重传效率）</li>
<li>WS–窗口计算指数（有点复杂的话先不用管）</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/1423013fe76719cfa3088ebc4704c023.png" alt="image.png"></p>
<p>全连接队列（accept queue）的长度是由 listen(sockfd, backlog) 这个函数里的 backlog 控制的，而该 backlog 的最大值则是 somaxconn。somaxconn 在 5.4 之前的内核中，默认都是 128（5.4 开始调整为了默认 4096）</p>
<p>当服务器中积压的全连接个数超过该值后，新的全连接就会被丢弃掉。Server 在将新连接丢弃时，有的时候需要发送 reset 来通知 Client，这样 Client 就不会再次重试了。不过，默认行为是直接丢弃不去通知 Client。至于是否需要给 Client 发送 reset，是由 tcp_abort_on_overflow 这个配置项来控制的，该值默认为 0，即不发送 reset 给 Client。推荐也是将该值配置为 0</p>
<blockquote>
<p>net.ipv4.tcp_abort_on_overflow &#x3D; 0</p>
</blockquote>
<p><strong>这就是tcp为什么要握手建立连接，就是为了解决tcp的可靠传输</strong></p>
<p>物理上没有一个连接的东西在这里，udp也类似会占用端口、ip，但是大家都没说过udp的连接。而本质上我们说tcp的连接是指tcp是拥有和维护一些状态信息的，这个状态信息就包含seq、ack、窗口&#x2F;buffer，tcp握手就是协商出来这些初始值。这些状态才是我们平时所说的tcp连接的本质。</p>
<h3 id="unres-qlen-和-握手"><a href="#unres-qlen-和-握手" class="headerlink" title="unres_qlen  和 握手"></a>unres_qlen  和 握手</h3><p>tcp connect 的本地流程是这样的：</p>
<p>1、tcp发出SYN建链报文后，报文到ip层需要进行路由查询</p>
<p>2、路由查询完成后，报文到arp层查询下一跳mac地址</p>
<p>3、如果本地没有对应网关的arp缓存，就需要缓存住这个报文，发起arp报文请求</p>
<p>4、arp层收到arp回应报文之后，从缓存中取出SYN报文，完成mac头填写并发送给驱动。</p>
<p>问题在于，arp层报文缓存队列长度默认为3。如果你运气不好，刚好赶上缓存已满，这个报文就会被丢弃。</p>
<p>TCP层发现SYN报文发出去3s（默认值）还没有回应，就会重发一个SYN。这就是为什么少数连接会3s后才能建链。</p>
<p>幸运的是，arp层缓存队列长度是可配置的，用 sysctl -a | grep unres_qlen 就能看到，默认值为3。</p>
<h3 id="建连接失败经常碰到的问题"><a href="#建连接失败经常碰到的问题" class="headerlink" title="建连接失败经常碰到的问题"></a>建连接失败经常碰到的问题</h3><p>内核扔掉syn的情况（握手失败，建不上连接）：</p>
<ul>
<li>rp_filter 命中(rp_filter&#x3D;1, 多网卡环境）， troubleshooting:  netstat -s | grep -i filter ;</li>
<li>snat&#x2F;dnat的时候宿主机port冲突，内核会扔掉 syn包。 troubleshooting: sudo conntrack -S | grep  insert_failed &#x2F;&#x2F;有不为0的</li>
<li>全连接队列满的情况</li>
<li>syn flood攻击</li>
<li>若远端服务器的内核参数 net.ipv4.tcp_tw_recycle 和 net.ipv4.tcp_timestamps 的值都为 1，则远端服务器会检查每一个报文中的时间戳（Timestamp），若 Timestamp 不是递增的关系，不会响应这个报文。配置 NAT 后，远端服务器看到来自不同的客户端的源 IP 相同，但 NAT 前每一台客户端的时间可能会有偏差，报文中的 Timestamp 就不是递增的情况。nat后的连接，开启timestamp。因为快速回收time_wait的需要，会校验时间该ip上次tcp通讯的timestamp大于本次tcp(nat后的不同机器经过nat后ip一样，保证不了timestamp递增）</li>
<li>NAT 哈希表满导致 ECS 实例丢包 nf_conntrack full</li>
</ul>
<h3 id="tcp断开连接的四次挥手"><a href="#tcp断开连接的四次挥手" class="headerlink" title="tcp断开连接的四次挥手"></a>tcp断开连接的四次挥手</h3><p>再来看java连上mysql后，执行了一个SQL： select sleep(2); 然后就断开了连接</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/b6f4a952cdf8ffbb8f6e9434d1432e05.png" alt="image.png"></p>
<p>四个红框表示断开连接的四次挥手：</p>
<ul>
<li>第一步： client主动发送fin包给server</li>
<li>第二步： server回复ack（对应第一步fin包的ack）给client，表示server知道client要断开了</li>
<li>第三步： server发送fin包给client，表示server也可以断开了</li>
<li>第四部： client回复ack给server，表示既然双发都发送fin包表示断开，那么就真的断开吧</li>
</ul>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/321f96243eef2f6437fe4e1559c15efe.png" alt="image.png"></p>
<p>除了 CLOSE_WAIT 状态外，其余两个状态都有对应的系统配置项来控制。</p>
<p>我们首先来看 FIN_WAIT_2 状态，TCP 进入到这个状态后，如果本端迟迟收不到对端的 FIN 包，那就会一直处于这个状态，于是就会一直消耗系统资源。Linux 为了防止这种资源的开销，设置了这个状态的超时时间 tcp_fin_timeout，默认为 60s，超过这个时间后就会自动销毁该连接。</p>
<p>至于本端为何迟迟收不到对端的 FIN 包，通常情况下都是因为对端机器出了问题，或者是因为太繁忙而不能及时 close()。所以，通常我们都建议将 tcp_fin_timeout 调小一些，以尽量避免这种状态下的资源开销。对于数据中心内部的机器而言，将它调整为 2s 足以：</p>
<blockquote>
<p>net.ipv4.tcp_fin_timeout &#x3D; 2</p>
</blockquote>
<p>TIME_WAIT 状态存在的意义是：最后发送的这个 ACK 包可能会被丢弃掉或者有延迟，这样对端就会再次发送 FIN 包。如果不维持 TIME_WAIT 这个状态，那么再次收到对端的 FIN 包后，本端就会回一个 Reset 包，这可能会产生一些异常。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/9fbe15fa8b913ba76048f3b2ad2b923a.png" alt="image.png"></p>
<h3 id="为什么握手三次、挥手四次"><a href="#为什么握手三次、挥手四次" class="headerlink" title="为什么握手三次、挥手四次"></a>为什么握手三次、挥手四次</h3><p>这个问题太恶心，面试官太喜欢问，其实大部分面试官只会背诵：因为TCP是双向的，所以关闭需要四次挥手……。</p>
<p>你要是想怼面试官的话可以问他握手也是双向的但是只需要三次呢？</p>
<p>我也不知道怎么回答。网上都说tcp是双向的，所以断开要四次。但是我认为建连接也是双向的（双向都协调告知对方自己的seq号），为什么不需要四次握手呢，所以网上说的不一定精准。</p>
<p>你再看三次握手的第二步发 syn+ack，如果拆分成两步先发ack再发syn完全也是可以的（效率略低），这样三次握手也变成四次握手了。</p>
<p>看起来挥手的时候多一次，主要是收到第一个fin包后单独回复了一个ack包，如果能回复fin+ack那么四次挥手也就变成三次了。 来看一个案例：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/9db33f9304f8236b1ebcb215064bb2af.png" alt="image.png"></p>
<p>图中第二个红框就是回复的fin+ack，这样四次挥手变成三次了（如果一个包就是一次的话）。</p>
<p>我的理解：之所以绝大数时候我们看到的都是四次挥手，是因为收到fin后，知道对方要关闭了，然后OS通知应用层要关闭，这里应用层可能需要做些准备工作，可能还有数据没发送完，所以内核先回ack，等应用准备好了主动调close时再发fin 。 握手过程没有这个准备过程所以可以立即发送syn+ack（把这里的两步合成一步了）。 内核收到对方的fin后，只能ack，不能主动替应用来fin，因为他不清楚应用能不能关闭。</p>
<h3 id="ack-x3D-seq-len"><a href="#ack-x3D-seq-len" class="headerlink" title="ack&#x3D;seq+len"></a>ack&#x3D;seq+len</h3><p>ack总是seq+len（包的大小），这样发送方明确知道server收到那些东西了</p>
<p>但是特例是三次握手和四次挥手，虽然len都是0，但是syn和fin都要占用一个seq号，所以这里的ack都是seq+1</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/45c6d36ce8b17a5c0442e66fce002ab4.png" alt="image.png"></p>
<p>看图中左边红框里的len+seq就是接收方回复的ack的数字，表示这个包接收方收到了。然后下一个包的seq就是前一个包的len+seq，依次增加，一旦中间发出去的东西没有收到ack就是丢包了，过一段时间（或者其他方式）触发重传，保障了tcp传输的可靠性。</p>
<h3 id="三次握手中协商的其它信息"><a href="#三次握手中协商的其它信息" class="headerlink" title="三次握手中协商的其它信息"></a>三次握手中协商的其它信息</h3><p>MSS 最大一个包中能传输的信息（不含tcp、ip包头），MSS+包头就是MTU（最大传输单元），如果MTU过大可能在传输的过程中被卡住过不去造成卡死（这个大小的包一直传输不过去），跟丢包还不一样</p>
<p>MSS的问题具体可以看我这篇文章： <a href="https://www.atatech.org/articles/60633" target="_blank" rel="noopener">scp某个文件的时候卡死问题的解决过程</a></p>
<p>SACK_PERM 用于丢包的话提升重传效率，比如client一次发了1、2、3、4、5 这5个包给server，实际server收到了 1、3、4、5这四个包，中间2丢掉了。这个时候server回复ack的时候，都只能回复2，表示2前面所有的包都收到了，给我发第二个包吧，如果server 收到3、4、5还是没有收到2的话，也是回复ack 2而不是回复ack 3、4、5、6的，表示快点发2过来。</p>
<p>但是这个时候client虽然知道2丢了，然后会重发2，但是不知道3、4、5有没有丢啊，实际3、4、5 server都收到了，如果支持sack，那么可以ack 2的时候同时告诉client 3、4、5都收到了，这样client重传的时候只重传2就可以，如果没有sack的话那么可能会重传2、3、4、5，这样效率就低了。</p>
<p>来看一个例子：</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/5322d0cf77a3a1ae6c87a972cc5843d0.png" alt="image.png"></p>
<p>图中的红框就是SACK。</p>
<p>知识点：ack数字表示这个数字前面的数据<strong>都</strong>收到了</p>
<h2 id="TIME-WAIT-和-CLOSE-WAIT"><a href="#TIME-WAIT-和-CLOSE-WAIT" class="headerlink" title="TIME_WAIT 和 CLOSE_WAIT"></a>TIME_WAIT 和 CLOSE_WAIT</h2><p>假设服务端和客户端跑在同一台机器上，服务端监听在 18080端口上，客户端使用18089端口建立连接。</p>
<p>如果client主动断开连接那么就会看到client端的连接在 TIME_WAIT：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 1808</span><br><span class="line">tcp        0      0 0.0.0.0:18080           0.0.0.0:*               LISTEN      </span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      TIME_WAIT</span><br></pre></td></tr></table></figure>

<p>如果Server主动断开连接(也就是18080）那么就会看到client端的连接在CLOSE_WAIT 而Server在FIN_WAIT2：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 1808</span><br><span class="line">tcp    0      0 192.168.1.79:18080      192.168.1.79:18089      FIN_WAIT2  --&lt;&lt; server</span><br><span class="line">tcp    0      0 192.168.1.79:18089      192.168.1.79:18080      CLOSE_WAIT --&lt;&lt; client</span><br></pre></td></tr></table></figure>

<p><strong>TIME_WAIT是主动断连方出现的状态（ 2MSL）</strong></p>
<h3 id="被动关闭方收到fin后有两种选择"><a href="#被动关闭方收到fin后有两种选择" class="headerlink" title="被动关闭方收到fin后有两种选择"></a>被动关闭方收到fin后有两种选择</h3><p>如下描述是server端主动关闭的情况</p>
<p>1 如果client也立即断开，那么Server的这个连接会进入 TIME_WAIT状态</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 1808</span><br><span class="line">tcp    0      0 0.0.0.0:18080           0.0.0.0:*            LISTEN  --&lt;&lt; server还在  </span><br><span class="line">tcp    0      0 192.168.1.79:18080      192.168.1.79:18089   TIME_WAIT --&lt;&lt; server</span><br></pre></td></tr></table></figure>

<p>2 client 坚持不断开过 Server 一段时间后（3.10：net.netfilter.nf_conntrack_tcp_timeout_fin_wait &#x3D; 120， 4.19：net.ipv4.tcp_fin_timeout &#x3D; 15）会结束这个连接但是client还是会 在CLOSE_WAIT 直到client进程退出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 1808</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      CLOSE_WAIT</span><br></pre></td></tr></table></figure>

<h3 id="CLOSE-WAIT"><a href="#CLOSE-WAIT" class="headerlink" title="CLOSE_WAIT"></a>CLOSE_WAIT</h3><p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
<p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/" target="_blank" rel="noopener">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<port>」来确认端口对应运行的是什么程序以及它的进程号是多少。</port></p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<h4 id="server端大量close-wait案例"><a href="#server端大量close-wait案例" class="headerlink" title="server端大量close_wait案例"></a>server端大量close_wait案例</h4><p>看了这么多理论，下面用个案例来检查自己对close_wait理论（tcp握手本质）的掌握。同时也可以看看自己从知识到问题的推理能力（跟文章最后的知识效率呼应一下）。</p>
<p>问题描述：</p>
<blockquote>
<p>服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/2703fc07dfc4dd5b6e1bb4c2ce620e59.png" alt="image.png"></p>
<p>结论：</p>
<blockquote>
<p>这个case的最终原因是因为<strong>OS的open files设置的是1024,达到了上限</strong>，进而导致server不能accept，但这个时候的tcp连接状态已经是ESTABLISHED了（这个状态变换是取决于内核收发包，跟应用是否accept()无关）。</p>
<p>同时从这里可以推断 netstat 即使看到一个tcp连接状态是ESTABLISHED也不能代表占用了 open files句柄。此时client可以正常发送数据了，只是应用服务在accept之前没法receive数据和close连接。</p>
</blockquote>
<h2 id="TCP连接状态图"><a href="#TCP连接状态图" class="headerlink" title="TCP连接状态图"></a>TCP连接状态图</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<h2 id="总结下"><a href="#总结下" class="headerlink" title="总结下"></a>总结下</h2><p>tcp所有特性基本上核心都是为了<strong>可靠传输</strong>这个目标来服务的，然后有一些是出于优化性能的目的</p>
<p>三次握手建连接的详细过程可以参考我这篇： <a href="https://www.atatech.org/articles/78858" target="_blank" rel="noopener">关于TCP 半连接队列和全连接队列</a></p>
<p>后续希望再通过几个案例来深化一下上面的知识。</p>
<hr>
<p>为什么要案例来深化一下上面的知识，说点关于学习的题外话</p>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举三反一，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理解理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>对于费曼（参考费曼学习法）这样的聪明人就是很容易看到一个理论知识就能理解这个理论知识背后的本质。</p>
<p>肯定知识效率最牛逼，但是拥有这种能力的人毕竟非常少。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快地掌握一个新知识。剩下的绝大部分只能拼时间(刷题)+方法+总结等也能掌握一些知识</p>
<p>非常遗憾我就是工程效率型，只能羡慕那些知识效率型的学霸。但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，即使灰色地带也行啊。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/06/02/就是要你懂TCP--wireshark-dup-ack-issue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/06/02/就是要你懂TCP--wireshark-dup-ack-issue/" itemprop="url">就是要你懂TCP--wireshark-dup-ack-issue</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-06-02T15:30:03+08:00">
                2017-06-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TCP/" itemprop="url" rel="index">
                    <span itemprop="name">TCP</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="就是要你懂TCP–wireshark-dup-ack-issue"><a href="#就是要你懂TCP–wireshark-dup-ack-issue" class="headerlink" title="就是要你懂TCP–wireshark-dup-ack-issue"></a>就是要你懂TCP–wireshark-dup-ack-issue</h1><h2 id="问题："><a href="#问题：" class="headerlink" title="问题："></a>问题：</h2><p>很多同学学会抓包后，经常拿着这样一个抓包来问我是怎么回事：</p>
<p>在wireshark中看到一个tcp会话中的两台机器突然一直互相发dup ack包，但是没有触发重传。每次重复ack都是间隔精确的20秒</p>
<h2 id="如下截图："><a href="#如下截图：" class="headerlink" title="如下截图："></a>如下截图：</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/bm3W68Q.png"></p>
<p>client都一直在回复收到2号包（ack&#x3D;2）了，可是server跟傻了一样居然还发seq&#x3D;1的包（按理，应该发比2大的包啊）</p>
<h2 id="系统配置："><a href="#系统配置：" class="headerlink" title="系统配置："></a>系统配置：</h2><pre><code>net.ipv4.tcp_keepalive_time = 20
net.ipv4.tcp_keepalive_probes = 5
net.ipv4.tcp_keepalive_intvl = 3
</code></pre>
<h2 id="原因："><a href="#原因：" class="headerlink" title="原因："></a>原因：</h2><p>抓包不全的话wireshark有缺陷，把keepalive包识别成了dup ack包，看内容这种dup ack和keepalive似乎是一样的，flags都是0x010。keep alive的定义的是后退一格(seq少1）。</p>
<p>2、4、6、8……号包，都有一个“tcp acked unseen segment”。这个一般表示它ack的这个包，没有被抓到。Wirshark如何作出此判断呢？前面一个包是seq&#x3D;1, len&#x3D;0，所以正常情况下是ack &#x3D; seq + len &#x3D; 1，然而Wireshark看到的确是ack &#x3D; 2, 它只能判断有一个seq &#x3D;1, len &#x3D; 1的包没有抓到。<br>dup ack也是类似道理，这些包完全符合dup ack的定义，因为“ack &#x3D; ” 某个数连续多次出现了。</p>
<p>这一切都是因为keep alive的特殊性导致的。打开66号包的tcp层（见后面的截图），可以看到它的 next sequence number &#x3D; 12583，表示正常情况下server发出的下一个包应该是seq &#x3D; 12583。可是在下一个包，也就是68号包中，却是seq &#x3D; 12582。keep alive的定义的确是这样，即后退一格。<br>Wireshark只有在抓到数据包（66号包）和keep alive包的情况下才有可能正确识别，前面的抓包中恰好在keep alive之前丢失了数据包，所以Wireshark就蒙了。</p>
<h2 id="构造重现"><a href="#构造重现" class="headerlink" title="构造重现"></a>构造重现</h2><p>如果用“frame.number &gt;&#x3D; 68” 过滤这个包，然后File–&gt;export specified packets保存成一个新文件，再打开那个新文件，就会发现Wireshark又蒙了。本来能够正常识别的keep alive包又被错看成dup ack了，所以一旦碰到这种情况不要慌要稳</p>
<p>下面是知识点啦</p>
<h2 id="Keepalive"><a href="#Keepalive" class="headerlink" title="Keepalive"></a>Keepalive</h2><p>TCP报文接收方必须回复的场景：</p>
<p>TCP携带字节数据<br>没有字节数据，携带SYN状态位<br>没有字节数据，携带FIN状态位</p>
<p>keepalive 提取历史发送的最后一个字节，充当心跳字节数据，依然使用该字节的最初序列号。也就是前面所说的seq回退了一个</p>
<p>对方收到后因为seq小于TCP滑动窗口的左侧，被判定为duplicated数据包，然后扔掉了，并回复一个duplicated ack</p>
<p>所以keepalive跟duplicated本质是一回事，就看wireshark能够正确识别了。</p>
<h2 id="Duplication-ack是指："><a href="#Duplication-ack是指：" class="headerlink" title="Duplication ack是指："></a>Duplication ack是指：</h2><p>server收到了3和8号包，但是没有收到中间的4&#x2F;5&#x2F;6&#x2F;7，那么server就会ack 3，如果client还是继续发8&#x2F;9号包，那么server会继续发dup ack 3#1 ; dup ack 3#2 来向客户端说明只收到了3号包，不要着急发后面的大包，把4&#x2F;5&#x2F;6&#x2F;7给我发过来</p>
<h2 id="TCP-Window-Update"><a href="#TCP-Window-Update" class="headerlink" title="TCP Window Update"></a>TCP Window Update</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/oss/1558941016099-bc4504f1-e9c7-4d84-85e1-a7f5c6554306.png"></p>
<p>如上图，当接收方的tcp Window Size不足一个MSS的时候，为了避免 Silly Window Syndrome，Client不再发小包，而是发送探测包（跟keepalive一样，发一个回退一格的包，触发server ack同时server ack的时候会带过来新的window size）探测包间隔时间是200&#x2F;400&#x2F;800&#x2F;1600……ms这样</p>
<h2 id="正常的keep-alive-Case："><a href="#正常的keep-alive-Case：" class="headerlink" title="正常的keep-alive Case："></a>正常的keep-alive Case：</h2><p><img src="https://cdn.jsdelivr.net/gh/plantegg/plantegg.github.io/images/951413iMgBlog/DsTWFZr.png"></p>
<p>keep-alive 通过发一个比实际seq小1的包，比如server都已经 ack 12583了，client故意发一个seq 12582来标识这是一个keep-Alive包</p>
<h2 id="Duplication-ack是指：-1"><a href="#Duplication-ack是指：-1" class="headerlink" title="Duplication ack是指："></a>Duplication ack是指：</h2><p>server收到了3和8号包，但是没有收到中间的4&#x2F;5&#x2F;6&#x2F;7，那么server就会ack 3，如果client还是继续发8&#x2F;9号包，那么server会继续发dup ack 3#1 ; dup ack 3#2 来向客户端说明只收到了3号包，不要着急发后面的大包，把4&#x2F;5&#x2F;6&#x2F;7给我发过来</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2017/03/24/docker swarm的Label使用/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="twitter @plantegg">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/03/24/docker swarm的Label使用/" itemprop="url">docker、swarm的Label使用</a></h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-03-24T17:30:03+08:00">
                2017-03-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index">
                    <span itemprop="name">docker</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="docker、swarm的Label使用"><a href="#docker、swarm的Label使用" class="headerlink" title="docker、swarm的Label使用"></a>docker、swarm的Label使用</h1><h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>广发银行需要把方舟集群部署在多个机房（多个机房组成一个大集群），这样物理机和容器vlan没法互相完全覆盖，</p>
<p>也就是可能会出现A机房的网络subnet:192.168.1.0&#x2F;24, B 机房的网络subnet：192.168.100.0&#x2F;24 但是他们属于同一个vlan，要求如果容器在A机房的物理机拉起，分到的是192.168.1.0&#x2F;24中的IP，B机房的容器分到的IP是：192.168.100.0&#x2F;24</p>
<p><strong>功能实现：</strong></p>
<ul>
<li><strong>本质就是对所有物理机打标签，同一个asw下的物理机用同样的标签，不同asw下的物理机标签不同；</strong></li>
<li><strong>创建容器网络的时候也加标签，不同asw下的网络标签不一样，同时跟这个asw下的物理机标签匹配；</strong></li>
<li><strong>创建容器的时候使用 –net&#x3D;driver:vlan 来动态选择多个vlan网络中的任意一个，然后swarm根据网络的标签要和物理机的标签一致，从而把容器调度到正确的asw下的物理机上。</strong></li>
</ul>
<p><strong>分为如下三个改造点</strong></p>
<p><strong>1：</strong></p>
<p>daemon启动的时候增加标签（其中一个就行）：</p>
<table>
<thead>
<tr>
<th>上联交换机组的名称，多个逗号隔开</th>
<th>com.alipay.acs.engine.asw.hostname</th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td></td>
</tr>
</tbody></table>
<p><strong>2：</strong><br>创建网络的时候使用对应的标签：</p>
<table>
<thead>
<tr>
<th>网络域交换机组asw列表的名称，多个逗号隔开</th>
<th>com.alipay.acs.network.asw.hostname</th>
</tr>
</thead>
<tbody><tr>
<td>该VLAN网络是否必须显式指定，默认为0即不必须，此时当传入–net driver:vlan时ACS会根据调度结果自行选择一个可用的VLAN网络并拼装到参数中</td>
<td>com.alipay.acs.network.explicit</td>
</tr>
</tbody></table>
<p><strong>3：</strong></p>
<p>Swarm manager增加可选启动选项netarch.multiscope，值为true</p>
<h3 id="功能实现逻辑"><a href="#功能实现逻辑" class="headerlink" title="功能实现逻辑"></a>功能实现逻辑</h3><ol>
<li>Swarm manager增加可选启动选项netarch.multiscope，当为1时，network create时强制要求必须指定label描述配置VLAN的ASW信息</li>
<li>Swarm manager在创建容器时检查网络类型，VLAN网络时则将网络ASW的label放入过滤器中，在调度时按照机器的ASW标签过滤</li>
<li>如果使用者如果不关心具体使用哪个VLAN，则可以指定–net&#x3D;”driver:vlan”，会自动查找driver&#x3D;vlan的network，并根据调度结果（Node所关联的ASW）自动选择合适的network填入Config.HostConfig.NetworkMode传递给Docker daemon.</li>
</ol>
<p>如果是现存的环境，修改zk来更新网络标签：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 21] get /Cluster/docker/network/v1.0/network/c79e533e4444294ac9cb7838608115c961c6e403d3610367ff4b197ef6b981fc </span><br><span class="line">&#123;&quot;addrSpace&quot;:&quot;GlobalDefault&quot;,&quot;enableIPv6&quot;:false,&quot;generic&quot;:&#123;&quot;com.docker.network.enable_ipv6&quot;:false,&quot;com.docker.network.generic&quot;:&#123;&quot;VlanId&quot;:&quot;192&quot;&#125;&#125;,&quot;id&quot;:&quot;c79e533e4444294ac9cb7838608115c961c6e403d3610367ff4b197ef6b981fc&quot;,&quot;inDelete&quot;:false,&quot;internal&quot;:false,&quot;ipamOptions&quot;:&#123;&quot;VlanId&quot;:&quot;192&quot;&#125;,&quot;ipamType&quot;:&quot;default&quot;,&quot;ipamV4Config&quot;:&quot;[&#123;\&quot;PreferredPool\&quot;:\&quot;192.168.8.0/24\&quot;,\&quot;SubPool\&quot;:\&quot;\&quot;,\&quot;Gateway\&quot;:\&quot;192.168.8.1\&quot;,\&quot;AuxAddresses\&quot;:null&#125;]&quot;,&quot;ipamV4Info&quot;:&quot;[&#123;\&quot;IPAMData\&quot;:\&quot;&#123;\\\&quot;AddressSpace\\\&quot;:\\\&quot;\\\&quot;,\\\&quot;Gateway\\\&quot;:\\\&quot;192.168.8.1/24\\\&quot;,\\\&quot;Pool\\\&quot;:\\\&quot;192.168.8.0/24\\\&quot;&#125;\&quot;,\&quot;PoolID\&quot;:\&quot;GlobalDefault/192.168.8.0/24\&quot;&#125;]&quot;,&quot;labels&quot;:&#123;&#125;,&quot;name&quot;:&quot;vlan192-8&quot;,&quot;networkType&quot;:&quot;vlan&quot;,&quot;persist&quot;:true,&quot;postIPv6&quot;:false,&quot;scope&quot;:&quot;global&quot;&#125;</span><br><span class="line">cZxid = 0x4100008cce</span><br><span class="line">ctime = Fri Mar 09 12:46:44 CST 2018</span><br><span class="line">mZxid = 0x4100008cce</span><br><span class="line">mtime = Fri Mar 09 12:46:44 CST 2018</span><br><span class="line">pZxid = 0x4100008cce</span><br><span class="line">cversion = 0</span><br><span class="line">dataVersion = 0</span><br><span class="line">aclVersion = 0</span><br><span class="line">ephemeralOwner = 0x0</span><br><span class="line">dataLength = 716</span><br><span class="line">numChildren = 0</span><br></pre></td></tr></table></figure>

<p>&#x2F;&#x2F;注意上面的网络还没有标签，修改如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[zk: localhost:2181(CONNECTED) 28] set /Cluster/docker/network/v1.0/network/c79e533e4444294ac9cb7838608115c961c6e403d3610367ff4b197ef6b981fc &#123;&quot;addrSpace&quot;:&quot;GlobalDefault&quot;,&quot;enableIPv6&quot;:false,&quot;generic&quot;:&#123;&quot;com.docker.network.enable_ipv6&quot;:false,&quot;com.docker.network.generic&quot;:&#123;&quot;VlanId&quot;:&quot;192&quot;&#125;&#125;,&quot;id&quot;:&quot;c79e533e4444294ac9cb7838608115c961c6e403d3610367ff4b197ef6b981fc&quot;,&quot;inDelete&quot;:false,&quot;internal&quot;:false,&quot;ipamOptions&quot;:&#123;&quot;VlanId&quot;:&quot;192&quot;&#125;,&quot;ipamType&quot;:&quot;default&quot;,&quot;ipamV4Config&quot;:&quot;[&#123;\&quot;PreferredPool\&quot;:\&quot;192.168.8.0/24\&quot;,\&quot;SubPool\&quot;:\&quot;\&quot;,\&quot;Gateway\&quot;:\&quot;192.168.8.1\&quot;,\&quot;AuxAddresses\&quot;:null&#125;]&quot;,&quot;ipamV4Info&quot;:&quot;[&#123;\&quot;IPAMData\&quot;:\&quot;&#123;\\\&quot;AddressSpace\\\&quot;:\\\&quot;\\\&quot;,\\\&quot;Gateway\\\&quot;:\\\&quot;192.168.8.1/24\\\&quot;,\\\&quot;Pool\\\&quot;:\\\&quot;192.168.8.0/24\\\&quot;&#125;\&quot;,\&quot;PoolID\&quot;:\&quot;GlobalDefault/192.168.8.0/24\&quot;&#125;]&quot;,**&quot;labels&quot;:&#123;&quot;com.alipay.acs.network.asw.hostname&quot;:&quot;238&quot;&#125;,**&quot;name&quot;:&quot;vlan192-8&quot;,&quot;networkType&quot;:&quot;vlan&quot;,&quot;persist&quot;:true,&quot;postIPv6&quot;:false,&quot;scope&quot;:&quot;global&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>example：</p>
<p>创建网络：&#x2F;&#x2F;–label&#x3D;”com.alipay.acs.network.asw.hostname&#x3D;vlan902-63”<br>docker network create -d vlan –label&#x3D;”com.alipay.acs.network.asw.hostname&#x3D;vlan902-63” –subnet&#x3D;11.162.63.0&#x2F;24  –gateway&#x3D;11.162.63.247  –opt VlanId&#x3D;902 –ipam-opt VlanId&#x3D;902 hanetwork2<br>跟daemon中的标签：com.alipay.acs.engine.asw.hostname&#x3D;vlan902-63 对应，匹配调度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$sudo cat /etc/docker/daemon.json</span><br><span class="line">&#123;&quot;labels&quot;:[&quot;com.alipay.acs.engine.hostname=11.239.142.46&quot;,&quot;com.alipay.acs.engine.ip=11.239.142.46&quot;,&quot;com.alipay.acs.engine.device_type=Server&quot;,&quot;com.alipay.acs.engine.status=free&quot;,&quot;ark.network.vlan.range=vlan902-63&quot;,&quot;com.alipay.acs.engine.asw.hostname=vlan902-63&quot;,&quot;com.alipay.acs.network.asw.hostname=vlan902-63&quot;]&#125;</span><br><span class="line">//不指定具体网络，有多个网络的时候自动调度  --net driver:vlan 必须是network打过标签了</span><br><span class="line">docker run -d -it --name=&quot;udp10&quot; --net driver:vlan --restart=always reg.docker.alibaba-inc.com/middleware.udp</span><br></pre></td></tr></table></figure>


          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/8/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/10/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/images/avatar.gif" alt="twitter @plantegg">
          <p class="site-author-name" itemprop="name">twitter @plantegg</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
           
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">190</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">18</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">281</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>

<span id="busuanzi_container_site_pv">
    本站总访问量<span id="busuanzi_value_site_pv_footer"></span>次
</span>
<span id="busuanzi_container_site_uv">
  本站访客数<span id="busuanzi_value_site_uv_footer"></span>人次
</span>


        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i> 访问人数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>


        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  




	





  





  





  






  





  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  

  

  

  

</body>
</html>
