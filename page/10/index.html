<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/page/10/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="技术,编程,博客">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://plantegg.github.io/page/10/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"page/10/index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">plantegg</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">282</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/" class="post-title-link" itemprop="url">举三反一--从理论知识到实际问题的推导</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-11-02 10:30:03" itemprop="dateCreated datePublished" datetime="2020-11-02T10:30:03+08:00">2020-11-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="举三反一–从理论知识到实际问题的推导"><a href="#举三反一–从理论知识到实际问题的推导" class="headerlink" title="举三反一–从理论知识到实际问题的推导"></a>举三反一–从理论知识到实际问题的推导</h1><blockquote>
<p>怎么样才能获取举三反一的秘籍， 普通人为什么要案例来深化对理论知识的理解。</p>
</blockquote>
<h2 id="什么是工程效率，什么是知识效率"><a href="#什么是工程效率，什么是知识效率" class="headerlink" title="什么是工程效率，什么是知识效率"></a>什么是工程效率，什么是知识效率</h2><p>有些人纯看理论就能掌握好一门技能，还能举三反一，这是知识效率，这种人非常少；</p>
<p>大多数普通人都是看点知识然后结合实践来强化理解理论，要经过反反复复才能比较好地掌握一个知识，这就是工程效率，讲究技巧、工具来达到目的。</p>
<p>对于费曼（参考费曼学习法）这样的聪明人就是很容易看到一个理论知识就能理解这个理论知识背后的本质。</p>
<p>肯定知识效率最牛逼，但是拥有这种能力的人毕竟非常少。从小我们周边那种不怎么学的学霸型基本都是这类，这种学霸都还能触类旁通非常快地掌握一个新知识。剩下的绝大部分只能拼时间(刷题)+方法+总结等也能掌握一些知识</p>
<p>但是这事又不能独立看待有些人在某些方向上是工程效率型，有些方向就又是知识效率型（有一种知识效率型是你掌握的实在太多也就比较容易触类旁通了，这算灰色知识效率型）</p>
<p>使劲挖掘自己在知识效率型方面的能力吧，即使灰色地带也行啊。</p>
<p>接下来看看TCP状态中的CLOSE_WAIT状态的含义</p>
<h2 id="先看TCP连接状态图"><a href="#先看TCP连接状态图" class="headerlink" title="先看TCP连接状态图"></a>先看TCP连接状态图</h2><p>这是网络、书本上凡是描述TCP状态一定会出现的状态图，理论上看这个图能解决任何TCP状态问题。</p>
<p><img src="/images/oss/b3d075782450b0c8d2615c5d2b75d923.png" alt="image.png"></p>
<p>反复看这个图的右下部分的CLOSE_WAIT ，从这个图里可以得到如下结论：</p>
<p><strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong></p>
<p>基本上这一结论要能帮助解决所有CLOSE_WAIT相关的问题，如果不能说明对这个知识点理解的不够。</p>
<h2 id="server端大量close-wait案例"><a href="#server端大量close-wait案例" class="headerlink" title="server端大量close_wait案例"></a>server端大量close_wait案例</h2><p>用实际案例来检查自己对CLOSE_WAIT 理论（<strong>CLOSE_WAIT是被动关闭端在等待应用进程的关闭</strong>）的掌握 – 能不能用这个结论来解决实际问题。同时也可以看看自己从知识到问题的推理能力（跟前面的知识效率呼应一下）。</p>
<h3 id="问题描述："><a href="#问题描述：" class="headerlink" title="问题描述："></a>问题描述：</h3><blockquote>
<p>服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn大小后 CLOSE_WAIT 也会跟着变成一样的值）</p>
</blockquote>
<p>根据这个描述先不要往下看，自己推理分析下可能的原因。</p>
<p>我的推理如下：</p>
<p>从这里看起来，client跟server成功建立了somaxconn个连接（somaxconn小于backlog，所以accept queue只有这么大），但是应用没有accept这个连接，导致这些连接一直在accept queue中。但是这些连接的状态已经是ESTABLISHED了，也就是client可以发送数据了，数据发送到server后OS ack了，并放在os的tcp buffer中，应用一直没有accept也就没法读取数据。client于是发送fin（可能是超时、也可能是简单发送数据任务完成了得结束连接），这时Server上这个连接变成了CLOSE_WAIT .</p>
<p>也就是从开始到结束这些连接都在accept queue中，没有被应用accept，很快他们又因为client 发送 fin 包变成了CLOSE_WAIT ，所以始终看到的是服务端出现大量CLOSE_WAIT 并且个数正好等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）。</p>
<p>如下图所示，在连接进入accept queue后状态就是ESTABLISED了，也就是可以正常收发数据和fin了。client是感知不到server是否accept()了，只是发了数据后server的os代为保存在OS的TCP buffer中，因为应用没来取自然在CLOSE_WAIT 后应用也没有close()，所以一直维持CLOSE_WAIT 。</p>
<p>得检查server 应用为什么没有accept。</p>
<p><img src="/images/951413iMgBlog/20190706093602331.png" alt="Recv-Q和Send-Q"></p>
<p>如上是老司机的思路靠经验缺省了一些理论推理，缺省还是对理论理解不够， 这个分析抓住了 大量CLOSE_WAIT 个数正好 等于somaxconn（调整somaxconn后 CLOSE_WAIT 也会跟着变成一样的值）但是没有抓住 CLOSE_WAIT 背后的核心原因</p>
<h3 id="更简单的推理"><a href="#更简单的推理" class="headerlink" title="更简单的推理"></a>更简单的推理</h3><p>如果没有任何实战经验，只看上面的状态图的学霸应该是这样推理的：</p>
<p>看到server上有大量的CLOSE_WAIT说明client主动断开了连接，server的OS收到client 发的fin，并回复了ack，这个过程不需要应用感知，进而连接从ESTABLISHED进入CLOSE_WAIT，此时在等待server上的应用调用close连关闭连接（处理完所有收发数据后才会调close()） —- 结论：server上的应用一直卡着没有调close().</p>
<p>同时这里很奇怪的现象： 服务端出现大量CLOSE_WAIT 个数正好 等于somaxconn，进而可以猜测是不是连接建立后很快accept队列满了（应用也没有accept() ), 导致 大量CLOSE_WAIT 个数正好 等于somaxconn —- 结论： server 上的应用不但没有调close(), 连close() 前面必须调用 accept() 都一直卡着没调 （这个结论需要有accept()队列的理论知识)</p>
<p><strong>从上面两个结论可以清楚地看到 server的应用卡住了</strong></p>
<h3 id="实际结论："><a href="#实际结论：" class="headerlink" title="实际结论："></a>实际结论：</h3><blockquote>
<p>这个case的最终原因是因为<strong>OS的open files设置的是1024,达到了上限</strong>，进而导致server不能accept，但这个时候的tcp连接状态已经是ESTABLISHED了（这个状态变换是取决于内核收发包，跟应用是否accept()无关）。</p>
<p>同时从这里可以推断 netstat 即使看到一个tcp连接状态是ESTABLISHED也不能代表占用了 open files句柄。此时client可以正常发送数据了，只是应用服务在accept之前没法receive数据和close连接。</p>
</blockquote>
<p>这个结论的图解如下：</p>
<p><img src="/images/oss/bcf463efeb677d5749d8d7571274ee79.png" alt="image.png"></p>
<p>假如全连接队列满了，握手第三步后对于client端来说是无法感知的，client端只需要回复ack后这个连接对于client端就是ESTABLISHED了，这时client是可以发送数据的。但是Server会扔掉收到的ack，回复syn+ack给client。</p>
<p>如果全连接队列没满，但是fd不够，那么在Server端这个Socket也是ESTABLISHED，但是只是暂存在全连接队列中，等待应用来accept，这个时候client端同样无法感知这个连接没有被accept，client是可以发送数据的，这个数据会保存在tcp receive memory buffer中，等到accept后再给应用。</p>
<p>如果自己无法得到上面的分析，那么再来看看如果把 CLOSE_WAIT 状态更细化地分析下(类似有老师帮你把知识点揉开跟实际案例联系下—-未必是上面的案例)，看完后再来分析下上面的案例。</p>
<h2 id="CLOSE-WAIT-状态拆解"><a href="#CLOSE-WAIT-状态拆解" class="headerlink" title="CLOSE_WAIT 状态拆解"></a>CLOSE_WAIT 状态拆解</h2><p>通常，CLOSE_WAIT 状态在服务器停留时间很短，如果你发现大量的 CLOSE_WAIT 状态，那么就意味着被动关闭的一方没有及时发出 FIN 包，一般有如下几种可能：</p>
<ul>
<li><strong>程序问题</strong>：如果代码层面忘记了 close 相应的 socket 连接，那么自然不会发出 FIN 包，从而导致 CLOSE_WAIT 累积；或者代码不严谨，出现死循环之类的问题，导致即便后面写了 close 也永远执行不到。</li>
<li>响应太慢或者超时设置过小：如果连接双方不和谐，一方不耐烦直接 timeout，另一方却还在忙于耗时逻辑，就会导致 close 被延后。响应太慢是首要问题，不过换个角度看，也可能是 timeout 设置过小。</li>
<li>BACKLOG 太大：此处的 backlog 不是 syn backlog，而是 accept 的 backlog，如果 backlog 太大的话，设想突然遭遇大访问量的话，即便响应速度不慢，也可能出现来不及消费的情况，导致多余的请求还在<a target="_blank" rel="noopener" href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/">队列</a>里就被对方关闭了。</li>
</ul>
<p>如果你通过「netstat -ant」或者「ss -ant」命令发现了很多 CLOSE_WAIT 连接，请注意结果中的「Recv-Q」和「Local Address」字段，通常「Recv-Q」会不为空，它表示应用还没来得及接收数据，而「Local Address」表示哪个地址和端口有问题，我们可以通过「lsof -i:<PORT>」来确认端口对应运行的是什么程序以及它的进程号是多少。</p>
<p>如果是我们自己写的一些程序，比如用 HttpClient 自定义的蜘蛛，那么八九不离十是程序问题，如果是一些使用广泛的程序，比如 Tomcat 之类的，那么更可能是响应速度太慢或者 timeout 设置太小或者 BACKLOG 设置过大导致的故障。</p>
<p>看完这段 CLOSE_WAIT 更具体深入点的分析后再来分析上面的案例看看，能否推导得到正确的结论。</p>
<h2 id="一些疑问"><a href="#一些疑问" class="headerlink" title="一些疑问"></a>一些疑问</h2><h3 id="连接都没有被accept-client端就能发送数据了？"><a href="#连接都没有被accept-client端就能发送数据了？" class="headerlink" title="连接都没有被accept(), client端就能发送数据了？"></a>连接都没有被accept(), client端就能发送数据了？</h3><p>答：是的。只要这个连接在OS看来是ESTABLISHED的了就可以，因为握手、接收数据都是由内核完成的，内核收到数据后会先将数据放在内核的tcp buffer中，然后os回复ack。另外三次握手之后client端是没法知道server端是否accept()了。</p>
<h3 id="CLOSE-WAIT与accept-queue有关系吗？"><a href="#CLOSE-WAIT与accept-queue有关系吗？" class="headerlink" title="CLOSE_WAIT与accept queue有关系吗？"></a>CLOSE_WAIT与accept queue有关系吗？</h3><p>答：没有关系。只是本案例中因为open files不够了，影响了应用accept(), 导致accept queue满了，同时因为即使应用不accept（三次握手后，server端是否accept client端无法感知），client也能发送数据和发 fin断连接，这些响应都是os来负责，跟上层应用没关系，连接从握手到ESTABLISHED再到CLOSE_WAIT都不需要fd，也不需要应用参与。CLOSE_WAIT只跟应用不调 close() 有关系。 </p>
<h3 id="CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？"><a href="#CLOSE-WAIT与accept-queue为什么刚好一致并且联动了？" class="headerlink" title="CLOSE_WAIT与accept queue为什么刚好一致并且联动了？"></a>CLOSE_WAIT与accept queue为什么刚好一致并且联动了？</h3><p>答：这里他们的数量刚好一致是因为所有新建连接都没有accept，堵在queue中。同时client发现问题后把所有连接都fin了，也就是所有queue中的连接从来没有被accept过，但是他们都是ESTABLISHED，过一阵子之后client端发了fin所以所有accept queue中的连接又变成了 CLOSE_WAIT, 所以二者刚好一致并且联动了</p>
<h3 id="openfiles和accept-的关系是？"><a href="#openfiles和accept-的关系是？" class="headerlink" title="openfiles和accept()的关系是？"></a>openfiles和accept()的关系是？</h3><p>答：accept()的时候才会创建文件句柄，消耗openfiles</p>
<h3 id="一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"><a href="#一个连接如果在accept-queue中了，但是还没有被应用-accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？" class="headerlink" title="一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？"></a>一个连接如果在accept queue中了，但是还没有被应用 accept，那么这个时候在server上看这个连接的状态他是ESTABLISHED的吗？</h3><p>答：是</p>
<h3 id="如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？"><a href="#如果server的os参数-open-files到了上限（就是os没法打开新的文件句柄了）会导致这个accept-queue中的连接一直没法被accept对吗？" class="headerlink" title="如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？"></a>如果server的os参数 open files到了上限（就是os没法打开新的文件句柄了）会导致这个accept queue中的连接一直没法被accept对吗？</h3><p>答：对</p>
<h3 id="如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？"><a href="#如果通过gdb-attach-应用进程，故意让进程accept，这个时候client还能连上应用吗？" class="headerlink" title="如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？"></a>如果通过gdb attach 应用进程，故意让进程accept，这个时候client还能连上应用吗？</h3><p>答： 能，这个时候在client和server两边看到的连接状态都是 ESTABLISHED，只是Server上的全连接队列占用加1。连接握手并切换到ESTABLISHED状态都是由OS来负责的，应用不参与，ESTABLISHED后应用才能accept，进而收发数据。也就是能放入到全连接队列里面的连接肯定都是 ESTABLISHED 状态的了</p>
<h3 id="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"><a href="#接着上面的问题，如果新连接继续连接进而全连接队列满了呢？" class="headerlink" title="接着上面的问题，如果新连接继续连接进而全连接队列满了呢？"></a>接着上面的问题，如果新连接继续连接进而全连接队列满了呢？</h3><p>答：那就连不上了，server端的OS因为全连接队列满了直接扔掉第一个syn握手包，这个时候连接在client端是SYN_SENT，Server端没有这个连接，这是因为syn到server端就直接被OS drop 了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">//如下图，本机测试，只有一个client端发起的syn_send, 3306的server端没有任何连接</span><br><span class="line">$netstat -antp  |grep -i 127.0.0.1:3306</span><br><span class="line">tcp     0   1 127.0.0.1:61106      127.0.0.1:3306    SYN_SENT    21352/telnet</span><br></pre></td></tr></table></figure>

<p>能进入到accept queue中的连接都是 ESTABLISHED，不管用户态有没有accept，用户态accept后队列大小减1</p>
<h3 id="如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？"><a href="#如果一个连接握手成功进入到accept-queue但是应用accept前被对方RESET了呢？" class="headerlink" title="如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？"></a>如果一个连接握手成功进入到accept queue但是应用accept前被对方RESET了呢？</h3><p>答： 如果此时收到对方的RESET了，那么OS会释放这个连接。但是内核认为所有 listen 到的连接, 必须要 accept 走, 因为用户有权利知道有过这么一个连接存在过。所以OS不会到全连接队列拿掉这个连接，全连接队列数量也不会减1，直到应用accept这个连接，然后read&#x2F;write才发现这个连接断开了，报communication failure异常</p>
<h3 id="什么时候连接状态变成-ESTABLISHED"><a href="#什么时候连接状态变成-ESTABLISHED" class="headerlink" title="什么时候连接状态变成 ESTABLISHED"></a>什么时候连接状态变成 ESTABLISHED</h3><p>三次握手成功就变成 ESTABLISHED 了，不需要用户态来accept，如果握手第三步的时候OS发现全连接队列满了，这时OS会扔掉这个第三次握手ack，并重传握手第二步的syn+ack, 在OS端这个连接还是 SYN_RECV 状态的，但是client端是 ESTABLISHED状态的了。</p>
<p>这是在4000（tearbase）端口上<strong>全连接队列没满，但是应用不再accept了</strong>，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //server</span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 ESTABLISHED //client</span><br><span class="line">[root@dcep-blockchain-1 cfl-sm2-sm3]# ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port         Peer Address:Port   </span><br><span class="line">LISTEN      73     1024            *:terabase                 *:* </span><br></pre></td></tr></table></figure>

<p>这是在4000（tearbase）端口上<strong>全连接队列满掉</strong>后，nc用12346端口去连4000（tearbase）端口的结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -at |grep &quot;:12346 &quot;  </span><br><span class="line">tcp   0      0 dcep-blockchai:terabase dcep-blockchain-1:12346 SYN_RECV    //server</span><br><span class="line">tcp   0      0 dcep-blockchain-1:12346 dcep-blockchai:terabase ESTABLISHED //client</span><br><span class="line"># ss -lt</span><br><span class="line">State       Recv-Q Send-Q      Local Address:Port       Peer Address:Port   </span><br><span class="line">LISTEN      1025   1024             *:terabase              *:* </span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/09/22/kubernetes%20service%20%E5%92%8C%20kube-proxy%E8%AF%A6%E8%A7%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/22/kubernetes%20service%20%E5%92%8C%20kube-proxy%E8%AF%A6%E8%A7%A3/" class="post-title-link" itemprop="url">kubernetes service 和 kube-proxy详解</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-22 17:30:03" itemprop="dateCreated datePublished" datetime="2020-09-22T17:30:03+08:00">2020-09-22</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解" class="headerlink" title="kubernetes service 和 kube-proxy详解"></a>kubernetes service 和 kube-proxy详解</h1><blockquote>
<p>service 是Kubernetes里面非常重要的一个功能，用以解决负载均衡、弹性伸缩、升级灰度等等 </p>
<p>本文先从概念介绍到实际负载均衡运转过程中追踪每个环节都做哪些处理，同时这些包会相应地怎么流转最终到达目标POD，以阐明service工作原理以及kube-proxy又在这个过程中充当了什么角色。</p>
</blockquote>
<h2 id="service-模式"><a href="#service-模式" class="headerlink" title="service 模式"></a>service 模式</h2><p>根据创建Service的<code>type</code>类型不同，可分成4种模式：</p>
<ul>
<li>ClusterIP： <strong>默认方式</strong>。根据是否生成ClusterIP又可分为普通Service和Headless Service两类：<ul>
<li><code>普通Service</code>：通过为Kubernetes的Service分配一个集群内部可访问的固定虚拟IP（Cluster IP），实现集群内的访问。为最常见的方式。</li>
<li><code>Headless Service</code>：该服务不会分配Cluster IP，也不通过kube-proxy做反向代理和负载均衡。而是通过DNS提供稳定的网络ID来访问，DNS会将headless service的后端直接解析为podIP列表。主要供StatefulSet中对应POD的序列用。</li>
</ul>
</li>
<li><code>NodePort</code>：除了使用Cluster IP之外，还通过将service的port映射到集群内每个节点的相同一个端口，实现通过nodeIP:nodePort从集群外访问服务。NodePort会RR转发给后端的任意一个POD，跟ClusterIP类似</li>
<li><code>LoadBalancer</code>：和nodePort类似，不过除了使用一个Cluster IP和nodePort之外，还会向所使用的公有云申请一个负载均衡器，实现从集群外通过LB访问服务。在公有云提供的 Kubernetes 服务里，都使用了一个叫作 CloudProvider 的转接层，来跟公有云本身的 API 进行对接。所以，在上述 LoadBalancer 类型的 Service 被提交后，Kubernetes 就会调用 CloudProvider 在公有云上为你创建一个负载均衡服务，并且把被代理的 Pod 的 IP 地址配置给负载均衡服务做后端。</li>
<li><code>ExternalName</code>：是 Service 的特例。此模式主要面向运行在集群外部的服务，通过它可以将外部服务映射进k8s集群，且具备k8s内服务的一些特征（如具备namespace等属性），来为集群内部提供服务。此模式要求kube-dns的版本为1.7或以上。这种模式和前三种模式（除headless service）最大的不同是重定向依赖的是dns层次，而不是通过kube-proxy。</li>
</ul>
<p>service yaml案例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ren</span><br><span class="line">  labels:</span><br><span class="line">    app: web</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line"># clusterIP: None  </span><br><span class="line">  ports:</span><br><span class="line">  - port: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line">    nodePort: 30080</span><br><span class="line">  selector:</span><br><span class="line">    app: ren</span><br></pre></td></tr></table></figure>

<p><code>ports</code> 字段指定服务的端口信息：</p>
<ul>
<li><code>port</code>：虚拟 ip 要绑定的 port，每个 service 会创建出来一个虚拟 ip，通过访问 <code>vip:port</code> 就能获取服务的内容。这个 port 可以用户随机选取，因为每个服务都有自己的 vip，也不用担心冲突的情况</li>
<li><code>targetPort</code>：pod 中暴露出来的 port，这是运行的容器中具体暴露出来的端口，一定不能写错–一般用name来代替具体的port</li>
<li><code>protocol</code>：提供服务的协议类型，可以是 <code>TCP</code> 或者 <code>UDP</code></li>
<li><code>nodePort</code>： 仅在type为nodePort模式下有用，宿主机暴露端口</li>
</ul>
<p>nodePort和loadbalancer可以被外部访问，loadbalancer需要一个外部ip，流量走外部ip进出</p>
<p>NodePort向外部暴露了多个宿主机的端口，外部可以部署负载均衡将这些地址配置进去。</p>
<p>默认情况下，服务会rr转发到可用的后端。如果希望保持会话（同一个 client 永远都转发到相同的 pod），可以把 <code>service.spec.sessionAffinity</code> 设置为 <code>ClientIP</code>。</p>
<h2 id="Service和kube-proxy的工作原理"><a href="#Service和kube-proxy的工作原理" class="headerlink" title="Service和kube-proxy的工作原理"></a>Service和kube-proxy的工作原理</h2><p>kube-proxy有两种主要的实现（userspace基本没有使用了）：</p>
<ul>
<li>iptables来做NAT以及负载均衡（默认方案）</li>
<li>ipvs来做NAT以及负载均衡</li>
</ul>
<p>Service 是由 kube-proxy 组件通过监听 Pod 的变化事件，在宿主机上维护iptables规则或者ipvs规则。</p>
<p>Kube-proxy 主要监听两个对象，一个是 Service，一个是 Endpoint，监听他们启停。以及通过selector将他们绑定。</p>
<p>IPVS 是专门为LB设计的。它用hash table管理service，对service的增删查找都是*O(1)*的时间复杂度。不过IPVS内核模块没有SNAT功能，因此借用了iptables的SNAT功能。IPVS 针对报文做DNAT后，将连接信息保存在nf_conntrack中，iptables据此接力做SNAT。该模式是目前Kubernetes网络性能最好的选择。但是由于nf_conntrack的复杂性，带来了很大的性能损耗。</p>
<h3 id="iptables-实现负载均衡的工作流程"><a href="#iptables-实现负载均衡的工作流程" class="headerlink" title="iptables 实现负载均衡的工作流程"></a>iptables 实现负载均衡的工作流程</h3><p>如果kube-proxy不是用的ipvs模式，那么主要靠iptables来做DNAT和SNAT以及负载均衡</p>
<p>iptables+clusterIP工作流程：</p>
<ol>
<li>集群内访问svc 10.10.35.224:3306 命中 kube-services iptables（两条规则，宿主机、以及pod内）</li>
<li>iptables 规则：KUBE-SEP-F4QDAAVSZYZMFXZQ 对应到  KUBE-SEP-F4QDAAVSZYZMFXZQ</li>
<li>KUBE-SEP-F4QDAAVSZYZMFXZQ 指示 DNAT到 宿主机：192.168.0.83:10379（在内核中将包改写了ip port）</li>
<li>从svc description中可以看到这个endpoint的地址 192.168.0.83:10379（pod使用Host network）</li>
</ol>
<p><img src="/images/oss/52e050ebb7841d70b7e3ea62e18d5b30.png" alt="image.png"></p>
<p>iptables规则解析如下（case不一样，所以看到的端口、ip都不一样）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">-t nat -A &#123;PREROUTING, OUTPUT&#125; -m conntrack --ctstate NEW -j KUBE-SERVICES</span><br><span class="line"></span><br><span class="line"># 宿主机访问 nginx Service 的流量，同时满足 4 个条件：</span><br><span class="line"># 1. src_ip 不是 Pod 网段</span><br><span class="line"># 2. dst_ip=3.3.3.3/32 (ClusterIP)</span><br><span class="line"># 3. proto=TCP</span><br><span class="line"># 4. dport=80</span><br><span class="line"># 如果匹配成功，直接跳转到 KUBE-MARK-MASQ；否则，继续匹配下面一条（iptables 是链式规则，高优先级在前）</span><br><span class="line"># 跳转到 KUBE-MARK-MASQ 是为了保证这些包出宿主机时，src_ip 用的是宿主机 IP。</span><br><span class="line">-A KUBE-SERVICES ! -s 1.1.0.0/16 -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line"># Pod 访问 nginx Service 的流量：同时满足 4 个条件：</span><br><span class="line"># 1. 没有匹配到前一条的，（说明 src_ip 是 Pod 网段）</span><br><span class="line"># 2. dst_ip=3.3.3.3/32 (ClusterIP)</span><br><span class="line"># 3. proto=TCP</span><br><span class="line"># 4. dport=80</span><br><span class="line">-A KUBE-SERVICES -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-NGINX</span><br><span class="line"></span><br><span class="line"># 以 50% 的概率跳转到 KUBE-SEP-NGINX1</span><br><span class="line">-A KUBE-SVC-NGINX -m statistic --mode random --probability 0.50 -j KUBE-SEP-NGINX1</span><br><span class="line"># 如果没有命中上面一条，则以 100% 的概率跳转到 KUBE-SEP-NGINX2</span><br><span class="line">-A KUBE-SVC-NGINX -j KUBE-SEP-NGINX2</span><br><span class="line"></span><br><span class="line"># 如果 src_ip=1.1.1.1/32，说明是 Service-&gt;client 流量，则</span><br><span class="line"># 需要做 SNAT（MASQ 是动态版的 SNAT），替换 src_ip -&gt; svc_ip，这样客户端收到包时，</span><br><span class="line"># 看到就是从 svc_ip 回的包，跟它期望的是一致的。</span><br><span class="line">-A KUBE-SEP-NGINX1 -s 1.1.1.1/32 -j KUBE-MARK-MASQ</span><br><span class="line"># 如果没有命令上面一条，说明 src_ip != 1.1.1.1/32，则说明是 client-&gt; Service 流量，</span><br><span class="line"># 需要做 DNAT，将 svc_ip -&gt; pod1_ip，</span><br><span class="line">-A KUBE-SEP-NGINX1 -p tcp -m tcp -j DNAT --to-destination 1.1.1.1:80</span><br><span class="line"># 同理，见上面两条的注释</span><br><span class="line">-A KUBE-SEP-NGINX2 -s 1.1.1.2/32 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-NGINX2 -p tcp -m tcp -j DNAT --to-destination 1.1.1.2:80</span><br></pre></td></tr></table></figure>

<p><img src="/images/951413iMgBlog/dc0aa14d0eedf9f8f6f8bca1eee34cf8.png" alt="image.png"></p>
<p>在对应的宿主机上可以清楚地看到容器中的mysqld进程正好监听着 10379端口</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@az1-drds-83 ~]# ss -lntp |grep 10379</span><br><span class="line">LISTEN     0      128         :::10379                   :::*                   users:((&quot;mysqld&quot;,pid=17707,fd=18))</span><br><span class="line">[root@az1-drds-83 ~]# ps auxff | grep 17707 -B2</span><br><span class="line">root     13606  0.0  0.0  10720  3764 ?        Sl   17:09   0:00  \_ containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97 -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc</span><br><span class="line"></span><br><span class="line">root     13624  0.0  0.0 103044 10424 ?        Ss   17:09   0:00  |   \_ python /entrypoint.py</span><br><span class="line">root     14835  0.0  0.0  11768  1636 ?        S    17:10   0:00  |   \_ /bin/sh /u01/xcluster/bin/mysqld_safe --defaults-file=/home/mysql/my10379.cnf</span><br><span class="line">alidb    17707  0.6  0.0 1269128 67452 ?       Sl   17:10   0:25  |       \_ /u01/xcluster_20200303/bin/mysqld --defaults-file=/home/mysql/my10379.cnf --basedir=/u01/xcluster_20200303 --datadir=/home/mysql/data10379/dbs10379 --plugin-dir=/u01/xcluster_20200303/lib/plugin --user=mysql --log-error=/home/mysql/data10379/mysql/master-error.log --open-files-limit=8192 --pid-file=/home/mysql/data10379/dbs10379/az1-drds-83.pid --socket=/home/mysql/data10379/tmp/mysql.sock --port=10379</span><br></pre></td></tr></table></figure>

<p>对应的这个pod的description：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">#kubectl describe pod apsaradbcluster010-cv6w</span><br><span class="line">Name:         apsaradbcluster010-cv6w</span><br><span class="line">Namespace:    default</span><br><span class="line">Priority:     0</span><br><span class="line">Node:         az1-drds-83/192.168.0.83</span><br><span class="line">Start Time:   Thu, 10 Sep 2020 17:09:33 +0800</span><br><span class="line">Labels:       alisql.clusterName=apsaradbcluster010</span><br><span class="line">              alisql.pod_name=apsaradbcluster010-cv6w</span><br><span class="line">              alisql.pod_role=leader</span><br><span class="line">Annotations:  apsara.metric.pod_name: apsaradbcluster010-cv6w</span><br><span class="line">Status:       Running</span><br><span class="line">IP:           192.168.0.83</span><br><span class="line">IPs:</span><br><span class="line">  IP:           192.168.0.83</span><br><span class="line">Controlled By:  ApsaradbCluster/apsaradbcluster010</span><br><span class="line">Containers:</span><br><span class="line">  engine:</span><br><span class="line">    Container ID:   docker://ead57b52b11902b9b5004db0b72abb060b56a1af7ee7ad7066bd09c946abcb97</span><br><span class="line">    Image:          reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-engine:develop-20200910140415</span><br><span class="line">    Image ID:       docker://sha256:7ad5cc53c87b34806eefec829d70f5f0192f4127c7ee4e867cb3da3bb6c2d709</span><br><span class="line">    Ports:          10379/TCP, 20383/TCP, 46846/TCP</span><br><span class="line">    Host Ports:     10379/TCP, 20383/TCP, 46846/TCP</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:</span><br><span class="line">      ALISQL_POD_NAME:  apsaradbcluster010-cv6w (v1:metadata.name)</span><br><span class="line">      ALISQL_POD_PORT:  10379</span><br><span class="line">    Mounts:</span><br><span class="line">      /dev/shm from devshm (rw)</span><br><span class="line">      /etc/localtime from etclocaltime (rw)</span><br><span class="line">      /home/mysql/data from data-dir (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</span><br><span class="line">  exporter:</span><br><span class="line">    Container ID:  docker://b49865b7798f9036b431203d54994ac8fdfcadacb01a2ab4494b13b2681c482d</span><br><span class="line">    Image:         reg.docker.alibaba-inc.com/apsaradb/alisqlcluster-exporter:latest</span><br><span class="line">    Image ID:      docker://sha256:432cdd0a0e7c74c6eb66551b6f6af9e4013f60fb07a871445755f6577b44da19</span><br><span class="line">    Port:          47272/TCP</span><br><span class="line">    Host Port:     47272/TCP</span><br><span class="line">    Args:</span><br><span class="line">      --web.listen-address=:47272</span><br><span class="line">      --collect.binlog_size</span><br><span class="line">      --collect.engine_innodb_status</span><br><span class="line">      --collect.info_schema.innodb_metrics</span><br><span class="line">      --collect.info_schema.processlist</span><br><span class="line">      --collect.info_schema.tables</span><br><span class="line">      --collect.info_schema.tablestats</span><br><span class="line">      --collect.slave_hosts</span><br><span class="line">    State:          Running</span><br><span class="line">      Started:      Thu, 10 Sep 2020 17:09:35 +0800</span><br><span class="line">    Ready:          True</span><br><span class="line">    Restart Count:  0</span><br><span class="line">    Environment:</span><br><span class="line">      ALISQL_POD_NAME:   apsaradbcluster010-cv6w (v1:metadata.name)</span><br><span class="line">      DATA_SOURCE_NAME:  root:@(127.0.0.1:10379)/</span><br><span class="line">    Mounts:</span><br><span class="line">      /dev/shm from devshm (rw)</span><br><span class="line">      /etc/localtime from etclocaltime (rw)</span><br><span class="line">      /home/mysql/data from data-dir (rw)</span><br><span class="line">      /var/run/secrets/kubernetes.io/serviceaccount from default-token-n2bmn (ro)</span><br></pre></td></tr></table></figure>

<p>DNAT 规则的作用，就是<strong>在 PREROUTING 检查点之前，也就是在路由之前，将流入 IP 包的目的地址和端口，改成–to-destination 所指定的新的目的地址和端口</strong>。可以看到，这个目的地址和端口，正是被代理 Pod 的 IP 地址和端口。</p>
<h4 id="如下是一个iptables来实现service的案例中的iptables流量分配规则："><a href="#如下是一个iptables来实现service的案例中的iptables流量分配规则：" class="headerlink" title="如下是一个iptables来实现service的案例中的iptables流量分配规则："></a>如下是一个iptables来实现service的案例中的iptables流量分配规则：</h4><p>三个pod，每个pod承担三分之一的流量</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">iptables-save | grep 3306</span><br><span class="line"></span><br><span class="line">iptables-save | grep KUBE-SERVICES</span><br><span class="line"></span><br><span class="line">#iptables-save |grep KUBE-SVC-RVEVH2XMONK6VC5O</span><br><span class="line">:KUBE-SVC-RVEVH2XMONK6VC5O - [0:0]</span><br><span class="line">-A KUBE-SERVICES -d 10.10.70.95/32 -p tcp -m comment --comment &quot;drds/mysql-read:mysql cluster IP&quot; -m tcp --dport 3306 -j KUBE-SVC-RVEVH2XMONK6VC5O</span><br><span class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-XC4TZYIZFYB653VI</span><br><span class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-MK4XPBZUIJGFXKED</span><br><span class="line">-A KUBE-SVC-RVEVH2XMONK6VC5O -m comment --comment &quot;drds/mysql-read:mysql&quot; -j KUBE-SEP-AAYXWGQJBDHUJUQ3</span><br></pre></td></tr></table></figure>

<p>到这里我们基本可以看到，利用iptables规则，宿主机内核把发到宿主机上的流量按照iptables规则做dnat后发给service后端的pod，同时iptables规则可以配置每个pod的流量大小。再辅助kube-proxy监听pod的起停和健康状态并相应地更新iptables规则，这样整个service实现逻辑就很清晰了。</p>
<p>看起来 service 是个完美的方案，可以解决服务访问的所有问题，但是 service 这个方案（iptables 模式）也有自己的缺点。</p>
<p>首先，<strong>如果转发的 pod 不能正常提供服务，它不会自动尝试另一个 pod</strong>，当然这个可以通过 <code>readiness probes</code> 来解决。每个 pod 都有一个健康检查的机制，当有 pod 健康状况有问题时，kube-proxy 会删除对应的转发规则。</p>
<p>另外，<code>nodePort</code> 类型的服务也无法添加 TLS 或者更复杂的报文路由机制。因为只做了NAT</p>
<h3 id="ipvs-实现负载均衡的原理"><a href="#ipvs-实现负载均衡的原理" class="headerlink" title="ipvs 实现负载均衡的原理"></a>ipvs 实现负载均衡的原理</h3><p>ipvs模式下，kube-proxy会先创建虚拟网卡，kube-ipvs0下面的每个ip都对应着svc的一个clusterIP：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># ip addr</span><br><span class="line">  ...</span><br><span class="line">5: kube-ipvs0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default </span><br><span class="line">    link/ether de:29:17:2a:8d:79 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.68.70.130/32 scope global kube-ipvs0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure>

<p>kube-ipvs0下面绑的这些ip就是在发包的时候让内核知道如果目标ip是这些地址的话，这些地址是自身的所以包不会发出去，而是给INPUT链，这样ipvs内核模块有机会改写包做完NAT后再发走。</p>
<p>ipvs会放置DNAT钩子在INPUT链上，因此必须要让内核识别 VIP 是本机的 IP。这样才会过INPUT 链，要不然就通过OUTPUT链出去了。k8s 通过kube-proxy将service cluster ip 绑定到虚拟网卡kube-ipvs0。</p>
<p>同时在路由表中增加一些ipvs 的路由条目：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># ip route show table local  //等于ip route list table local</span><br><span class="line">local 10.68.0.1 dev kube-ipvs0 proto kernel scope host src 10.68.0.1 </span><br><span class="line">local 10.68.0.2 dev kube-ipvs0 proto kernel scope host src 10.68.0.2 </span><br><span class="line">local 10.68.70.130 dev kube-ipvs0 proto kernel scope host src 10.68.70.130 -- ipvs</span><br><span class="line">broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 </span><br><span class="line">local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 </span><br><span class="line">local 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 </span><br><span class="line">broadcast 127.255.255.255 dev lo proto kernel scope link src 127.0.0.1 </span><br><span class="line">broadcast 172.17.0.0 dev docker0 proto kernel scope link src 172.17.0.1 </span><br><span class="line">local 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 </span><br><span class="line">broadcast 172.17.255.255 dev docker0 proto kernel scope link src 172.17.0.1 </span><br><span class="line">local 172.20.185.192 dev tunl0 proto kernel scope host src 172.20.185.192 </span><br><span class="line">broadcast 172.20.185.192 dev tunl0 proto kernel scope link src 172.20.185.192 </span><br><span class="line">broadcast 172.26.128.0 dev eth0 proto kernel scope link src 172.26.137.117 </span><br><span class="line">local 172.26.137.117 dev eth0 proto kernel scope host src 172.26.137.117 </span><br><span class="line">broadcast 172.26.143.255 dev eth0 proto kernel scope link src 172.26.137.117 </span><br><span class="line"></span><br><span class="line">//访问本机IP（不是 127.0.0.1），内核在路由项查找的时候判断类型是 RTN_LOCAL，仍然会使用 net-&gt;loopback_dev。也就是 lo 虚拟网卡。</span><br></pre></td></tr></table></figure>

<p>而接下来，kube-proxy 就会通过 Linux 的 IPVS 模块，为这个 IP 地址设置三个 IPVS 虚拟主机，并设置这三个虚拟主机之间使用轮询模式 (rr) 来作为负载均衡策略。我们可以通过 ipvsadm 查看到这个设置，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ipvsadm -ln |grep 10.68.114.131 -A5</span><br><span class="line">TCP  10.68.114.131:3306 rr</span><br><span class="line">  -&gt; 172.20.120.143:3306          Masq    1      0          0         </span><br><span class="line">  -&gt; 172.20.185.209:3306          Masq    1      0          0         </span><br><span class="line">  -&gt; 172.20.248.143:3306          Masq    1      0          0  </span><br></pre></td></tr></table></figure>

<p>172.20.<em>.</em> 是后端真正pod的ip， 10.68.114.131 是cluster-ip.</p>
<p>完整的工作流程如下：</p>
<ol>
<li>因为service cluster ip 绑定到虚拟网卡kube-ipvs0上，内核可以识别访问的 VIP 是本机的 IP.</li>
<li>数据包到达INPUT链.</li>
<li>ipvs监听到达input链的数据包，比对数据包请求的服务是为集群服务，修改数据包的目标IP地址为对应pod的IP，然后将数据包发至POSTROUTING链.</li>
<li>数据包经过POSTROUTING链选路由后，将数据包通过tunl0网卡(calico网络模型)发送出去。从tunl0虚拟网卡获得源IP.</li>
<li>经过tunl0后进行ipip封包，丢到物理网络，路由到目标node（目标pod所在的node）</li>
<li>目标node进行ipip解包后给pod对应的网卡</li>
<li>pod接收到请求之后，构建响应报文，改变源地址和目的地址，返回给客户端。</li>
</ol>
<p><img src="/images/oss/51695ebb1c6b30d95f8ac8d5dcb8dd7f.png" alt="image.png"></p>
<h4 id="ipvs实际案例"><a href="#ipvs实际案例" class="headerlink" title="ipvs实际案例"></a>ipvs实际案例</h4><p>ipvs负载均衡下一次完整的syn握手抓包。</p>
<p>宿主机上访问 curl clusterip+port 后因为这个ip绑定在kube-ipvs0上，本来是应该发出去的包（prerouting）但是内核认为这个包是访问自己，于是给INPUT链，接着被ipvs放置在INPUT中的DNAT钩子勾住，将dest ip根据负载均衡逻辑改成pod-ip，然后将数据包再发至POSTROUTING链。这时因为目标ip是POD-IP了，根据ip route 选择到出口网卡是tunl0。</p>
<p>可以看下内核中的路由规则：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ip route get 10.68.70.130</span><br><span class="line">local 10.68.70.130 dev lo src 10.68.70.130  //这条规则指示了clusterIP是发给自己的</span><br><span class="line">    cache &lt;local&gt; </span><br><span class="line"># ip route get 172.20.185.217</span><br><span class="line">172.20.185.217 via 172.26.137.117 dev tunl0 src 172.20.22.192  //这条规则指示clusterIP替换成POD IP后发给本地tunl0做ipip封包</span><br></pre></td></tr></table></figure>

<p>于是cip变成了tunl0的IP，这个tunl0是ipip模式，于是将这个包打包成ipip，也就是外层sip、dip都是宿主机ip，再将这个包丢入到物理网络</p>
<p><img src="/images/oss/84bbd3f10de9e7ec2266a82520876c8c.png"></p>
<p>网络收包到达内核后的处理流程如下，核心都是查路由表，出包也会查路由表（判断是否本机内部通信，或者外部通信的话需要选用哪个网卡）</p>
<h4 id="ipvs的一些分析"><a href="#ipvs的一些分析" class="headerlink" title="ipvs的一些分析"></a>ipvs的一些分析</h4><p>ipvs是一个内核态的四层负载均衡，支持NAT以及IPIP隧道模式，但LB和RS不能跨子网，IPIP性能次之，通过ipip隧道解决跨网段传输问题，因此能够支持跨子网。而NAT模式没有限制，这也是唯一一种支持端口映射的模式。</p>
<p>但是ipvs只有NAT（也就是DNAT），NAT也俗称三角模式，要求RS和LVS 在一个二层网络，并且LVS是RS的网关，这样回包一定会到网关，网关再次做SNAT，这样client看到SNAT后的src ip是LVS ip而不是RS-ip。默认实现不支持ful-NAT，所以像公有云厂商为了适应公有云场景基本都会定制实现ful-NAT模式的lvs。</p>
<p>我们不难猜想，由于Kubernetes Service需要使用端口映射功能，因此kube-proxy必然只能使用ipvs的NAT模式。</p>
<p>如下Masq表示MASQUERADE（也就是SNAT），跟iptables里面的 MASQUERADE 是一个意思</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ipvsadm -L -n  |grep 70.130 -A12</span><br><span class="line">TCP  10.68.70.130:12380 rr</span><br><span class="line">  -&gt; 172.20.185.217:9376          Masq    1      0          0</span><br></pre></td></tr></table></figure>

<h2 id="为什么clusterIP不能ping通"><a href="#为什么clusterIP不能ping通" class="headerlink" title="为什么clusterIP不能ping通"></a>为什么clusterIP不能ping通</h2><p><a target="_blank" rel="noopener" href="https://cizixs.com/2017/03/30/kubernetes-introduction-service-and-kube-proxy/">集群内访问cluster ip（不能ping，只能cluster ip+port）就是在到达网卡之前被内核iptalbes做了dnat&#x2F;snat</a>, cluster IP是一个虚拟ip，可以针对具体的服务固定下来，这样服务后面的pod可以随便变化。</p>
<p>iptables模式的svc会ping不通clusterIP，可以看如下iptables和route（留意：–reject-with icmp-port-unreachable）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#ping 10.96.229.40</span><br><span class="line">PING 10.96.229.40 (10.96.229.40) 56(84) bytes of data.</span><br><span class="line">^C</span><br><span class="line">--- 10.96.229.40 ping statistics ---</span><br><span class="line">2 packets transmitted, 0 received, 100% packet loss, time 999ms</span><br><span class="line"></span><br><span class="line">#iptables-save |grep 10.96.229.40</span><br><span class="line">-A KUBE-SERVICES -d 10.96.229.40/32 -p tcp -m comment --comment &quot;***-service:https has no endpoints&quot; -m tcp --dport 8443 -j REJECT --reject-with icmp-port-unreachable</span><br><span class="line"></span><br><span class="line">#ip route get 10.96.229.40</span><br><span class="line">10.96.229.40 via 11.164.219.253 dev eth0  src 11.164.219.119 </span><br><span class="line">    cache </span><br></pre></td></tr></table></figure>

<p>如果用ipvs实现的clusterIP是可以ping通的：</p>
<ul>
<li>如果用iptables 来做转发是ping不通的，因为iptables里面这条规则只处理tcp包，reject了icmp</li>
<li>ipvs实现的clusterIP都能ping通</li>
<li>ipvs下的clusterIP ping通了也不是转发到pod，ipvs负载均衡只转发tcp协议的包</li>
<li>ipvs 的clusterIP在本地配置了route路由到回环网卡，这个包是lo网卡回复的</li>
</ul>
<p>ipvs实现的clusterIP，在本地有添加路由到lo网卡</p>
<p><img src="/images/oss/1f5539eb4c5fa16b2f66f44056d80d7a.png" alt="image.png"></p>
<p>然后在本机抓包（到ipvs后端的pod上抓不到icmp包）：</p>
<p><img src="/images/oss/1caea5b0eb23a47241191d1b5d8c5001.png" alt="image.png"></p>
<p>从上面可以看出显然ipvs只会转发tcp包到后端pod，所以icmp包不会通过ipvs转发到pod上，同时在本地回环网卡lo上抓到了进去的icmp包。因为本地添加了一条路由规则，目标clusterIP被指示发到lo网卡上，lo网卡回复了这个ping包，所以通了。</p>
<h2 id="NodePort-Service"><a href="#NodePort-Service" class="headerlink" title="NodePort Service"></a>NodePort Service</h2><p>这种类型的 Service 也能被宿主机和 pod 访问，但与 ClusterIP 不同的是，<strong>它还能被 集群外的服务访问</strong>。</p>
<ul>
<li>External node IP + port in NodePort range to any endpoint (pod), e.g. 10.0.0.1:31000</li>
<li>Enables access from outside</li>
</ul>
<p>实现上，kube-apiserver 会<strong>从预留的端口范围内分配一个端口给 Service</strong>，然后 <strong>每个宿主机上的 kube-proxy 都会创建以下规则</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">-t nat -A &#123;PREROUTING, OUTPUT&#125; -m conntrack --ctstate NEW -j KUBE-SERVICES</span><br><span class="line"></span><br><span class="line">-A KUBE-SERVICES ! -s 1.1.0.0/16 -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 3.3.3.3/32 -p tcp -m tcp --dport 80 -j KUBE-SVC-NGINX</span><br><span class="line"># 如果前面两条都没匹配到（说明不是 ClusterIP service 流量），并且 dst 是 LOCAL，跳转到 KUBE-NODEPORTS</span><br><span class="line">-A KUBE-SERVICES -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br><span class="line"></span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m tcp --dport 31000 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m tcp --dport 31000 -j KUBE-SVC-NGINX</span><br><span class="line"></span><br><span class="line">-A KUBE-SVC-NGINX -m statistic --mode random --probability 0.50 -j KUBE-SEP-NGINX1</span><br><span class="line">-A KUBE-SVC-NGINX -j KUBE-SEP-NGINX2</span><br></pre></td></tr></table></figure>

<ol>
<li>前面几步和 ClusterIP Service 一样；如果没匹配到 ClusterIP 规则，则跳转到 <code>KUBE-NODEPORTS</code> chain。</li>
<li><code>KUBE-NODEPORTS</code> chain 里做 Service 匹配，但<strong>这次只匹配协议类型和目的端口号</strong>。</li>
<li>匹配成功后，转到对应的 <code>KUBE-SVC-</code> chain，后面的过程跟 ClusterIP 是一样的。</li>
</ol>
<h3 id="NodePort-的一些问题"><a href="#NodePort-的一些问题" class="headerlink" title="NodePort 的一些问题"></a>NodePort 的一些问题</h3><ul>
<li>首先endpoint回复不能走node 1给client，因为会被client reset（如果在node1上将src ip替换成node2的ip可能会路由不通）。回复包在 node1上要snat给node2</li>
<li>经过snat后endpoint没法拿到client ip（slb之类是通过option带过来）</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">          client</span><br><span class="line">            \ ^</span><br><span class="line">             \ \</span><br><span class="line">              v \</span><br><span class="line">  node 1 &lt;--- node 2</span><br><span class="line">   | ^   SNAT</span><br><span class="line">   | |   ---&gt;</span><br><span class="line">   v |</span><br><span class="line">endpoint</span><br></pre></td></tr></table></figure>

<p>可以将 Service 的 spec.externalTrafficPolicy 字段设置为 local，这就保证了所有 Pod 通过 Service 收到请求之后，一定可以看到真正的、外部 client 的源地址。</p>
<p>而这个机制的实现原理也非常简单：这时候，<strong>一台宿主机上的 iptables 规则，会设置为只将 IP 包转发给运行在这台宿主机上的 Pod</strong>。所以这时候，Pod 就可以直接使用源地址将回复包发出，不需要事先进行 SNAT 了。这个流程，如下所示：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">      client</span><br><span class="line">      ^ /   \</span><br><span class="line">     / /     \</span><br><span class="line">    / v       X</span><br><span class="line">  node 1     node 2</span><br><span class="line">   ^ |</span><br><span class="line">   | |</span><br><span class="line">   | v</span><br><span class="line">endpoint</span><br></pre></td></tr></table></figure>

<p>当然，这也就意味着如果在一台宿主机上，没有任何一个被代理的 Pod 存在，比如上图中的 node 2，那么你使用 node 2 的 IP 地址访问这个 Service，就是无效的。此时，你的请求会直接被 DROP 掉。</p>
<h2 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h2><p>在 Kubernetes v1.0 版本，代理完全在 userspace 实现。Kubernetes v1.1 版本新增了 <a target="_blank" rel="noopener" href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#iptables-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F">iptables 代理模式</a>，但并不是默认的运行模式。从 Kubernetes v1.2 起，默认使用 iptables 代理。在 Kubernetes v1.8.0-beta.0 中，添加了 <a target="_blank" rel="noopener" href="https://jimmysong.io/kubernetes-handbook/concepts/service.html#ipvs-%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F">ipvs 代理模式</a></p>
<p>kube-proxy相当于service的管理方，业务流量不会走到kube-proxy，业务流量的负载均衡都是由内核层面的iptables或者ipvs来分发。</p>
<p>kube-proxy的三种模式：</p>
<p><img src="/images/oss/075e2955c5fbd08986bd34afaa5034ba.png" alt="image.png"></p>
<p><strong>一直以来，基于 iptables 的 Service 实现，都是制约 Kubernetes 项目承载更多量级的 Pod 的主要障碍。</strong></p>
<p>ipvs 就是用于解决在大量 Service 时，iptables 规则同步变得不可用的性能问题。与 iptables 比较像的是，ipvs 的实现虽然也基于 netfilter 的钩子函数，但是它却使用哈希表作为底层的数据结构并且工作在内核态，这也就是说 ipvs 在重定向流量和同步代理规则有着更好的性能。</p>
<p>除了能够提升性能之外，ipvs 也提供了多种类型的负载均衡算法，除了最常见的 Round-Robin 之外，还支持最小连接、目标哈希、最小延迟等算法，能够很好地提升负载均衡的效率。</p>
<p>而相比于 iptables，IPVS 在内核中的实现其实也是基于 Netfilter 的 NAT 模式，所以在转发这一层上，理论上 IPVS 并没有显著的性能提升。但是，IPVS 并不需要在宿主机上为每个 Pod 设置 iptables 规则，而是把对这些“规则”的处理放到了内核态，从而极大地降低了维护这些规则的代价。这也正印证了我在前面提到过的，“将重要操作放入内核态”是提高性能的重要手段。</p>
<p><strong>IPVS 模块只负责上述的负载均衡和代理功能。而一个完整的 Service 流程正常工作所需要的包过滤、SNAT 等操作，还是要靠 iptables 来实现。只不过，这些辅助性的 iptables 规则数量有限，也不会随着 Pod 数量的增加而增加。</strong></p>
<p>ipvs 和 iptables 都是基于 Netfilter 实现的。</p>
<p>Kubernetes 中已经使用 ipvs 作为 kube-proxy 的默认代理模式。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/opt/kube/bin/kube-proxy --bind-address=172.26.137.117 --cluster-cidr=172.20.0.0/16 --hostname-override=172.26.137.117 --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig --logtostderr=true --proxy-mode=ipvs</span><br></pre></td></tr></table></figure>

<p><img src="/images/oss/c44c8b3fbb1b2e0910872a6aecef790c.png" alt="image.png"></p>
<h2 id="port-forward"><a href="#port-forward" class="headerlink" title="port-forward"></a>port-forward</h2><p>port-forward后外部也能够像nodePort一样访问到，但是port-forward不适合大流量，一般用于管理端口，启动的时候port-forward会固定转发到一个具体的Pod上，也没有负载均衡的能力。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#在本机监听1080端口，并转发给后端的svc/nginx-ren(总是给发给svc中的一个pod)</span><br><span class="line">kubectl port-forward --address 0.0.0.0 svc/nginx-ren 1080:80</span><br></pre></td></tr></table></figure>

<p><code>kubectl</code> looks up a Pod from the service information provided on the command line and forwards directly to a Pod rather than forwarding to the ClusterIP&#x2F;Service port and allowing the cluster to load balance the service like regular service traffic.</p>
<p>The <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L225">portforward.go <code>Complete</code> function</a> is where <code>kubectl portforward</code> does the first look up for a pod from options via <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L254"><code>AttachablePodForObjectFn</code></a>:</p>
<p>The <code>AttachablePodForObjectFn</code> is defined as <code>attachablePodForObject</code> in <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/interface.go#L39-L40">this interface</a>, then here is the <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go"><code>attachablePodForObject</code> function</a>.</p>
<p>To my (inexperienced) Go eyes, it appears the <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/6d12ae1ac20bee2d3b5fb7a664de76d7fc134a63/pkg/polymorphichelpers/attachablepodforobject.go"><code>attachablePodForObject</code></a> is the thing <code>kubectl</code> uses to look up a Pod to from a Service defined on the command line.</p>
<p>Then from there on everything deals with filling in the Pod specific <a target="_blank" rel="noopener" href="https://github.com/kubernetes/kubectl/blob/c53c16a548eb34f54f673efee2b9b09c52ec15b5/pkg/cmd/portforward/portforward.go#L46-L58"><code>PortForwardOptions</code></a> (which doesn’t include a service) and is passed to the kubernetes API.</p>
<h2 id="Service-和-DNS-的关系"><a href="#Service-和-DNS-的关系" class="headerlink" title="Service 和 DNS 的关系"></a>Service 和 DNS 的关系</h2><p>Service 和 Pod 都会被分配对应的 DNS A 记录（从域名解析 IP 的记录）。</p>
<p>对于 ClusterIP 模式的 Service 来说（比如我们上面的例子），它的 A 记录的格式是：..svc.cluster.local。当你访问这条 A 记录的时候，它解析到的就是该 Service 的 VIP 地址。</p>
<p>而对于指定了 clusterIP&#x3D;None 的 Headless Service 来说，它的 A 记录的格式也是：..svc.cluster.local。但是，当你访问这条 A 记录的时候，它返回的是所有被代理的 Pod 的 IP 地址的集合。当然，如果你的客户端没办法解析这个集合的话，它可能会只会拿到第一个 Pod 的 IP 地址。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">#kubectl get pod -l app=mysql-r -o wide</span><br><span class="line">NAME        READY   STATUS    RESTARTS IP               NODE          </span><br><span class="line">mysql-r-0   2/2     Running   0        172.20.120.143   172.26.137.118</span><br><span class="line">mysql-r-1   2/2     Running   4        172.20.248.143   172.26.137.116</span><br><span class="line">mysql-r-2   2/2     Running   0        172.20.185.209   172.26.137.117</span><br><span class="line"></span><br><span class="line">/ # nslookup mysql-r-1.mysql-r</span><br><span class="line">Server:    10.68.0.2</span><br><span class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      mysql-r-1.mysql-r</span><br><span class="line">Address 1: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</span><br><span class="line">/ # </span><br><span class="line">/ # nslookup mysql-r-2.mysql-r</span><br><span class="line">Server:    10.68.0.2</span><br><span class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      mysql-r-2.mysql-r</span><br><span class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">#如果service是headless(也就是明确指定了 clusterIP: None)</span><br><span class="line">/ # nslookup mysql-r</span><br><span class="line">Server:    10.68.0.2</span><br><span class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      mysql-r</span><br><span class="line">Address 1: 172.20.185.209 mysql-r-2.mysql-r.default.svc.cluster.local</span><br><span class="line">Address 2: 172.20.248.143 mysql-r-1.mysql-r.default.svc.cluster.local</span><br><span class="line">Address 3: 172.20.120.143 mysql-r-0.mysql-r.default.svc.cluster.local</span><br><span class="line"></span><br><span class="line">#如果service 没有指定 clusterIP: None，也就是会分配一个clusterIP给集群</span><br><span class="line">/ # nslookup mysql-r</span><br><span class="line">Server:    10.68.0.2</span><br><span class="line">Address 1: 10.68.0.2 kube-dns.kube-system.svc.cluster.local</span><br><span class="line"></span><br><span class="line">Name:      mysql-r</span><br><span class="line">Address 1: 10.68.90.172 mysql-r.default.svc.cluster.local</span><br></pre></td></tr></table></figure>

<p>不是每个pod都会向DNS注册，只有：</p>
<ul>
<li>StatefulSet中的POD会向dns注册，因为他们要保证顺序行</li>
<li>POD显式指定了hostname和subdomain，说明要靠hostname&#x2F;subdomain来解析</li>
<li>Headless Service代理的POD也会注册</li>
</ul>
<h2 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h2><p> <code>kube-proxy</code> 只能路由 Kubernetes 集群内部的流量，而我们知道 Kubernetes 集群的 Pod 位于 <a target="_blank" rel="noopener" href="https://jimmysong.io/kubernetes-handbook/concepts/cni.html">CNI</a> 创建的外网络中，集群外部是无法直接与其通信的，因此 Kubernetes 中创建了 <a target="_blank" rel="noopener" href="https://jimmysong.io/kubernetes-handbook/concepts/ingress.html">ingress</a> 这个资源对象，它由位于 Kubernetes <a target="_blank" rel="noopener" href="https://jimmysong.io/kubernetes-handbook/practice/edge-node-configuration.html">边缘节点</a>（这样的节点可以是很多个也可以是一组）的 Ingress controller 驱动，负责管理<strong>南北向流量</strong>，Ingress 必须对接各种 Ingress Controller 才能使用，比如 <a target="_blank" rel="noopener" href="https://github.com/kubernetes/ingress-nginx">nginx ingress controller</a>、<a target="_blank" rel="noopener" href="https://traefik.io/">traefik</a>。Ingress 只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 <a target="_blank" rel="noopener" href="https://kubernetes.github.io/ingress-nginx/user-guide/exposing-tcp-udp-services/">nginx ingress controller</a>，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。</p>
<p>Ingress是授权入站连接到达集群服务的规则集合。 你可以给Ingress配置提供外部可访问的URL、负载均衡、SSL、基于名称的虚拟主机等。 用户通过POST Ingress资源到API server的方式来请求ingress。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> internet</span><br><span class="line">     |</span><br><span class="line">[ Ingress ]</span><br><span class="line">--|-----|--</span><br><span class="line">[ Services ]</span><br></pre></td></tr></table></figure>

<p>可以将 Ingress 配置为服务提供外部可访问的 URL、负载均衡流量、终止 SSL&#x2F;TLS，以及提供基于名称的虚拟主机等能力。 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/concepts/services-networking/ingress-controllers">Ingress 控制器</a> 通常负责通过负载均衡器来实现 Ingress，尽管它也可以配置边缘路由器或其他前端来帮助处理流量。</p>
<p>Ingress 不会公开任意端口或协议。 将 HTTP 和 HTTPS 以外的服务公开到 Internet 时，通常使用 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#nodeport">Service.Type&#x3D;NodePort</a> 或 <a target="_blank" rel="noopener" href="https://kubernetes.io/zh/docs/concepts/services-networking/service/#loadbalancer">Service.Type&#x3D;LoadBalancer</a> 类型的服务。</p>
<p>Ingress 其实不是Service的一个类型，但是它可以作用于多个Service，作为集群内部服务的入口。Ingress 能做许多不同的事，比如根据不同的路由，将请求转发到不同的Service上等等。</p>
<p><img src="/images/oss/0e100056910df8cfc45403a05838dd34.png" alt="image.png"></p>
<p> Ingress 对象，其实就是 Kubernetes 项目对“反向代理”的一种抽象。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: extensions/v1beta1</span><br><span class="line">kind: Ingress</span><br><span class="line">metadata:</span><br><span class="line">  name: cafe-ingress</span><br><span class="line">spec:</span><br><span class="line">  tls:</span><br><span class="line">  - hosts:</span><br><span class="line">    - cafe.example.com</span><br><span class="line">    secretName: cafe-secret</span><br><span class="line">  rules:</span><br><span class="line">  - host: cafe.example.com</span><br><span class="line">    http:</span><br><span class="line">      paths:</span><br><span class="line">      - path: /tea              --入口url路径</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: tea-svc  --对应的service</span><br><span class="line">          servicePort: 80</span><br><span class="line">      - path: /coffee</span><br><span class="line">        backend:</span><br><span class="line">          serviceName: coffee-svc</span><br><span class="line">          servicePort: 80</span><br></pre></td></tr></table></figure>

<p>在实际的使用中，你只需要从社区里选择一个具体的 Ingress Controller，把它部署在 Kubernetes 集群里即可。然后，这个 Ingress Controller 会根据你定义的 Ingress 对象，提供对应的代理能力。</p>
<p>目前，业界常用的各种反向代理项目，比如 Nginx、HAProxy、Envoy、Traefik 等，都已经为 Kubernetes 专门维护了对应的 Ingress Controller。</p>
<p>一个 Ingress Controller 可以根据 Ingress 对象和被代理后端 Service 的变化，来自动进行更新的 Nginx 负载均衡器。</p>
<h2 id="eBPF（extended-Berkeley-Packet-Filter）和网络"><a href="#eBPF（extended-Berkeley-Packet-Filter）和网络" class="headerlink" title="eBPF（extended Berkeley Packet Filter）和网络"></a>eBPF（extended Berkeley Packet Filter）和网络</h2><p>eBPF允许程序<strong>对内核本身进行编程</strong>（即 通过程序动态修改内核的行为。传统方式要么是<strong>给内核打补丁</strong>，要么是<strong>修改内核源码 重新编译</strong>）。一句话来概括：<strong>编写代码监听内核事件，当事件发生时，BPF 代码就会在内核执行</strong>。</p>
<p>eBPF 最早出现在 3.18 内核中，此后原来的 BPF 就被称为 <strong>“经典” BPF</strong>（classic BPF, cBPF），cBPF 现在基本已经废弃了。很多人知道 cBPF 是因为它是 <code>tcpdump</code> 的包过滤语言。<strong>现在，Linux 内核只运行 eBPF，内核会将加载的 cBPF 字节码 透明地转换成 eBPF 再执行</strong>。如无特殊说明，本文中所说的 BPF 都是泛指 BPF 技术。</p>
<p>2015年<strong>eBPF 添加了一个新 fast path：XDP</strong>，XDP 是 eXpress DataPath 的缩写，支持在网卡驱动中运行 eBPF 代码（在软件中最早可以处理包的位置），而无需将包送 到复杂的协议栈进行处理，因此处理代价很小，速度极快。</p>
<p>BPF 当时用于 tcpdump，在内核中尽量前面的位置抓包，它不会 crash 内核；</p>
<p>bcc 是 tracing frontend for eBPF。</p>
<p>内核添加了一个新 socket 类型 AF_XDP。它提供的能力是：在零拷贝（ zero-copy）的前提下将包从网卡驱动送到用户空间。</p>
<p>AF_XDP 提供的能力与 DPDK 有点类似，不过：</p>
<ul>
<li>DPDK 需要重写网卡驱动，需要额外维护用户空间的驱动代码。</li>
<li>AF_XDP 在复用内核网卡驱动的情况下，能达到与 DPDK 一样的性能。</li>
</ul>
<p>而且由于复用了内核基础设施，所有的网络管理工具还都是可以用的，因此非常方便， 而 DPDK 这种 bypass 内核的方案导致绝大大部分现有工具都用不了了。</p>
<p>由于所有这些操作都是发生在 XDP 层的，因此它称为 AF_XDP。插入到这里的 BPF 代码 能直接将包送到 socket。</p>
<p>Facebook 公布了生产环境 XDP+eBPF 使用案例（DDoS &amp; LB）</p>
<ul>
<li>用 XDP&#x2F;eBPF 重写了原来基于 IPVS 的 L4LB，性能 10x。</li>
<li>eBPF 经受住了严苛的考验：从 2017 开始，每个进入 facebook.com 的包，都是经过了 XDP &amp; eBPF 处理的。</li>
</ul>
<p><strong>Cilium 1.6 发布</strong> 第一次支持完全干掉基于 iptables 的 kube-proxy，全部功能基于 eBPF。Cilium 1.8 支持基于 XDP 的 Service 负载均衡和 host network policies。</p>
<p>传统的 kube-proxy 处理 Kubernetes Service 时，包在内核中的 转发路径是怎样的？如下图所示：</p>
<p><img src="/images/oss/67851ecb88fca18b9745dae4948947a5.png" alt="image.png"></p>
<p>步骤：</p>
<ol>
<li>网卡收到一个包（通过 DMA 放到 ring-buffer）。</li>
<li>包经过 XDP hook 点。</li>
<li>内核给包分配内存，此时才有了大家熟悉的 skb（包的内核结构体表示），然后 送到内核协议栈。</li>
<li>包经过 GRO 处理，对分片包进行重组。</li>
<li>包进入 tc（traffic control）的 ingress hook。接下来，所有橙色的框都是 Netfilter 处理点。</li>
<li>Netfilter：在 PREROUTING hook 点处理 raw table 里的 iptables 规则。</li>
<li>包经过内核的连接跟踪（conntrack）模块。</li>
<li>Netfilter：在 PREROUTING hook 点处理 mangle table 的 iptables 规则。</li>
<li>Netfilter：在 PREROUTING hook 点处理 nat table 的 iptables 规则。</li>
<li>进行路由判断（FIB：Forwarding Information Base，路由条目的内核表示，译者注） 。接下来又是四个 Netfilter 处理点。</li>
<li>Netfilter：在 FORWARD hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 FORWARD hook 点处理 filter table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 mangle table 里的iptables 规则。</li>
<li>Netfilter：在 POSTROUTING hook 点处理 nat table 里的iptables 规则。</li>
<li>包到达 TC egress hook 点，会进行出方向（egress）的判断，例如判断这个包是到本 地设备，还是到主机外。</li>
<li>对大包进行分片。根据 step 15 判断的结果，这个包接下来可能会：发送到一个本机 veth 设备，或者一个本机 service endpoint， 或者，如果目的 IP 是主机外，就通过网卡发出去。</li>
</ol>
<h3 id="Cilium-如何处理POD之间的流量（东西向流量）"><a href="#Cilium-如何处理POD之间的流量（东西向流量）" class="headerlink" title="Cilium 如何处理POD之间的流量（东西向流量）"></a>Cilium 如何处理POD之间的流量（东西向流量）</h3><p><img src="/images/oss/f6efb2e51abbd2c88a099ee9dc942d37.png" alt="image.png"></p>
<p>如上图所示，Socket 层的 BPF 程序主要处理 Cilium 节点的东西向流量（E-W）。</p>
<ul>
<li>将 Service 的 IP:Port 映射到具体的 backend pods，并做负载均衡。</li>
<li>当应用发起 connect、sendmsg、recvmsg 等请求（系统调用）时，拦截这些请求， 并根据请求的IP:Port 映射到后端 pod，直接发送过去。反向进行相反的变换。</li>
</ul>
<p>这里实现的好处：性能更高。</p>
<ul>
<li>不需要包级别（packet leve）的地址转换（NAT）。在系统调用时，还没有创建包，因此性能更高。</li>
<li>省去了 kube-proxy 路径中的很多中间节点（intermediate node hops） 可以看出，应用对这种拦截和重定向是无感知的（符合 Kubernetes Service 的设计）。</li>
</ul>
<h3 id="Cilium处理外部流量（南北向流量）"><a href="#Cilium处理外部流量（南北向流量）" class="headerlink" title="Cilium处理外部流量（南北向流量）"></a>Cilium处理外部流量（南北向流量）</h3><p><img src="/images/oss/e013d356145d1be6d6a69e2f1b32bdc8.png" alt="image.png"></p>
<p>集群外来的流量到达 node 时，由 XDP 和 tc 层的 BPF 程序进行处理， 它们做的事情与 socket 层的差不多，将 Service 的 IP:Port 映射到后端的 PodIP:Port，如果 backend pod 不在本 node，就通过网络再发出去。发出去的流程我们 在前面 Cilium eBPF 包转发路径 讲过了。</p>
<p>这里 BPF 做的事情：执行 DNAT。这个功能可以在 XDP 层做，也可以在 TC 层做，但 在XDP 层代价更小，性能也更高。</p>
<p>总结起来，Cilium的核心理念就是：</p>
<ul>
<li>将东西向流量放在离 socket 层尽量近的地方做。</li>
<li>将南北向流量放在离驱动（XDP 和 tc）层尽量近的地方做。</li>
</ul>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>测试环境：两台物理节点，一个发包，一个收包，收到的包做 Service loadbalancing 转发给后端 Pods。</p>
<p><img src="/images/oss/1b69dfd206a91dc4007781163fd55f41.png" alt="image.png"></p>
<p>可以看出：</p>
<ul>
<li>Cilium XDP eBPF 模式能处理接收到的全部 10Mpps（packets per second）。</li>
<li>Cilium tc eBPF 模式能处理 3.5Mpps。</li>
<li>kube-proxy iptables 只能处理 2.3Mpps，因为它的 hook 点在收发包路径上更后面的位置。</li>
<li>kube-proxy ipvs 模式这里表现更差，它相比 iptables 的优势要在 backend 数量很多的时候才能体现出来。</li>
</ul>
<p>cpu：</p>
<ul>
<li>XDP 性能最好，是因为 XDP BPF 在驱动层执行，不需要将包 push 到内核协议栈。</li>
<li>kube-proxy 不管是 iptables 还是 ipvs 模式，都在处理软中断（softirq）上消耗了大量 CPU。</li>
</ul>
<h2 id="利用-ebpf-sockmap-redirection-提升-socket-性能"><a href="#利用-ebpf-sockmap-redirection-提升-socket-性能" class="headerlink" title="利用 ebpf sockmap&#x2F;redirection 提升 socket 性能"></a><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/socket-acceleration-with-ebpf-zh/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io#11-bpf-%E5%9F%BA%E7%A1%80">利用 ebpf sockmap&#x2F;redirection 提升 socket 性能</a></h2><p>通过bpf监听socket来拦截所有sendmsg事件，如果是发送到本地另一个socket那么bpf就绕过TCP&#x2F;IP协议栈，直接将msg送给对方socket。依赖用cgroups来指定监听的sockets事件</p>
<p><img src="/images/951413iMgBlog/sock-redir.png" alt="img"></p>
<p>实现这个功能依赖两个东西：</p>
<ol>
<li><p>sockmap：这是一个存储 socket 信息的映射表。作用：</p>
<ul>
<li>一段 BPF 程序<strong>监听所有的内核 socket 事件</strong>，并将新建的 socket 记录到这个 map；</li>
<li>另一段 BPF 程序<strong>拦截所有 <code>sendmsg</code> 系统调用</strong>，然后去 map 里查找 socket 对端，之后 调用 BPF 函数绕过 TCP&#x2F;IP 协议栈，直接将数据发送到对端的 socket queue。</li>
</ul>
</li>
<li><p>cgroups：绑定cgroup下的进程，从而将这些进程下的所有 socket 加入到sockmap。</p>
</li>
</ol>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://imroc.cc/blog/2019/08/12/troubleshooting-with-kubernetes-network">https://imroc.cc/blog/2019/08/12/troubleshooting-with-kubernetes-network</a> Kubernetes 网络疑难杂症排查方法</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36183935/article/details/90734936">https://blog.csdn.net/qq_36183935/article/details/90734936</a>  kube-proxy ipvs模式详解</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/ebpf-and-k8s-zh/">http://arthurchiao.art/blog/ebpf-and-k8s-zh/</a>  大规模微服务利器：eBPF 与 Kubernetes</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/">http://arthurchiao.art/blog/cilium-life-of-a-packet-pod-to-service-zh/</a>  Life of a Packet in Cilium：实地探索 Pod-to-Service 转发路径及 BPF 处理逻辑</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/">http://arthurchiao.art/blog/understanding-ebpf-datapath-in-cilium-zh/</a>  深入理解 Cilium 的 eBPF 收发包路径（datapath）（KubeCon, 2019）</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/socket-acceleration-with-ebpf-zh/?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io#11-bpf-%E5%9F%BA%E7%A1%80">利用 ebpf sockmap&#x2F;redirection 提升 socket 性能</a> </p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/cilium-scale-k8s-service-with-bpf-zh/">利用 eBPF 支撑大规模 K8s Service (LPC, 2019)</a></p>
<p><a target="_blank" rel="noopener" href="https://jiayu0x.com/2014/12/02/iptables-essential-summary/">https://jiayu0x.com/2014/12/02/iptables-essential-summary/</a></p>
<p><a target="_blank" rel="noopener" href="https://k8s.imroc.io/">imroc 电子书</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/09/16/RT%E9%83%BD%E5%8E%BB%E5%93%AA%E4%BA%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/09/16/RT%E9%83%BD%E5%8E%BB%E5%93%AA%E4%BA%86/" class="post-title-link" itemprop="url">delay ack拉高实际rt的case</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-09-16 17:30:03" itemprop="dateCreated datePublished" datetime="2020-09-16T17:30:03+08:00">2020-09-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/performance/" itemprop="url" rel="index"><span itemprop="name">performance</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="delay-ack拉高实际rt的case"><a href="#delay-ack拉高实际rt的case" class="headerlink" title="delay ack拉高实际rt的case"></a>delay ack拉高实际rt的case</h2><h2 id="案例描述"><a href="#案例描述" class="headerlink" title="案例描述"></a>案例描述</h2><blockquote>
<p>开发人员发现client到server的rtt是2.5ms，每个请求1ms server就能处理完毕，但是监控发现的rt不是3.5（1+2.5），而是6ms，想知道这个6ms怎么来的？</p>
</blockquote>
<p>如下业务监控图：实际处理时间（逻辑服务时间1ms，rtt2.4ms，加起来3.5ms），但是系统监控到的rt（蓝线）是6ms，如果一个请求分很多响应包串行发给client，这个6ms是正常的（1+2.4*N），但实际上如果send buffer足够的话，按我们前面的理解多个响应包会并发发出去，所以如果整个rt是3.5ms才是正常的。</p>
<p><img src="/images/oss/d56f87a19a10b0ac9a3b7009641247a0.png" alt="image.png"></p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>抓包来分析原因：</p>
<p><img src="/images/oss/d5e2e358dd1a24e104f54815c84875c9.png" alt="image.png"></p>
<p>实际看到大量的response都是3.5ms左右，符合我们的预期，但是有少量rt被delay ack严重影响了</p>
<p>从下图也可以看到有很多rtt超过3ms的，这些超长时间的rtt会最终影响到整个服务rt</p>
<p><img src="/images/oss/48eae3dcd7c78a68b0afd5c66f783f23.png" alt="image.png"></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/08/31/kubernetes%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/08/31/kubernetes%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">kubernetes容器网络</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-08-31 11:30:03" itemprop="dateCreated datePublished" datetime="2020-08-31T11:30:03+08:00">2020-08-31</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="kubernetes-容器网络"><a href="#kubernetes-容器网络" class="headerlink" title="kubernetes 容器网络"></a>kubernetes 容器网络</h1><h2 id="cni-网络"><a href="#cni-网络" class="headerlink" title="cni 网络"></a>cni 网络</h2><blockquote>
<p> <strong>cni0</strong> is a Linux network bridge device, all <strong>veth</strong> devices will connect to this bridge, so all Pods on the same node can communicate with each other, as explained in <strong>Kubernetes Network Model</strong> and the hotel analogy above.</p>
</blockquote>
<h3 id="cni（Container-Network-Interface）"><a href="#cni（Container-Network-Interface）" class="headerlink" title="cni（Container Network Interface）"></a>cni（Container Network Interface）</h3><p>CNI 全称为 Container Network Interface，是用来定义容器网络的一个 <a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni/blob/master/SPEC.md">规范</a>。<a target="_blank" rel="noopener" href="https://github.com/containernetworking/cni">containernetworking&#x2F;cni</a> 是一个 CNCF 的 CNI 实现项目，包括基本额 bridge,macvlan等基本网络插件。</p>
<p>一般将cni各种网络插件的可执行文件二进制放到 <code>/opt/cni/bin</code> ，在 <code>/etc/cni/net.d/</code> 下创建配置文件，剩下的就交给 K8s 或者 containerd 了，我们不关心也不了解其实现。</p>
<p>比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#ls -lh /opt/cni/bin/</span><br><span class="line">总用量 90M</span><br><span class="line">-rwxr-x--- 1 root root 4.0M 12月 23 09:39 bandwidth</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico</span><br><span class="line">-rwxr-x--- 1 root root  35M 12月 23 09:39 calico-ipam</span><br><span class="line">-rwxr-x--- 1 root root 3.0M 12月 23 09:39 flannel</span><br><span class="line">-rwxr-x--- 1 root root 3.5M 12月 23 09:39 host-local</span><br><span class="line">-rwxr-x--- 1 root root 3.1M 12月 23 09:39 loopback</span><br><span class="line">-rwxr-x--- 1 root root 3.8M 12月 23 09:39 portmap</span><br><span class="line">-rwxr-x--- 1 root root 3.3M 12月 23 09:39 tuning</span><br><span class="line"></span><br><span class="line">[root@hygon3 15:55 /root]</span><br><span class="line">#ls -lh /etc/cni/net.d/</span><br><span class="line">总用量 12K</span><br><span class="line">-rw-r--r-- 1 root root  607 12月 23 09:39 10-calico.conflist</span><br><span class="line">-rw-r----- 1 root root  292 12月 23 09:47 10-flannel.conflist</span><br><span class="line">-rw------- 1 root root 2.6K 12月 23 09:39 calico-kubeconfig</span><br></pre></td></tr></table></figure>

<p>CNI 插件都是直接通过 exec 的方式调用，而不是通过 socket 这样 C&#x2F;S 方式，所有参数都是通过环境变量、标准输入输出来实现的。</p>
<p>Step-by-step communication from <strong>Pod 1</strong> to <strong>Pod 6</strong>:</p>
<ol>
<li><em>Package leaves</em> *<strong>Pod 1 netns*</strong> <em>through the</em> *<strong>eth1*</strong> <em>interface and reaches the</em> <em><strong>root netns*</strong> <em>through the virtual interface</em> <em><strong>veth1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>veth1*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> <em><strong>cni0*</strong> <em>and is redirected to</em> <em><strong>eth0*</strong></em>;</em></li>
<li><em>Package leaves</em> *<strong>eth0*</strong> <em>from</em> <em><strong>Master 1*</strong> <em>and reaches the</em> <em><strong>gateway*</strong></em>;</em></li>
<li><em>Package leaves the</em> *<strong>gateway*</strong> <em>and reaches the</em> *<strong>root netns*</strong> <em>through the</em> <em><strong>eth0*</strong> <em>interface on</em> <em><strong>Worker 1*</strong></em>;</em></li>
<li><em>Package leaves</em> <em><strong>eth0*</strong> <em>and reaches</em> <em><strong>cni0*</strong></em>, looking for</em> <em><strong>Pod 6*</strong></em>’s* <em>address;</em></li>
<li><em>Package leaves</em> *<strong>cni0*</strong> <em>and is redirected to the</em> *<strong>veth6*</strong> <em>virtual interface;</em></li>
<li><em>Package leaves the</em> *<strong>root netns*</strong> <em>through</em> *<strong>veth6*</strong> <em>and reaches the</em> *<strong>Pod 6 netns*</strong> <em>though the</em> *<strong>eth6*</strong> <em>interface;</em></li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115124747936.png" alt="image-20220115124747936"></p>
<h2 id="flannel-网络"><a href="#flannel-网络" class="headerlink" title="flannel 网络"></a>flannel 网络</h2><p>假如POD1访问POD4：</p>
<ol>
<li>从POD1中出来的包先到Bridge cni0上（因为POD1对应的veth挂在了cni0上）</li>
<li>然后进入到宿主机网络，宿主机有路由 10.244.2.0&#x2F;24 via 10.244.2.0 dev flannel.1 onlink ，也就是目标ip 10.244.2.3的包交由 flannel.1 来处理</li>
<li>flanneld 进程将包封装成vxlan 丢到eth0从宿主机1离开（封装后的目标ip是192.168.2.91）</li>
<li>这个封装后的vxlan udp包正确路由到宿主机2</li>
<li>然后经由 flanneld 解包成 10.244.2.3 ，命中宿主机2上的路由：10.244.2.0&#x2F;24 dev cni0 proto kernel scope link src 10.244.2.1 ，交给cni0（<strong>这里会过宿主机iptables</strong>）</li>
<li>cni0将包送给POD4</li>
</ol>
<p><img src="/images/951413iMgBlog/image-20220115132938290.png" alt="image-20220115132938290"></p>
<p>对应宿主机查询到的ip、路由信息（和上图不是对应的）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">#ip -d -4 addr show cni0</span><br><span class="line">475: cni0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default qlen 1000</span><br><span class="line">    link/ether 8e:34:ba:e2:a4:c6 brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    bridge forward_delay 1500 hello_time 200 max_age 2000 ageing_time 30000 stp_state 0 priority 32768 vlan_filtering 0 vlan_protocol 802.1Q bridge_id 8000.8e:34:ba:e2:a4:c6 designated_root 8000.8e:34:ba:e2:a4:c6 root_port 0 root_path_cost 0 topology_change 0 topology_change_detected 0 hello_timer    0.00 tcn_timer    0.00 topology_change_timer    0.00 gc_timer  161.46 vlan_default_pvid 1 vlan_stats_enabled 0 group_fwd_mask 0 group_address 01:80:c2:00:00:00 mcast_snooping 1 mcast_router 1 mcast_query_use_ifaddr 0 mcast_querier 0 mcast_hash_elasticity 4 mcast_hash_max 512 mcast_last_member_count 2 mcast_startup_query_count 2 mcast_last_member_interval 100 mcast_membership_interval 26000 mcast_querier_interval 25500 mcast_query_interval 12500 mcast_query_response_interval 1000 mcast_startup_query_interval 3124 mcast_stats_enabled 0 mcast_igmp_version 2 mcast_mld_version 1 nf_call_iptables 0 nf_call_ip6tables 0 nf_call_arptables 0 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.1/24 brd 192.168.3.255 scope global cni0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line">#ip -d -4 addr show flannel.1</span><br><span class="line">474: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default</span><br><span class="line">    link/ether fe:49:64:ae:36:af brd ff:ff:ff:ff:ff:ff promiscuity 0 minmtu 68 maxmtu 65535</span><br><span class="line">    vxlan id 1 local 10.133.2.252 dev bond0 srcport 0 0 dstport 8472 nolearning ttl auto ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535</span><br><span class="line">    inet 192.168.3.0/32 brd 192.168.3.0 scope global flannel.1</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">       </span><br><span class="line">[root@hygon239 20:06 /root]</span><br><span class="line">#kubectl describe node hygon252 | grep -C5 -i vtep  //可以看到VetpMAC 以及对应的宿主机IP（vxlan封包后的IP）</span><br><span class="line">Labels:             beta.kubernetes.io/arch=amd64</span><br><span class="line">                    beta.kubernetes.io/os=linux</span><br><span class="line">                    kubernetes.io/arch=amd64</span><br><span class="line">                    kubernetes.io/hostname=hygon252</span><br><span class="line">                    kubernetes.io/os=linux</span><br><span class="line">Annotations:        flannel.alpha.coreos.com/backend-data: &#123;&quot;VNI&quot;:1,&quot;VtepMAC&quot;:&quot;fe:49:64:ae:36:af&quot;&#125;</span><br><span class="line">                    flannel.alpha.coreos.com/backend-type: vxlan</span><br><span class="line">                    flannel.alpha.coreos.com/kube-subnet-manager: true</span><br><span class="line">                    flannel.alpha.coreos.com/public-ip: 10.133.2.252</span><br><span class="line">                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock</span><br><span class="line">                    node.alpha.kubernetes.io/ttl: 0       </span><br></pre></td></tr></table></figure>

<p><img src="/images/951413iMgBlog/image-20220115133500854.png" alt="image-20220115133500854"></p>
<h2 id="kubernetes-calico-网络"><a href="#kubernetes-calico-网络" class="headerlink" title="kubernetes calico 网络"></a>kubernetes calico 网络</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml</span><br><span class="line"></span><br><span class="line">#或者老版本的calico</span><br><span class="line">curl https://docs.projectcalico.org/v3.15/manifests/calico.yaml -o calico.yaml</span><br></pre></td></tr></table></figure>

<p>默认calico用的是ipip封包（这个性能跟原生网络差多少有待验证，本质也是overlay网络，比flannel那种要好很多吗？）</p>
<p>跨宿主机的两个容器之间的流量链路是：</p>
<blockquote>
<p>cali-容器eth0-&gt;宿主机cali27dce37c0e8-&gt;tunl0-&gt;内核ipip模块封包-&gt;物理网卡（ipip封包后）—远程–&gt; 物理网卡-&gt;内核ipip模块解包-&gt;tunl0-&gt;cali-容器</p>
</blockquote>
<p><img src="/images/oss/a1767a5f2cbc2c48c1a35da9f3232a2c.png" alt="image.png"></p>
<p>Calico IPIP模式对物理网络无侵入，符合云原生容器网络要求；使用IPIP封包，性能略低于Calico BGP模式；无法使用传统防火墙管理、也无法和存量网络直接打通。Pod在Node做SNAT访问外部，Pod流量不易被监控。</p>
<h2 id="calico-ipip网络不通"><a href="#calico-ipip网络不通" class="headerlink" title="calico ipip网络不通"></a>calico ipip网络不通</h2><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，部分节点之间不通。每台机器部署好calico网络后，会分配一个 &#x2F;26 CIRD 子网（64个ip）。</p>
<h3 id="案例1"><a href="#案例1" class="headerlink" title="案例1"></a>案例1</h3><p>目标机是10.122.127.128（宿主机ip 192.168.3.112），如果从10.122.17.64（宿主机ip 192.168.3.110） ping 10.122.127.128不通，查看10.122.127.128路由表：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 10.122.127.128 dev tunl0  //这条路由不通</span><br><span class="line">[root@az3-k8s-13 ~]# ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br><span class="line"></span><br><span class="line">[root@az3-k8s-13 ~]# ip route |grep tunl0</span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink //这样就通了 </span><br></pre></td></tr></table></figure>

<p>在10.122.127.128抓包如下，明显可以看到icmp request到了 tunl0网卡，tunl0网卡也回复了，但是回复包没有经过kernel ipip模块封装后发到eth1上：</p>
<p><img src="/images/oss/d3111417ce646ca1475def5bea01e6b9.png" alt="image.png"></p>
<p>正常机器应该是这样，上图不正常的时候缺少红框中的reply：</p>
<p><img src="/images/oss/9ea9041af1211b2a5b8de4e216044465.png" alt="image.png"></p>
<p>解决：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip route del 10.122.17.64/26 via 10.122.127.128 dev tunl0 ; </span><br><span class="line">ip route add 10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink</span><br></pre></td></tr></table></figure>

<p>删除错误路由增加新的路由就可以了，新增路由的意思是从tunl0发给10.122.17.64&#x2F;26的包下一跳是 192.168.3.110。</p>
<p> via 192.168.3.110 表示下一跳的ip</p>
<p>onlink参数的作用：<br>使用这个参数将会告诉内核，不必检查网关是否可达。因为在linux内核中，网关与本地的网段不同是被认为不可达的，从而拒绝执行添加路由的操作。</p>
<p>因为tunl0网卡ip的 CIDR 是32，也就是不属于任何子网，那么这个网卡上的路由没有网关，配置路由的话必须是onlink, 内核存也没法根据子网来选择到这块网卡，所以还会加上 dev 指定网卡。</p>
<h3 id="案例2"><a href="#案例2" class="headerlink" title="案例2"></a>案例2</h3><p>集群有五台机器192.168.0.110-114, 同时每个node都有另外一个ip：192.168.3.110-114，只有node2没有192.168.3.111这个ip，结果node2跟其他节点都不通：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#calicoctl node status</span><br><span class="line">Calico process is running.</span><br><span class="line"></span><br><span class="line">IPv4 BGP status</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| PEER ADDRESS  |     PEER TYPE     | STATE |   SINCE    |    INFO     |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br><span class="line">| 192.168.0.111 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.112 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.113 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">| 192.168.3.114 | node-to-node mesh | up    | 2020-08-29 | Established |</span><br><span class="line">+---------------+-------------------+-------+------------+-------------+</span><br></pre></td></tr></table></figure>

<p>从node4 ping node2，然后在node2上抓包，可以看到 icmp request都发到了node2上，但是node2收到后没有发给tunl0：</p>
<p><img src="/images/oss/16fda9322e9a59c37c11629acc611bf3.png" alt="image.png"></p>
<p>所以icmp没有回复，这里的问题在于<strong>kernel收到包后为什么不给tunl0</strong></p>
<p>同样，在node2上ping node4，同时在node2上抓包，可以看到发给node4的request包和reply包：</p>
<p><img src="/images/oss/c6d1706b6f8162cfac528ddf5319c8e2.png" alt="image.png"></p>
<p>从request包可以看到src ip 是0.111， dest ip是 3.113，<strong>因为 node2 没有192.168.3.111这个ip</strong></p>
<p>非常关键的我们看到node4的回复包 src ip 不是3.113，而是0.113（根据node4的路由就应该是0.113）</p>
<p><img src="/images/oss/5c7172e2422579eb99c66e881d47bf99.png" alt="image.png"></p>
<p>这就是问题所在，从node4过来的ipip包src ip都是0.113，实际这里ipip能认识的只是3.113. </p>
<p>如果这个时候在3.113机器上把0.113网卡down掉，那么3.113上的：</p>
<p>10.122.124.128&#x2F;26 via 192.168.0.111 dev tunl0 proto bird onlink 路由被自动删除，3.113将不再回复request。这是因为calico记录的node2的ip是192.168.0.111，所以会自动增加</p>
<p>解决办法，在node4上删除这条路由记录，也就是强制让回复包走3.113网卡，这样收发的ip就能对应上了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ip route del 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.113</span><br><span class="line">//同时将默认路由改到3.113</span><br><span class="line">ip route del default via 192.168.0.253 dev eth0; </span><br><span class="line">ip route add default via 192.168.3.253 dev eth1</span><br></pre></td></tr></table></figure>

<p>最终OK后，node4上的ip route是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@az3-k8s-14 ~]# ip route</span><br><span class="line">default via 192.168.3.253 dev eth1 </span><br><span class="line">10.122.17.64/26 via 192.168.3.110 dev tunl0 proto bird onlink </span><br><span class="line">10.122.124.128/26 via 192.168.0.111 dev tunl0 proto bird onlink </span><br><span class="line">10.122.127.128/26 via 192.168.3.112 dev tunl0 proto bird onlink </span><br><span class="line">blackhole 10.122.157.128/26 proto bird </span><br><span class="line">10.122.157.129 dev cali19f6ea143e3 scope link </span><br><span class="line">10.122.157.130 dev cali09e016ead53 scope link </span><br><span class="line">10.122.157.131 dev cali0ad3225816d scope link </span><br><span class="line">10.122.157.132 dev cali55a5ff1a4aa scope link </span><br><span class="line">10.122.157.133 dev cali01cf8687c65 scope link </span><br><span class="line">10.122.157.134 dev cali65232d7ada6 scope link </span><br><span class="line">10.122.173.128/26 via 192.168.3.114 dev tunl0 proto bird onlink </span><br><span class="line">172.17.0.0/16 dev docker0 proto kernel scope link src 172.17.0.1 </span><br><span class="line">192.168.3.0/24 dev eth1 proto kernel scope link src 192.168.3.113</span><br></pre></td></tr></table></figure>

<p>正常后的抓包, 注意这里drequest的est ip 和reply的 src ip终于一致了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">//request</span><br><span class="line">00:16:3e:02:06:1e &gt; ee:ff:ff:ff:ff:ff, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 57971, offset 0, flags [DF], proto IPIP (4), length 104)</span><br><span class="line">    192.168.0.111 &gt; 192.168.3.110: (tos 0x0, ttl 64, id 18953, offset 0, flags [DF], proto ICMP (1), length 84)</span><br><span class="line">    10.122.124.128 &gt; 10.122.17.64: ICMP echo request, id 22001, seq 4, length 64</span><br><span class="line">    </span><br><span class="line">//reply    </span><br><span class="line">ee:ff:ff:ff:ff:ff &gt; 00:16:3e:02:06:1e, ethertype IPv4 (0x0800), length 118: (tos 0x0, ttl 64, id 2565, offset 0, flags [none], proto IPIP (4), length 104)</span><br><span class="line">    192.168.3.110 &gt; 192.168.0.111: (tos 0x0, ttl 64, id 26374, offset 0, flags [none], proto ICMP (1), length 84)</span><br><span class="line">    10.122.17.64 &gt; 10.122.124.128: ICMP echo reply, id 22001, seq 4, length 64</span><br></pre></td></tr></table></figure>

<p>总结下来这两个案例都还是对路由不够了解，特别是案例2，因为有了多个网卡后导致路由更复杂。calico ipip的基本原理就是利用内核进行ipip封包，然后修改路由来保证网络的畅通。</p>
<h2 id="flannel网络不通"><a href="#flannel网络不通" class="headerlink" title="flannel网络不通"></a>flannel网络不通</h2><h3 id="firewalld"><a href="#firewalld" class="headerlink" title="firewalld"></a>firewalld</h3><p>在麒麟系统的物理机上通过kubeadm setup集群，发现有的环境flannel网络不通，在宿主机上ping 其它物理机flannel.0网卡的ip，通过在对端宿主机抓包发现icmp收到后被防火墙扔掉了，抓包中可以看到错误信息：icmp unreachable - admin prohibited</p>
<p>下图中正常的icmp是直接ping 物理机ip</p>
<p><img src="/images/951413iMgBlog/image-20211228203650921.png" alt="image-20211228203650921"></p>
<blockquote>
<p>The “admin prohibited filter” seen in the tcpdump output means there is a firewall blocking a connection. It does it by sending back an ICMP packet meaning precisely that: the admin of that firewall doesn’t want those packets to get through. It could be a firewall at the destination site. It could be a firewall in between. It could be iptables on the Linux system.</p>
</blockquote>
<p>发现有问题的环境中宿主机的防火墙设置报错了：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &#x27;/usr/sbin/iptables -w10 -t filter -X DOCKER-ISOLATION-STAGE-1&#x27; failed: iptables: No chain/target/match by that name.</span><br><span class="line">12月 28 23:35:08 hygon253 firewalld[10493]: WARNING: COMMAND_FAILED: &#x27;/usr/sbin/iptables -w10 -t filter -F DOCKER-ISOLATION-STAGE-2&#x27; failed: iptables: No chain/target/match by that name.</span><br></pre></td></tr></table></figure>

<p>应该是因为启动docker的时候 firewalld 是运行着的</p>
<blockquote>
<p>Do you have firewalld enabled, and was it (re)started after docker was started? If so, then it’s likely that firewalld wiped docker’s IPTables rules. Restarting the docker daemon should re-create those rules.</p>
</blockquote>
<p>停掉 firewalld 服务可以解决这个问题</p>
<h3 id="掉电重启后flannel网络不通"><a href="#掉电重启后flannel网络不通" class="headerlink" title="掉电重启后flannel网络不通"></a><a target="_blank" rel="noopener" href="https://github.com/flannel-io/flannel/issues/799">掉电重启后flannel网络不通</a></h3><p>flannel能收到包，但是cni0收不到包，说明包进到了目标宿主机，但是从flannel解开udp转送到cni的时候出了问题，大概率是iptables 拦截了包</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">It seems docker version &gt;=1.13 will add iptables rule like below,and it make this issue happen:</span><br><span class="line">iptables -P FORWARD DROP</span><br><span class="line"></span><br><span class="line">All you need to do is add a rule below:</span><br><span class="line">iptables -P FORWARD ACCEPT</span><br></pre></td></tr></table></figure>



<h2 id="清理"><a href="#清理" class="headerlink" title="清理"></a><a target="_blank" rel="noopener" href="https://serverfault.com/questions/247767/cannot-delete-gre-tunnel">清理</a></h2><p>cni信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/cni/net.d/*</span><br><span class="line">/var/lib/cni/ 下存放有ip分配信息</span><br></pre></td></tr></table></figure>

<p>calico创建的tunl0网卡是个tunnel，可以通过 ip tunnel show来查看，清理不掉（重启可以清理掉tunl0）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ip link set dev tunl0 name tunl0_fallback</span><br><span class="line"></span><br><span class="line">或者</span><br><span class="line">/sbin/ip link set eth1 down</span><br><span class="line">/sbin/ip link set eth1 name eth123</span><br><span class="line">/sbin/ip link set eth123 up</span><br></pre></td></tr></table></figure>

<p>flannel</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ip link delete flannel.1</span><br><span class="line">ip link delete cni0</span><br></pre></td></tr></table></figure>



<h2 id="netns"><a href="#netns" class="headerlink" title="netns"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/lscMpc5BWAEzjgYw6H0wBw">netns</a></h2><p>以下case创建一个名为 ren 的netns，然后在里面增加一对虚拟网卡veth1 veth1_p,  veth1放置在ren里面，veth1_p 放在物理机上，给他们配置上ip并up就能通了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">1004  [2021-10-27 10:49:08] ip netns add ren</span><br><span class="line">1005  [2021-10-27 10:49:12] ip netns show</span><br><span class="line">1006  [2021-10-27 10:49:22] ip netns exec ren route   //为空</span><br><span class="line">1007  [2021-10-27 10:49:29] ip netns exec ren iptables -L</span><br><span class="line">1008  [2021-10-27 10:49:55] ip link add veth1 type veth peer name veth1_p //此时宿主机上能看到这两块网卡</span><br><span class="line">1009  [2021-10-27 10:50:07] ip link set veth1 netns ren //将veth1从宿主机默认网络空间挪到ren中，宿主机中看不到veth1了</span><br><span class="line">1010  [2021-10-27 10:50:18] ip netns exec ren route  </span><br><span class="line">1011  [2021-10-27 10:50:25] ip netns exec ren iptables -L</span><br><span class="line">1012  [2021-10-27 10:50:39] ifconfig</span><br><span class="line">1013  [2021-10-27 10:50:51] ip link list</span><br><span class="line">1014  [2021-10-27 10:51:29] ip netns exec ren ip link list</span><br><span class="line">1017  [2021-10-27 10:53:27] ip netns exec ren ip addr add 172.19.0.100/24 dev veth1 </span><br><span class="line">1018  [2021-10-27 10:53:31] ip netns exec ren ip link list</span><br><span class="line">1019  [2021-10-27 10:53:39] ip netns exec ren ifconfig</span><br><span class="line">1020  [2021-10-27 10:53:42] ip netns exec ren ifconfig -a</span><br><span class="line">1021  [2021-10-27 10:54:13] ip netns exec ren ip link set dev veth1 up</span><br><span class="line">1022  [2021-10-27 10:54:16] ip netns exec ren ifconfig</span><br><span class="line">1023  [2021-10-27 10:54:22] ping 172.19.0.100</span><br><span class="line">1024  [2021-10-27 10:54:35] ifconfig -a</span><br><span class="line">1025  [2021-10-27 10:55:03] ip netns exec ren ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line">1026  [2021-10-27 10:55:10] ip addr add 172.19.0.101/24 dev veth1_p</span><br><span class="line">1027  [2021-10-27 10:55:16] ifconfig veth1_p</span><br><span class="line">1028  [2021-10-27 10:55:30] ip link set dev veth1_p up</span><br><span class="line">1029  [2021-10-27 10:55:32] ifconfig veth1_p</span><br><span class="line">1030  [2021-10-27 10:55:38] ping 172.19.0.101</span><br><span class="line">1031  [2021-10-27 10:55:43] ping 172.19.0.100</span><br><span class="line">1032  [2021-10-27 10:55:53] ip link set dev veth1_p down</span><br><span class="line">1033  [2021-10-27 10:55:54] ping 172.19.0.100</span><br><span class="line">1034  [2021-10-27 10:55:58] ping 172.19.0.101</span><br><span class="line">1035  [2021-10-27 10:56:08] ifconfig veth1_p</span><br><span class="line">1036  [2021-10-27 10:56:32] ping 172.19.0.101</span><br><span class="line">1037  [2021-10-27 10:57:04] ip netns exec ren route</span><br><span class="line">1038  [2021-10-27 10:57:52] ip netns exec ren ping 172.19.0.101</span><br><span class="line">1039  [2021-10-27 10:57:58] ip link set dev veth1_p up</span><br><span class="line">1040  [2021-10-27 10:57:59] ip netns exec ren ping 172.19.0.101</span><br><span class="line">1041  [2021-10-27 10:58:06] ip netns exec ren ping 172.19.0.100</span><br><span class="line">1042  [2021-10-27 10:58:14] ip netns exec ren ifconfig</span><br><span class="line">1043  [2021-10-27 10:58:19] ip netns exec ren route</span><br><span class="line">1044  [2021-10-27 10:58:26] ip netns exec ren ping 172.19.0.100 -I veth1</span><br><span class="line">1045  [2021-10-27 10:58:58] ifconfig veth1_p</span><br><span class="line">1046  [2021-10-27 10:59:10] ping 172.19.0.100</span><br><span class="line">1047  [2021-10-27 10:59:26] ip netns exec ren ping 172.19.0.101 -I veth1</span><br><span class="line"></span><br><span class="line">把网卡加入到docker0的bridge下</span><br><span class="line">1160  [2021-10-27 12:17:37] brctl show</span><br><span class="line">1161  [2021-10-27 12:18:05] ip link set dev veth3_p master docker0</span><br><span class="line">1162  [2021-10-27 12:18:09] ip link set dev veth1_p master docker0</span><br><span class="line">1163  [2021-10-27 12:18:13] ip link set dev veth2 master docker0</span><br><span class="line">1164  [2021-10-27 12:18:15] brctl show</span><br><span class="line"></span><br><span class="line">btctl showmacs br0</span><br></pre></td></tr></table></figure>

<p>Linux 上存在一个默认的网络命名空间，Linux 中的 1 号进程初始使用该默认空间。Linux 上其它所有进程都是由 1 号进程派生出来的，在派生 clone 的时候如果没有额外特别指定，所有的进程都将共享这个默认网络空间。</p>
<p>所有的网络设备刚创建出来都是在宿主机默认网络空间下的。可以通过 <code>ip link set 设备名 netns 网络空间名</code> 将设备移动到另外一个空间里去，socket也是归属在某一个网络命名空间下的，由创建socket进程所在的netns来决定socket所在的netns</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/socket.c</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">sock_create</span><span class="params">(<span class="type">int</span> family, <span class="type">int</span> type, <span class="type">int</span> protocol, <span class="keyword">struct</span> socket **res)</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="keyword">return</span> __sock_create(current-&gt;nsproxy-&gt;net_ns, family, type, protocol, res, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: include/net/sock.h</span></span><br><span class="line"><span class="type">static</span> <span class="keyword">inline</span></span><br><span class="line"><span class="type">void</span> <span class="title function_">sock_net_set</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="keyword">struct</span> net *net)</span></span><br><span class="line">&#123;</span><br><span class="line"> write_pnet(&amp;sk-&gt;sk_net, net);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>内核提供了三种操作命名空间的方式，分别是 clone、setns 和 unshare。ip netns add 使用的是 unshare，原理和 clone 是类似的。</p>
<p><img src="/images/951413iMgBlog/640-5304524." alt="Image"></p>
<p>每个 net 下都包含了自己的路由表、iptable 以及内核参数配置等等</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://morven.life/notes/networking-3-ipip/">https://morven.life/notes/networking-3-ipip/</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bakari/p/10564347.html">https://www.cnblogs.com/bakari/p/10564347.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/goldsunshine/p/10701242.html">https://www.cnblogs.com/goldsunshine/p/10701242.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/07/03/MySQL%20JDBC%20StreamResult%20%E5%92%8C%20net_write_timeout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/03/MySQL%20JDBC%20StreamResult%20%E5%92%8C%20net_write_timeout/" class="post-title-link" itemprop="url">MySQL JDBC StreamResult 和 net_write_timeout</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-07-03 17:30:03" itemprop="dateCreated datePublished" datetime="2020-07-03T17:30:03+08:00">2020-07-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="MySQL-JDBC-StreamResult-和-net-write-timeout"><a href="#MySQL-JDBC-StreamResult-和-net-write-timeout" class="headerlink" title="MySQL JDBC StreamResult 和 net_write_timeout"></a>MySQL JDBC StreamResult 和 net_write_timeout</h1><h2 id="MySQL-JDBC-拉取数据的三种方式"><a href="#MySQL-JDBC-拉取数据的三种方式" class="headerlink" title="MySQL JDBC 拉取数据的三种方式"></a>MySQL JDBC 拉取数据的三种方式</h2><p>MySQL JDBC 在从 MySQL 拉取数据的时候有<a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000016724645">三种方式</a>：</p>
<ol>
<li>简单模式，也就是默认模式，数据都先要从MySQL Server发到client的OS TCP buffer，然后JDBC把 OS buffer读取到JVM内存中，读取到JVM内存的过程中憋着不让client读取，全部读完再通知inputStream.read(). 数据大的话容易导致JVM OOM</li>
<li><strong>useCursorFetch&#x3D;true</strong>，配合FetchSize，也就是MySQL Server把查到的数据先缓存到本地磁盘，然后按照FetchSize挨个发给client。这需要占用MySQL很高的IOPS（先写磁盘缓存），其次每次Fetch需要一个RTT，效率不高。</li>
<li>Stream读取，Stream读取是在执行SQL前设置FetchSize：statement.setFetchSize(Integer.MIN_VALUE)，同时确保游标是只读、向前滚动的（为游标的默认值），MySQL JDBC内置的操作方法是将Statement强制转换为：com.mysql.jdbc.StatementImpl，调用其方法：enableStreamingResults()，这2者达到的效果是一致的，都是启动Stream流方式读取数据。这个时候MySQL不停地发数据，inputStream.read()不停地读取。一般来说发数据更快些，很快client的OS TCP recv buffer就满了，这时MySQL停下来等buffer有空闲就继续发数据。等待过程中如果超过 net_write_timeout MySQL就会报错，中断这次查询。</li>
</ol>
<p>从这里的描述来看，数据小的时候第一种方式还能接受，但是数据大了容易OOM，方式三看起来不错，但是要特别注意 net_write_timeout。</p>
<p>1和3对MySQL Server来说处理上没有啥区别，也感知不到这两种方式的不同。只是对1来说从OS Buffer中的数据复制到JVM内存中速度快，JVM攒多了数据内存就容易爆掉；对3来说JDBC一条条将OS Buffer中的数据复制到JVM(内存复制速度快)同时返回给execute挨个处理（慢），一般来说挨个处理要慢一些，这就导致了从OS Buffer中复制数据较慢，容易导致 TCP Receive Buffer满了，那么MySQL Server感知到的就是TCP 传输窗口为0了，导致暂停传输数据。</p>
<p>在数据量很小的时候方式三没什么优势，因为总是多一次set net_write_tiemout，也就是多了一次RTT。</p>
<p><img src="/images/951413iMgBlog/70.png" alt="img"></p>
<h2 id="MySQL-timeout"><a href="#MySQL-timeout" class="headerlink" title="MySQL timeout"></a><a target="_blank" rel="noopener" href="https://www.cubrid.org/blog/3826470">MySQL timeout</a></h2><ol>
<li>Creates a statement by calling <code>Connection.createStatement()</code>.</li>
<li>Calls <code>Statement.executeQuery()</code>.</li>
<li>The statement transmits the Query to MySqlServer by using the internal connection.</li>
<li>The statement creates a new timeout-execution thread for timeout process.</li>
<li>For version 5.1.x, it changes to assign 1 thread for each connection.</li>
<li>Registers the timeout execution to the thread.</li>
<li>Timeout occurs.</li>
<li>The timeout-execution thread creates a connection that has the same configurations as the statement.</li>
<li>Transmits the cancel Query (KILL QUERY “connectionId“) by using the connection.</li>
</ol>
<p><img src="/images/951413iMgBlog/1f6df479e83fd2c14ecac4ee6be64a29.png" alt="Figure 6: QueryTimeout Execution Process for MySQL JDBC Statement (5.0.8)."></p>
<p>参考<a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1162225">《揭秘JDBC超时机制》</a></p>
<h2 id="net-read-timeout"><a href="#net-read-timeout" class="headerlink" title="net_read_timeout"></a>net_read_timeout</h2><table>
<thead>
<tr>
<th align="left">Command-Line Format</th>
<th><code>--net-read-timeout=#</code></th>
</tr>
</thead>
<tbody><tr>
<td align="left">System Variable</td>
<td><code>net_read_timeout</code></td>
</tr>
<tr>
<td align="left">Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td align="left">Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td align="left">Type</td>
<td>Integer</td>
</tr>
<tr>
<td align="left">Default Value</td>
<td><code>30</code></td>
</tr>
<tr>
<td align="left">Minimum Value</td>
<td><code>1</code></td>
</tr>
<tr>
<td align="left">Maximum Value</td>
<td><code>31536000</code></td>
</tr>
<tr>
<td align="left">Unit</td>
<td>seconds</td>
</tr>
</tbody></table>
<p>The number of seconds to wait for more data from a connection before aborting the read. When the server is reading from the client, <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_read_timeout"><code>net_read_timeout</code></a> is the timeout value controlling when to abort.</p>
<p>如下图，MySQL Server监听3017端口，195228号包 客户端发一个SQL 给 MySQL Server，但是似乎这个时候正好网络异常，30秒钟后（从 SQL 请求的前一个 ack 开始算，Server应该一直都没有收到），Server 端触发 net_read_timeout 超时异常（疑问：这里没有 net_read_timeout 描述的读取了一半的现象）</p>
<p><img src="/images/951413iMgBlog/image-20230209155545142.png" alt="image-20230209155545142"></p>
<p>解决方案：建议调大 net_read_timeout 以应对可能出现的网络丢包</p>
<h2 id="net-write-timeout"><a href="#net-write-timeout" class="headerlink" title="net_write_timeout"></a>net_write_timeout</h2><p>show processlist 看到的State的值一直处于**“Sending to client”**，说明SQL这个语句已经执行完毕，而此时由于请求的数据太多，MySQL不停写入net buffer，而net buffer又不停的将数据写入服务端的网络棧，服务器端的网络栈（socket send buffer）被写满了，又没有被客户端读取并消化，这时读数据的流程就被MySQL暂停了。直到客户端完全读取了服务端网络棧的数据，这个状态才会消失。</p>
<p>先看下 <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_write_timeout"><code>net_write_timeout</code></a>的解释：The number of seconds to wait for a block to be written to a connection before aborting the write. 只针对执行查询中的等待超时，网络不好，tcp buffer满了（应用迟迟不读走数据）等容易导致mysql server端报net_write_timeout错误，指的是mysql server hang在那里长时间无法发送查询结果。</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Command-Line Format</td>
<td><code>--net-write-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>net_write_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>60</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
</tbody></table>
<p>报这个错就是RDS等了net_write_timeout这么久没写数据，可能是客户端卡死没有读走数据，也可能是从多个分片挨个拉取，还没开始拉第7片前面6片拉取耗时就超过了net_write_timeout。</p>
<blockquote>
<p><strong>案例</strong>：DRDS 到 MySQL 多个分片拉取数据生成了许多 cursor 并发执行,但拉数据的时候是串行拉取的,如果用户端拉取数据过慢会导致最后一个 cursor 执行完成之后要等待很久.会超过 MySQL 的 net_write_timeout 配置从而引发报错. 也就是最后一个cursor打开后一直没有去读取数据，直到MySQL  Server 触发 net_write_timeout 异常</p>
<p>首先可以尝试在 DRDS jdbcurl 配置 netTimeoutForStreamingResults 参数,设置为 0 可以使其一直等待,或设置一个合理的值(秒).</p>
</blockquote>
<p>从JDBC驱动中可以看到，当调用PreparedStatement的executeQuery() 方法的时候，如果我们是去获取流式resultset的话，就会默认执行SET net_write_timeout&#x3D; ？ 这个命令去重新设置timeout时间。源代码如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">if (doStreaming &amp;&amp; this.connection.getNetTimeoutForStreamingResults() &gt; 0) &#123;  </span><br><span class="line">            java.sql.Statement stmt = null;  </span><br><span class="line">            try &#123;  </span><br><span class="line">                stmt = this.connection.createStatement();                    ((com.mysql.jdbc.StatementImpl)stmt).executeSimpleNonQuery(this.connection, &quot;SET net_write_timeout=&quot;   </span><br><span class="line">                        + this.connection.getNetTimeoutForStreamingResults());  </span><br><span class="line">            &#125; finally &#123;  </span><br><span class="line">                if (stmt != null) &#123;  </span><br><span class="line">                    stmt.close();  </span><br><span class="line">                &#125;  </span><br><span class="line">            &#125;  </span><br><span class="line">        &#125;  </span><br><span class="line">        </span><br><span class="line">//另外DRDS代码 AppLoader.java 中写死了net_write_timeout 8小时</span><br><span class="line">ds.putConnectionProperties(ConnectionProperties.NET_WRITE_TIMEOUT, 28800);</span><br></pre></td></tr></table></figure>

<p>而 this.connection.getNetTimeoutForStreamingResults() 默认是600秒，或者在JDBC连接串种通过属性 netTimeoutForStreamingResults 来指定。</p>
<p>netTimeoutForStreamingResults 默认值：</p>
<p>What value should the driver automatically set the server setting ‘net_write_timeout’ to when the streaming result sets feature is in use? Value has unit of seconds, the value “0” means the driver will not try and adjust this value.</p>
<table>
<thead>
<tr>
<th align="left">Default Value</th>
<th>600</th>
</tr>
</thead>
<tbody><tr>
<td align="left">Since Version</td>
<td>5.1.0</td>
</tr>
</tbody></table>
<p>一般在数据导出场景中容易出现 net_write_timeout 这个错误，比如这个错误堆栈：</p>
<p><img src="/images/oss/8fe715d3ebb6929afecd19aadbe53e5e.png"></p>
<p>或者：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">ErrorMessage:</span><br><span class="line">Communications link failure</span><br><span class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">The last packet successfully received from the server was 7 milliseconds ago.  The last packet sent successfully to the server was 709,806 milliseconds ago.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1036)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3427)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3327)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3814)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:870)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1928)</span><br><span class="line">	at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:378)</span><br><span class="line">	at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:358)</span><br><span class="line">	at com.mysql.jdbc.ResultSetImpl.next(ResultSetImpl.java:6337)</span><br><span class="line">	at com.alibaba.datax.plugin.rdbms.reader.CommonRdbmsReader$Task.startRead(CommonRdbmsReader.java:275)</span><br><span class="line">	at com.alibaba.datax.plugin.reader.drdsreader.DrdsReader$Task.startRead(DrdsReader.java:148)</span><br><span class="line">	at com.alibaba.datax.core.taskgroup.runner.ReaderRunner.run(ReaderRunner.java:62)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:834)</span><br><span class="line">Caused by: java.io.EOFException: Can not read response from server. Expected to read 258 bytes, read 54 bytes before connection was unexpectedly lost.</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2914)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3387)</span><br><span class="line">	... 11 more</span><br></pre></td></tr></table></figure>

<h3 id="特别注意"><a href="#特别注意" class="headerlink" title="特别注意"></a>特别注意</h3><p>JDBC驱动报如下错误</p>
<blockquote>
<p>Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server.  </p>
</blockquote>
<p>不一定是 <code>net_write_timeout</code> 设置过小导致的，JDBC 在 streaming 流模式下只要连接异常就会报如上错误，比如：</p>
<ul>
<li>连接被 TCP reset</li>
<li>连接因为某种原因(比如 QueryTimeOut、比如用户监控kill 慢查询) 触发 kill Query导致连接中断</li>
</ul>
<p><a href="https://plantegg.github.io/2022/10/10/Linux%20BUG%E5%86%85%E6%A0%B8%E5%AF%BC%E8%87%B4%E7%9A%84%20TCP%E8%BF%9E%E6%8E%A5%E5%8D%A1%E6%AD%BB/">比如出现内核bug，内核卡死不发包的话，客户端同样报 net_write_timeout 错误</a></p>
<p><a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_allowed_packet"><code>max_allowed_packet</code></a>: 单个SQL或者单条记录的最大大小</p>
<h2 id="一些其他的-Timeout"><a href="#一些其他的-Timeout" class="headerlink" title="一些其他的 Timeout"></a>一些其他的 Timeout</h2><p>connectTimeout：表示等待和MySQL数据库建立socket链接的超时时间，默认值0，表示不设置超时，单位毫秒，建议30000。 JDBC驱动连接属性</p>
<p>queryTimeout：超时后jdbc驱动触发新建一个连接来发送一个 kill 给DB</p>
<p>socketTimeout：JDBC参数，表示客户端发送请求给MySQL数据库后block在read的等待数据的超时时间，linux系统默认的socketTimeout为30分钟，可以不设置。<strong>socketTimeout 超时不会触发发kill，只会断开tcp连接</strong>。</p>
<p>要特别注意socketTimeout仅仅是指等待socket数据时间，如果在传输数据那么这个值就没有用了。<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/7/docs/api/java/net/SocketOptions.html#SO_TIMEOUT">socketTimeout通过mysql-connector中的NativeProtocol最终设置在socketOptions上</a></p>
<p><img src="/images/951413iMgBlog/image-20211024171459127.png" alt="image-20211024171459127"></p>
<blockquote>
<p>static final int SO_TIMEOUT。 <strong>Set a timeout on blocking Socket operations</strong>:</p>
<p> ServerSocket.accept();<br> SocketInputStream.read();<br> DatagramSocket.receive();</p>
<p>The option must be set prior to entering a blocking operation to take effect. If the timeout expires and the operation would continue to block, <strong>java.io.InterruptedIOException</strong> is raised. The Socket is not closed in this case.</p>
</blockquote>
<p>Statement Timeout：用来限制statement的执行时长，timeout的值通过调用JDBC的java.sql.Statement.setQueryTimeout(int timeout) API进行设置。不过现在开发者已经很少直接在代码中设置，而多是通过框架来进行设置。</p>
<p><a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_execution_time"><code>max_execution_time</code></a>：The execution timeout for <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/select.html"><code>SELECT</code></a> statements, in milliseconds. If the value is 0, timeouts are not enabled.  MySQL 属性，可以set修改，一般用来设置一个查询最长不超过多少秒，避免一个慢查询一直在跑，跟statement timeout对应。</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Command-Line Format</td>
<td><code>--max-execution-time=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>max_execution_time</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>0</code></td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout"><code>wait_timeout</code></a> The number of seconds the server waits for activity on a noninteractive connection before closing it. MySQL 属性，一般设置tcp keepalive后这个值基本不会超时（这句话存疑 202110）。</p>
<p>On thread startup, the session <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout"><code>wait_timeout</code></a> value is initialized from the global <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_wait_timeout"><code>wait_timeout</code></a> value or from the global <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout"><code>interactive_timeout</code></a> value, depending on the type of client (as defined by the <code>CLIENT_INTERACTIVE</code> connect option to <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/mysql-real-connect.html"><code>mysql_real_connect()</code></a>). See also <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_interactive_timeout"><code>interactive_timeout</code></a>.</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody><tr>
<td>Command-Line Format</td>
<td><code>--wait-timeout=#</code></td>
</tr>
<tr>
<td>System Variable</td>
<td><code>wait_timeout</code></td>
</tr>
<tr>
<td>Scope</td>
<td>Global, Session</td>
</tr>
<tr>
<td>Dynamic</td>
<td>Yes</td>
</tr>
<tr>
<td>Type</td>
<td>Integer</td>
</tr>
<tr>
<td>Default Value</td>
<td><code>28800</code></td>
</tr>
<tr>
<td>Minimum Value</td>
<td><code>1</code></td>
</tr>
<tr>
<td>Maximum Value (Other)</td>
<td><code>31536000</code></td>
</tr>
<tr>
<td>Maximum Value (Windows)</td>
<td><code>2147483</code></td>
</tr>
</tbody></table>
<p>一般来说应该设置： max_execution_time&#x2F;statement timeout &lt; Tranction Timeout &lt; socketTimeout</p>
<h3 id="SocketTimeout"><a href="#SocketTimeout" class="headerlink" title="SocketTimeout"></a>SocketTimeout</h3><p><a target="_blank" rel="noopener" href="https://issues.apache.org/jira/browse/HTTPCLIENT-1478">这个 httpclient 的bug</a> 就是在 TCP 连接握手成功(只受ConnectTimeout影响，SocketTimeout还不起作用)后，还需要进行 SSL的数据交换(HandShake)，但因为httpclient是在连接建立后(含 SSL HandShake)才设置的 SocketTimeout，导致在SSL HandShake的时候卡在了读数据，此时恰好还没设置SocketTimeout，导致连接永久卡死在SSL HandShake的读数据</p>
<p>所以代码的fix方案就是在建连接前就设置好 SocketTimeout。</p>
<h2 id="一次-PreparedStatement-执行"><a href="#一次-PreparedStatement-执行" class="headerlink" title="一次 PreparedStatement 执行"></a>一次 PreparedStatement 执行</h2><p>useServerPrepStmts&#x3D;true&amp;cachePrepStmts&#x3D;true</p>
<p>5.0.5版本后的驱动 useServerPrepStmts 默认值是false；<strong>另外跨Statement是没法重用PreparedStatement预编译的</strong>，还需要设置 <a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/32286518/whats-the-difference-between-cacheprepstmts-and-useserverprepstmts-in-mysql-jdb">cachePrepStmts 才可以</a>。</p>
<p>对于打开预编译的URL（String url &#x3D; “jdbc:mysql:&#x2F;&#x2F;localhost:3306&#x2F;studb?useServerPrepStmts&#x3D;true&amp;cachePrepStmts&#x3D;true”）获取数据库连接之后，本质是获取预编译语句 **pstmt &#x3D; conn.prepareStatement(sql)**时会向MySQL服务端发送一个RPC，发送一个预编译的SQL模板（驱动会拼接MySQL预编译语句prepare s1 from ‘select * from user where id &#x3D; ?’），然后MySQL服务端会编译好收到的SQL模板，再会为此预编译模板语句分配一个 <strong>serverStatementId</strong>发送给JDBC驱动，这样以后PreparedStatement就会持有当前预编译语句的服务端的serverStatementId,并且会把此 PreparedStatement缓存在当前数据库连接中，以后对于相同SQL模板的操作 <strong>pstmt.executeUpdate()</strong>，都用相同的PreparedStatement，执行SQL时只需要发送 <strong>serverStatementId</strong> <strong>和参数</strong>，节省一次SQL编译, 直接执行。并且对于每一个连接(驱动端及MySQL服务端)都有自己的prepare cache,具体的源码实现是在com.mysql.jdbc.ServerPreparedStatement中实现。</p>
<p>根据SQL模板和设置的参数，解析成一条完整的SQL语句，最后根据MySQL协议，序列化成字节流，RPC发送给MySQL服务端</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 解析封装需要发送的SQL语句,序列化成MySQL协议对应的字节流</span></span><br><span class="line"><span class="type">Buffer</span> <span class="variable">sendPacket</span> <span class="operator">=</span> fillSendPacket();</span><br></pre></td></tr></table></figure>

<p>准备好需要发送的MySQL协议的字节流（sendPacket）后，就可以一路通过ConnectionImpl.execSQL –&gt; MysqlIO.sqlQueryDirect –&gt; MysqlIO.send – &gt; OutPutStram.write把字节流数据通过Socket发送给MySQL服务器，然后线程阻塞等待服务端返回结果数据，拿到数据后再根据MySQL协议反序列化成我们熟悉的ResultSet对象。</p>
<p><img src="/images/951413iMgBlog/image-20230802101859567.png" alt="image-20230802101859567"></p>
<h2 id="SocketTimeoutException-Read-timed-out"><a href="#SocketTimeoutException-Read-timed-out" class="headerlink" title="SocketTimeoutException: Read timed out"></a>SocketTimeoutException: Read timed out</h2><p>如果SQL 超过 SocketTimeout 报错如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#java -cp .:./mysql-connector-java-5.1.45.jar Test &quot;jdbc:mysql://127.0.0.1:3306/test?useSSL=false&amp;useServerPrepStmts=true&amp;cachePrepStmts=true&amp;connectTimeout=15000&amp;socketTimeout=3700&quot; root 123 &quot;select sleep(10), id from sbtest1 where id= ?&quot; 100</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet successfully received from the server was 3,705 milliseconds ago.  The last packet sent successfully to the server was 3,705 milliseconds ago.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3559)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3459)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3900)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1283)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:783)</span><br><span class="line">	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:1966)</span><br><span class="line">	at Test.main(Test.java:31)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: Read timed out</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3008)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3469)</span><br><span class="line">	... 7 more</span><br></pre></td></tr></table></figure>



<h2 id="连接超时"><a href="#连接超时" class="headerlink" title="连接超时"></a>连接超时</h2><p>在防火墙里设置了 3306 的包都 drop</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#java -cp .:./mysql-connector-java-5.1.45.jar Test &quot;jdbc:mysql://127.0.0.1:3306/test?useSSL=false&amp;useServerPrepStmts=true&amp;cachePrepStmts=true&amp;connectTimeout=15000&amp;socketTimeout=3700&quot; root 123 &quot;select sleep(10), id from sbtest1 where id= ?&quot; 100</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet sent successfully to the server was 0 milliseconds ago. The driver has not received any packets from the server.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:341)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2186)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2219)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2014)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:776)</span><br><span class="line">	at com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:47)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:386)</span><br><span class="line">	at com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:330)</span><br><span class="line">	at java.sql.DriverManager.getConnection(DriverManager.java:664)</span><br><span class="line">	at java.sql.DriverManager.getConnection(DriverManager.java:247)</span><br><span class="line">	at Test.main(Test.java:26)</span><br><span class="line">Caused by: java.net.ConnectException: 连接超时 (Connection timed out)</span><br><span class="line">	at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class="line">	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)</span><br><span class="line">	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</span><br><span class="line">	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)</span><br><span class="line">	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)</span><br><span class="line">	at java.net.Socket.connect(Socket.java:607)</span><br><span class="line">	at com.mysql.jdbc.StandardSocketFactory.connect(StandardSocketFactory.java:211)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.&lt;init&gt;(MysqlIO.java:300)</span><br><span class="line">	... 15 more</span><br></pre></td></tr></table></figure>



<h2 id="SocketException-connection-timed-out"><a href="#SocketException-connection-timed-out" class="headerlink" title="SocketException connection timed out"></a>SocketException connection timed out</h2><p><img src="/images/951413iMgBlog/image-20240522165928366.png" alt="image-20240522165928366"></p>
<p><img src="/images/951413iMgBlog/image-20240522165849942.png" alt="image-20240522165849942"></p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>设置JDBC参数不合理（不设置的话默认值是：queryTimeout&#x3D;10s，socketTimeout&#x3D;10s），会导致在异常情况下，第二条get获得了第一条的结果，拿到了错误的数据，数据库则表现正常</p>
<p>触发：</p>
<p>同事设置了queryTimeout 和socketTimeout，当queryTimeout 先触发，并发送了 kill id 给 Server，此时标志链接是 Cancle 状态，在网络不太好的场景下 Server 没有收到或者收到这个 kill 晚了，这时 socketTimeout 到达触发时间，连接抛CommunicationsException（严重异常，触发后连接应该断开）, 但JDBC会检查请求是否被cancle了，如果Cancle 就会抛出MySQLTimeoutException异常，这是一个普通异常，连接会被重新放回连接池重用（导致下一个获取这个连接的线程可能会得到前一个请求的response）。</p>
<p>解决办法：</p>
<ol>
<li>TDDL：在TDDL JDBC接口层面屏蔽掉用户所有的queryTimeout设置，全部使用socketTimeout进行设置，同时配置好正确的mysql exception sorter(断开超时链接).</li>
<li>JDBC Driver：判断这个query是否被cancel时，同时判断当前 SQLException的errorCode，如果errorCode是ER_QUERY_INTERRUPTED且callingStatement被cancel掉，才会用MySQLTimeoutException覆盖原来的SQLexception。避免了CommunicationsException被覆盖的问题</li>
<li>MYSQL Server：添加了 max_statement_time，来替代queryTimeout，当执行时间超过 max_statement_time 时，Server 直接 kill，不需要客户端发送 kill</li>
</ol>
<p>queryTimeout（queryTimeoutKillsConnection&#x3D;True–来强制关闭连接）会触发启动一个新的连接向server发送 kill id的命令，<strong>MySQL5.7增加了max_statement_time&#x2F;max_execution_time来做到在server上直接检测到这种查询，然后结束掉</strong>。</p>
<h3 id="jdbc-和-RDS之间-socket-timeout"><a href="#jdbc-和-RDS之间-socket-timeout" class="headerlink" title="jdbc 和 RDS之间 socket_timeout"></a>jdbc 和 RDS之间 socket_timeout</h3><p>jdbc驱动设置socketTimeout&#x3D;1459，如果是socketTimeout触发客户端断开后，server端的SQL会继续执行，如果是client被kill则server端的SQL会被终止</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"># java -cp /home/admin/drds-server/lib/*:. Test &quot;jdbc:mysql://172.16.40.215:3008/bank_000000?socketTimeout=1459&quot; &quot;user&quot; &quot;pass&quot; &quot;select sleep(2)&quot; &quot;1&quot;</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet successfully received from the server was 1,461 milliseconds ago.  The last packet sent successfully to the server was 1,461 milliseconds ago.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2811)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2806)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2764)</span><br><span class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1399)</span><br><span class="line">	at Test.main(Test.java:29)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: Read timed out</span><br><span class="line">	at java.net.SocketInputStream.socketRead0(Native Method)</span><br><span class="line">	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:171)</span><br><span class="line">	at java.net.SocketInputStream.read(SocketInputStream.java:141)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 8 more</span><br><span class="line">	</span><br><span class="line">	或者开协程后的错误堆栈</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet successfully received from the server was 1,460 milliseconds ago.  The last packet sent successfully to the server was 1,459 milliseconds ago.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2811)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2806)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2764)</span><br><span class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1399)</span><br><span class="line">	at Test.main(Test.java:29)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: time out</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 8 more</span><br></pre></td></tr></table></figure>

<p>对应抓包，没有 kill动作</p>
<img src="/images/951413iMgBlog/image-20220601141709318.png" alt="image-20220601141709318" style="zoom:50%;" />



<h3 id="CN-和-DN-间socket-timeout案例"><a href="#CN-和-DN-间socket-timeout案例" class="headerlink" title="CN 和 DN 间socket_timeout案例"></a>CN 和 DN 间socket_timeout案例</h3><p>设置CN到DN的socket_timeout为2秒，然后执行一个sleep</p>
<p>CN上抓包分析(stream 5是客户端到CN、stream6是CN到DN）如下，首先CN会计时2秒钟后发送quit给DN，然后断开和DN的连接，并返回一个错误给client，client发送quit断开连接：</p>
<img src="/images/951413iMgBlog/image-20220601122556415.png" alt="image-20220601122556415" style="zoom:50%;" />

<p>CN完整报错堆栈：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br></pre></td><td class="code"><pre><span class="line">2022-06-01 12:10:00.178 [ServerExecutor-bucket-2-19-thread-181] ERROR com.alibaba.druid.pool.DruidPooledStatement - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank] CommunicationsException, druid version 1.1.24, jdbcUrl : jdbc:mysql://172.16.40.215:3008/bank_000000?maintainTimeStats=false&amp;rewriteBatchedStatements=false&amp;failOverReadOnly=false&amp;cacheResultSetMetadata=true&amp;allowMultiQueries=true&amp;clobberStreamingResults=true&amp;autoReconnect=false&amp;usePsMemOptimize=true&amp;useServerPrepStmts=true&amp;netTimeoutForStreamingResults=0&amp;useSSL=false&amp;metadataCacheSize=256&amp;readOnlyPropagatesToServer=false&amp;prepStmtCacheSqlLimit=4096&amp;connectTimeout=5000&amp;socketTimeout=9000000&amp;cachePrepStmts=true&amp;characterEncoding=utf8&amp;prepStmtCacheSize=256, testWhileIdle true, idle millis 11861, minIdle 5, poolingCount 4, timeBetweenEvictionRunsMillis 60000, lastValidIdleMillis 11861, driver com.mysql.jdbc.Driver, exceptionSorter com.alibaba.polardbx.common.jdbc.sorter.MySQLExceptionSorter</span><br><span class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] ERROR com.alibaba.druid.pool.DruidDataSource - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank] discard connection</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</span><br><span class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</span><br><span class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</span><br><span class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</span><br><span class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</span><br><span class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</span><br><span class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</span><br><span class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</span><br><span class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:874)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: time out</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] [1461cdf8b2809000]Execute ERROR on GROUP: BANK_000000_GROUP, ATOM: dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000, MERGE_UNION_SIZE:1, SQL: /*DRDS /10.101.32.6/1461cdf8b2809000/0// */SELECT SLEEP(?) AS `sleep(236)`, PARAM: [236], ERROR: Communications link failure, tddl version: 5.4.13-16522656</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</span><br><span class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</span><br><span class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</span><br><span class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</span><br><span class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</span><br><span class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</span><br><span class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</span><br><span class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</span><br><span class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:874)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: time out</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] Reset conn socketTimeout failed, lastSocketTimeout is 9000000, tddl version: 5.4.13-16522656</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.Util.getInstance(Util.java:408)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:918)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:897)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:886)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:860)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.throwConnectionClosedException(ConnectionImpl.java:1326)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.checkClosed(ConnectionImpl.java:1321)</span><br><span class="line">	at com.mysql.jdbc.ConnectionImpl.setNetworkTimeout(ConnectionImpl.java:5888)</span><br><span class="line">	at com.alibaba.polardbx.atom.utils.NetworkUtils.setNetworkTimeout(NetworkUtils.java:18)</span><br><span class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectConnection.setNetworkTimeout(TGroupDirectConnection.java:433)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.resetPhyConnSocketTimeout(MyJdbcHandler.java:721)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1173)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</span><br><span class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</span><br><span class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</span><br><span class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</span><br><span class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</span><br><span class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:874)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">2022-06-01 12:10:00.179 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.executor.ExecutorHelper - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] PhyQuery(node=&quot;BANK_000000_GROUP&quot;, sql=&quot;SELECT SLEEP(?) AS `sleep(236)`&quot;)</span><br><span class="line">, tddl version: 5.4.13-16522656</span><br><span class="line">2022-06-01 12:10:00.180 [ServerExecutor-bucket-2-19-thread-181] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=43947,schema=bank]  [TDDL] [ERROR-CODE: 3009][1461cdf8b2809000] SQL:  /*+TDDL:node(0)  and SOCKET_TIMEOUT=2000 */ select sleep(236), tddl version: 5.4.13-16522656</span><br><span class="line">com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [TDDL-4614][ERR_EXECUTE_ON_MYSQL] Error occurs when execute on GROUP &#x27;BANK_000000_GROUP&#x27; ATOM &#x27;dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000&#x27;: Communications link failure</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.handleException(MyJdbcHandler.java:1935)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.generalHandlerException(MyJdbcHandler.java:1911)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1168)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.doInit(MyPhyQueryCursor.java:83)</span><br><span class="line">	at com.alibaba.polardbx.executor.cursor.AbstractCursor.init(AbstractCursor.java:53)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyPhyQueryCursor.&lt;init&gt;(MyPhyQueryCursor.java:67)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.CursorFactoryMyImpl.repoCursor(CursorFactoryMyImpl.java:42)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.handler.MyPhyQueryHandler.handle(MyPhyQueryHandler.java:24)</span><br><span class="line">	at com.alibaba.polardbx.executor.handler.HandlerCommon.handlePlan(HandlerCommon.java:102)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.executeInner(AbstractGroupExecutor.java:58)</span><br><span class="line">	at com.alibaba.polardbx.executor.AbstractGroupExecutor.execByExecPlanNode(AbstractGroupExecutor.java:36)</span><br><span class="line">	at com.alibaba.polardbx.executor.TopologyExecutor.execByExecPlanNode(TopologyExecutor.java:34)</span><br><span class="line">	at com.alibaba.polardbx.transaction.TransactionExecutor.execByExecPlanNode(TransactionExecutor.java:120)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.executeByCursor(ExecutorHelper.java:155)</span><br><span class="line">	at com.alibaba.polardbx.executor.ExecutorHelper.execute(ExecutorHelper.java:70)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execByExecPlanNodeByOne(PlanExecutor.java:130)</span><br><span class="line">	at com.alibaba.polardbx.executor.PlanExecutor.execute(PlanExecutor.java:75)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeQuery(TConnection.java:682)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TConnection.executeSQL(TConnection.java:457)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.executeSQL(TPreparedStatement.java:65)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TStatement.executeInternal(TStatement.java:133)</span><br><span class="line">	at com.alibaba.polardbx.matrix.jdbc.TPreparedStatement.execute(TPreparedStatement.java:50)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1131)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:883)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:850)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.execute(ServerConnection.java:844)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:82)</span><br><span class="line">	at com.alibaba.polardbx.server.handler.SelectHandler.handle(SelectHandler.java:31)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeSql(ServerQueryHandler.java:155)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.executeStatement(ServerQueryHandler.java:133)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerQueryHandler.queryRaw(ServerQueryHandler.java:118)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.query(FrontendConnection.java:460)</span><br><span class="line">	at com.alibaba.polardbx.net.handler.FrontendCommandHandler.handle(FrontendCommandHandler.java:49)</span><br><span class="line">	at com.alibaba.polardbx.net.FrontendConnection.lambda$handleData$0(FrontendConnection.java:753)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.RunnableWithCpuCollector.run(RunnableWithCpuCollector.java:36)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool$RunnableAdapter.run(ServerThreadPool.java:793)</span><br><span class="line">	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)</span><br><span class="line">	at java.util.concurrent.FutureTask.run(FutureTask.java:266)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">	at java.lang.Thread.run(Thread.java:874)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runOutsideWisp(WispTask.java:277)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.runCommand(WispTask.java:252)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</span><br><span class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</span><br><span class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</span><br><span class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</span><br><span class="line">	... 44 common frames omitted</span><br><span class="line">Caused by: java.net.SocketTimeoutException: time out</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:244)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="应用和-DB-间丢包导致-keepalive-心跳失败"><a href="#应用和-DB-间丢包导致-keepalive-心跳失败" class="headerlink" title="应用和 DB 间丢包导致 keepalive 心跳失败"></a>应用和 DB 间丢包导致 keepalive 心跳失败</h3><p>应用使用了 Druid 连接池来维护到 DB 间的所有长连接</p>
<p>应用和 DB 间丢包导致 keepalive 心跳失败，进而 OS会断开这个连接</p>
<p><img src="/images/951413iMgBlog/image-20230322171621838.png" alt="image-20230322171621838"></p>
<p>一个连接归还给Druid连接池都要做清理动作，就是第一个红框的rollback&#x2F;autocommit&#x3D;1</p>
<p>归还后OS 层面会探活TCP 连接，DB(4381端口)多次后多次不响应keepalive 后，OS 触发reset tcp断开连接，此时上层应用(比如Druid连接池、比如Tomcat)还不知道此连接在OS 层面已经断开</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#sysctl -a |grep -i keepalive</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 3</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 60</span><br><span class="line">net.ipv4.tcp_keepalive_time = 20</span><br></pre></td></tr></table></figure>

<p>继续过来一个新连接，业务取到这个连接执行查询就会报如下错误：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet successfully received from the server was 162,776 milliseconds ago.  The last packet sent successfully to the server was 162,776 milliseconds ago.</span><br><span class="line"></span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line"></span><br><span class="line">The last packet successfully received from the server was 162,776 milliseconds ago.  The last packet sent successfully to the server was 162,776 milliseconds ago.</span><br></pre></td></tr></table></figure>

<p>这个错误就是因为OS层面连接断开了，并且断开了162秒(和截图时间戳能对应上)</p>
<p>对应的错误堆栈：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Caused by: java.net.SocketException: Connection timed out (Write failed)</span><br><span class="line">        at java.net.SocketOutputStream.socketWrite0(Native Method)</span><br><span class="line">        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)</span><br><span class="line">        at java.net.SocketOutputStream.write(SocketOutputStream.java:155)</span><br><span class="line">        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)</span><br><span class="line">        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)</span><br><span class="line">        at com.mysql.jdbc.MysqlIO.send(MysqlIO.java:3725)</span><br><span class="line">        ... 46 common frames omitted</span><br></pre></td></tr></table></figure>

<h3 id="kill-案例"><a href="#kill-案例" class="headerlink" title="kill 案例"></a>kill 案例</h3><h4 id="kill-mysql-client"><a href="#kill-mysql-client" class="headerlink" title="kill mysql client"></a>kill mysql client</h4><p>mysql client连cn执行一个很慢的SQL，然后kill掉mysql client</p>
<p>cn报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">2022-06-01 11:45:59.063 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.druid.pool.DruidPooledStatement - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank] CommunicationsException, druid version 1.1.24, jdbcUrl : jdbc:mysql://172.16.40.215:3008/bank_000000?maintainTimeStats=false&amp;rewriteBatchedStatements=false&amp;failOverReadOnly=false&amp;cacheResultSetMetadata=true&amp;allowMultiQueries=true&amp;clobberStreamingResults=true&amp;autoReconnect=false&amp;usePsMemOptimize=true&amp;useServerPrepStmts=true&amp;netTimeoutForStreamingResults=0&amp;useSSL=false&amp;metadataCacheSize=256&amp;readOnlyPropagatesToServer=false&amp;prepStmtCacheSqlLimit=4096&amp;connectTimeout=5000&amp;socketTimeout=9000000&amp;cachePrepStmts=true&amp;characterEncoding=utf8&amp;prepStmtCacheSize=256, testWhileIdle true, idle millis 72028, minIdle 5, poolingCount 4, timeBetweenEvictionRunsMillis 60000, lastValidIdleMillis 345734, driver com.mysql.jdbc.Driver, exceptionSorter com.alibaba.polardbx.common.jdbc.sorter.MySQLExceptionSorter</span><br><span class="line">2022-06-01 11:45:59.064 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.druid.pool.DruidDataSource - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank] discard connection</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	…………</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: java.net.SocketException: Socket is closed</span><br><span class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] [1461c86bbe809001]Execute ERROR on GROUP: BANK_000000_GROUP, ATOM: dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000, MERGE_UNION_SIZE:1, SQL: /*DRDS /10.101.32.6/1461c86bbe809001/0// */SELECT SLEEP(?) AS `sleep(236)`, PARAM: [236], ERROR: Communications link failure, tddl version: 5.4.13-16522656</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">…………</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: java.net.SocketException: Socket is closed</span><br><span class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Reset conn socketTimeout failed, lastSocketTimeout is 9000000, tddl version: 5.4.13-16522656</span><br><span class="line">com.mysql.jdbc.exceptions.jdbc4.MySQLNonTransientConnectionException: No operations allowed after connection closed.</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:80)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">…………</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">2022-06-01 11:45:59.065 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.executor.ExecutorHelper - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] PhyQuery(node=&quot;BANK_000000_GROUP&quot;, sql=&quot;SELECT SLEEP(?) AS `sleep(236)`&quot;)</span><br><span class="line">, tddl version: 5.4.13-16522656</span><br><span class="line">2022-06-01 11:45:59.066 [ServerExecutor-bucket-0-17-thread-158] ERROR com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Interrupted unexpectedly for 1461c86bbe809001, tddl version: 5.4.13-16522656</span><br><span class="line">java.lang.InterruptedException: null</span><br><span class="line">	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1310)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.BooleanMutex$Sync.innerGet(BooleanMutex.java:136)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.BooleanMutex.get(BooleanMutex.java:53)</span><br><span class="line">	at com.alibaba.polardbx.common.utils.thread.ServerThreadPool.waitByTraceId(ServerThreadPool.java:445)</span><br><span class="line">	at com.alibaba.polardbx.server.ServerConnection.innerExecute(ServerConnection.java:1291)</span><br><span class="line">	……</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask.access$100(WispTask.java:33)</span><br><span class="line">	at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">2022-06-01 11:45:59.066 [ServerExecutor-bucket-0-17-thread-158] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] [ERROR-CODE: 3009][1461c86bbe809001] SQL:  /*+TDDL:node(0)  and SOCKET_TIMEOUT=40000 */ select sleep(236), tddl version: 5.4.13-16522656</span><br><span class="line">com.alibaba.polardbx.common.exception.TddlRuntimeException: ERR-CODE: [TDDL-4614][ERR_EXECUTE_ON_MYSQL] Error occurs when execute on GROUP &#x27;BANK_000000_GROUP&#x27; ATOM &#x27;dskey_bank_000000_group#pxc-xdb-s-pxcunrcbmk4g9lcpk0f24#172.16.40.215-3008#bank_000000&#x27;: Communications link failure</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.handleException(MyJdbcHandler.java:1935)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.generalHandlerException(MyJdbcHandler.java:1911)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1168)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQuery(MyJdbcHandler.java:990)</span><br><span class="line">	…………</span><br><span class="line">		at com.alibaba.wisp.engine.WispTask$CacheableCoroutine.run(WispTask.java:223)</span><br><span class="line">	at java.dyn.CoroutineBase.startInternal(CoroutineBase.java:60)</span><br><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure</span><br><span class="line">	at sun.reflect.GeneratedConstructorAccessor72.newInstance(Unknown Source)</span><br><span class="line">	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2658)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.serverExecute(ServerPreparedStatement.java:1281)</span><br><span class="line">	at com.mysql.jdbc.ServerPreparedStatement.executeInternal(ServerPreparedStatement.java:782)</span><br><span class="line">	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1367)</span><br><span class="line">	at com.alibaba.druid.pool.DruidPooledPreparedStatement.execute(DruidPooledPreparedStatement.java:497)</span><br><span class="line">	at com.alibaba.polardbx.group.jdbc.TGroupDirectPreparedStatement.execute(TGroupDirectPreparedStatement.java:84)</span><br><span class="line">	at com.alibaba.polardbx.repo.mysql.spi.MyJdbcHandler.executeQueryInner(MyJdbcHandler.java:1133)</span><br><span class="line">	... 44 common frames omitted</span><br><span class="line">Caused by: java.net.SocketException: Socket is closed</span><br><span class="line">	at java.net.Socket.getSoTimeout(Socket.java:1291)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read0(WispSocketImpl.java:249)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:208)</span><br><span class="line">	at sun.nio.ch.WispSocketImpl$1$1.read(WispSocketImpl.java:201)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:101)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:144)</span><br><span class="line">	at com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:174)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3183)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3659)</span><br><span class="line">	... 53 common frames omitted</span><br><span class="line">2022-06-01 11:45:59.071 [KillExecutor-15-thread-49] WARN  com.alibaba.polardbx.server.ServerConnection - [user=polardbx_root,host=10.101.32.6,port=50684,schema=bank]  [TDDL] Connection Killed, tddl version: 5.4.13-16522656</span><br></pre></td></tr></table></figure>

<p>mysqld报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2022-06-01T11:45:58.915371+08:00 8218735 [Note] Aborted connection 8218735 to db: &#x27;bank_000000&#x27; user: &#x27;rds_polardb_x&#x27; host: &#x27;172.16.40.214&#x27; (Got an error reading communication packets)</span><br></pre></td></tr></table></figure>

<p>172.16.40.214是客户端IP</p>
<p>抓包看到CN收到mysql client发过来的fin，CN回复fin断开连接</p>
<p>CN会给DN在新的连接上发Kill Query（stream 1596），同时会在原来的连接(stream 583)上发fin，然后原来的连接收到DN的response（被kill），然后CN发reset给DN</p>
<img src="/images/951413iMgBlog/image-20220601120626629.png" alt="image-20220601120626629" style="zoom:50%;" />

<p>下图是sleep 连接的收发包</p>
<img src="/images/951413iMgBlog/image-20220601120417026.png" alt="image-20220601120417026" style="zoom:50%;" />

<h4 id="Kill-jdbc-client"><a href="#Kill-jdbc-client" class="headerlink" title="Kill jdbc client"></a>Kill jdbc client</h4><p>Java jdbc client被kill后没有错误堆栈，kill后触发socket.close(对应client发送fin断开连接），kill后server端SQL也被立即中断</p>
<p>抓包：</p>
<img src="/images/951413iMgBlog/image-20220601143200253.png" alt="image-20220601143200253" style="zoom:50%;" />

<p>server端报错信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2022-06-01T14:33:52.204848+08:00 8288839 [Note] Aborted connection 8288839 to db: &#x27;bank_000000&#x27; user: &#x27;user&#x27; host: &#x27;172.16.40.214&#x27; (Got an error reading communication packets)</span><br></pre></td></tr></table></figure>

<h3 id="Statement-timeout"><a href="#Statement-timeout" class="headerlink" title="Statement timeout"></a>Statement timeout</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># java -cp /home/admin/drds-server/lib/*:. Test &quot;jdbc:mysql://172.16.40.215:3008/bank_000000?socketTimeout=5459&quot; &quot;user&quot; &quot;pass&quot; &quot;select sleep(180)&quot; &quot;1&quot; 3</span><br><span class="line">com.mysql.jdbc.exceptions.MySQLTimeoutException: Statement cancelled due to timeout or client request</span><br><span class="line">	at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1419)</span><br><span class="line">	at Test.main(Test.java:31)</span><br></pre></td></tr></table></figure>

<p>statement会设置一个timer，到时间还没有返回结果就创建一个新连接发送kill query</p>
<p>server 端收到kill后终止SQL执行，抓包看到Server端主动提前返回了错误</p>
<img src="/images/951413iMgBlog/image-20220601152401387.png" alt="image-20220601152401387" style="zoom:50%;" />

<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/xieyuooo/article/details/83109971">MySQL JDBC StreamResult通信原理浅析</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/07/01/%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E8%BF%9E%E8%87%AA%E5%B7%B1%E7%9A%84TCP%E8%BF%9E%E6%8E%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/07/01/%E5%A6%82%E4%BD%95%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AA%E8%87%AA%E5%B7%B1%E8%BF%9E%E8%87%AA%E5%B7%B1%E7%9A%84TCP%E8%BF%9E%E6%8E%A5/" class="post-title-link" itemprop="url">如何创建一个自己连自己的TCP连接</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-07-01 17:30:03" itemprop="dateCreated datePublished" datetime="2020-07-01T17:30:03+08:00">2020-07-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="如何创建一个自己连自己的TCP连接"><a href="#如何创建一个自己连自己的TCP连接" class="headerlink" title="如何创建一个自己连自己的TCP连接"></a>如何创建一个自己连自己的TCP连接</h1><blockquote>
<p>能不能建立一个tcp连接， src-ip:src-port 等于dest-ip:dest-port 呢？</p>
</blockquote>
<p>最近有同时找我，说是发现了一个奇怪的问题，他的 MySQLD listen 28350 端口， Sysbench 和 MySQLD 部署在同一台机器上，然后压 MySQL，只要 MySQL 一挂掉就再也起不来，起不来是因为 28350 端口被 Sysbench 抢走了，如下图，对 Sysbench 来说他已经连上 28350 的 MySQL 了（注意 ESTABLISHED 状态）：</p>
<p><img src="/images/951413iMgBlog/ignore-error,1" alt="img"></p>
<p>所以问题就是能不能建立一个自己连自己的连接呢？建立后有什么现象和后果？</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># nc 192.168.0.79 18082 -p 18082</span><br></pre></td></tr></table></figure>

<p>然后就能看到</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 18082</span><br><span class="line">tcp        0      0 192.168.0.79:18082      192.168.0.79:18082      ESTABLISHED</span><br></pre></td></tr></table></figure>

<p>比较神奇，这个连接的srcport等于destport，并且完全可以工作，也能收发数据。这有点颠覆大家的理解，端口能重复使用？</p>
<h2 id="port-range"><a href="#port-range" class="headerlink" title="port range"></a>port range</h2><p>我们都知道linux下本地端口范围由参数控制</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cat /proc/sys/net/ipv4/ip_local_port_range </span><br><span class="line">10000	65535</span><br></pre></td></tr></table></figure>

<p>所以也经常看到一个<strong>误解</strong>：一台机器上最多能创建65535个TCP连接</p>
<h2 id="到底一台机器上最多能创建多少个TCP连接"><a href="#到底一台机器上最多能创建多少个TCP连接" class="headerlink" title="到底一台机器上最多能创建多少个TCP连接"></a>到底一台机器上最多能创建多少个TCP连接</h2><p>在内存、文件句柄足够的话可以创建的连接是没有限制的，那么&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range指定的端口范围到底是什么意思呢？</p>
<p>一个TCP连接只要保证四元组(src-ip src-port dest-ip dest-port)唯一就可以了，而不是要求src port唯一，比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># netstat -ant |grep 18089</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:22         ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18089      192.168.1.79:18080      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.0.79:18089      192.168.0.79:22         TIME_WAIT </span><br><span class="line">tcp        0      0 192.168.1.79:22         192.168.1.79:18089      ESTABLISHED</span><br><span class="line">tcp        0      0 192.168.1.79:18080      192.168.1.79:18089      ESTABLISHED</span><br></pre></td></tr></table></figure>

<p>从前三行可以清楚地看到18089被用了三次，第一第二行src-ip、dest-ip也是重复的，但是dest port不一样，第三行的src-port还是18089，但是src-ip变了。</p>
<p>所以一台机器能创建的TCP连接是没有限制的，而ip_local_port_range是指没有bind的时候OS随机分配端口的范围，但是分配到的端口要同时满足五元组唯一，这样 ip_local_port_range 限制的是连同一个目标（dest-ip和dest-port一样）的port的数量（请忽略本地多网卡的情况，因为dest-ip为以后route只会选用一个本地ip）。</p>
<p>但是如果程序调用的是bind函数(bind(ip,port&#x3D;0))这个时候是让系统绑定到某个网卡和自动分配的端口，此时系统没有办法确定接下来这个socket是要去connect还是listen. 如果是listen的话，那么肯定是不能出现端口冲突的，如果是connect的话，只要满足4元组唯一即可。在这种情况下，系统只能尽可能满足更强的要求，就是先要求端口不能冲突，即使之后去connect的时候4元组是唯一的。</p>
<p>bind()的时候内核是还不知道四元组的，只知道src_ip、src_port，所以这个时候单网卡下src_port是没法重复的，但是connect()的时候已经知道了四元组的全部信息，所以只要保证四元组唯一就可以了，那么这里的src_port完全是可以重复使用的。</p>
<h2 id="自己连自己的连接"><a href="#自己连自己的连接" class="headerlink" title="自己连自己的连接"></a>自己连自己的连接</h2><p>我们来看自己连自己发生了什么</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># strace nc 192.168.0.79 18084 -p 18084</span></span><br><span class="line">execve(<span class="string">&quot;/usr/bin/nc&quot;</span>, [<span class="string">&quot;nc&quot;</span>, <span class="string">&quot;192.168.0.79&quot;</span>, <span class="string">&quot;18084&quot;</span>, <span class="string">&quot;-p&quot;</span>, <span class="string">&quot;18084&quot;</span>], [/* 31 vars */]) = 0</span><br><span class="line">brk(NULL)                               = 0x23d4000</span><br><span class="line">mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f213f394000</span><br><span class="line">access(<span class="string">&quot;/etc/ld.so.preload&quot;</span>, R_OK)      = -1 ENOENT (No such file or directory)</span><br><span class="line">open(<span class="string">&quot;/etc/ld.so.cache&quot;</span>, O_RDONLY|O_CLOEXEC) = 3</span><br><span class="line">fstat(3, &#123;st_mode=S_IFREG|0644, st_size=23295, ...&#125;) = 0</span><br><span class="line">mmap(NULL, 23295, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7f213f38e000</span><br><span class="line">close(3)                                = 0</span><br><span class="line">open(<span class="string">&quot;/lib64/libssl.so.10&quot;</span>, O_RDONLY|O_CLOEXEC) = 3</span><br><span class="line">………………</span><br><span class="line">munmap(0x7f213f393000, 4096)            = 0</span><br><span class="line">open(<span class="string">&quot;/usr/share/ncat/ca-bundle.crt&quot;</span>, O_RDONLY) = -1 ENOENT (No such file or directory)</span><br><span class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</span><br><span class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</span><br><span class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</span><br><span class="line"><span class="built_in">bind</span>(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">&quot;0.0.0.0&quot;</span>)&#125;, 16) = 0</span><br><span class="line">//注意这里<span class="built_in">bind</span>后直接就是connect，没有listen</span><br><span class="line">connect(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(<span class="string">&quot;192.168.0.79&quot;</span>)&#125;, 16) = -1 EINPROGRESS (Operation now <span class="keyword">in</span> progress)</span><br><span class="line"><span class="keyword">select</span>(4, [3], [3], [3], &#123;10, 0&#125;)       = 1 (out [3], left &#123;9, 999998&#125;)</span><br><span class="line">getsockopt(3, SOL_SOCKET, SO_ERROR, [0], [4]) = 0</span><br><span class="line"><span class="keyword">select</span>(4, [0 3], [], [], NULL</span><br></pre></td></tr></table></figure>

<p>抓包看看，正常三次握手，但是syn的seq和syn+ack的seq是一样的</p>
<p><img src="/images/oss/341f2891253baa4eebdaeaf34aa60c4b.png" alt="image.png"></p>
<p>这个连接算是常说的TCP simultaneous open，simultaneous open指的是两个不同port同时发syn建连接。而这里是先创建了一个socket，然后socket bind到18084端口上（作为local port，因为nc指定了local port），然后执行 connect, 连接到的目标也是192.168.0.79:18084，而这个目标正好是刚刚创建的socket，也就是自己连自己（连接双方总共只有一个socket）。因为一个socket充当了两个角色（client、server），握手的时候发syn，自己收到自己发的syn，就相当于两个角色simultaneous open了。</p>
<p>正常一个连接一定需要两个socket参与（这两个socket不一定要在两台机器上），而这个连接只用了一个socket就创建了，还能正常传输数据。但是仔细观察发数据的时候发放的seq增加（注意tcp_len 11那里的seq），收方的seq也增加了11，这是因为本来这就是用的同一个socket。正常两个socket通讯不是这样的。</p>
<p>那么这种情况为什么没有当做bug被处理呢？</p>
<h2 id="TCP-simultanous-open"><a href="#TCP-simultanous-open" class="headerlink" title="TCP simultanous open"></a>TCP simultanous open</h2><p>在tcp连接的定义中，通常都是一方先发起连接，假如两边同时发起连接，也就是两个socket同时给对方发 syn 呢？ 这在内核中是支持的，就叫同时打开（simultaneous open）。</p>
<p><img src="/images/oss/b9a0144a3835759c844f697bc45103fa.png" alt="image.png"></p>
<p>​							                                           摘自《tcp&#x2F;ip卷1》</p>
<p>可以清楚地看到这个连接建立用了四次握手，然后连接建立了，当然也有 simultanous close(3次挥手成功关闭连接)。如下内核代码 net&#x2F;ipv4&#x2F;tcp_input.c 的5924行中就说明了允许这种自己连自己的连接（当然也允许simultanous open). 也就是允许一个socket本来应该收到 syn+ack(发出syn后), 结果收到了syn的情况，而一个socket自己连自己又是这种情况的特例。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">	static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,</span><br><span class="line">                     const struct tcphdr *th)</span><br><span class="line">	&#123;</span><br><span class="line">5916         /* PAWS check. */</span><br><span class="line">						 //PAWS机制全称Protect Againest Wrapped Sequence numbers，</span><br><span class="line">						 //目的是为了解决在高带宽下，TCP序号可能被重复使用而带来的问题。</span><br><span class="line">5917         if (tp-&gt;rx_opt.ts_recent_stamp &amp;&amp; tp-&gt;rx_opt.saw_tstamp &amp;&amp;</span><br><span class="line">5918             tcp_paws_reject(&amp;tp-&gt;rx_opt, 0))</span><br><span class="line">5919                 goto discard_and_undo;</span><br><span class="line">5920         //在socket发送syn后收到了一个syn(正常应该收到syn+ack),这里是允许的。</span><br><span class="line">5921         if (th-&gt;syn) &#123;</span><br><span class="line">5922                 /* We see SYN without ACK. It is attempt of</span><br><span class="line">5923                  * simultaneous connect with crossed SYNs.</span><br><span class="line">5924                  * Particularly, it can be connect to self.  //自己连自己</span><br><span class="line">5925                  */</span><br><span class="line">5926                 tcp_set_state(sk, TCP_SYN_RECV);</span><br><span class="line">5927 </span><br><span class="line">5928                 if (tp-&gt;rx_opt.saw_tstamp) &#123;</span><br><span class="line">5929                         tp-&gt;rx_opt.tstamp_ok = 1;</span><br><span class="line">5930                         tcp_store_ts_recent(tp);</span><br><span class="line">5931                         tp-&gt;tcp_header_len =</span><br><span class="line">5932                                 sizeof(struct tcphdr) + TCPOLEN_TSTAMP_ALIGNED;</span><br><span class="line">5933                 &#125; else &#123;</span><br><span class="line">5934                         tp-&gt;tcp_header_len = sizeof(struct tcphdr);</span><br><span class="line">5935                 &#125;</span><br><span class="line">5936 </span><br><span class="line">5937                 tp-&gt;rcv_nxt = TCP_SKB_CB(skb)-&gt;seq + 1;</span><br><span class="line">5938                 tp-&gt;copied_seq = tp-&gt;rcv_nxt;</span><br><span class="line">5939                 tp-&gt;rcv_wup = TCP_SKB_CB(skb)-&gt;seq + 1;</span><br><span class="line">5940 </span><br><span class="line">5941                 /* RFC1323: The window in SYN &amp; SYN/ACK segments is</span><br><span class="line">5942                  * never scaled.</span><br><span class="line">5943                  */</span><br></pre></td></tr></table></figure>

<p>也就是在发送syn进入SYN_SENT状态之后，收到对端发来的syn包后不会RST，而是处理流程如下，调用tcp_set_state(sk, TCP_SYN_RECV)进入SYN_RECV状态，以及调用tcp_send_synack(sk)向对端发送syn+ack。</p>
<h2 id="自己连自己的原理解释"><a href="#自己连自己的原理解释" class="headerlink" title="自己连自己的原理解释"></a>自己连自己的原理解释</h2><p>第一我们要理解Kernel是支持simultaneous open（同时打开）的，也就是说socket发走syn后，本来应该收到一个syn+ack的，但是实际收到了一个syn（没有ack），这是允许的。这叫TCP连接同时打开（同时给对方发syn），四次握手然后建立连接成功。</p>
<p>自己连自己又是simultaneous open的一个特例，特别在这个连接只有一个socket参与，发送、接收都是同一个socket，自然也会是发syn后收到了自己的syn（自己发给自己），然后依照simultaneous open连接也能创建成功。</p>
<p>这个bind到18084 local port的socket又要连接到 18084 port上，而这个18084 socket已经bind到了socket（也就是自己），就形成了两个socket 的simultaneous open一样，内核又允许这种simultaneous open，所以就形成了自己连自己，也就是一个socket在自己给自己收发数据，所以看到收方和发放的seq是一样的。</p>
<p>可以用python来重现这个连接连自己的过程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import socket</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">connected=False</span><br><span class="line">while (not connected):</span><br><span class="line">        try:</span><br><span class="line">          sock = socket.socket(socket.AF_INET,socket.SOCK_STREAM)</span><br><span class="line">          sock.setsockopt(socket.IPPROTO_TCP,socket.TCP_NODELAY,1)</span><br><span class="line">          sock.bind((&#x27;&#x27;, 18084))               //sock 先bind到18084</span><br><span class="line">          sock.connect((&#x27;127.0.0.1&#x27;,18084))    //然后同一个socket连自己</span><br><span class="line">          connected=True</span><br><span class="line">        except socket.error,(value,message):</span><br><span class="line">        	print message</span><br><span class="line"></span><br><span class="line">        if not connected:</span><br><span class="line">        	print &quot;reconnect&quot;</span><br><span class="line">               </span><br><span class="line">print &quot;tcp self connection occurs!&quot;</span><br><span class="line">print &quot;netstat -an|grep 18084&quot;</span><br><span class="line">time.sleep(1800)             </span><br></pre></td></tr></table></figure>

<p>这里connect前如果没有bind那么系统就会从 local port range 分配一个可用port。</p>
<p>bind成功后会将ip+port放入hash表来判重，这就是我们常看到的 Bind to *** failed (IOD #1): Address already in use 异常。所以一台机器上，如果有多个ip，是可以将同一个port bind多次的，但是bind的时候如果不指定ip，也就是bind(‘0’, port) 还是会冲突。</p>
<p>connect成功后会将四元组放入ehash来判定连接的重复性。如果connect四元组冲突了就会报如下错误</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># nc 192.168.0.82 8080 -p 29798 -s 192.168.0.79</span><br><span class="line">Ncat: Cannot assign requested address.</span><br></pre></td></tr></table></figure>

<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a><strong>问题解决</strong></h2><p>知道原因就好解决了，有如下两个方案</p>
<ol>
<li>正常应该通过 port_range 限制随机端口的使用范围(就是给 Sysbench 这些客户端使用的)，而 Listen 使用的端口在 port_range 之外，这样就不会出现自己连自己的连接了</li>
<li>将 listen 端口添加到 &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_reserved_ports 中</li>
</ol>
<p>方案2示例(推荐该方案)如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># echo 3306,32768,1024-3000,32769-65535 &gt;/proc/sys/net/ipv4/ip_local_reserved_ports</span><br><span class="line">//合并连续的区间</span><br><span class="line"># cat /proc/sys/net/ipv4/ip_local_reserved_ports</span><br><span class="line">1024-3000,3306,32768-65535</span><br></pre></td></tr></table></figure>

<p>以上两个方法都可以解决这个问题，方案2 简直是为这种情况量身打造的</p>
<h2 id="bind-和-connect、listen"><a href="#bind-和-connect、listen" class="headerlink" title="bind 和 connect、listen"></a>bind 和 connect、listen</h2><p>当对一个TCP socket调用connect函数时，如果这个socket没有bind指定的端口号，操作系统会为它选择一个当前未被使用的端口号，这个端口号被称为ephemeral port, 范围可以在&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_local_port_range里查看。假设30000这个端口被选为ephemeral port。</p>
<p>如果这个socket指定了local port那么socket创建后会执行bind将这个socket bind到这个port。比如：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) = 3</span><br><span class="line">fcntl(3, F_GETFL)                       = 0x2 (flags O_RDWR)</span><br><span class="line">fcntl(3, F_SETFL, O_RDWR|O_NONBLOCK)    = 0</span><br><span class="line">setsockopt(3, SOL_SOCKET, SO_REUSEADDR, [1], 4) = 0</span><br><span class="line">bind(3, &#123;sa_family=AF_INET, sin_port=htons(18084), sin_addr=inet_addr(&quot;0.0.0.0&quot;)&#125;, 16) = 0</span><br></pre></td></tr></table></figure>

<p><img src="/images/oss/5373ecfe0d4496d106c64d3f370c893c.png" alt="image.png"></p>
<h3 id="listen"><a href="#listen" class="headerlink" title="listen"></a>listen</h3><p><img src="/images/oss/4d188cab03e919f055bb9dbe3da0188c.png" alt="image-20200702131215819"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000002396411">https://segmentfault.com/a/1190000002396411</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/a364572/article/details/40628171">linux中TCP的socket、bind、listen、connect和accept的实现</a></p>
<p><a target="_blank" rel="noopener" href="https://ops.tips/blog/how-linux-tcp-introspection/">How Linux allows TCP introspection The inner workings of bind and listen on Linux.</a></p>
<p><a target="_blank" rel="noopener" href="https://idea.popcount.org/2014-04-03-bind-before-connect/">https://idea.popcount.org/2014-04-03-bind-before-connect/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/05/24/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/05/24/%E7%A8%8B%E5%BA%8F%E5%91%98%E5%A6%82%E4%BD%95%E5%AD%A6%E4%B9%A0%E5%92%8C%E6%9E%84%E5%BB%BA%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/" class="post-title-link" itemprop="url">程序员如何学习和构建网络知识体系</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-05-24 17:30:03" itemprop="dateCreated datePublished" datetime="2020-05-24T17:30:03+08:00">2020-05-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/network/" itemprop="url" rel="index"><span itemprop="name">network</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="程序员如何学习和构建网络知识体系"><a href="#程序员如何学习和构建网络知识体系" class="headerlink" title="程序员如何学习和构建网络知识体系"></a>程序员如何学习和构建网络知识体系</h1><p>大家学习网络知识的过程中经常发现当时看懂了，很快又忘了，最典型的比如TCP三次握手、为什么要握手，大家基本都看过，但是种感觉还差那么一点点。都要看是因为面试官总要问，所以不能不知道啊。</p>
<p>我们来看一个典型的面试问题：</p>
<blockquote>
<p>问：为什么TCP是可靠的？<br>答：因为TCP有连接（或者回答因为TCP有握手）</p>
<p>追问：为什么有连接就可靠了？（面试的人估计心里在骂，你这不是傻逼么，有连接就可靠啊）</p>
<p>追问：这个TCP连接的本质是什么？网络上给你保留了一个带宽所以能可靠？<br>答：……懵了（或者因为TCP有ack，所以可靠）</p>
<p>追问：握手的本质是什么？为什么握手就可靠了<br>答：因为握手需要ack<br>追问：那这个ack也只是保证握手可靠，握手是怎么保证后面可靠的？握手本质做了什么事情？</p>
<p>追问：有了ack可靠后还会带来什么问题（比如发一个包ack一下，肯定是可行的，但是效率不行，面试官想知道的是这里TCP怎么传输的，从而引出各个buffer、拥塞窗口的概念）</p>
</blockquote>
<p>基本上我发现99%的程序员会回答TCP相对UDP是可靠的，70%以上的程序员会告诉你可靠是因为有ack（其他的会告诉你可靠是因为握手或者有连接），再追问下次就开始王顾左右而言他、胡言乱语。</p>
<p>我的理解：</p>
<blockquote>
<p>物理上没有一个连接的东西在这里，udp也类似会占用端口、ip，但是大家都没说过udp的连接。而本质上我们说tcp的握手是指tcp是协商和维护一些状态信息的，这个状态信息就包含seq、ack、窗口&#x2F;buffer，tcp握手就是协商出来这些初始值。这些状态才是我们平时所说的tcp连接的本质。</p>
</blockquote>
<p>这说明大部分程序员对问题的本质的理解上出了问题，或者教科书描述的过于教条不够接地气所以看完书本质没get到。</p>
<p>想想 <code>费曼学习方法</code> 中对<strong>事物本质</strong>的理解的重要性。</p>
<h2 id="重点掌握如下两篇文章"><a href="#重点掌握如下两篇文章" class="headerlink" title="## 重点掌握如下两篇文章"></a>## 重点掌握如下两篇文章</h2><p><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">一个网络包是如何到达目的地的 – </a>  这篇可以帮你掌握网络如何运转，在本机上从端口、ip、mac地址如何一层层封上去，链路上每一个点拆开mac看看，拆看ip看看，然后替换mac地址继续扔到链路的下一跳，这样一跳跳到达目的。</p>
<p><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/">对BDP、Buffer、各种窗口、rt的理解和运用 </a> 这一篇可以让你入门TCP</p>
<p>以上两篇都是站在程序员的角度来剖析关于网络我们应该掌握哪些，也许第一篇有点像网工要掌握的，实际我不这么认为，目前很流行的微服务化、云原生对网络的要求更高了，大多时候需要程序员去掌握这些，也就是在网络包从你的网卡离开你才有资格呼叫网工，否则成本很高！</p>
<p>我本周还碰到了网络不通的问题</p>
<blockquote>
<p>我的测试机器不能连外网(公司安全策略)</p>
<p>走流程申请开通，开通后会在测试机器安装客户端以及安全配置文件</p>
<p>但仍然不通，客户端自检都能通</p>
<p>我的排查就是第一篇文章：ping 公网ip；ip route get 公网-ip；ping 网关；</p>
<p>很快就发现是路由的问题，公网ip正好命中了docker 容器添加的某个路由，以及默认路由缺失</p>
<p>如果我自己不会那就开工单、描述问题、call各种人、申请权限……</p>
</blockquote>
<p>我碰到的程序员一看到网络连接异常就吓尿了，不关我的事，网络不通，但是在call人前你至少可以做：</p>
<ol>
<li>ping ip 通不通(也有个别禁掉了icmp)</li>
<li>telnet ip port通不通</li>
<li>网络包发出去没有(抓包)</li>
<li>是不是都不通还是只有你的机器不通</li>
</ol>
<h2 id="来看一个案例"><a href="#来看一个案例" class="headerlink" title="来看一个案例"></a>来看一个案例</h2><p>我第一次看<a target="_blank" rel="noopener" href="https://tools.ietf.org/html/rfc1180">RFC1180</a>的时候是震惊的，觉得讲述的太好了，2000字就把一本教科书的知识阐述的无比清晰、透彻。但是实际上我发现很快就忘了，而且大部分程序员基本都是这样</p>
<blockquote>
<p>RFC1180写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于95%的程序员没有什么用，当时看的时候很爽、也觉得自己理解了、学会了，实际上看完几周后就忘得差不多了。问题出在这种RFC偏理论多一点看起来完全没有体感无法感同身受，所以即使似乎当时看懂了，但是忘得也快，需要一篇结合实践的文章来帮助理解</p>
</blockquote>
<p>在这个问题上，让我深刻地理解到：</p>
<blockquote>
<p>一流的人看RFC就够了，差一些的人看《TCP&#x2F;IP卷1》，再差些的人要看一个个案例带出来的具体知识的书籍了，比如<a target="_blank" rel="noopener" href="https://book.douban.com/subject/26268767/">《wireshark抓包艺术》</a>，人和人的学习能力有差别必须要承认。</p>
</blockquote>
<p>也就是我们要认识到每个个人的<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA">学习能力的差异</a>，我超级认同这篇文章中的一个评论</p>
<blockquote>
<p>看完深有感触，尤其是后面的知识效率和工程效率型的区别。以前总是很中二的觉得自己看一遍就理解记住了，结果一次次失败又怀疑自己的智商是不是有问题，其实就是把自己当作知识效率型来用了。一个不太恰当的形容就是，有颗公主心却没公主命！</p>
</blockquote>
<p>嗯，大部分时候我们都觉得自己看一遍就理解了记住了能实用解决问题了，实际上了是马上忘了，停下来想想自己是不是这样的？在网络的相关知识上大部分看RFC、TCP卷1等东西是很难实际理解的，还是要靠实践来建立对知识的具体的理解，而网络相关的东西基本离大家有点远（大家不回去读tcp、ip源码，纯粹是靠对书本的理解），所以很难建立具体的概念，所以这里有个必杀技就是学会抓包和用wireshark看包，同时针对实际碰到的文题来抓包、看包分析。</p>
<p>比如这篇《<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA">从计算机知识到落地能力，你欠缺了什么？</a>》就对上述问题最好的阐述，程序员最常碰到的网络问题就是网络为啥不通？</p>
<p>这是最好建立对网络知识具体理解和实践的机会，你把《<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/x-ScSwEm3uQ2SFv-nAzNaA">从计算机知识到落地能力，你欠缺了什么？</a>》实践完再去看<a target="_blank" rel="noopener" href="https://tools.ietf.org/html/rfc1180">RFC1180</a> 就明白了。通过案例把RFC1180抽象的描述给它具体化、场景化了，理解起来就很轻松不容易忘记了。</p>
<blockquote>
<p>经验一: 通过具体的东西(案例、抓包)来建立对网络基础的理解</p>
</blockquote>
<p><img src="/images/951413iMgBlog/image-20220221151815993.png" alt="image-20220221151815993"></p>
<h2 id="不要追求知识的广度"><a href="#不要追求知识的广度" class="headerlink" title="不要追求知识的广度"></a>不要追求知识的广度</h2><p>学习网络知识过程中，不建议每个知识点都去看，因为很快会忘记，我的方法是只看经常碰到的问题点，碰到一个点把他学透理解明白。</p>
<p>比如我曾经碰到过 <a href="/2019/01/09/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82ping--nslookup-OK-but-ping-fail/">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的知识体系中扎根下来</a> , 这个问题Google上很多人在搜索，说明很普遍，但是没找到有资料能把这个问题说清楚，所以借着这个机会就把 Linux下的 NSS（name service switch）的原理搞懂了。要不然碰到问题老司机告诉你改下 &#x2F;etc&#x2F;hosts 或者  &#x2F;etc&#x2F;nsswitch 或者 &#x2F;etc&#x2F;resolv.conf 之类的问题就能解决，但是你一直不知道这三个文件怎么起作用的，也就是你碰到过这种问题也解决过但是下次碰到类似的问题你不一定能解决。</p>
<p>当然对我来说为了解决这个问题最后写了4篇跟域名解析相关的文章，从windows到linux，涉及到vpn、glibc、docker等各种场景，我把他叫做场景驱动。后来换来工作环境从windows换到mac后又补了一篇mac下的路由、dns文章。</p>
<p>关于<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/JlXWLpQSyj3Z_KMyUmzBPA">场景驱动学习的方法可以看这篇总结</a></p>
<h2 id="TCP是最复杂的，要从实用出发"><a href="#TCP是最复杂的，要从实用出发" class="headerlink" title="TCP是最复杂的，要从实用出发"></a>TCP是最复杂的，要从实用出发</h2><p>比如拥塞算法基本大家不会用到，了解下就行，你想想你有碰到过因为拥塞算法导致的问题吗？极少是吧。还有拥塞窗口、慢启动，这个实际中碰到的概率不高，面试要问你基本上是属于炫技类型。</p>
<p>实际碰到更多的是传输效率（<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/fKWJrDNSAZjLsyobolIQKw">对BDP、Buffer、各种窗口、rt的理解和运用</a>），还有为什么连不通、<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/yH3PzGEFopbpA-jw4MythQ">连接建立不起来</a>、为什么收到包不回复、为什么要reset、为什么丢包了之类的问题。</p>
<p>关于为什么连不通，我碰到了<a href="/2019/05/16/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%B8%8D%E9%80%9A%E6%98%AF%E4%B8%AA%E5%A4%A7%E9%97%AE%E9%A2%98--%E5%8D%8A%E5%A4%9C%E9%B8%A1%E5%8F%AB/">这个问题</a>，随后在这个问题的基础上进行了总结，得到客户端建连接的时候抛异常，可能的原因（握手失败，建不上连接）：</p>
<ul>
<li>网络不通，<strong>诊断</strong>：ping ip</li>
<li>端口不通,  <strong>诊断</strong>：telnet ip port</li>
<li>rp_filter 命中(rp_filter&#x3D;1, 多网卡环境）， <strong>诊断</strong>:  netstat -s | grep -i filter </li>
<li>防火墙、命中iptables 被扔掉了，可以试试22端口起sshd 能否正常访问，能的话说明是端口被干了</li>
<li>snat&#x2F;dnat的时候宿主机port冲突，内核会扔掉 syn包。<strong>诊断</strong>: sudo conntrack -S | grep  insert_failed &#x2F;&#x2F;有不为0的</li>
<li>Firewalld 或者 iptables</li>
<li>全连接队列满的情况，<strong>诊断</strong>： netstat -s | egrep “listen|LISTEN” </li>
<li>syn flood攻击, <strong>诊断</strong>：同上</li>
<li>服务端的内核参数 net.ipv4.tcp_tw_recycle(<a target="_blank" rel="noopener" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc">4.12内核</a>删除这个参数了) 和 net.ipv4.tcp_timestamps 的值都为 1时，服务器会检查每一个 SYN报文中的时间戳（Timestamp，跟同一ip下最近一次 FIN包时间对比），若 <a target="_blank" rel="noopener" href="https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux">Timestamp 不是递增的关系</a>，就扔掉这个SYN包（<strong>诊断</strong>：netstat -s | grep “ passive connections rejected because of time stamp”），常见触发时间戳非递增场景：<ol>
<li><a target="_blank" rel="noopener" href="https://lwn.net/Articles/708021/">4.10 内核</a>，一直必现大概率性丢包。<a target="_blank" rel="noopener" href="https://github.com/torvalds/linux/commit/95a22caee396cef0bb2ca8fafdd82966a49367bb">4.11 改成了</a> per-destination host的算法</li>
<li>tcpping 这种时间戳按连接随机的，必现大概率持续丢包</li>
<li><strong>同一个客户端通过直连或者 DNAT 后两条链路到同一个服务端</strong>，客户端生成时间戳是 by dst ip，导致大概率持续丢包</li>
<li>经过NAT&#x2F;LVS 后多个客户端被当成一个客户端，小概率偶尔出现</li>
<li>网路链路复杂&#x2F;链路长容易导致包乱序，进而出发丢包，取决于网络会小概率出现——通过 tc qdisc 可以来构造丢包重现该场景</li>
<li>客户端修改 net.ipv4.tcp_timestamps  <ul>
<li>1-&gt;0，触发持续60秒大概率必现的丢包，60秒后恢复</li>
<li>0-&gt;1 持续大概率一直丢包60秒; 60秒过后如果网络延时略高且客户端并发大一直有上一次 FIN 时间戳大于后续SYN 会一直概率性丢包持续下去；如果停掉所有流量，重启客户端流量，恢复正常</li>
<li>2-&gt;1 丢包，情况同2</li>
<li>1-&gt;2 不触发丢包</li>
</ul>
</li>
</ol>
</li>
<li>若服务器所用<a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1262180">端口是 time_wait 状态</a>，这时新连接刚好和 time_wait 5元组重复，一般服务器不会回复syn+ack 而是回复time_wait 前的ack </li>
<li>NAT 哈希表满导致 ECS 实例丢包 nf_conntrack full， <strong>诊断</strong>: dmesg |grep conntrack</li>
</ul>
<p>为什么 drop SYN 包时不去看四元组？因为tiem_wait 状态是 per-host </p>
<p>0-&gt;1 60秒后仍然持续丢包：</p>
<p><img src="/images/951413iMgBlog/image-20240803095126448.png" alt="image-20240803095126448"></p>
<p>2-&gt;1 60秒后持续丢包：(非常神奇：在310客户端改不影响自己，导致510客户端（网络延时大）一直丢包，直到510 客户端重启流量才能恢复)</p>
<p><img src="/images/951413iMgBlog/image-20240803093817441.png" alt="image-20240803093817441"></p>
<p>tcp_reuse 参数只对客户端有效(客户端是指主动发起 fin 的一方)，启用后会回收超过 1 秒的 time_wait 状态端口重复使用：参考：<a target="_blank" rel="noopener" href="https://ata.atatech.org/articles/11020082442">https://ata.atatech.org/articles/11020082442</a></p>
<h2 id="如果服务端是Time-wait-状态时收到-SYN-包怎么办？"><a href="#如果服务端是Time-wait-状态时收到-SYN-包怎么办？" class="headerlink" title="如果服务端是Time_wait 状态时收到 SYN 包怎么办？"></a>如果服务端是Time_wait 状态时收到 SYN 包怎么办？</h2><p><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1262180">https://developer.aliyun.com/article/1262180</a> </p>
<p>tcp connect 的流程是这样的：</p>
<p>1、tcp发出SYN建链报文后，报文到ip层需要进行路由查询</p>
<p>2、路由查询完成后，报文到arp层查询下一跳mac地址</p>
<p>3、如果本地没有对应网关的arp缓存，就需要缓存住这个报文，发起arp请求</p>
<p>4、arp层收到arp回应报文之后，从缓存中取出SYN报文，完成mac头填写并发送给驱动。</p>
<p>问题在于，arp层报文缓存队列长度默认为3。如果你运气不好，刚好赶上缓存已满，这个报文就会被丢弃。</p>
<p>TCP层发现SYN报文发出去3s（1s+2s）还没有回应，就会重发一个SYN。这就是为什么少数连接会3s后才能建链。</p>
<p>幸运的是，arp层缓存队列长度是可配置的，用 sysctl -a | grep unres_qlen 就能看到，默认值为3</p>
<h2 id="Time-Wait"><a href="#Time-Wait" class="headerlink" title="Time_Wait"></a>Time_Wait</h2><p>socket.close 默认是四次挥手，但如果tw bucket 满了就直接走 reset，比如很多机器设置的是 5000 net.ipv4.tcp_max_tw_buckets &#x3D; 5000</p>
<p>bucket 溢出对应的监控指标：TCPTimeWaitOverflow</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#netstat -s | grep -i overflow</span><br><span class="line">    439 times the listen queue of a socket overflowed</span><br><span class="line">    TCPTimeWaitOverflow: 377310115</span><br><span class="line"></span><br><span class="line">#netstat -s | grep -i overflow</span><br><span class="line">    439 times the listen queue of a socket overflowed</span><br><span class="line">    TCPTimeWaitOverflow: 377314175</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>一定要会用tcpdump和wireshark（纯工具，没有任何门槛，用不好只有一个原因: 懒）</li>
<li>多实践（因为网络知识离我们有点远、有点抽象）,用好各种工具，工具能帮我们看到、摸到</li>
<li>不要追求知识面的广度，深抠几个具体的知识点然后让这些点建立体系</li>
<li>不要为那些基本用不到的偏门知识花太多精力，天天用的都学不过来对吧。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>per-connection random offset：<a target="_blank" rel="noopener" href="https://lwn.net/Articles/708021/">https://lwn.net/Articles/708021/</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/04/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97--%E9%98%BF%E9%87%8C%E6%8A%80%E6%9C%AF%E5%85%AC%E4%BC%97%E5%8F%B7%E7%89%88%E6%9C%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/04/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97--%E9%98%BF%E9%87%8C%E6%8A%80%E6%9C%AF%E5%85%AC%E4%BC%97%E5%8F%B7%E7%89%88%E6%9C%AC/" class="post-title-link" itemprop="url">就是要你懂TCP--半连接队列和全连接队列</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-04-07 17:30:03" itemprop="dateCreated datePublished" datetime="2020-04-07T17:30:03+08:00">2020-04-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="关于TCP-半连接队列和全连接队列"><a href="#关于TCP-半连接队列和全连接队列" class="headerlink" title="关于TCP 半连接队列和全连接队列"></a>关于TCP 半连接队列和全连接队列</h1><blockquote>
<p>最近碰到一个client端连接服务器总是抛异常的问题，然后定位分析并查阅各种资料文章，对TCP连接队列有个深入的理解</p>
<p>查资料过程中发现没有文章把这两个队列以及怎么观察他们的指标说清楚，希望通过这篇文章能把他们说清楚</p>
</blockquote>
<h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><pre><code>场景：JAVA的client和server，使用socket通信。server使用NIO。

1.间歇性的出现client向server建立连接三次握手已经完成，但server的selector没有响应到这连接。
2.出问题的时间点，会同时有很多连接出现这个问题。
3.selector没有销毁重建，一直用的都是一个。
4.程序刚启动的时候必会出现一些，之后会间歇性出现。
</code></pre>
<h3 id="分析问题"><a href="#分析问题" class="headerlink" title="分析问题"></a>分析问题</h3><h4 id="正常TCP建连接三次握手过程："><a href="#正常TCP建连接三次握手过程：" class="headerlink" title="正常TCP建连接三次握手过程："></a>正常TCP建连接三次握手过程：</h4><p><img src="/images/oss/159a331ff8cdd4b8994dfe6a209d035f.png" alt="image.png"></p>
<ul>
<li>第一步：client 发送 syn 到server 发起握手；</li>
<li>第二步：server 收到 syn后回复syn+ack给client；</li>
<li>第三步：client 收到syn+ack后，回复server一个ack表示收到了server的syn+ack（此时client的56911端口的连接已经是established）</li>
</ul>
<p>从问题的描述来看，有点像TCP建连接的时候全连接队列（accept队列，后面具体讲）满了，尤其是症状2、4. 为了证明是这个原因，马上通过 netstat -s | egrep “listen” 去看队列的溢出统计数据：</p>
<pre><code>667399 times the listen queue of a socket overflowed
</code></pre>
<p>反复看了几次之后发现这个overflowed 一直在增加，那么可以明确的是server上全连接队列一定溢出了</p>
<p>接着查看溢出后，OS怎么处理：</p>
<pre><code># cat /proc/sys/net/ipv4/tcp_abort_on_overflow
0
</code></pre>
<p><strong>tcp_abort_on_overflow 为0表示如果三次握手第三步的时候全连接队列满了那么server扔掉client 发过来的ack（在server端认为连接还没建立起来）</strong></p>
<p>为了证明客户端应用代码的异常跟全连接队列满有关系，我先把tcp_abort_on_overflow修改成 1，1表示第三步的时候如果全连接队列满了，server发送一个reset包给client，表示废掉这个握手过程和这个连接（本来在server端这个连接就还没建立起来）。</p>
<p>接着测试，这时在客户端异常中可以看到很多connection reset by peer的错误，<strong>到此证明客户端错误是这个原因导致的（逻辑严谨、快速证明问题的关键点所在）</strong>。</p>
<p>于是开发同学翻看java 源代码发现socket 默认的backlog（这个值控制全连接队列的大小，后面再详述）是50，于是改大重新跑，经过12个小时以上的压测，这个错误一次都没出现了，同时观察到 overflowed 也不再增加了。</p>
<p>到此问题解决，<strong>简单来说TCP三次握手后有个accept队列，进到这个队列才能从Listen变成accept，默认backlog 值是50，很容易就满了</strong>。满了之后握手第三步的时候server就忽略了client发过来的ack包（隔一段时间server重发握手第二步的syn+ack包给client），如果这个连接一直排不上队就异常了。</p>
<blockquote>
<p>但是不能只是满足问题的解决，而是要去复盘解决过程，中间涉及到了哪些知识点是我所缺失或者理解不到位的；这个问题除了上面的异常信息表现出来之外，还有没有更明确地指征来查看和确认这个问题。</p>
</blockquote>
<h3 id="深入理解TCP握手过程中建连接的流程和队列"><a href="#深入理解TCP握手过程中建连接的流程和队列" class="headerlink" title="深入理解TCP握手过程中建连接的流程和队列"></a>深入理解TCP握手过程中建连接的流程和队列</h3><p><img src="/images/oss/2703fc07dfc4dd5b6e1bb4c2ce620e59.png" alt="image.png"><br>（图片来源：<a target="_blank" rel="noopener" href="http://www.cnxct.com/something-about-phpfpm-s-backlog/%EF%BC%89">http://www.cnxct.com/something-about-phpfpm-s-backlog/）</a></p>
<p>如上图所示，这里有两个队列：syns queue(半连接队列）；accept queue（全连接队列）</p>
<p>三次握手中，在第一步server收到client的syn后，把这个连接信息放到半连接队列中，同时回复syn+ack给client（第二步）；</p>
<pre><code>题外话，比如syn floods 攻击就是针对半连接队列的，攻击方不停地建连接，但是建连接的时候只做第一步，第二步中攻击方收到server的syn+ack后故意扔掉什么也不做，导致server上这个队列满其它正常请求无法进来
</code></pre>
<p>第三步的时候server收到client的ack，如果这时全连接队列没满，那么从半连接队列拿出这个连接的信息放入到全连接队列中，否则按tcp_abort_on_overflow指示的执行。</p>
<p>这时如果全连接队列满了并且tcp_abort_on_overflow是0的话，server过一段时间再次发送syn+ack给client（也就是重新走握手的第二步），如果client超时等待比较短，client就很容易异常了。</p>
<p>在我们的os中retry 第二步的默认次数是2（centos默认是5次）：</p>
<pre><code>net.ipv4.tcp_synack_retries = 2
</code></pre>
<h3 id="如果TCP连接队列溢出，有哪些指标可以看呢？"><a href="#如果TCP连接队列溢出，有哪些指标可以看呢？" class="headerlink" title="如果TCP连接队列溢出，有哪些指标可以看呢？"></a>如果TCP连接队列溢出，有哪些指标可以看呢？</h3><p>上述解决过程有点绕，听起来蒙逼，那么下次再出现类似问题有什么更快更明确的手段来确认这个问题呢？</p>
<p>（<em>通过具体的、感性的东西来强化我们对知识点的理解和吸收</em>）</p>
<h4 id="netstat-s"><a href="#netstat-s" class="headerlink" title="netstat -s"></a>netstat -s</h4><pre><code>[root@server ~]#  netstat -s | egrep &quot;listen|LISTEN&quot; 
667399 times the listen queue of a socket overflowed
667399 SYNs to LISTEN sockets ignored
</code></pre>
<p>比如上面看到的 667399 times ，表示全连接队列溢出的次数，隔几秒钟执行下，如果这个数字一直在增加的话肯定全连接队列偶尔满了。</p>
<h4 id="ss-命令"><a href="#ss-命令" class="headerlink" title="ss 命令"></a>ss 命令</h4><pre><code>[root@server ~]# ss -lnt
Recv-Q Send-Q Local Address:Port  Peer Address:Port 
0        50               *:3306             *:* 
</code></pre>
<p><strong>上面看到的第二列Send-Q 值是50，表示第三列的listen端口上的全连接队列最大为50，第一列Recv-Q为全连接队列当前使用了多少</strong></p>
<p><strong>全连接队列的大小取决于：min(backlog, somaxconn) . backlog是在socket创建的时候传入的，somaxconn是一个os级别的系统参数</strong></p>
<p>这个时候可以跟我们的代码建立联系了，比如Java创建ServerSocket的时候会让你传入backlog的值：</p>
<pre><code>ServerSocket()
	Creates an unbound server socket.
ServerSocket(int port)
	Creates a server socket, bound to the specified port.
ServerSocket(int port, int backlog)
	Creates a server socket and binds it to the specified local port number, with the specified backlog.
ServerSocket(int port, int backlog, InetAddress bindAddr)
	Create a server with the specified port, listen backlog, and local IP address to bind to.
</code></pre>
<p>（来自JDK帮助文档：<a target="_blank" rel="noopener" href="https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html%EF%BC%89">https://docs.oracle.com/javase/7/docs/api/java/net/ServerSocket.html）</a></p>
<p><strong>半连接队列的大小取决于：max(64,  &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_max_syn_backlog)。 不同版本的os会有些差异</strong></p>
<blockquote>
<p>我们写代码的时候从来没有想过这个backlog或者说大多时候就没给他值（那么默认就是50），直接忽视了他，首先这是一个知识点的忙点；其次也许哪天你在哪篇文章中看到了这个参数，当时有点印象，但是过一阵子就忘了，这是知识之间没有建立连接，不是体系化的。但是如果你跟我一样首先经历了这个问题的痛苦，然后在压力和痛苦的驱动自己去找为什么，同时能够把为什么从代码层推理理解到OS层，那么这个知识点你才算是比较好地掌握了，也会成为你的知识体系在TCP或者性能方面成长自我生长的一个有力抓手</p>
</blockquote>
<h4 id="netstat-命令"><a href="#netstat-命令" class="headerlink" title="netstat 命令"></a>netstat 命令</h4><p>netstat跟ss命令一样也能看到Send-Q、Recv-Q这些状态信息，不过如果这个连接不是<strong>Listen状态</strong>的话，Recv-Q就是指收到的数据还在缓存中，还没被进程读取，这个值就是还没被进程读取的 bytes；而 Send 则是发送队列中没有被远程主机确认的 bytes 数</p>
<pre><code>$netstat -tn  
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address   Foreign Address State  
tcp0  0 server:8182  client-1:15260 SYN_RECV   
tcp0 28 server:22    client-1:51708  ESTABLISHED
tcp0  0 server:2376  client-1:60269 ESTABLISHED
</code></pre>
<p> **netstat -tn 看到的 Recv-Q 跟全连接半连接没有关系，这里特意拿出来说一下是因为容易跟 ss -lnt 的 Recv-Q 搞混淆，顺便建立知识体系，巩固相关知识点 **  </p>
<h5 id="Recv-Q-和-Send-Q-的说明"><a href="#Recv-Q-和-Send-Q-的说明" class="headerlink" title="Recv-Q 和 Send-Q 的说明"></a>Recv-Q 和 Send-Q 的说明</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Recv-Q</span><br><span class="line">Established: The count of bytes not copied by the user program connected to this socket.</span><br><span class="line">Listening: Since Kernel 2.6.18 this column contains the current syn backlog.</span><br><span class="line"></span><br><span class="line">Send-Q</span><br><span class="line">Established: The count of bytes not acknowledged by the remote host.</span><br><span class="line">Listening: Since Kernel 2.6.18 this column contains the maximum size of the syn backlog. </span><br></pre></td></tr></table></figure>



<h6 id="通过-netstat-发现问题的案例"><a href="#通过-netstat-发现问题的案例" class="headerlink" title="通过 netstat 发现问题的案例"></a>通过 netstat 发现问题的案例</h6><p>自身太慢，比如如下netstat -t 看到的Recv-Q有大量数据堆积，那么一般是CPU处理不过来导致的：</p>
<p><img src="/images/oss/77ed9ba81f70f7940546f0a22dabf010.png" alt="image.png"></p>
<p>下面的case是接收方太慢，从应用机器的netstat统计来看，也是压力端回复太慢（本机listen 9108端口)</p>
<img src="/images/oss/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;" />

<p>send-q表示回复从9108发走了，没收到对方的ack，<strong>基本可以推断PTS到9108之间有瓶颈</strong></p>
<p>上面是通过一些具体的工具、指标来认识全连接队列（工程效率的手段）   </p>
<h3 id="实践验证一下上面的理解"><a href="#实践验证一下上面的理解" class="headerlink" title="实践验证一下上面的理解"></a>实践验证一下上面的理解</h3><p>把java中backlog改成10（越小越容易溢出），继续跑压力，这个时候client又开始报异常了，然后在server上通过 ss 命令观察到：</p>
<pre><code>Fri May  5 13:50:23 CST 2017
Recv-Q Send-QLocal Address:Port  Peer Address:Port
11         10         *:3306               *:*
</code></pre>
<p>按照前面的理解，这个时候我们能看到3306这个端口上的服务全连接队列最大是10，但是现在有11个在队列中和等待进队列的，肯定有一个连接进不去队列要overflow掉，同时也确实能看到overflow的值在不断地增大。</p>
<h4 id="Tomcat和Nginx中的Accept队列参数"><a href="#Tomcat和Nginx中的Accept队列参数" class="headerlink" title="Tomcat和Nginx中的Accept队列参数"></a>Tomcat和Nginx中的Accept队列参数</h4><p>Tomcat默认短连接，backlog（Tomcat里面的术语是Accept count）Ali-tomcat默认是200, Apache Tomcat默认100. </p>
<pre><code>#ss -lnt
Recv-Q Send-Q   Local Address:Port Peer Address:Port
0       100                 *:8080            *:*
</code></pre>
<p>Nginx默认是511</p>
<pre><code>$sudo ss -lnt
State  Recv-Q Send-Q Local Address:PortPeer Address:Port
LISTEN    0     511              *:8085           *:*
LISTEN    0     511              *:8085           *:*
</code></pre>
<p>因为Nginx是多进程模式，所以看到了多个8085，也就是多个进程都监听同一个端口以尽量避免上下文切换来提升性能   </p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>全连接队列、半连接队列溢出这种问题很容易被忽视，但是又很关键，特别是对于一些短连接应用（比如Nginx、PHP，当然他们也是支持长连接的）更容易爆发。 一旦溢出，从cpu、线程状态看起来都比较正常，但是压力上不去，在client看来rt也比较高（rt&#x3D;网络+排队+真正服务时间），但是从server日志记录的真正服务时间来看rt又很短。</p>
<p>jdk、netty等一些框架默认backlog比较小，可能有些情况下导致性能上不去，比如这个 <a target="_blank" rel="noopener" href="https://www.atatech.org/articles/12919">《netty新建连接并发数很小的case》 </a><br>都是类似原因</p>
<p>希望通过本文能够帮大家理解TCP连接过程中的半连接队列和全连接队列的概念、原理和作用，更关键的是有哪些指标可以明确看到这些问题（<strong>工程效率帮助强化对理论的理解</strong>）。</p>
<p>另外每个具体问题都是最好学习的机会，光看书理解肯定是不够深刻的，请珍惜每个具体问题，碰到后能够把来龙去脉弄清楚，每个问题都是你对具体知识点通关的好机会。</p>
<h3 id="最后提出相关问题给大家思考"><a href="#最后提出相关问题给大家思考" class="headerlink" title="最后提出相关问题给大家思考"></a>最后提出相关问题给大家思考</h3><ol>
<li>全连接队列满了会影响半连接队列吗？</li>
<li>netstat -s看到的overflowed和ignored的数值有什么联系吗？</li>
<li>如果client走完了TCP握手的第三步，在client看来连接已经建立好了，但是server上的对应连接实际没有准备好，这个时候如果client发数据给server，server会怎么处理呢？（有同学说会reset，你觉得呢？）</li>
</ol>
<blockquote>
<p>提出这些问题就是以这个知识点为抓手，让你的知识体系开始自我生长</p>
</blockquote>
<hr>
<p>参考文章：</p>
<p><a target="_blank" rel="noopener" href="http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html">http://veithen.github.io/2014/01/01/how-tcp-backlog-works-in-linux.html</a></p>
<p><a target="_blank" rel="noopener" href="http://www.cnblogs.com/zengkefu/p/5606696.html">http://www.cnblogs.com/zengkefu/p/5606696.html</a></p>
<p><a target="_blank" rel="noopener" href="http://www.cnxct.com/something-about-phpfpm-s-backlog/">http://www.cnxct.com/something-about-phpfpm-s-backlog/</a></p>
<p><a target="_blank" rel="noopener" href="http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/">http://jaseywang.me/2014/07/20/tcp-queue-%E7%9A%84%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/</a></p>
<p><a target="_blank" rel="noopener" href="http://jin-yang.github.io/blog/network-synack-queue.html#">http://jin-yang.github.io/blog/network-synack-queue.html#</a></p>
<p><a target="_blank" rel="noopener" href="http://blog.chinaunix.net/uid-20662820-id-4154399.html">http://blog.chinaunix.net/uid-20662820-id-4154399.html</a></p>
<p><a target="_blank" rel="noopener" href="https://www.atatech.org/articles/12919">https://www.atatech.org/articles/12919</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/xiaolincoding/p/12995358.html">https://www.cnblogs.com/xiaolincoding/p/12995358.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/03/01/%E9%BB%84%E5%A5%87%E5%B8%86%E7%9A%84%E5%A4%8D%E6%97%A6%E7%BB%8F%E6%B5%8E%E8%AF%BE--%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/03/01/%E9%BB%84%E5%A5%87%E5%B8%86%E7%9A%84%E5%A4%8D%E6%97%A6%E7%BB%8F%E6%B5%8E%E8%AF%BE--%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">分析与思考——黄奇帆的复旦经济课笔记</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-03-01 17:30:03" itemprop="dateCreated datePublished" datetime="2020-03-01T17:30:03+08:00">2020-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%85%B6%E5%AE%83/" itemprop="url" rel="index"><span itemprop="name">其它</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="分析与思考——黄奇帆的复旦经济课笔记"><a href="#分析与思考——黄奇帆的复旦经济课笔记" class="headerlink" title="分析与思考——黄奇帆的复旦经济课笔记"></a>分析与思考——黄奇帆的复旦经济课笔记</h1><p>这是一次奇特的读书经历，因为黄奇帆这本书的内容主要是17-19年的一些报告内容，里面给出了他的一些看法以及各种数据，所以在3年后的2021年来读的话，我能够搜索当前的数据来印证他的判断，这种穿越的感觉很好。</p>
<p>黄比较厉害的是思路、逻辑清晰，然后各种数据比较丰富，所以他对问题的判断和看法比较准确。另外一个准确的是任大炮。相较另外一些教授、专家就混乱无比了，比如<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%98%93%E5%AE%AA%E5%AE%B9/10443874">易宪容</a></p>
<p>比如，去杠杆比他预估的要差多了，杠杆这几年不但没去成反而加大了；政府债务占比也没有按他的预期减少，稳中有增；房地产开工面积也在增加，当然增速很慢了，占GDP比重也在缓慢增加。</p>
<p>以下引用内容都是从网络搜索所得，其它内容为黄书中直接复制出来的，附录内容没有读。</p>
<hr>
<h2 id="去杠杆"><a href="#去杠杆" class="headerlink" title="去杠杆"></a>去杠杆</h2><ul>
<li>国家M2。2017年中国M2已经达到170万亿元，这几个月下来到5月底已经是176万亿元，我们的GDP 2017年是82万亿元，M2与GDP之比已经是2.1∶1。美国的M2跟它的GDP之比是0.9∶1，美国GDP是20万亿美元，他们M2统统加起来，尽管已经有了三次（Q1、Q2、Q3）的宽松，现在的M2其实也就18万亿美元，所以我们这个指标就显然是非常非常的高。</li>
<li>我们国家金融业的增加值。2017年年底占GDP总量的7.9%，2016年年底是8.4%，2017年五六月份到了8.8%，下半年开始努力地约束金融业夸张性的发展或者说太高速的发展，把这个约束了一下，所以到2017年年底是7.9%，2018年1—5月份还是在7.8%左右。这个指标也是世界最高，全世界金融增加值跟全世界GDP来比的话，平均是在4%左右。像日本尽管有泡沫危机，从20世纪80年代一直到现在，基本上在百分之五点几。美国从1980年到2000年也是百分之五点几，2000年以来，一直到次贷危机才逐渐增加。2008年崩盘之前占GDP的百分之八点几。这几年约束了以后，现在是在7%左右。它是世界金融的中心，全世界的金融资源集聚在华尔街，集聚在美国，产生的金融增加值也就是7%，我们并没有把世界的金融资源增加值、效益利润集聚到中国来，中国的金融业为何能够占中国80多万亿元GDP的百分之八点几？中国在十余年前，也就是2005年的时候，金融增加值占当时GDP的5%不到，百分之四点几，快速增长恰恰是这些年异常扩张、高速发展的结果。这说明我们金融发达吗？不对，其实是脱实就虚，许多金融GDP把实体经济的利润转移过来，使得实体经济异常辛苦，从这个意义上说，这个指标是泡沫化的表现。</li>
<li>我们国家宏观经济的杠杆率。非银行非金融的企业负债，政府部门的负债(40%多)，加上居民部门的负债(50%)，三方面加起来是GDP的2.5倍，250%，在世界100多个国家里我们是前5位，是偏高的，我们跟美国相当，美国也是250%，日本是最高的，现在是440%，英国也比较高，当然欧洲一些国家，比如意大利或者西班牙，以及像希腊等一些债务财政出问题的小的国家，他们也异常的高。即使这样，我们的债务杠杆率排在世界前5位，也是异常的高。</li>
<li>每年全社会新增的融资。我们的企业每年都要融资，除了存量借新还旧，存量之外，有个增量，我们在十年前每年全社会新增融资量是五六万亿元，五年前新增的量在10万亿—12万亿元，2017年新增融资18万亿元。每年新增的融资里面，股权资本金性质的融资只占总的融资量的10%不到一点，也就是说91%是债权，要么是银行贷款，要么是信托，要么是小贷公司，或者直接是金融的债券。大家可以想象，如果每年新增的融资总是90%以上是债权，10%是股权的话，这个数学模型推它十年，十年以后中国的债务不会缩小，只会越来越高。</li>
</ul>
<blockquote>
<p><strong>2021年4月末，广义货币(M2)余额226.21万亿元,同比增长8.1%，增速分别比上月末和上年同期低1.3个和3个百分点</strong>；狭义货币(M1)余额60.54万亿元,同比增长6.2%，增速比上月末低0.9个百分点，比上年同期高0.7个百分点；流通中货币(M0)余额8.58万亿元,同比增长5.3%。当月净回笼现金740亿元。</p>
</blockquote>
<p>杠杆率是250%，在全世界来说是排在前面，是比较高的。这个指标里面又分成三个方面，其中政府的债务占GDP不到50%，国家统计公布的数据是40%多，但是有些隐性债务没算进去，就算算进去也不到50%。第二个方面是老百姓的债务，十年前还只占10%，五年前到了20%，我印象中有一年中国人民银行也说了，中国居民部门的债务还可以放一点杠杆，这两年按揭贷款异常发展起来，居民债务两年就上升到50%。老百姓这一块的债务，主要是房产的债务，也包括信用卡和其他投资，总的也占GDP50%左右。两个方面加起来就等于GDP，剩下的160%是企业债务，美国企业负债是美国GDP的60%，而中国企业的负债是GDP的160%，这个指标是有问题的</p>
<p>中国政府的40多万亿元债务是中央政府的债务有十几万亿元，地方政府的债务有20多万亿元，加在一起40多万亿元，占GDP 50%左右，我们是把区县、地市、省级政府到国家级统算在一起的。所以，我们中国政府的债务算得是比较充分的。</p>
<blockquote>
<p>人民网北京2021年4月7日电 （记者王震）国务院新闻办公室4月7日就贯彻落实“十四五”规划纲要，加快建立现代财税体制有关情况举行发布会。财政部部长助理欧文汉介绍，截至2020年末，地方政府债务余额25.66万亿元，控制在全国人大批准的限额28.81万亿元之内，加上纳入预算管理的中央政府债务余额20.89万亿元，全国政府债务余额46.55万亿元，政府债务余额与GDP的比重为45.8%，低于国际普遍认同的60%警戒线，风险总体可控。</p>
<p><img src="/images/951413iMgBlog/1626778550752-653e12e4-aa1c-4048-b72f-5c253ab560c9.png" alt="image.png"></p>
</blockquote>
<p>去企业负债杠杆方法：第一是坚定不移地把没有任何前途的、过剩的企业，破产关闭，伤筋动骨、壮士断腕，该割肿瘤就要割掉一块(5%)。第二是通过收购兼并，资产重组去掉一部分坏账，去掉一部分债务，同时又保护生产力(5%)。第三是优势的企业融资，股权融资从新增融资的10%增加到30%、40%、50%，这应该是一个要通过五年、十年实现的中长期目标。第四是柔性地、柔和地通货膨胀，稀释债务，一年2个点，五年就是10个点，也很可观。第五是在基本面上保持M2增长率和GDP增长率与物价指数增长率之和大体相当。我相信通过这五方面措施，假以时日，务实推进，那么有个三五年、近十年，我们去杠杆四五十个百分点的宏观目标就会实现。</p>
<p>怎么把股市融资和私募投资从10%上升到40%、50%，这就是我们中国投融资体制，金融体制要发生一个坐标转换。这里最重要的，实际上是两件事。第一件事，要把证券市场、资本市场搞好。十几年前上证指数两三千点，现在还是两三千点。美国股市指数翻了两番，香港股市指数也翻了两番。我国国民经济总量翻了两番，为什么股市指数不增长？这里面要害是什么呢？可以说有散户的结构问题，有长期资本缺乏的问题，有违规运作处罚不力的问题，有注册制不到位的问题，各种问题都可以说。归根到底最重要的一个问题是什么呢？就是退市制度没搞好。</p>
<h2 id="供应侧结构化改革三去一降一补"><a href="#供应侧结构化改革三去一降一补" class="headerlink" title="供应侧结构化改革三去一降一补"></a>供应侧结构化改革三去一降一补</h2><p>供应侧结构化改革三去一降一补：去产能、去库存、去杠杆、降成本、补短板</p>
<p>中国所有的货物运输量占GDP的比重是15%，美国、欧洲都在7%，日本只有百分之五点几。我们占15%就比其他国家额外多了几万亿元的运输成本。中国交通运输的物流成本高，除了基础设施很大一部分是新建投资、折旧成本较高以外，相当大的部分是管理体制造成的。由于我们的管理、软件、系统协调性、无缝对接等方面存在很多问题，造成了各种物流成本抬高。在这个问题上，各个地方，各个系统，各个行业都把这方面问题重视一下、协调一下，人家7%，我们哪怕降不到7%的GDP占比，能够降3%—4%的占比，就省了3万亿—4万亿元</p>
<p>按国际惯例，个人所得税率一般低于企业所得税率，我国的个人所得税采取超额累进税率与比例税率相结合的方式征收，工资薪金类为超额累进税率5%—45%。最高边际税率45%，是在1950年定的，当时我国企业所得税率是55%，个人所得税率定在45%有它的理由。现在企业所得税率已经降到25%，个人所得税率还保持在45%，明显高于前者，也高于大多数国家25%左右的水平。</p>
<p>到2018年底，中国个人住房贷款余额25.75万亿元，而公积金个人住房贷款余额为4.98万亿元，在整个贷款余额中不到20%，其为人们购房提供低息贷款的功能完全可以交由商业银行按揭贷款来解决。可以考虑公积金合并为年金</p>
<p>互联网金融平台、物联网金融平台、物联网+金融形成的平台会在这里起颠覆性的、全息性的、五个全方位信息（全产业链的信息、全流程的信息、全空间布局的信息、全场景的信息、全价值链的信息）的配置作用</p>
<p>“三元悖论”，即安全、廉价、便捷三者不可能同时存在</p>
<p>鉴于互联网商业平台公司的商业模式已经远远超出传统商业规模所能达成的社会影响力，所以，互联网商业平台公司与其说是在从事商业经营，不如说是在从事网络社会的经营和管理。正因如此，国家有必要通过立法，构建一种由网络安全、金融安全、社会安全、财政安全等相关部门参加的“互联网技术研发信息日常跟踪制度”。</p>
<h2 id="货币"><a href="#货币" class="headerlink" title="货币"></a>货币</h2><p>“二战”后建立的“布雷顿森林体系”，即“美元与黄金挂钩，其他国家货币与美元挂钩”的“双挂钩”制度，其实质也是一种“金本位”制度，而1971年美国总统尼克松宣布美元与黄金脱钩也正式标志着美元放弃了以黄金为本位的货币制度，随之采取的是“主权信用货币制”</p>
<p>近十余年来，美国为了摆脱金融危机，政府债务总量从2007年的9万亿美元上升到2019年的22万亿美元，已经超过美国GDP</p>
<p>从1970年开始，欧美、日本等大部分世界发达国家逐步采用了“<strong>主权信用货币制</strong>”，在实践中总体表现较好，货币的发行量与经济增量相匹配，保持了经济的健康增长和物价的稳定。<strong>这种货币制度通常以M3、经济增长率、通胀率、失业率等作为“间接锚”，并不是完全的无锚制货币。</strong></p>
<p>从1970年到2008年，美国政府在应用主权信用货币制度的过程中基本遵守货币发行纪律，货币的增长与GDP增长、政府债务始终保持适度的比例。1970年，美国基础货币约为700亿美元，2007年底约为8200亿美元，大约增长了12倍。与此同时，美国GDP从1970年的1.1万亿美元增长到2007年的14.5万亿美元，大概增长了13倍。美元在全世界外汇储备中的占比稳定在65%以上</p>
<p>从2008年年底至2014年10月，美联储先后出台三轮量化宽松政策，总共购买资产约3.9万亿美元。美联储持有的资产规模占国内生产总值的比例从2007年年底的6.1%大幅升至2014年年底的25.3%，资产负债表扩张到前所未有的水平。</p>
<p>从2008年到2019年，美国基础货币供应量从8200亿美元飙升到4万亿美元，整体约增长了5倍，与此同时，美国GDP仅增长了1.5倍，基础货币的发行增速几乎是同期GDP增速的3倍以上。在这种货币政策的驱动下，美国股市开启了十年长牛之路，股市从6000点涨到28000点。各类资产价格开始重新走上上涨之路，美国经济沉浸在一片欣欣向荣之中。</p>
<p>1913年美国《联邦储备法案》规定，美元的发行权归美联储所有。美国政府没有发行货币的权力，只有发行国债的权力。但实际上，美国政府可以通过发行国债间接发行货币。美国国会批准国债发行规模，财政部将设计好的不同种类的国债债券拿到市场上进行拍卖，最后拍卖交易中没有卖出去的由美联储照单全收，并将相应的美元现金交给财政部。这个过程中，财政部把国债卖给美联储取得现金，美联储通过买进国债获得利息，两全其美，皆大欢喜。</p>
<p>2008年前美国以国家信用为担保发行美债，美债余额始终控制在GDP的70%比例之内，国家信用良好。美债作为全世界交易规模最大的政府债券，长久以来保持稳定的收益，成为黄金以外另一种可靠的无风险资产。美国的货币供给总体上与世界经济的需求也保持着适当的比例，进一步加强了美元的信用。</p>
<p>美国GDP占全球GDP的比重已经从50%下降到24%，但美元仍然是主要的国际交易结算货币。尽管近年来美元在国际储备货币中的占比逐渐从70%左右滑落到62%，但美元的地位短时间内仍然看不到动摇的迹象</p>
<p>布雷顿森林体系解体后，各国以美元为货币“名义锚”的强制性随之弱化，但在自由选择条件下，绝大多数发展中国家仍然选择美元为“名义锚”，实行了锚定美元，允许一定浮动的货币调控制度。另有一些发展中国家选择锚定原来的宗主国，以德国马克、法国法郎和英镑等货币为“名义锚”。而主要的发达国家在货币寻锚的过程中，经历了一些波折之后，大多选择以“利率、货币发行量、通货膨胀率”等指标作为货币发行中间目标，实际上锚定的是国内资产。总之，从当前来看，世界的货币大致形成了两类发行制度：以“其他货币”为名义锚的货币发行体制和以“本国资产”为名义锚的货币发行体系，也称为主权信用货币制度。</p>
<p>香港采用的货币制度很独特，被称为“联系汇率”制度，又被称为“钞票局”制度，据说是19世纪一位英国爵士的发明。其基本内容是香港以某一种国际货币为锚（20世纪70年代以前以英镑为本位，80年代后改以美元为本位），即以该货币为储备发行港币，通过中央银行吞吐储备货币来达到稳定本币汇率的目标。在这种货币制度下，不仅需要储备相当规模的锚货币，其还有一个重大缺陷是必须放弃独立的货币政策，即本位货币加降息时，其也必须跟随。因此，这种货币制度只适用于小国或者小型经济体，对大国或大型经济体则不适用。</p>
<p>从新中国成立到如今，人民币发行制度经历了从“物资本位制”到“汇兑本位制”两个阶段，这两种不同时期实施的货币制度在当时都有效地促进了国民经济的发展。1995年以后，面对新的形势，中国人民银行探索通过改革实行了新的货币制度——“汇兑本位制”，即通过发行人民币对流入的外汇强制结汇，人民币汇率采取盯住美元的策略，从而保持人民币的汇率基本稳定。</p>
<p>在“汇兑本位制”下，我国主要有两种渠道完成人民币的发行。第一种，当外资到我国投资时，需要将其持有的外币兑换成人民币，这就是被结汇。结汇以后，在中国人民银行资产负债表上，一边增加了外汇资产，另一边增加了存款准备金的资产，这实际上就是基础货币发出的过程。中央银行的资产负债表中关于金融资产分为两部分，一部分是外币资产，另一部分是基础货币。基础货币包括M0和存款准备金，而这部分的准备金就是因为外汇占款而出现的。</p>
<p>第二种则是贸易顺差。中国企业由于进出口业务产生贸易顺差，实际上是外汇结余。企业将多余的外汇卖给商业银行，再由央行通过发行基础货币来购买商业银行收到的外汇。商业银行收到央行用于购买外汇的基础货币，就会通过M0流入社会。长此以往，就会增加通货膨胀的风险。央行为规避通货膨胀的风险，就会通过提高准备金率将多出的基础货币回收。</p>
<p>在“汇兑本位制”下，外汇储备可以视作人民币发行的基础或储备，且由于实行强制结汇，外汇占款逐渐成为我国基础货币发行的主要途径，到2013年末达到83%的峰值，此后略有下降，截至2019年7月末，外汇占款占中国人民银行资产总规模达到59.35%，说明有近六成人民币仍然通过外汇占款的方式发行。</p>
<blockquote>
<p><strong>现代货币理论</strong>（缩写<strong>MMT</strong>）是一种<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%9D%9E%E4%B8%BB%E6%B5%81%E7%B6%93%E6%BF%9F%E5%AD%B8">非主流</a>[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%8E%B0%E4%BB%A3%E8%B4%A7%E5%B8%81%E7%90%86%E8%AE%BA#cite_note-Heterodox_Views_of_Money_and_Modern_Monetary_Theory_(MMT):_Phil_Armstrong-1">1]</a><a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%AE%8F%E8%A7%82%E7%BB%8F%E6%B5%8E%E5%AD%A6">宏观经济学</a>理论，认为现代货币体系实际上是一种政府信用货币体系。[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%8E%B0%E4%BB%A3%E8%B4%A7%E5%B8%81%E7%90%86%E8%AE%BA#cite_note-2">2]</a> 现代货币理论即<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E4%B8%BB%E6%AC%8A%E5%9C%8B%E5%AE%B6">主权国家</a>的货币并不与任何商品和其他外币挂钩，只与未来<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%A8%8E%E6%94%B6">税收</a>与<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E5%85%AC%E5%80%BA">公债</a>相对应。[<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E7%8E%B0%E4%BB%A3%E8%B4%A7%E5%B8%81%E7%90%86%E8%AE%BA#cite_note-3">3]</a>因为主权货币具有<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E6%97%A0%E9%99%90%E6%B3%95%E5%81%BF">无限法偿</a>性质，没有名义预算约束，只存在<a target="_blank" rel="noopener" href="https://zh.wikipedia.org/wiki/%E9%80%9A%E8%B2%A8%E8%86%A8%E8%84%B9">通货膨胀</a>的实际约束。–基本上就是：主权信用货币制度</p>
</blockquote>
<p>“汇兑本位制”的实质：锚定美元</p>
<p>我国的人民币汇率制度基于两个环节。第一，人民币的汇率是人民币和外币之间的交换比率，是人民币与一篮子货币形成的一个比价。我国在同其他国家进行投资、贸易时，人民币按照汇率进行兑换。由于美元是目前世界上最主要的货币，所以虽然人民币与一篮子货币形成相对均衡的比价，但由于美元在一篮子货币中占有较大的比重，人民币最重要的比价货币是美元。第二，我国实行结汇制，即我国的商业银行、企业，基本上不能保存外汇，必须将收到的外汇卖给央行。因此，由于我国的货币发行的基础是外汇，而美元在我国的外汇占款、一篮子货币中占比较高，因此可以说人民币是间接锚定美元发行的。</p>
<p>截至2018年12月底，中国人民银行资产总规模为37.25万亿元，其中外汇占款达21.25万亿元。外汇占款在货币发行中的份额已经从2013年的83%降低至2008年初的57%左右。与此同时，央行对其他存款性公司债权迅速扩张，从2014年到2016年底扩张了2.4倍，占总资产份额从7.4%升至24.7%。这说明了随着外汇占款成为基础货币回笼而非投放的主渠道，央行主要通过公开市场操作和各类再贷款便利操作购买国内资产来投放货币，不失为在外汇占款不足的情况下供给货币的明智选择。同时，央行连续降低法定存款准备金率，提高了货币乘数，一定程度上也缓解了国内流动性不足的问题。</p>
<p>外汇占款在货币发行存量中的比重仍然接近60%，只是通过一些货币政策工具缓解了原来“汇兑本位制”的问题。一旦日后出现大量贸易顺差导致外汇储备增加，货币发行制度就会又回到老路上。</p>
<p>实施“主权信用货币制度”是大国崛起的必然选择</p>
<p>从根本上来说，税收是货币的信用，财政可以是货币发行的手段，而且是最高效公平的手段，央行买国债是能够自主收放的货币政策手段。一旦货币超发后，央行只需要提高利率、提高存款准备金率回收基础货币，而财政部门也可以通过增加税收、注销政府债券的方式来消除多余的货币、避免通货膨胀。</p>
<p>实际上，信用货币制度最大的问题在于锚的不清晰、不稳定，缺乏刚性。</p>
<blockquote>
<p><strong>特别提款权</strong>（Special Drawing Right，<strong>SDR</strong>），亦称“纸黄金”（Paper Gold），最早发行于1969年，是国际货币基金组织根据会员国认缴的份额分配的，可用于偿还国际货币基金组织债务、弥补会员国政府之间国际收支逆差的一种账面资产。 其价值由美元、欧元、人民币、日元和英镑组成的一篮子储备货币决定。</p>
</blockquote>
<p>主权信用货币背景下，人民币是由国债做锚的。中央银行为了发行基础货币，需要购买财政部发行的国债，但中央银行不能购买财政部为了弥补财政亏空发行的国债。《中华人民共和国中国人民银行法》规定，中国人民银行不得直接认购、包销国债和其他政府债券。这意味着中国人民银行不能以政府的债权作为抵押发行货币，只能参与国债二级市场的交易而不能参与国债一级市场的发行，央行直接购买国债来发行基础货币的方式就被法律禁止了。因此，建立以国债为基础的人民币发行制度，必须对相关法律法规进行修改。</p>
<h2 id="2017年5月-房地产"><a href="#2017年5月-房地产" class="headerlink" title="2017年5月 房地产"></a>2017年5月 房地产</h2><p>中国房地产和实体经济存在“十大失衡”——土地供求失衡、土地价格失衡、房地产投资失衡、房地产融资比例失衡、房地产税费占地方财力比重失衡、房屋销售租赁比失衡、房价收入比失衡、房地产内部结构失衡、房地产市场秩序失衡、政府房地产调控失衡</p>
<p>土地调控得当、法律制度到位、土地金融规范、税制结构改革和公租房制度保障，并特别强调了“地票制度”对盘活土地存量，提高耕地增量的重要意义</p>
<p>为国家粮食战略安全计，我国土地供应应逐步收紧，2015年供地770万亩，2016年700万亩，今年计划供应600万亩</p>
<p>国家每年批准供地中，约有三分之一用于农村建设性用地，比如水利基础设施、高速公路等，真正用于城市的只占三分之二，这部分又一分为三：55%左右用于各种基础设施和公共设施，30%左右给了工业，实际给房地产开发的建设用地只有15%。这是三分之二城市建设用地中的15%，摊到全部建设用地中只占到10%左右，这个比例是不平衡的</p>
<p>对于供过于求的商品，哪怕货币泛滥，也可能价格跌掉一半。货币膨胀只是房价上涨的必要条件而非充分条件，只是外部因素而非内部因素。内因只能是供需关系</p>
<p>住房作为附着在土地上的不动产，地价高房价必然会高，地价低房价自然会低，地价是决定房价的根本性因素。如果只有货币这个外因存在，地价这个内因不配合，房价想涨也是涨不起来的。控制房价的关键就是要控制地价。</p>
<p>拍卖机制，加上新供土地短缺，旧城改造循环，这三个因素相互叠加，地价就会不断上升—核心加大供地可解地价过高</p>
<p>按经济学的经验逻辑，一个城市的固定资产投资中房地产投资每年不应超过25%</p>
<p>正常情况下，一个家庭用于租房的支出最好不要超过月收入的六分之一，超过了就会影响正常生活。买房也如此，不能超过职工全部工作年限收入的六分之一，按每个人一生工作40年左右时间算，“6—7年的家庭年收入买一套房”是合理的。—-中国每个人体制外算20年工作时间，体制内可算35年</p>
<p>从均价看，一线城市北京、上海、广州、深圳、杭州等，房价收入比往往已到40年左右。这个比例在世界已经处于很高的水平了。考虑房价与居民收入比，必须高收入对高房价，低收入对低房价，均价对均价。有人说，纽约房子比上海还贵，伦敦海德公园的房价也比上海高。但伦敦城市居民的人均收入要高出上海几倍。就均价而言，伦敦房价收入比还是在10年以内。</p>
<p>每年固定资产投资不应超过GDP的60%。如果GDP有1万亿元，固定资产投资达到1.3万亿元甚至1.5万亿元，一年两年可以，长远就会不可持续。固定资产投资不超过GDP的60%，再按“房地产投资不超过固定资产投资的25%”，也符合“房地产投资不超过GDP六分之一”这一基本逻辑。</p>
<p>大陆31个省会城市和直辖市中，房地产投资连续多年占GDP 60%以上的有5个，占40%以上的有16个，显然偏高</p>
<blockquote>
<p>房地产17-18w亿,大概8.5w亿是直接留给卖地的地方政府<br>之后3-4w亿是各种建筑商供应商的辛苦钱<br>还有1w亿+流向的银行贷款的利息<br>1w亿+是各种非银金融机构,如信托和平安保险等<br>之后又是一轮税收,然后才是房地产商和房地产人<br>搞死房地产也许容易,但是你得指条明路,让这些人找到新地方做业务活着啊..</p>
<p><a target="_blank" rel="noopener" href="http://m.fangchan.com/data/13/2020-03-09/6642690492314489641.html#:~:text=%E8%AF%A5%E6%8A%A5%E5%91%8A%E6%98%BE%E7%A4%BA%3A,%E3%80%8119.3%%E5%92%8C18.8%%E3%80%82">上海易居房地产研究院3月9日发布《2019年区域房地产依赖度》。该报告显示</a>:房地产开发投资占GDP比重可以用来衡量当地经济对房地产的依赖程度，占比越高，说明经济对房地产的依赖度越高。2019年，房地产开发投资占GDP比重排名前三位的省市分别是海南、天津和重庆，占比分别为25.2%、19.3%和18.8%。</p>
<p>2020年杭州市GDP总量达到16106亿元，比2019年增长3.9%</p>
<p>有15个城市去年房地产开发投资额超过1000亿元，其中超过2000亿元的共8个，分别是杭州、郑州、广州、武汉、成都、南京、西安和昆明，杭州、郑州、广州三城更是超过了3000亿元。杭州以3397.27亿元在26城中位居榜首</p>
<p>2019年，除了杭州土地出让金继续领跑外，数据显示南京的卖地收入达到了1696.8亿元，同比增长77.32%，福州增幅为63.37%，昆明为59.85%，武汉为27.89%。去年土地出让金额没有超过1000亿元的城市中，合肥土地出让收入增长了33.32%，长沙增长了33.75%，贵阳增长了52%。</p>
<p>2020年，GDP前10强的城市依次为：上海、北京、深圳、广州、重庆、苏州、成都、杭州、武汉、南京。</p>
<p><img src="/images/951413iMgBlog/1627013968961-98b92fc1-b93c-47db-bbb2-03c84f5071c3.png" alt="image.png"></p>
</blockquote>
<p>2011年，全国人民币贷款余额54.8万亿元，其中房地产贷款余额10.7万亿元，占比不到20%。这一比例逐年走高，2016年全国106万亿元的贷款余额中，房地产贷款余额26.9万亿元，占比超过25%。也就是说，房地产占用了全部金融资金量的25%，而房地产贡献的GDP只有7%左右。2016年全国贷款增量的45%来自房地产，一些国有大型银行甚至70%—80%的增量是房地产。从这个意义上讲，房地产绑架了太多的金融资源，导致众多金融“活水”没有进入到实体经济，就是“脱实就虚”的具体表现。</p>
<p>这些年，中央加地方的全部财政收入中，房地产税费差不多占了35%，乍一看来，这一比例感觉还不高。但考虑到房地产税费属地方税、地方费，和中央财力无关，把房地产税费与地方财力相比较，则显得比重太高。全国10万亿元地方税中，有40%也就是4万亿是与房地产关联的，再加上土地出让金3.7万亿元，全部13万亿元左右的地方财政预算收入中就有近8万亿元与房地产有关（60%）。政府的活动太依赖房地产，地方政府财力离了房地产是会断粮的，这也是失衡的。</p>
<p>一般中等城市每2万元GDP造1平方米就够了，再多必过剩。对大城市而言，每平方米写字楼成本高一些，其资源利用率也会高一些，大体按每平方米4万元GDP来规划。</p>
<p>一个城市的土地供应总量一般可按每人100平方米来控制，这应该成为一个法制化原则。100万城市人口就供应100平方千米。爬行钉住，后发制人。</p>
<p>人均100平方米的城市建设用地，该怎么分配呢？不能都拿来搞基础设施、公共设施，也不能都拿来搞商业住宅。大体上，应该有55平方米用于交通、市政、绿地等基础设施和学校、医院、文化等公共设施，这是城市环境塑造的基本需要。对工业用地，应该控制在20平方米以内，每平方千米要做到100亿元产值。剩下的25平用于房地产开发</p>
<p>房产税应包括五个要点：（1）对各种房子存量、增量一网打尽，增量、存量一起收；（2）根据房屋升值额度计税，如果1%的税率，价值100万元的房屋就征收1万元，升值到500万元税额就涨到5万元；（3）越高档的房屋持有成本越高，税率也要相对提高；（4）低端的、中端的房屋要有抵扣项，使得全社会70%—80%的中低端房屋的交税压力不大；（5）房产税实施后，已批租土地70年到期后可不再二次缴纳土地出让金，实现制度的有序接替。这五条是房产税应考虑的基本原则。</p>
<p>房地产调控的长效机制：一是金融；二是土地；三是财税；四是投资；五是立法。</p>
<p>在1990年之前，中国是没有商品房交易的，那时候一年就是1000多万平方米。在1998年和1999年的时候，中国房地产一年新建房的交易销售量实际上刚刚达到1亿平方米。从1998年到2008年，这十年里平均涨了6倍，有的城市实际上涨到8倍以上，十年翻三番。2007年，销售量本来已经到了差不多7亿平方米，2008年全球金融危机发生了，在这个冲击下，中国的房产交易量也下降了，萎缩到6亿平方米。后来又过了5年，到了2012年前后，房地产的交易量翻了一番，从6亿平方米增长到12亿平方米。从2012年到2018年，又增加了5亿平方米。总之在过去的20年，中国房地产每年的新房销售交易量差不多从1亿平方米增长到17亿平方米，翻了四番多。</p>
<p>今后十几年，中国每年的房地产新房的交易量不仅不会继续增长翻番，还会每年小比例地有所萎缩，或者零增长，或者负增长。十几年以后，每年房地产的新房销售交易量可能下降到10亿平方米以内，大体上减少40%的总量。</p>
<p>今后十几年的房地产业发展趋势，不会是17亿平方米、18亿平方米、20亿平方米、30亿平方米，而是逐渐萎缩，当然这个萎缩不会在一年里面大规模萎缩20%、30%，大体上有十几年的过程，每年往下降。十几年后产生的销售量下降到10亿平方米以下</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="http://www.ce.cn/cysc/fdc/fc/202101/18/t20210118_36234860.shtml#:~:text=%E5%BD%93%E6%97%A5%E5%8F%91%E5%B8%83%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%BE%E7%A4%BA,%E7%9A%8415.97%E4%B8%87%E4%BA%BF%E5%85%83%E3%80%82">http://www.ce.cn/cysc/fdc/fc/202101/18/t20210118_36234860.shtml#:~:text&#x3D;%E5%BD%93%E6%97%A5%E5%8F%91%E5%B8%83%E7%9A%84%E6%95%B0%E6%8D%AE%E6%98%BE%E7%A4%BA,%E7%9A%8415.97%E4%B8%87%E4%BA%BF%E5%85%83%E3%80%82</a>:  <strong>2020</strong>年,<strong>中国</strong>商品房<strong>销售</strong>面积176086万平方米,比上年增长2.6%,2019年为下降0.1%。 商品房<strong>销售</strong>额173613亿元,增长8.7%,增速比上年提高2.2个百分点。 此前,<strong>中国</strong>商品房<strong>销售</strong>面积和<strong>销售</strong>额的最高纪录分为2018年的近17.17亿平方米和2019年的15.97万亿元。</p>
<p><img src="/images/951413iMgBlog/1627372037702-30d2c2b6-6c58-48a5-8891-071e918b47c1.png" alt="image.png"></p>
</blockquote>
<p>1990年，中国人均住房面积只有6平方米；到2000年，城市人均住房面积也仅十几平方米，现在城市人均住房面积已近50平方米。人均住房面积偏小，也会产生改善性的购房需求。</p>
<blockquote>
<p>根据国家统计局公布的数据，1982年至2019年，我国常住人口城镇<strong>化率</strong>从21.1%上升至60.6%，上升超过39个百分点；同期，户籍人口城镇<strong>化率</strong>仅从17.6%上升至44.4%，上升不到27个百分点。</p>
<p>经济日报-<strong>中国</strong>经济网北京2月28日讯国家统计局网站2月28日发布我国<strong>2020</strong>年国民经济和社会发展统计公报。 公报显示，<strong>2020</strong>年末，我国常住人口城镇<strong>化率</strong>超过60%。Feb 28, 2021</p>
<p><img src="/images/951413iMgBlog/1627371470209-2e0d97e4-bccc-4910-acc1-024da9f647af.png" alt="image.png"></p>
<p>官方数据显示，2020年，我国的城镇化率高达63.89%，比发达国家80%的平均水平低了16.11%，与美国82.7%的城镇化水平还有18.81%的距离。</p>
<p><img src="/images/951413iMgBlog/1627371544599-b6933e76-7231-4b80-882f-104e3767a7b3.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/1627371610846-ed83be5e-08f1-4685-ab5e-b4f04c706c20.png" alt="image.png"></p>
</blockquote>
<p>当前我国人均住房面积已经达到近50平方米</p>
<p>2012年，住建部下发了一个关于住宅和写字楼等各种商品性房屋的建筑质量标准，把原来中国住宅商品房30年左右的安全标准提升到了至少70年，甚至100年。这意味着从2010年以后，新建造的各种城市商品房，理论上符合质量要求的话，可以使用70年到100年，这也就是说老城市的折旧改造量会大量减少。</p>
<blockquote>
<p>实际据说12年后因为利润率的原因房子质量在下降？！待证</p>
</blockquote>
<p>中国各个省的省会城市大体上发展规律都会遵循“一二三四”的逻辑。所谓“一二三四”，就是这个省会城市往往占有这个省土地面积的10%不到，一般是5%—10%；但是它的人口一般会等于这个省总人口的20%；它的GDP有可能达到这个省总GDP的30%；它的服务业，不论是学校、医院、文化等政府主导的公共服务，还是金融、商业、旅游等市场化的服务业，一般会占到这个省总量的40%。</p>
<p>河南省有1亿人口，郑州目前只有1000万人口。作为省会城市，应承担全省20%的人口，所以十几年、20年以后郑州发展成2000万人口一点不用惊讶。同样的道理，郑州的GDP现在到了1万亿元，整个河南5万亿元，它贡献了20%，如果要30%的话应该是1.5万亿元，还相差甚远。对于服务业，一个地方每100万人应该有一个三甲医院，如果河南省1亿人口要有100个的话，郑州就应该有40个，它现在才9个三甲医院，每造一个三甲医院投资20多亿元，产生的营业额也是20多亿元，作为服务业，营业额对增加值贡献比率在80%以上。</p>
<p>大家可以关注现在近十个人口超过1000万的国家级超级大城市，根据这些省总的经济人口规模去算一下，它们都有十几年以后人口增长500万以上的可能。只要人口增长了，城市住宅房地产就会跟上去。所以我刚才说的大都市、超级大城市，人口在1000万—2000万之间的有一批城市还会扩张，过了2000万的，可能上面要封顶，但是在1000万—2000万之间的不会封顶，会形成它的趋势。</p>
<p>在今后的十几年，房地产开发不再是四处开花，而会相对集聚在省会城市及同等级区域性中心城市、都市圈中的中小城市和城市群中的大中型城市三个热点地区。</p>
<blockquote>
<p>根据住房和城乡建设部于2020年底最新公布的《<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/2019%E5%B9%B4%E5%9F%8E%E5%B8%82%E5%BB%BA%E8%AE%BE%E7%BB%9F%E8%AE%A1%E5%B9%B4%E9%89%B4/56070783">2019年城市建设统计年鉴</a>》，符合中国“超大城市”标准的共有<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E4%B8%8A%E6%B5%B7/114606">上海</a>、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%8C%97%E4%BA%AC/128981">北京</a>、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E9%87%8D%E5%BA%86/23586">重庆</a>、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B9%BF%E5%B7%9E/72101">广州</a>、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%B7%B1%E5%9C%B3/140588">深圳</a>、<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%A4%A9%E6%B4%A5/132308">天津</a>。</p>
<p>东莞、武汉、成都、杭州、南京、郑州、西安、济南、沈阳和青岛这10个<strong>城市</strong>的城区<strong>人口</strong>处于<strong>500万</strong>到1000<strong>万</strong>之间，属于特大<strong>城市</strong>。Jan 12, 2021</p>
<p>根据住建部最新数据，2019年底，<strong>长沙城区人口</strong>384.75万人，建成区面积377.95平方公里。 与2018年的374.43万人相比，2019年底<strong>长沙城区人口</strong>增加约10万人。 注意，这个<strong>城区</strong>的统计范围包括雨花、岳麓、芙蓉、天心、开福、望城六区，应该不包括<strong>长沙</strong>县，因为<strong>长沙</strong>县目前不是设区<strong>市</strong>，也不是县级<strong>市</strong>。Jan 26, 2021</p>
<p>长沙市总人口810万</p>
<p><img src="/images/951413iMgBlog/1627372812022-e9a7792a-234e-43f9-ae31-d25851761097.png" alt="image.png"></p>
<p><img src="/images/951413iMgBlog/1627372683037-b3f0c7df-c4c4-4b31-8ec1-7de1e4fe5f53.png" alt="image.png"></p>
</blockquote>
<p>从通货膨胀看，我国M2已经到了190万亿元，会不会今后十年M2再翻两番？不可能，这两年国家去杠杆、稳金融已经做到了让M2的增长率大体上等于GDP的增长率加物价指数，这几年的GDP增长率百分之六点几，物价指数加两个点，所以M2在2017年、2018年都是八点几，2019年1—6月份是8.5%，基本上是这样。M2如果是八点几的话，今后十几年，基本上是GDP增长率加物价指数，保持均衡的增长。如果中国的GDP今后十几年平均增长率大体在5%—6%，房地产价格的增长大体上不会超过M2的增长率，也不会超过GDP的增长率，一般会小于老百姓家庭收入的增长率。</p>
<p>9万多个房产企业中，排名在前的15%大开发商在去年的开发，实际的施工、竣工、销售的面积，在17亿平方米里面它们可能占了85%。意思是什么呢？15%的企业解决了17亿平方米的85%，就是14亿多平方米，剩下的企业只干了那么2亿多平方米，有大量的空壳公司。</p>
<p>中国的房地产企业，我刚才说9万多个，9万多个房产商的总负债率2018年是84%。中国前十位的销售规模在1万亿元左右的房产商，它们的负债率也是在81%。</p>
<blockquote>
<p>REITs: Real Estate Investment Trusts，译为房地产投资信托基金</p>
</blockquote>
<p>地王的产生都是因为房产商背后有银行，所以政府的土地部门，只要资格审查的时候查定金从哪儿来，拍卖的时候资金从哪儿来，只要审查管住这个，就一定能管住地王现象的出现</p>
<blockquote>
<p>2000年，中国房地产增加值仅为4141亿元，在当年GDP中占比4.1%。二十年后，2020年中国房地产增加值跃升至74553亿元，GDP占比7.3%。在20年时间里，房地产增加值大涨70412亿元，增长率达78%。</p>
<p>房地产增加值从1万亿元增加到2万亿元用了5年，从2万亿元到3万亿元用了3年，从5万亿元到7万亿年只用了4年。房地产新创造价值的增长速度越来越快。</p>
<p><img src="/images/951413iMgBlog/1627376113361-25fa29fb-1293-4a31-ad62-dabf28616be6.png" alt="image.png"></p>
</blockquote>
<blockquote>
<p>香港公租房面积：現時<strong>公</strong>屋單位的<strong>人均</strong>室內<strong>面積</strong>不得小於七平方米，惟房委會近年興建單位時，<strong>面積</strong>均僅停留在合格線，供一至二人入住的甲類單位，<strong>面積</strong>只及14平方米，二至三人的乙類單位也只有21平方米。 尤有甚者，有報章整理房委會資料，2020至2024年度的甲、乙類單位佔52%，總數3.44萬個，較跟2015至2019年度落成單位高11個百分點。Jan 19, 2021</p>
</blockquote>
<h2 id="对外开放"><a href="#对外开放" class="headerlink" title="对外开放"></a>对外开放</h2><p>近40年以来世界贸易的格局，国际贸易的产品结构、企业组织和管理的方式，国家和国家之间贸易有关的政策均发生了重要的变化。货物贸易中的中间品的比重上升到70%以上，在总贸易量中服务贸易的比重从百分之几变成了30%。</p>
<p>产品交易和贸易格局的变化，导致跨国公司的组织管理方式发生变化，谁控制着产业链的集群、供应链的纽带、价值链的枢纽，谁就是龙头老大。由于世界贸易格局特征的变化，由于跨国公司管理世界级的产品的管理模式的变化，也就是“三链”这种特征性的发展，引出了世界贸易新格局中的一个新的国际贸易规则制度的变化，就是零关税、零壁垒和零补助“三零”原则的提出，并将是大势所趋。中国自贸试验区的核心，也就是“三零”原则在自己这个区域里先行先试，等到国家签订FTA的时候，自贸试验区就为国家签订FTA提供托底的经验。</p>
<p>现在一个产品，涉及几千个零部件，由上千个企业在几百个城市、几十个国家，形成一个游走的逻辑链，那么谁牵头、谁在管理、谁把众多的几百个上千个中小企业产业链中的企业组织在一起，谁就是这个世界制造业的大头、领袖、集群的灵魂。</p>
<p>能提出行业标准、产品标准的企业往往是产品技术最大的发明者。谁控制供应链，谁其实就是供应链的纽带。你在组织整个供应链体系，几百个、上千个企业，都跟着你的指挥棒，什么时间、什么地点、到哪儿，一天的间隙都不差，在几乎没有零部件库存的背景下，几百个工厂，非常有组织、非常高效地在世界各地形成一个组合。在这个意义上讲，供应链的纽带也十分重要。</p>
<p>50年前关税平均是50%—60%。到了20世纪八九十年代，关税一般都降到了WTO要求的关税水平，降到了10%以下。WTO要求中国的关税也要下降。以前我们汽车进口关税最高达到170%。后来降到50%。现在我们汽车进口关税还在20%的水平。但我们整个中国的加权平均的关税率，20世纪八九十年代是在40%—50%，到了90年代末加入WTO的时候到了百分之十几。WTO给我们一个过渡期，要求我们15年内降到10%以内。我们到2015年的确降到9.5%，到去年已经降到7.5%。现在整个世界的贸易平均关税已经降到了5%以内，美国现在是2.5%。</p>
<blockquote>
<p>目前对于<strong>进口汽车</strong>收取的<strong>关税</strong>税率是25%，还有对<strong>进口</strong>车收取17%的增值税，而根据<strong>汽车</strong>的排量收取不同的消费税税率。 排量在在1.5升(含)以下3%，1.5升至2.0升(含) 5%，2.0升至2.5升(含) 9%，2.5升至3.0升(含)12%，3.0升至4.0升(含) 15%，4.0升以上20%。Jun 26, 2020</p>
<p><img src="/images/951413iMgBlog/xUmW-hcaquev3441323.png" alt="进口货物的增值税则在2018年5月1日进行了下调，由17%降为16%。"></p>
<p>假定这辆到岸价24万的进口车为中规进口车（原厂授权，4S店销售），排气量为4.0以上，且到岸时间为5月1日前，套入相关计算公式，则其所要缴纳税费为：</p>
<p>　　关税：24万×25% &#x3D;6万</p>
<p>　　消费税：（24万+6万）÷（1-40%）×40% &#x3D;20万</p>
<p>　　增值税：（24万+6万+20万）×17% &#x3D;8.5万</p>
<p>　　税费合计34.5万，加上24万的到岸价，总共58.5万，即这辆进口汽车的抵岸价。</p>
<p>　　常规而言，这个价格与90万指导价间的31.5万价差，即为运输等成本费用和国内经销商的利润。</p>
<p><img src="/images/951413iMgBlog/nBJ2-hcaquev3441409.png" alt="24万的进口车为何国内要卖90万？"></p>
</blockquote>
<p>在这七八年，FTA，双边贸易体的讨论，或者是一个地区，五六个国家、七八个国家形成一个贸易体的讨论就不断增加，成为趋势。给人感觉好像发达国家都在进行双边谈判，把WTO边缘化了</p>
<blockquote>
<p>所谓自由贸易协定（Free Trade Agrement:<strong>FTA</strong>）是指两个或两个以上的国家（包括独立关税地区）根据WTO相关规则，为实现相互之间的贸易自由化所进行的地区性贸易安排。 由自由贸易协定的缔约方所形成的区域称为自由贸易区。 <strong>FTA</strong>的传统<strong>含义</strong>是缔约国之间相互取消货物贸易关税和非关税贸易壁垒。</p>
<p>2019年10月8日，日本驻美国大使杉山晋辅与美国贸易谈判代表莱特希泽在白宫正式签署新日美贸易协定。美国总统特朗普不仅亲自出席见证签字，还邀请多位西北部农业州农民代表参加，声称自己为美国农民赢得了巨大市场，巧妙地将国际贸易协定变成了国内拉票筹码。</p>
</blockquote>
<p>中国已经形成了世界产业链里面最大的产业链集群，但是这个集群里面，我们掌控纽带的，掌控标准的，掌控结算枢纽的，掌控价值链枢纽的企业并不多。比如华为，华为的零部件，由3600多家大大小小供应链上的企业生产。这全球的3000多家企业每年都来开供应链大会。华为就是掌控标准。它的供应链企业比苹果多两倍。为什么？苹果主要做手机，华为既做手机又做服务器、通信设备。通信设备里面的零部件原材料更多。所以，它掌控产业链上中下游的集群，掌控标准，也掌控价值链中的牵制中枢。</p>
<p>零关税第一个好处：对进口中间品实行零关税，将降低企业成本，提高产品的国际竞争力。</p>
<p>当中国制造业实施零关税的时候，事实上对于整个制造业产业链的完整化、集群化和纽带、控制能力有好处，对于中国制造业的产业链、供应链、价值链在中国形成枢纽、形成纽带、形成集团的龙头等各方面会有提升作用，这是第二个好处。</p>
<p>关税下降，会促进中国的生产力结构的提升，促进我们企业的竞争能力的加强，使得我们工商企业的成本下降。</p>
<p>我们现在差不多有6.6亿吨农作物粮食是在中国的土地上生产出来的，但是我们现在每年要进口农产品1亿吨。加在一起，也就是中国14亿人，一年要吃7.6亿吨农作物。这1亿吨里面，有个基本的分类。我们现在进口的1亿吨里面，有8000多万吨进口的是大豆、300多万吨小麦、300多万吨玉米、300多万吨糖，另外就是进口的猪肉、牛肉和其他的肉类，也有几百万吨。</p>
<p>从2010年起步，当年人民币只有近千亿元的结算量，从这些年发展来看，2018年已经到7万亿元了。也就是说，中国进出口贸易里面有7万亿元人民币是人家收了人民币而不去收美元</p>
<h3 id="自贸区"><a href="#自贸区" class="headerlink" title="自贸区"></a>自贸区</h3><p>在过去的40年，我们国家的开放有五个基本特点：</p>
<p>第一个就是以出口导向为基础，利用国内的资源优势和劳动力的比较优势，推动出口发展，带动中国经济更好地发展；</p>
<p>第二个就是以引进外资为主，弥补我们中国当时还十分贫困的经济和财力；</p>
<p>第三个就是以沿海开放为主，各种开发区或者各种特区，包括新区、保税区，都以沿海地区先行，中西部内陆逐步跟进；</p>
<p>第四个就是开放的领域主要是工业、制造业、房地产业、建筑业等先行开放，至于服务业、服务贸易、金融保险业务开放的程度比较低，即以制造业、建筑业等第二产业开放为主；</p>
<p>第五个就是我们国家最初几十年的开放以适应国际经济规则为主，用国际经济规则、国际惯例倒逼国内营商环境改革、改善，倒逼国内的各种机制体制变化，是用开放倒逼改革的这样一个过程。</p>
<p>2012年以后我们每年退休的人员平均在1500万人左右，但每年能够上岗的劳动力，不管农村的、城市的，新生的劳动力是1200万左右。实际最近五年，我们每年少了300万劳动力补充。</p>
<p>本来GDP应该每掉1个点退出200万就业人口。为什么几年下来没有感觉到有500万、1000万下岗工人出现呢？就是因为人口出现了对冲性均衡，正好这边下降，要退出人员，跟那边补充的人员不足，形成了平衡，所以实际上我们基础性劳动力条件发生了变化，人口红利逐步退出。</p>
<p>如果一个国家在五到十年里，连续都是世界第一、第二、第三的进口大国，那一定成为世界经济的强国。要知道进口大国是和经济强国连在一起的，美国是世界第一大进口国，它也理所当然是世界最大的经济强国。</p>
<h2 id="中美贸易战"><a href="#中美贸易战" class="headerlink" title="中美贸易战"></a>中美贸易战</h2><p>关于中国加入WTO，莱特希泽有五个观点：一是中国入世美国吃亏论；二是中国没有兑现入世承诺；三是中国强制美国企业转让技术；四是中国的巨额外汇顺差造成了美国2008年的金融危机；五是中国买了大量美国国债，操纵了汇率。</p>
<p>2008年美国金融危机原因是2001年科技互联网危机后，当时股市一年里跌了50%以上，再加上“9·11”事件，美国政府一是降息，从6%降到1%，二是采取零按揭刺激房地产，三是将房地产次贷在资本市场1∶20加杠杆搞CDS，最终导致泡沫经济崩盘。2007年，美国房地产总市值24.3万亿美元、占GDP比重达到173%；股市总市值达到了20万亿美元、占GDP比重达到135%。2008年金融危机后，美国股市缩水50%，剩下10万亿美元左右；房地产总市值缩水40%，从2008年的25万亿美元下降到2009年的15万亿美元。</p>
<p>一个成熟的经济体，政府每年总有占GDP　20%—30%的财政收入要支出使用，通常会生成15%左右的GDP，这部分国有经济产生的GDP是通过政府的投资和消费产生的，美国和欧洲各国都是如此。比如2017年，美国的GDP中有13.5%是美国政府财力支出形成的GDP。中国政府除了财政税收以外，还有土地批租等预算外收入，所以，中国政府财力支出占GDP的比重相对高一点，占17%左右</p>
<p>自1971年布雷顿森林体系解体，美元脱离了金本位，形成“无锚货币”，美元的货币发行体制转化为政府发债，美联储购买发行基础货币之后，全球的基础货币总量如脱缰野马，快速增长。从1970年不到1000亿美元，到1980年的3500亿美元，到1990年的7000亿美元，到2000年的1.5万亿美元，到2008年的4万亿美元，到2017年的21万亿美元。其中，美元的基础货币也从20世纪70年代的几百亿美元发展到今天的6万亿美元。</p>
<blockquote>
<p>报告还预计，截至2021财年底，<strong>美国</strong>联邦公共<strong>债务</strong>将达23万亿美元，约占<strong>美国GDP</strong>的103%；到2031财年，<strong>美国</strong>联邦公共<strong>债务</strong>占<strong>GDP</strong>的比重将进一步升至106%。Jul 2, 2021</p>
</blockquote>
<blockquote>
<p><img src="/images/951413iMgBlog/image-20210729172410379.png" alt="image-20210729172410379"></p>
<p>以下资料来源：<a target="_blank" rel="noopener" href="https://pdf.dfcfw.com/pdf/H3_AP202106171498408427_1.pdf?1623944100000.pdf">https://pdf.dfcfw.com/pdf/H3_AP202106171498408427_1.pdf?1623944100000.pdf</a></p>
<p><img src="/images/951413iMgBlog/image-20210729172820145.png" alt="image-20210729172820145"></p>
<p><img src="/images/951413iMgBlog/image-20210729173011549.png" alt="image-20210729173011549"></p>
<p><img src="/images/951413iMgBlog/image-20210729173116617.png" alt="image-20210729173116617"></p>
</blockquote>
<p>2020-01</p>
<p>上市公司几千家，几十家金融企业每年利润几乎占了几千家实体经济企业利润的50%，这个比重太高，造成我们脱实向虚。三是金融企业占GDP的比重是百分之八点几，是全世界最高的。世界平均金融业GDP占全球GDP的5%左右，欧洲也好、美国也好、日本也好，只要一到7%、8%，就会冒出一场金融危机，自己消除坏账后萎缩到5%、6%，过了几年，又扩张达到7%、8%，又崩盘。</p>
<blockquote>
<p>SWIFT 是 Society for Worldwide Interbank Financial Telecommunications 的缩写，翻译成中文叫做「环球银行金融电讯协会」。看名字就知道，他是个搞通讯的，还冠冕堂皇的是一个非盈利性组织。</p>
<p>另外，SWIFT 官网上是这么用中文介绍自己的：SWIFT 为社群提供报文传送平台和通信标准，并在连接、集成、身份识别、数据分析和合规等领域的产品和服务（我一字未改，这多语言做的……语句都不通顺，好在不影响理解）；用英文则是这么介绍的：SWIFT is a global member-owned cooperative and the world’s leading provider of secure financial messaging services。</p>
</blockquote>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2020/01/26/TCP%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2020/01/26/TCP%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/" class="post-title-link" itemprop="url">TCP相关参数解释</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-01-26 17:30:03" itemprop="dateCreated datePublished" datetime="2020-01-26T17:30:03+08:00">2020-01-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="TCP相关参数解释"><a href="#TCP相关参数解释" class="headerlink" title="TCP相关参数解释"></a>TCP相关参数解释</h1><p>读懂TCP参数前得先搞清楚内核中出现的HZ、Tick、Jiffies三个值是什么意思</p>
<h2 id="HZ"><a href="#HZ" class="headerlink" title="HZ"></a>HZ</h2><p>它可以理解为1s，所以120*HZ就是120秒，HZ&#x2F;5就是200ms。</p>
<p>HZ表示CPU一秒种发出多少次时间中断–IRQ-0，Linux中通常用HZ来做时间片的计算（<a target="_blank" rel="noopener" href="http://blog.csdn.net/bdc995/article/details/4144031">参考</a>）。</p>
<p>这个值在内核编译的时候可设定100、250、300或1000，一般设置的是1000</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#cat /boot/config-`uname -r` |grep &#x27;CONFIG_HZ=&#x27;</span><br><span class="line">CONFIG_HZ=1000 //一般默认1000, Linux核心每隔固定周期会发出timer interrupt (IRQ 0)，HZ是用来定义</span><br><span class="line">每一秒有几次timer interrupts。举例来说，HZ为1000，代表每秒有1000次timer interrupts</span><br></pre></td></tr></table></figure>

<p>HZ的设定：<br>#make menuconfig<br>processor type and features—&gt;Timer frequency (250 HZ)—&gt;</p>
<p>HZ的不同值会影响timer （节拍）中断的频率</p>
<h2 id="Tick"><a href="#Tick" class="headerlink" title="Tick"></a>Tick</h2><p>Tick是HZ的倒数，意即timer interrupt每发生一次中断的间隔时间。如HZ为250时，tick为4毫秒(millisecond)。</p>
<h2 id="Jiffies"><a href="#Jiffies" class="headerlink" title="Jiffies"></a>Jiffies</h2><p>Jiffies为Linux核心变数(32位元变数，unsigned long)，它被用来记录系统自开机以来，已经过多少的tick。每发生一次timer interrupt，Jiffies变数会被加一。值得注意的是，Jiffies于系统开机时，并非初始化成零，而是被设为-300*HZ (arch&#x2F;i386&#x2F;kernel&#x2F;time.c)，即代表系统于开机五分钟后，jiffies便会溢位。那溢出怎么办?事实上，Linux核心定义几个macro(timer_after、time_after_eq、time_before与time_before_eq)，即便是溢位，也能藉由这几个macro正确地取得jiffies的内容。</p>
<p>另外，80x86架构定义一个与jiffies相关的变数jiffies_64 ，此变数64位元，要等到此变数溢位可能要好几百万年。因此要等到溢位这刻发生应该很难吧。那如何经由jiffies_64取得jiffies呢?事实上，jiffies被对应至jiffies_64最低的32位元。因此，经由jiffies_64可以完全不理会溢位的问题便能取得jiffies。</p>
<h2 id="数据取自于4-19内核代码中的-include-net-tcp-h"><a href="#数据取自于4-19内核代码中的-include-net-tcp-h" class="headerlink" title="数据取自于4.19内核代码中的 include&#x2F;net&#x2F;tcp.h"></a>数据取自于4.19内核代码中的 include&#x2F;net&#x2F;tcp.h</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">//rto的定义，不让修改，到每个ip的rt都不一样，必须通过rtt计算所得, HZ 一般是1000</span><br><span class="line">#define TCP_RTO_MAX     ((unsigned)(120*HZ))</span><br><span class="line">#define TCP_RTO_MIN     ((unsigned)(HZ/5)) //在rt很小的环境中计算下来RTO基本等于TCP_RTO_MIN</span><br><span class="line"></span><br><span class="line">/* Maximal number of ACKs sent quickly to accelerate slow-start. */</span><br><span class="line">#define TCP_MAX_QUICKACKS       16U //默认前16个ack必须quick ack来加速慢启动</span><br><span class="line"></span><br><span class="line">//默认delay ack不能超过200ms</span><br><span class="line">#define TCP_DELACK_MAX  ((unsigned)(HZ/5))  /* maximal time to delay before sending an ACK */</span><br><span class="line">#if HZ &gt;= 100</span><br><span class="line">//默认 delay ack 40ms，不能修改和关闭</span><br><span class="line">#define TCP_DELACK_MIN  ((unsigned)(HZ/25))     /* minimal time to delay before sending an ACK */</span><br><span class="line">#define TCP_ATO_MIN     ((unsigned)(HZ/25))</span><br><span class="line">#else</span><br><span class="line">#define TCP_DELACK_MIN  4U</span><br><span class="line">#define TCP_ATO_MIN     4U</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">#define TCP_SYNQ_INTERVAL       (HZ/5)  /* Period of SYNACK timer */</span><br><span class="line">#define TCP_KEEPALIVE_TIME      (120*60*HZ)     /* two hours */</span><br><span class="line">#define TCP_KEEPALIVE_PROBES    9               /* Max of 9 keepalive probes    */</span><br><span class="line">#define TCP_KEEPALIVE_INTVL     (75*HZ)</span><br><span class="line"></span><br><span class="line">/* cwnd init 默认大小是10个拥塞窗口，也可以通过sysctl_tcp_init_cwnd来设置，要求内核编译的时候支持*/</span><br><span class="line">#if IS_ENABLED(CONFIG_TCP_INIT_CWND_PROC)</span><br><span class="line">extern u32 sysctl_tcp_init_cwnd;</span><br><span class="line">/* TCP_INIT_CWND is rvalue */</span><br><span class="line">#define TCP_INIT_CWND           (sysctl_tcp_init_cwnd + 0)</span><br><span class="line">#else</span><br><span class="line">/* TCP initial congestion window as per rfc6928 */</span><br><span class="line">#define TCP_INIT_CWND           10</span><br><span class="line">#endif</span><br><span class="line"></span><br><span class="line">/* Flags in tp-&gt;nonagle 默认nagle算法关闭的*/</span><br><span class="line">#define TCP_NAGLE_OFF           1       /* Nagle&#x27;s algo is disabled */</span><br><span class="line">#define TCP_NAGLE_CORK          2       /* Socket is corked         */</span><br><span class="line">#define TCP_NAGLE_PUSH          4       /* Cork is overridden for already queued data */</span><br><span class="line"></span><br><span class="line">//对应time_wait, alios 增加了tcp_tw_timeout 参数可以来设置这个值，当前网络质量更好了这个值可以减小一些</span><br><span class="line">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT</span><br><span class="line">                                  * state, about 60 seconds     */</span><br><span class="line">                                  </span><br><span class="line">#define TCP_SYN_RETRIES  6      /* This is how many retries are done</span><br><span class="line">                                 * when active opening a connection.</span><br><span class="line">                                 * RFC1122 says the minimum retry MUST</span><br><span class="line">                                 * be at least 180secs.  Nevertheless</span><br><span class="line">                                 * this value is corresponding to</span><br><span class="line">                                 * 63secs of retransmission with the</span><br><span class="line">                                 * current initial RTO.</span><br><span class="line">                                 */</span><br><span class="line"></span><br><span class="line">#define TCP_SYNACK_RETRIES 5    /* This is how may retries are done</span><br><span class="line">                                 * when passive opening a connection.</span><br><span class="line">                                 * This is corresponding to 31secs of</span><br><span class="line">                                 * retransmission with the current</span><br><span class="line">                                 * initial RTO.</span><br><span class="line">                                 */                                  </span><br></pre></td></tr></table></figure>

<p>rto 不能设置，而是根据到不同server的rtt计算得到，即使RTT很小（比如0.8ms），但是因为RTO有下限，最小必须是200ms，所以这是RTT再小也白搭；RTO最小值是内核编译是决定的，socket程序中无法修改，Linux TCP也没有任何参数可以改变这个值。</p>
<h3 id="delay-ack"><a href="#delay-ack" class="headerlink" title="delay ack"></a>delay ack</h3><p>正常情况下ack可以quick ack也可以delay ack，redhat在sysctl中可以设置这两个值</p>
<blockquote>
<p>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_ato_min</p>
</blockquote>
<p>默认都是推荐delay ack的，一定要修改成quick ack的话（3.10.0-327之后的内核版本）：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$sudo ip route show</span><br><span class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024</span><br><span class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</span><br><span class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</span><br><span class="line"></span><br><span class="line">$sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</span><br><span class="line"></span><br><span class="line">$sudo ip route show</span><br><span class="line">default via 10.0.207.253 dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</span><br><span class="line">10.0.192.0/20 dev eth0 proto kernel scope link src 10.0.200.23</span><br><span class="line">10.0.207.253 dev eth0 proto dhcp scope link src 10.0.200.23 metric 1024</span><br></pre></td></tr></table></figure>

<p>默认开启delay ack的抓包情况如下，可以清晰地看到有几个40ms的ack</p>
<p><img src="/images/oss/7f4590cccf73fd672268dbf0e6a1b309.png" alt="image.png"></p>
<p>第一个40ms 的ack对应的包， 3306收到 update请求后没有ack，而是等了40ms update也没结束，就ack了</p>
<p><img src="/images/oss/b06d3148450fc24fa26b2a9cdfe07831.png" alt="image.png"></p>
<p>同样的机器，执行quick ack后的抓包</p>
<blockquote>
<p>sudo ip route change default via 10.0.207.253  dev eth0 proto dhcp src 10.0.200.23 metric 1024 quickack 1</p>
</blockquote>
<p><img src="/images/oss/9fba9819e769494bc09a2a11245e4769.png" alt="image.png"></p>
<p><strong>同样场景下，改成quick ack后基本所有的ack都在0.02ms内发出去了。</strong></p>
<p>比较奇怪的是在delay ack情况下不是每个空ack都等了40ms，这么多包只看到4个delay了40ms，其它的基本都在1ms内就以空包就行ack了。</p>
<p>将 quick ack去掉后再次抓包仍然抓到了很多的40ms的ack。</p>
<p>Java中setNoDelay是指关掉nagle算法，但是delay ack还是存在的。</p>
<p>C代码中关闭的话：At the application level with the <code>TCP_QUICKACK</code> socket option. See <code>man 7 tcp</code> for further details. This option needs to be set with <code>setsockopt()</code> after each operation of TCP on a given socket</p>
<p>连接刚建立前16个包一定是quick ack的，目的是加快慢启动</p>
<p>一旦后面进入延迟ACK模式后，<a target="_blank" rel="noopener" href="https://www.cnblogs.com/lshs/p/6038635.html">如果接收的还没有回复ACK确认的报文总大小超过88bytes的时候就会立即回复ACK报文</a>。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/407743">https://access.redhat.com/solutions/407743</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/lshs/p/6038635.html">https://www.cnblogs.com/lshs/p/6038635.html</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <a class="extend prev" rel="prev" title="上一页" aria-label="上一页" href="/page/9/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><span class="page-number current">10</span><a class="page-number" href="/page/11/">11</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/11/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
