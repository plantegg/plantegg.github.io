<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="java mysql tcp performance network docker Linux">
<meta property="og:type" content="website">
<meta property="og:title" content="plantegg">
<meta property="og:url" content="https://plantegg.github.io/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="java mysql tcp performance network docker Linux">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="技术,编程,博客">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://plantegg.github.io/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">plantegg</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">184</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">274</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2117/06/07/%E5%85%B3%E4%BA%8E%E6%9C%AC%E5%8D%9A/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2117/06/07/%E5%85%B3%E4%BA%8E%E6%9C%AC%E5%8D%9A/" class="post-title-link" itemprop="url">关于本博</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2117-06-07 18:30:03" itemprop="dateCreated datePublished" datetime="2117-06-07T18:30:03+08:00">2117-06-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/others/" itemprop="url" rel="index"><span itemprop="name">others</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h2 id="关于本博"><a href="#关于本博" class="headerlink" title="关于本博"></a>关于本博</h2><p>find me on twitter: <a target="_blank" rel="noopener" href="https://twitter.com/plantegg">@plantegg</a></p>
<p>知识星球：<a target="_blank" rel="noopener" href="https://t.zsxq.com/0cSFEUh2J">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>Github: <a target="_blank" rel="noopener" href="https://github.com/plantegg/programmer_case">欢迎star</a> </p>
<p>关注基础知识，一次把问题搞清楚，从案例出发深挖相关知识。</p>
<p>以前觉得自己一看就懂，实际是一问就打鼓，一用就糊涂。所以现在开始记录并总结再联系案例，一般是先把零散知识记录下来（看到过），慢慢地相关知识积累更多，直到碰到实践案例或是有点领悟到于是发现这块知识可以整理成一篇系统些的文章（基本快懂了）。</p>
<p>“技术变化太快，容易过时”，我的看法是网络知识、操作系统、计算机原理等核心概念知识的寿命会比你的职业生涯还长。这些都是40岁之后还会还会很有用</p>
<p><a href="https://plantegg.github.io/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/">如何在工作中学习</a> 所有方法我都记录在这篇文章中了，希望对你能有所帮助。</p>
<p>所有新文章从<a href="https://plantegg.github.io/archives">这里可以看到</a>，即使再简单的一篇总结我可以持续总结三五年，有新的发现、感悟都是直接在原文上增减，不会发表新的文章。</p>
<p><img src="/images/951413iMgBlog/image-20220421102225491.png" alt="image-20220421102225491"></p>
<p>为什么写博客而不是公众号，我见过20年前的互联网，深度依赖搜索引擎，所以还是喜欢博客。另外技术类文章更适合电脑阅读（随时摘录、实验）</p>
<h2 id="精华文章推荐"><a href="#精华文章推荐" class="headerlink" title="精华文章推荐"></a>精华文章推荐</h2><h4 id="在2010年前后MySQL、PG、Oracle数据库在使用NUMA的时候碰到了性能问题，流传最广的这篇-MySQL-–-The-MySQL-“swap-insanity”-problem-and-the-effects-of-the-NUMA-architecture-http-blog-jcole-us-2010-09-28-mysql-swap-insanity-and-the-numa-architecture-文章描述了性能问题的原因-文章中把原因找错了-以及解决方案：关闭NUMA。-实际这个原因是kernel实现的一个低级bug，这个Bug在2014年修复了https-github-com-torvalds-linux-commit-4f9b16a64753d0bb607454347036dc997fd03b82，但是修复这么多年后仍然以讹传讹，这篇文章希望正本清源、扭转错误的认识。"><a href="#在2010年前后MySQL、PG、Oracle数据库在使用NUMA的时候碰到了性能问题，流传最广的这篇-MySQL-–-The-MySQL-“swap-insanity”-problem-and-the-effects-of-the-NUMA-architecture-http-blog-jcole-us-2010-09-28-mysql-swap-insanity-and-the-numa-architecture-文章描述了性能问题的原因-文章中把原因找错了-以及解决方案：关闭NUMA。-实际这个原因是kernel实现的一个低级bug，这个Bug在2014年修复了https-github-com-torvalds-linux-commit-4f9b16a64753d0bb607454347036dc997fd03b82，但是修复这么多年后仍然以讹传讹，这篇文章希望正本清源、扭转错误的认识。" class="headerlink" title="在2010年前后MySQL、PG、Oracle数据库在使用NUMA的时候碰到了性能问题，流传最广的这篇  MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/ 文章描述了性能问题的原因(文章中把原因找错了)以及解决方案：关闭NUMA。 实际这个原因是kernel实现的一个低级bug，这个Bug在2014年修复了https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82，但是修复这么多年后仍然以讹传讹，这篇文章希望正本清源、扭转错误的认识。"></a><a href="https://plantegg.github.io/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">在2010年前后MySQL、PG、Oracle数据库在使用NUMA的时候碰到了性能问题，流传最广的这篇  MySQL – The MySQL “swap insanity” problem and the effects of the NUMA architecture http://blog.jcole.us/2010/09/28/mysql-swap-insanity-and-the-numa-architecture/ 文章描述了性能问题的原因(文章中把原因找错了)以及解决方案：关闭NUMA。 实际这个原因是kernel实现的一个低级bug，这个Bug在2014年修复了https://github.com/torvalds/linux/commit/4f9b16a64753d0bb607454347036dc997fd03b82，但是修复这么多年后仍然以讹传讹，这篇文章希望正本清源、扭转错误的认识。</a></h4><p><img src="/images/951413iMgBlog/image-20210517082233798.png" alt="image-20210517082233798"></p>
<h4 id="CPU的制造和概念-从最底层的沙子开始用8篇文章来回答关于CPU的各种疑问以及大量的实验对比案例和测试数据来展示了CPU的各种原理，比如多核、超线程、NUMA、睿频、功耗、GPU、大小核再到分支预测、cache-line失效、加锁代价、IPC等各种指标（都有对应的代码和测试数据）。"><a href="#CPU的制造和概念-从最底层的沙子开始用8篇文章来回答关于CPU的各种疑问以及大量的实验对比案例和测试数据来展示了CPU的各种原理，比如多核、超线程、NUMA、睿频、功耗、GPU、大小核再到分支预测、cache-line失效、加锁代价、IPC等各种指标（都有对应的代码和测试数据）。" class="headerlink" title="CPU的制造和概念 从最底层的沙子开始用8篇文章来回答关于CPU的各种疑问以及大量的实验对比案例和测试数据来展示了CPU的各种原理，比如多核、超线程、NUMA、睿频、功耗、GPU、大小核再到分支预测、cache_line失效、加锁代价、IPC等各种指标（都有对应的代码和测试数据）。"></a><a href="https://plantegg.github.io/2021/06/01/CPU%E7%9A%84%E5%88%B6%E9%80%A0%E5%92%8C%E6%A6%82%E5%BF%B5/">CPU的制造和概念</a> 从最底层的沙子开始用8篇文章来回答关于CPU的各种疑问以及大量的实验对比案例和测试数据来展示了CPU的各种原理，比如多核、超线程、NUMA、睿频、功耗、GPU、大小核再到分支预测、cache_line失效、加锁代价、IPC等各种指标（都有对应的代码和测试数据）。</h4><p><img src="/images/951413iMgBlog/image-20210802161410524-1011377.png" alt="image-20210802161410524"> </p>
<h4 id="《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"><a href="#《Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》-从一个参数引起的rt抖动定位到OS锁等待再到CPU-Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大" class="headerlink" title="《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大"></a><a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/">《Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的》 从一个参数引起的rt抖动定位到OS锁等待再到CPU Pause指令，以及不同CPU型号对Pause使用cycles不同的影响，最终反馈到应用层面的rt全过程。在MySQL内核开发的时候考虑了Pause，但是没有考虑不同的CPU型号，所以换了CPU型号后性能差异比较大</a></h4><p><img src="/images/oss/d567449fe52725a9d0b9d4ec9baa372c.png" alt="image.png"></p>
<h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一次全栈性能优化过程的详细记录和分析。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一次全栈性能优化过程的详细记录和分析。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一次全栈性能优化过程的详细记录和分析。"></a><a href="https://plantegg.github.io/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，一次全栈性能优化过程的详细记录和分析。</h4><p><img src="/images/oss/05703c168e63e96821ea9f921d83712b.png" alt="image.png"></p>
<h4 id="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"><a href="#就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET"></a><a href="https://plantegg.github.io/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/">就是要你懂TCP–半连接队列和全连接队列：偶发性的连接reset异常、重启服务后短时间的连接异常，通过一篇文章阐明TCP连接的半连接队列和全连接队大小是怎么影响连接创建的，以及用什么工具来观察队列有没有溢出、连接为什么会RESET</a></h4><img src="/images/oss/1579241362064-807d8378-6c54-4a2c-a888-ff2337df817c.png" alt="image.png" style="zoom:80%;" />



<h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的"></a><a href="https://plantegg.github.io/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a></h4><p><img src="/images/oss/e177d59ecb886daef5905ed80a84dfd2.png"></p>
<h4 id="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"><a href="#就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。-同时可以跟讲这块的RFC1180比较一下，RFC1180-写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90-的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用" class="headerlink" title="就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。  同时可以跟讲这块的RFC1180比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用"></a><a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">就是要你懂网络–一个网络包的旅程：教科书式地阐述书本中的路由、网关、子网、Mac地址、IP地址是如何一起协作让网络包最终传输到目标机器上。</a>  同时可以跟讲这块的<a target="_blank" rel="noopener" href="https://tools.ietf.org/html/rfc1180">RFC1180</a>比较一下，RFC1180 写的确实很好，清晰简洁，图文并茂，结构逻辑合理，但是对于90%的程序员没有什么卵用，看完几周后就忘得差不多，因为他不是从实践的角度来阐述问题，中间没有很多为什么，所以一般资质的程序员看完当时感觉很好，实际还是不会灵活运用</h4><p><img src="/images/oss/8f5d8518c1d92ed68d23218028e3cd11.png"></p>
<h4 id="国产CPU和Intel、AMD性能PK-从Intel、AMD、海光、鲲鹏920、飞腾2500-等CPU在TPCC、sysbench下的性能对比来分析他们的性能差距，同时分析内存延迟对性能的影响"><a href="#国产CPU和Intel、AMD性能PK-从Intel、AMD、海光、鲲鹏920、飞腾2500-等CPU在TPCC、sysbench下的性能对比来分析他们的性能差距，同时分析内存延迟对性能的影响" class="headerlink" title="国产CPU和Intel、AMD性能PK 从Intel、AMD、海光、鲲鹏920、飞腾2500 等CPU在TPCC、sysbench下的性能对比来分析他们的性能差距，同时分析内存延迟对性能的影响"></a><a href="https://plantegg.github.io/2022/01/13/%E4%B8%8D%E5%90%8CCPU%E6%80%A7%E8%83%BD%E5%A4%A7PK/">国产CPU和Intel、AMD性能PK</a> 从Intel、AMD、海光、鲲鹏920、飞腾2500 等CPU在TPCC、sysbench下的性能对比来分析他们的性能差距，同时分析内存延迟对性能的影响</h4><p><img src="/images/951413iMgBlog/image-20220319115644219.png" alt="image-20220319115644219"></p>
<h4 id="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"><a href="#从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》" class="headerlink" title="从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》"></a><a href="/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/">从网络路由连通性的原理上来看负载均衡lvs的DR、NAT、FullNAT到底搞了些什么鬼，以及为什么要这么搞，和带来的优缺点：《就是要你懂负载均衡–lvs和转发模式》</a></h4><p><img src="/images/oss/94d55b926b5bb1573c4cab8353428712.png"></p>
<h4 id="LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"><a href="#LVS-20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。" class="headerlink" title="LVS 20倍的负载不均衡，原来是内核的这个Bug，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。"></a><a href="/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/">LVS 20倍的负载不均衡，原来是内核的这个Bug</a>，这个内核bug现在还在，可以稳定重现，有兴趣的话去重现一下，然后对照源代码以及抓包分析一下就清楚了。</h4><h4 id="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"><a href="#就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理" class="headerlink" title="就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理"></a><a href="/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/">就是要你懂TCP–握手和挥手，不是你想象中三次握手、四次挥手就理解了TCP，本文从握手的本质–握手都做了什么事情、连接的本质是什么等来阐述握手、挥手的原理</a></h4><p><img src="/images/oss/6d66dadecb72e11e3e5ab765c6c3ea2e.png"></p>
<h4 id="nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"><a href="#nslookup-OK-but-ping-fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来" class="headerlink" title="nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来"></a><a href="/2019/01/09/nslookup-OK-but-ping-fail/">nslookup OK but ping fail–看看老司机是如何解决问题的，解决问题的方法肯定比知识点重要多了，同时透过一个问题怎么样通篇来理解一大块知识，让这块原理真正在你的只是提示中扎根下来</a></h4><p><img src="/images/oss/ca466bb6430f1149958ceb41b9ffe591.png"></p>
<h4 id="如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"><a href="#如何在工作中学习-一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？" class="headerlink" title="如何在工作中学习 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？"></a><a href="/2018/05/23/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%A5%E4%BD%9C%E4%B8%AD%E5%AD%A6%E4%B9%A0/">如何在工作中学习</a> 一篇很土但是很务实可以复制的方法论文章。不要讲举一反三、触类旁通，谁都知道要举一反三、触类旁通，但是为什么我总是不能够举一反三、触类旁通？</h4><h4 id="举三反一–从理论知识到实际问题的推导-坚决不让思路跑偏，如何从一个理论知识点推断可能的问题"><a href="#举三反一–从理论知识到实际问题的推导-坚决不让思路跑偏，如何从一个理论知识点推断可能的问题" class="headerlink" title="举三反一–从理论知识到实际问题的推导 坚决不让思路跑偏，如何从一个理论知识点推断可能的问题"></a><a href="/2020/11/02/%E4%B8%BE%E4%B8%89%E5%8F%8D%E4%B8%80--%E4%BB%8E%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86%E5%88%B0%E5%AE%9E%E9%99%85%E9%97%AE%E9%A2%98%E7%9A%84%E6%8E%A8%E5%AF%BC/">举三反一–从理论知识到实际问题的推导</a> 坚决不让思路跑偏，如何从一个理论知识点推断可能的问题</h4><h2 id="性能相关（2015-2018年）"><a href="#性能相关（2015-2018年）" class="headerlink" title="性能相关（2015-2018年）"></a>性能相关（2015-2018年）</h2><h4 id="就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常"><a href="#就是要你懂TCP–半连接队列和全连接队列-偶发性的连接reset异常、重启服务后短时间的连接异常" class="headerlink" title="就是要你懂TCP–半连接队列和全连接队列  偶发性的连接reset异常、重启服务后短时间的连接异常"></a><a href="/2017/06/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E5%8D%8A%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97%E5%92%8C%E5%85%A8%E8%BF%9E%E6%8E%A5%E9%98%9F%E5%88%97/">就是要你懂TCP–半连接队列和全连接队列</a>  偶发性的连接reset异常、重启服务后短时间的连接异常</h4><h4 id="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"><a href="#就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的-发送窗口大小-Buffer-、接收窗口大小-Buffer-对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响" class="headerlink" title="就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响"></a><a href="/2019/09/28/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E5%92%8C%E5%8F%91%E9%80%81%E6%8E%A5%E6%94%B6Buffer%E7%9A%84%E5%85%B3%E7%B3%BB/">就是要你懂TCP–性能和发送接收Buffer的关系：发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响。BDP、RT、带宽对传输速度又是怎么影响的</a>  发送窗口大小(Buffer)、接收窗口大小(Buffer)对TCP传输速度的影响，以及怎么观察窗口对传输速度的影响</h4><h4 id="就是要你懂TCP–性能优化大全"><a href="#就是要你懂TCP–性能优化大全" class="headerlink" title="就是要你懂TCP–性能优化大全"></a><a href="/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E5%A4%A7%E5%85%A8/">就是要你懂TCP–性能优化大全</a></h4><h4 id="就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack"><a href="#就是要你懂TCP–TCP性能问题-Nagle算法和delay-ack" class="headerlink" title="就是要你懂TCP–TCP性能问题 Nagle算法和delay ack"></a><a href="/2018/06/14/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E6%9C%80%E7%BB%8F%E5%85%B8%E7%9A%84TCP%E6%80%A7%E8%83%BD%E9%97%AE%E9%A2%98/">就是要你懂TCP–TCP性能问题</a> Nagle算法和delay ack</h4><h4 id="10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"><a href="#10倍性能提升全过程-在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。" class="headerlink" title="10倍性能提升全过程 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。"></a><a href="/2018/01/23/10+%E5%80%8D%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87%E5%85%A8%E8%BF%87%E7%A8%8B/">10倍性能提升全过程</a> 在双11的紧张流程下，将系统tps从500优化到5500，从网络到snat、再到Spring和StackTrace，看看一个性能全栈工程师如何在各种工具加持下发现各种问题的。</h4><h2 id="CPU系列文章（2021年完成）"><a href="#CPU系列文章（2021年完成）" class="headerlink" title="CPU系列文章（2021年完成）"></a>CPU系列文章（2021年完成）</h2><h4 id="CPU的制造和概念"><a href="#CPU的制造和概念" class="headerlink" title="CPU的制造和概念"></a><a href="/2021/06/01/CPU%E7%9A%84%E5%88%B6%E9%80%A0%E5%92%8C%E6%A6%82%E5%BF%B5/">CPU的制造和概念</a></h4><h4 id="十年后数据库还是不敢拥抱NUMA？"><a href="#十年后数据库还是不敢拥抱NUMA？" class="headerlink" title="十年后数据库还是不敢拥抱NUMA？"></a><a href="/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">十年后数据库还是不敢拥抱NUMA？</a></h4><h4 id="Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的-2019-12-16-Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的"><a href="#Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的-2019-12-16-Intel-PAUSE指令变化是如何影响自旋锁以及MySQL的性能的" class="headerlink" title="[Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的](&#x2F;2019&#x2F;12&#x2F;16&#x2F;Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的&#x2F;)"></a>[Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的](&#x2F;2019&#x2F;12&#x2F;16&#x2F;Intel PAUSE指令变化是如何影响自旋锁以及MySQL的性能的&#x2F;)</h4><h4 id="Perf-IPC以及CPU性能-2021-05-16-Perf-IPC以及CPU利用率"><a href="#Perf-IPC以及CPU性能-2021-05-16-Perf-IPC以及CPU利用率" class="headerlink" title="[Perf IPC以及CPU性能](&#x2F;2021&#x2F;05&#x2F;16&#x2F;Perf IPC以及CPU利用率&#x2F;)"></a>[Perf IPC以及CPU性能](&#x2F;2021&#x2F;05&#x2F;16&#x2F;Perf IPC以及CPU利用率&#x2F;)</h4><h4 id="CPU性能和CACHE"><a href="#CPU性能和CACHE" class="headerlink" title="CPU性能和CACHE"></a><a href="https://plantegg.github.io/2021/07/19/CPU%E6%80%A7%E8%83%BD%E5%92%8CCACHE/">CPU性能和CACHE</a></h4><h4 id="CPU-性能和Cache-Line-2021-05-16-CPU-Cache-Line-和性能"><a href="#CPU-性能和Cache-Line-2021-05-16-CPU-Cache-Line-和性能" class="headerlink" title="[CPU 性能和Cache Line](&#x2F;2021&#x2F;05&#x2F;16&#x2F;CPU Cache Line 和性能&#x2F;)"></a>[CPU 性能和Cache Line](&#x2F;2021&#x2F;05&#x2F;16&#x2F;CPU Cache Line 和性能&#x2F;)</h4><h4 id="AMD-Zen-CPU-架构-以及-AMD、海光、Intel、鲲鹏的性能对比"><a href="#AMD-Zen-CPU-架构-以及-AMD、海光、Intel、鲲鹏的性能对比" class="headerlink" title="AMD Zen CPU 架构 以及 AMD、海光、Intel、鲲鹏的性能对比"></a><a href="/2021/08/13/AMD_Zen_CPU%E6%9E%B6%E6%9E%84/">AMD Zen CPU 架构 以及 AMD、海光、Intel、鲲鹏的性能对比</a></h4><h4 id="Intel、海光、鲲鹏920、飞腾2500-CPU性能对比"><a href="#Intel、海光、鲲鹏920、飞腾2500-CPU性能对比" class="headerlink" title="Intel、海光、鲲鹏920、飞腾2500 CPU性能对比"></a><a href="/2021/06/18/%E5%87%A0%E6%AC%BECPU%E6%80%A7%E8%83%BD%E5%AF%B9%E6%AF%94/">Intel、海光、鲲鹏920、飞腾2500 CPU性能对比</a></h4><h2 id="网络相关基础知识（2017年完成）"><a href="#网络相关基础知识（2017年完成）" class="headerlink" title="网络相关基础知识（2017年完成）"></a>网络相关基础知识（2017年完成）</h2><h4 id="就是要你懂网络–一个网络包的旅程"><a href="#就是要你懂网络–一个网络包的旅程" class="headerlink" title="就是要你懂网络–一个网络包的旅程"></a><a href="/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">就是要你懂网络–一个网络包的旅程</a></h4><h4 id="通过案例来理解MSS、MTU等相关TCP概念"><a href="#通过案例来理解MSS、MTU等相关TCP概念" class="headerlink" title="通过案例来理解MSS、MTU等相关TCP概念"></a><a href="/2018/05/07/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E9%80%9A%E8%BF%87%E6%A1%88%E4%BE%8B%E6%9D%A5%E5%AD%A6%E4%B9%A0MSS%E3%80%81MTU/">通过案例来理解MSS、MTU等相关TCP概念</a></h4><h4 id="就是要你懂TCP–握手和挥手"><a href="#就是要你懂TCP–握手和挥手" class="headerlink" title="就是要你懂TCP–握手和挥手"></a><a href="/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--%E8%BF%9E%E6%8E%A5%E5%92%8C%E6%8F%A1%E6%89%8B/">就是要你懂TCP–握手和挥手</a></h4><h4 id="wireshark-dup-ack-issue-and-keepalive"><a href="#wireshark-dup-ack-issue-and-keepalive" class="headerlink" title="wireshark-dup-ack-issue and keepalive"></a><a href="/2017/06/02/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82TCP--wireshark-dup-ack-issue/">wireshark-dup-ack-issue and keepalive</a></h4><h4 id="一个没有遵守tcp规则导致的问题"><a href="#一个没有遵守tcp规则导致的问题" class="headerlink" title="一个没有遵守tcp规则导致的问题"></a><a href="/2018/11/26/%E4%B8%80%E4%B8%AA%E6%B2%A1%E6%9C%89%E9%81%B5%E5%AE%88tcp%E8%A7%84%E5%88%99%E5%AF%BC%E8%87%B4%E7%9A%84%E9%97%AE%E9%A2%98/">一个没有遵守tcp规则导致的问题</a></h4><h4 id="kubernetes-service-和-kube-proxy详解-2020-09-22-kubernetes-service-和-kube-proxy详解"><a href="#kubernetes-service-和-kube-proxy详解-2020-09-22-kubernetes-service-和-kube-proxy详解" class="headerlink" title="[kubernetes service 和 kube-proxy详解](&#x2F;2020&#x2F;09&#x2F;22&#x2F;kubernetes service 和 kube-proxy详解&#x2F;)"></a>[kubernetes service 和 kube-proxy详解](&#x2F;2020&#x2F;09&#x2F;22&#x2F;kubernetes service 和 kube-proxy详解&#x2F;)</h4><h2 id="DNS相关"><a href="#DNS相关" class="headerlink" title="DNS相关"></a>DNS相关</h2><h4 id="就是要你懂DNS–一文搞懂域名解析相关问题"><a href="#就是要你懂DNS–一文搞懂域名解析相关问题" class="headerlink" title="就是要你懂DNS–一文搞懂域名解析相关问题"></a><a href="/2019/06/09/%E4%B8%80%E6%96%87%E6%90%9E%E6%87%82%E5%9F%9F%E5%90%8D%E8%A7%A3%E6%9E%90%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98/">就是要你懂DNS–一文搞懂域名解析相关问题</a></h4><h4 id="nslookup-OK-but-ping-fail"><a href="#nslookup-OK-but-ping-fail" class="headerlink" title="nslookup OK but ping fail"></a><a href="/2019/01/09/nslookup-OK-but-ping-fail/">nslookup OK but ping fail</a></h4><h4 id="Docker中的DNS解析过程"><a href="#Docker中的DNS解析过程" class="headerlink" title="Docker中的DNS解析过程"></a><a href="/2019/01/12/Docker%E4%B8%AD%E7%9A%84DNS%E8%A7%A3%E6%9E%90%E8%BF%87%E7%A8%8B/">Docker中的DNS解析过程</a></h4><h4 id="windows7的wifi总是报DNS域名异常无法上网"><a href="#windows7的wifi总是报DNS域名异常无法上网" class="headerlink" title="windows7的wifi总是报DNS域名异常无法上网"></a><a href="/2019/01/10/windows7%E7%9A%84wifi%E6%80%BB%E6%98%AF%E6%8A%A5DNS%E5%9F%9F%E5%90%8D%E5%BC%82%E5%B8%B8%E6%97%A0%E6%B3%95%E4%B8%8A%E7%BD%91/">windows7的wifi总是报DNS域名异常无法上网</a></h4><h2 id="LVS-负载均衡"><a href="#LVS-负载均衡" class="headerlink" title="LVS 负载均衡"></a>LVS 负载均衡</h2><h4 id="就是要你懂负载均衡–lvs和转发模式"><a href="#就是要你懂负载均衡–lvs和转发模式" class="headerlink" title="就是要你懂负载均衡–lvs和转发模式"></a><a href="/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/">就是要你懂负载均衡–lvs和转发模式</a></h4><h4 id="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"><a href="#就是要你懂负载均衡–负载均衡调度算法和为什么不均衡" class="headerlink" title="就是要你懂负载均衡–负载均衡调度算法和为什么不均衡"></a><a href="/2019/07/19/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%92%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E5%9D%87%E8%A1%A1/">就是要你懂负载均衡–负载均衡调度算法和为什么不均衡</a></h4><h2 id="网络工具"><a href="#网络工具" class="headerlink" title="网络工具"></a>网络工具</h2><h4 id="就是要你懂Unix-Socket-进行抓包解析"><a href="#就是要你懂Unix-Socket-进行抓包解析" class="headerlink" title="就是要你懂Unix Socket 进行抓包解析"></a><a href="/2018/01/01/%E9%80%9A%E8%BF%87tcpdump%E5%AF%B9Unix%20Socket%20%E8%BF%9B%E8%A1%8C%E6%8A%93%E5%8C%85%E8%A7%A3%E6%9E%90/">就是要你懂Unix Socket 进行抓包解析</a></h4><h4 id="就是要你懂网络监控–ss用法大全"><a href="#就是要你懂网络监控–ss用法大全" class="headerlink" title="就是要你懂网络监控–ss用法大全"></a><a href="/2016/10/12/ss%E7%94%A8%E6%B3%95%E5%A4%A7%E5%85%A8/">就是要你懂网络监控–ss用法大全</a></h4><h4 id="就是要你懂抓包–WireShark之命令行版tshark"><a href="#就是要你懂抓包–WireShark之命令行版tshark" class="headerlink" title="就是要你懂抓包–WireShark之命令行版tshark"></a><a href="/2019/06/21/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E6%8A%93%E5%8C%85--WireShark%E4%B9%8B%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88tshark/">就是要你懂抓包–WireShark之命令行版tshark</a></h4><h4 id="netstat-timer-keepalive-explain"><a href="#netstat-timer-keepalive-explain" class="headerlink" title="netstat timer keepalive explain"></a><a href="/2017/08/28/netstat%20%E7%AD%89%E7%BD%91%E7%BB%9C%E5%B7%A5%E5%85%B7/">netstat timer keepalive explain</a></h4><h4 id="Git-HTTP-Proxy-and-SSH-Proxy"><a href="#Git-HTTP-Proxy-and-SSH-Proxy" class="headerlink" title="Git HTTP Proxy and SSH Proxy"></a><a href="/2018/03/14/%E5%A6%82%E4%BD%95%E8%AE%BE%E7%BD%AEgit%20Proxy/">Git HTTP Proxy and SSH Proxy</a></h4>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2025/01/29/%E8%B7%A8Die%E7%83%AD%E8%BF%81%E7%A7%BB%E8%BF%81%E7%A7%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/29/%E8%B7%A8Die%E7%83%AD%E8%BF%81%E7%A7%BB%E8%BF%81%E7%A7%BB/" class="post-title-link" itemprop="url">跨 Die 热迁移迁移导致的性能问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-29 17:30:03" itemprop="dateCreated datePublished" datetime="2025-01-29T17:30:03+08:00">2025-01-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="跨-Die-热迁移迁移导致的性能问题"><a href="#跨-Die-热迁移迁移导致的性能问题" class="headerlink" title="跨 Die 热迁移迁移导致的性能问题"></a>跨 Die 热迁移迁移导致的性能问题</h1><h2 id="场景"><a href="#场景" class="headerlink" title="场景"></a>场景</h2><p>在 ARM 上，本来业务跑在 Die0 上，如果通过taskset 将 CPU 绑到 Die1，经过一段时间运行，内存也会慢慢随着释放&#x2F;新分配慢慢都迁移到 Die1， 但是这之后仍然发现性能比在 Die0 上要差很多。而在 Intel X86 上没碰到过这个问题</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>下图是内存带宽使用率监控数据，可以看到跨 Die 绑定后原来的 Die0 带宽急剧增加（7.5% -&gt; 13%）这是因为内存都需要跨 Die 访问了，随着跨 Die 访问并慢慢将内存迁移到 Die1 后内存带宽使用率回落到 5%：</p>
<p><img src="/images/951413iMgBlog/893058e1-0f19-4fc1-a76c-49b48f49317a.png" alt="img"></p>
<p>同时可以看到 CPU 使用率也从 35% 飙升到 76%，随着内存的迁移完毕，CPU  使用率回落到 48%，但仍然比最开始的 35% 高不少：</p>
<p><img src="/images/951413iMgBlog/6b396de3-36cb-42d9-b14b-56be896509f8.png" alt="img"></p>
<h3 id="Intel-X86-下跨-Die-迁移"><a href="#Intel-X86-下跨-Die-迁移" class="headerlink" title="Intel X86 下跨 Die 迁移"></a>Intel X86 下跨 Die 迁移</h3><p>Intel 基本都是一路就是一个 Die，所以你可以理解这里的跨 Die 就是跨路&#x2F;Socket 迁移：</p>
<p><img src="/images/951413iMgBlog/22e39654-d8d3-4d5d-ae88-d2c137be71a0.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/5b4477a5-a8d9-45b2-84e4-fb01cf8cc0b7.png" alt="img"></p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>ARM 比 X86 差这么多的原因是内存页表跨 Die 访问导致的，业务同学测试了开启 THP 负载的影响，从结果看，THP on 可有效降低cpu水位，但是依然存在跨die迁移cpu水位上升的问题。</p>
<p>alios 对应的 patch：<a target="_blank" rel="noopener" href="https://gitee.com/anolis/cloud-kernel/pulls/2254/commits">https://gitee.com/anolis/cloud-kernel/pulls/2254/commits</a>  不区分 x86 和 arm</p>
<p>页表是进程创建时在内核空间分配好的，所以迁移内存时并不会迁移页表。通过测试页表跨die迁移的poc，验证了跨die页表访问是导致本文所述问题的根本原因</p>
<p>课后作业：</p>
<p>去了解下内存页表&#x2F;THP(透明大页)</p>
<p>学习 CPU 访问内存延时是怎么回事，然后学习 CPU 和内存速度的差异，最后再去看看大学的计算机组成原理</p>
<p>学习下 taskset&#x2F;perf 等命令</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2025/01/09/CPU%20%E4%BD%BF%E7%94%A8%E7%8E%87%E9%AB%98%E5%B0%B1%E4%B8%80%E5%AE%9A%E6%9C%89%E6%95%88%E7%8E%87%E5%90%97/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/01/09/CPU%20%E4%BD%BF%E7%94%A8%E7%8E%87%E9%AB%98%E5%B0%B1%E4%B8%80%E5%AE%9A%E6%9C%89%E6%95%88%E7%8E%87%E5%90%97/" class="post-title-link" itemprop="url">CPU 使用率高就一定有效率吗？</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-01-09 17:30:03" itemprop="dateCreated datePublished" datetime="2025-01-09T17:30:03+08:00">2025-01-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="CPU-使用率高就一定有效率吗？"><a href="#CPU-使用率高就一定有效率吗？" class="headerlink" title="CPU 使用率高就一定有效率吗？"></a>CPU 使用率高就一定有效率吗？</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>最近碰到一个客户业务跑在8C ECS 上，随着业务压力增加 CPU使用率也即将跑满，于是考虑将 8C 升级到16C，事实是升级后业务 RT 反而略有增加，这个事情也超出了所有程序员们的预料，所以我们接下来分析下这个场景</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>通过采集升配前后、以前和正常时段的火焰图对比发现CPU 增加主要是消耗在 自旋锁上了：</p>
<p><img src="/images/951413iMgBlog/image-20240202085410669.png" alt="image-20240202085410669"></p>
<p>用一个案例来解释下自旋锁和锁，如果我们要用多线程对一个整数进行计数，要保证线程安全的话，可以加锁(synchronized), 这个加锁操作也有人叫悲观锁，抢不到锁就让出这个线程的CPU 调度(代价上下文切换一次，几千个时钟周期)</p>
<p>另外一种是用自旋锁(CAS、spin_lock) 来实现，抢不到锁就耍赖占住CPU 死磕不停滴抢(CPU 使用率一直100%)，自旋锁的设计主要是针对抢锁概率小、并发低的场景。这两种方案针对场景不一样各有优缺点</p>
<p>假如你的机器是8C，你有100个线程来对这个整数进行计数的话，你用synchronized 方式来实现会发现CPU 使用率永远达不到50%</p>
<p><img src="/images/951413iMgBlog/image-20240202090428778.png" alt="image-20240202090428778"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#taskset -a -c 56-63 java LockAccumulator 100 1000000000</span><br><span class="line">累加结果: 1000000000 and time:84267</span><br><span class="line"></span><br><span class="line">Performance counter stats for &#x27;taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 100 100000000&#x27;:</span><br><span class="line"></span><br><span class="line">      17785.271791      task-clock (msec)         #    2.662 CPUs utilized</span><br><span class="line">           110,351      context-switches          #    0.006 M/sec</span><br><span class="line">            10,094      cpu-migrations            #    0.568 K/sec</span><br><span class="line">            11,724      page-faults               #    0.659 K/sec</span><br><span class="line">    44,187,609,686      cycles                    #    2.485 GHz</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">    22,588,807,670      instructions              #    0.51  insns per cycle</span><br><span class="line">     6,919,355,610      branches                  #  389.050 M/sec</span><br><span class="line">        28,707,025      branch-misses             #    0.41% of all branches</span><br></pre></td></tr></table></figure>

<p>如果我们改成自旋锁版本的实现，8个核CPU 都是100%</p>
<p><img src="/images/951413iMgBlog/image-20240202090714845.png" alt="image-20240202090714845"></p>
<p>以下代码累加次数只有加锁版本的10%，时间还长了很多，也就是效率产出实在是低</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 100 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 106593 毫秒</span><br><span class="line"></span><br><span class="line"> Performance counter stats for &#x27;taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 100 100000000&#x27;:</span><br><span class="line"></span><br><span class="line">      85363.429249      task-clock (msec)         #    7.909 CPUs utilized</span><br><span class="line">            23,010      context-switches          #    0.270 K/sec</span><br><span class="line">             1,262      cpu-migrations            #    0.015 K/sec</span><br><span class="line">            13,403      page-faults               #    0.157 K/sec</span><br><span class="line">   213,191,037,155      cycles                    #    2.497 GHz</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-frontend</span><br><span class="line">   &lt;not supported&gt;      stalled-cycles-backend</span><br><span class="line">    43,523,454,723      instructions              #    0.20  insns per cycle</span><br><span class="line">    10,306,663,291      branches                  #  120.739 M/sec</span><br><span class="line">        14,704,466      branch-misses             #    0.14% of all branches</span><br></pre></td></tr></table></figure>



<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><p>我<a target="_blank" rel="noopener" href="https://github.com/plantegg/programmer_case/tree/main/code/spin_lock">放在了github 上</a>，有个带调X86 平台 pause 指令的汇编，Java 中要用JNI 来调用，ChatGPT4帮我写的，并给了编译、运行方案：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">javac SpinLockAccumulator.java</span><br><span class="line">javah -jni SpinLockAccumulator</span><br><span class="line"></span><br><span class="line"># Assuming GCC is installed and the above C code is in SpinLockAccumulator.c</span><br><span class="line">gcc -shared -o libpause.so -fPIC SpinLockAccumulator.c</span><br><span class="line"></span><br><span class="line">java -Djava.library.path=. SpinLockAccumulator</span><br><span class="line"></span><br><span class="line">实际gcc编译要带上jdk的头文件：</span><br><span class="line">gcc -I/opt/openjdk/include/ -I/opt/openjdk/include/linux/ -shared -o libpause.so  -fPIC SpinLockAccumulator.c</span><br></pre></td></tr></table></figure>



<h2 id="在MySQL-INNODB-里怎么优化这个自旋锁"><a href="#在MySQL-INNODB-里怎么优化这个自旋锁" class="headerlink" title="在MySQL INNODB 里怎么优化这个自旋锁"></a>在MySQL INNODB 里怎么优化这个自旋锁</h2><p>MySQL 在自旋锁抢锁的时候每次会调 ut_delay（底层会掉CPU指令，让CPU暂停一下但是不让出——避免上下文切换），发现性能好了几倍。这是MySQL 的官方文档：<a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html">https://dev.mysql.com/doc/refman/5.7/en/innodb-performance-spin_lock_polling.html</a></p>
<p>所以我们继续在以上代码的基础上在自旋的时候故意让CPU pause(50个), 这个优化详细案例：<a href="https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/">https://plantegg.github.io/2019/12/16/Intel%20PAUSE%E6%8C%87%E4%BB%A4%E5%8F%98%E5%8C%96%E6%98%AF%E5%A6%82%E4%BD%95%E5%BD%B1%E5%93%8D%E8%87%AA%E6%97%8B%E9%94%81%E4%BB%A5%E5%8F%8AMySQL%E7%9A%84%E6%80%A7%E8%83%BD%E7%9A%84/</a></p>
<h2 id="该你动手了"><a href="#该你动手了" class="headerlink" title="该你动手了"></a>该你动手了</h2><p>随便找一台x86 机器，笔记本也可以，macOS 也行，核数多一些效果更明显。只要有Java环境，就用我编译好的class、libpause.so 理论上也行，不兼容的话按代码那一节再重新编译一下</p>
<p>可以做的实验：</p>
<ul>
<li>重复我前面两个运行，看CPU 使用率以及最终耗时</li>
<li>尝试优化待pause版本的自旋锁实现，是不是要比没有pause性能反而要好</li>
<li>尝试让线程sleep 一下，效果是不是要好？</li>
<li>尝试减少线程数量，慢慢是不是发现自旋锁版本的性能越来越好了</li>
</ul>
<p>改变线程数量运行对比：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">//自旋锁版本线程数对总时间影响很明显，且线程少的话性能要比加锁版本好，这符合自旋锁的设定:大概率不需要抢就用自旋锁</span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 1 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 2542 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 2 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 2773 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 4 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 4109 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 8 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 11931 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 16 100000000</span><br><span class="line">累加结果: 100000000</span><br><span class="line">操作耗时: 13476 毫秒</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//加锁版本线程数变化对总时间影响不那么大</span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 16 100000000</span><br><span class="line">累加结果: 100000000 and time:9074</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 8 100000000</span><br><span class="line">累加结果: 100000000 and time:8832</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 4 100000000</span><br><span class="line">累加结果: 100000000 and time:7330</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 2 100000000</span><br><span class="line">累加结果: 100000000 and time:6298</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. LockAccumulator 1 100000000</span><br><span class="line">累加结果: 100000000 and time:3143</span><br></pre></td></tr></table></figure>

<p>设定100并发下，改变机器核数对比：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">//16核机器跑3次 耗时稳定在12秒以上，CPU使用率 1600%</span><br><span class="line">#taskset -a -c 48-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 12860 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 48-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 12949 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 48-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 13692 毫秒</span><br><span class="line"></span><br><span class="line">//8核机器跑3次，耗时稳定5秒左右，CPU使用率 800%</span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 6773 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 5557 毫秒</span><br><span class="line"></span><br><span class="line">#taskset -a -c 56-63 java -Djava.library.path=. SpinLockAccumulator 100 10000000</span><br><span class="line">累加结果: 10000000</span><br><span class="line">操作耗时: 2724 毫秒</span><br></pre></td></tr></table></figure>



<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以后应该不会再对升配后CPU 使用率也上去了，但是最终效率反而没变展现得很惊诧了</p>
<p>从CPU 使用率、上下文切换上理解自旋锁(乐观锁)和锁(悲观锁)</p>
<p>MySQL 里对自旋锁的优化，增加配置 <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_spin_wait_delay"><code>innodb_spin_wait_delay</code></a>  来增加不同场景下DBA 的干预手段</p>
<p>这篇文章主要功劳要给 ChatGPT4 ，里面所有演示代码都是它完成的</p>
<h2 id="相关阅读"><a href="#相关阅读" class="headerlink" title="相关阅读"></a>相关阅读</h2><p><a target="_blank" rel="noopener" href="https://articles.zsxq.com/id_332xqfhuxern.html">流量一样但为什么CPU使用率差别很大</a> 同样也是跟CPU 要效率，不过这个案例不是因为自旋锁导致CPU 率高，而是内存延时导致的</p>
<p><a target="_blank" rel="noopener" href="https://t.zsxq.com/16PqQ3p4x">今日短平快，ECS从16核升配到48核后性能没有任何提升（Netflix）</a> 也是CPU 使用率高没有产出，cacheline伪共享导致的</p>
<p><a href="https://plantegg.github.io/2022/03/15/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%90%AC%E9%A3%8E%E6%89%87%E5%A3%B0%E9%9F%B3%E6%9D%A5%E5%AE%9A%E4%BD%8D%E6%80%A7%E8%83%BD/">听风扇声音来定位性能瓶颈</a></p>
<p>你要是把这个案例以及上面三个案例综合看明白了，相当于把计算机组成原理就学明白了。这里最核心的就是“内存墙”，也就是内存速度没有跟上CPU的发展速度，导致整个计算机内绝大多场景下读写内存缓慢成为主要的瓶颈</p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a target="_blank" rel="noopener" href="https://twitter.com/plantegg">@plantegg</a></p>
<p>知识星球：<a target="_blank" rel="noopener" href="https://t.zsxq.com/0cSFEUh2J">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>
<img src="/images/951413iMgBlog/image-20240324161113874-5525714.png" alt="image-20240324161113874" style="zoom:50%;" />


      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/11/02/tcp%E4%BC%9A%E5%81%B6%E5%B0%943%E7%A7%92timeout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/11/02/tcp%E4%BC%9A%E5%81%B6%E5%B0%943%E7%A7%92timeout/" class="post-title-link" itemprop="url">tcp会偶尔3秒timeout的分析以及如何用php规避这个问题</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-11-02 17:30:03" itemprop="dateCreated datePublished" datetime="2024-11-02T17:30:03+08:00">2024-11-02</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/TCP/" itemprop="url" rel="index"><span itemprop="name">TCP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="tcp会偶尔3秒timeout的分析以及如何用php规避这个问题"><a href="#tcp会偶尔3秒timeout的分析以及如何用php规避这个问题" class="headerlink" title="tcp会偶尔3秒timeout的分析以及如何用php规避这个问题"></a><a target="_blank" rel="noopener" href="https://web.archive.org/web/20170317084941/http://mogu.io/tcp-three-second-timeout-with-php-3">tcp会偶尔3秒timeout的分析以及如何用php规避这个问题</a></h1><blockquote>
<p>这是一篇好文章，随着蘑菇街的完蛋，蘑菇街技术博客也没了，所以特意备份一下这篇</p>
</blockquote>
<ul>
<li><p>作者：蚩尤 </p>
</li>
<li><p>时间：May 27, 2014</p>
</li>
</ul>
<p>2年前做一个cache中间件调用的时候，发现很多通过php的curl调用一个的服务会出现偶尔的connect_time超时, 表现为get_curlinfo的connect_time在3秒左右, 本来没怎么注意, 因为客户端的curl_timeout设置的就是3秒, 某天, 我把这个timeout改到了5秒后, 发现了一个奇怪的现象, 很多慢请求依旧表现为connect_time在3秒左右..看来这个3秒并不是因为客户端设置的timeout引起的.于是开始查找这个原因.</p>
<hr>
<p>首先, 凭借经验调整了linux内核关于tcp的几个参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.core.netdev_max_backlog = 862144</span><br><span class="line">net.core.somaxconn = 262144</span><br></pre></td></tr></table></figure>

<p>经过观察发现依旧会有3秒超时, 而且数量并没有减少.</p>
<p>第二步, 排除是大并发导致的问题, 在一台空闲机器上也部署同样的服务, 仅让线上一台机器跑空闲机器的服务, 结果发现依旧会有报错.排除并发导致的问题.</p>
<p>最后, 通过查了大量的资料才发现并不是我们才遇到过这个问题, 而且这个问题并不是curl的问题, 它影响到所有tcp的调用, 网上各种说法, 但结论都指向linux内核对于tcp的实现.(某些版本会出现这些问题), 有兴趣的可以看下下面这两个资料.<br><a target="_blank" rel="noopener" href="https://web.archive.org/web/20170317084941/http://www.spinics.net/lists/linux-net/msg17545.html">资料1</a><br><a target="_blank" rel="noopener" href="https://web.archive.org/web/20170317084941/http://marc.info/?t=120655182600018&r=1&w=2">资料2</a></p>
<p>一看深入到linux内核..不管怎样修改的成本一定很大..于是乎, 发挥我们手中的php来规避这个问题的时间到了.</p>
<p>原本的代码, 简单实现，常规curl调用:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">function curl_call($p1, $p2 ...) &#123;</span><br><span class="line">   $ch = curl_init();</span><br><span class="line">   curl_setopt($ch, CURLOPT_TIMEOUT, 5);</span><br><span class="line">   curl_setopt($ch, CURLOPT_URL, &#x27;http://demon.at&#x27;);</span><br><span class="line">   $res = curl_exec($ch);</span><br><span class="line">   if (false === $res) &#123;</span><br><span class="line">      //失败..抛异常..</span><br><span class="line">   &#125;</span><br><span class="line">   return $res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>可以看出, 如果用上面的代码, 无法避免3秒connect_time的问题..这种实现对curl版本会有要求(CURLOPT_CONNECTTIMEOUT_MS)，主要的思路是，通过对链接时间进行毫秒级的控制(因为超时往往发生在connect的时候)，加上失败重试机制，来最大限度保证调用的正确性。所以,下面的代码就诞生了:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">function curl_call($p1, $p2, $times = 1) &#123;</span><br><span class="line">   $ch = curl_init();</span><br><span class="line">   curl_setopt($ch, CURLOPT_TIMEOUT, 5);</span><br><span class="line">   curl_setopt($ch, CURLOPT_URL, &#x27;http://demon.at&#x27;);</span><br><span class="line">   $curl_version = curl_version();</span><br><span class="line">   if ($curl_version[&#x27;version_number&#x27;] &gt;= 462850) &#123;</span><br><span class="line">      curl_setopt($ch, CURLOPT_CONNECTTIMEOUT_MS, 20);</span><br><span class="line">      curl_setopt($ch, CURLOPT_NOSIGNAL, 1);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">      throw new Exception(&#x27;this curl version is too low, version_num : &#x27; </span><br><span class="line">                         . $curl_version[&#x27;version&#x27;]);</span><br><span class="line">   &#125;</span><br><span class="line">   $res = curl_exec($ch);</span><br><span class="line">   curl_close($ch);</span><br><span class="line">   if (false === $res) &#123;</span><br><span class="line">      if (curl_errno($ch) == CURLE_OPERATION_TIMEOUTED</span><br><span class="line">             and $times != 最大重试阀值 ) &#123;</span><br><span class="line">         $times += 1;</span><br><span class="line">         return curl_call($p1, $p2, $times);</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   return $res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>上面这段代码只是一个规避的简单实例, 一些小细节并没有可以完善..比如抛出异常常以后curl资源的手动释放等等..这里不做讨论..当然还漏了一点要说的是，对重试次数最好加上限制 :)</p>
<p>说明一下上面几个数字值的含义:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">462850 //因为php的CURLOPT_CONNECTTIMEOUT_MS需要 curl_version 7.16.2,这个值就是这个版本的数字版本号，还需要注意的是, php版本要大于5.2.3</span><br><span class="line">20 //连接超时的时间, 单位:ms</span><br></pre></td></tr></table></figure>

<hr>
<p>这样这个问题就这样通过php的代码来规避开了.<br>如果有对这个问题有更好的解决方法，欢迎指教.</p>
<hr>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/-pRA12sLJktbXa-srWn02w">tcp connect 的流程是这样的</a>：<br>1、tcp发出SYN建链报文后，报文到ip层需要进行路由查询<br>2、路由查询完成后，报文到arp层查询下一跳mac地址<br>3、如果本地没有对应网关的arp缓存，就需要缓存住这个报文，发起arp请求<br>4、arp层收到arp回应报文之后，从缓存中取出SYN报文，完成mac头填写并发送给驱动。</p>
<p>问题在于，arp层缓存队列长度默认为3。如果你运气不好，刚好赶上缓存已满，这个报文就会被丢弃。</p>
<p>TCP层发现SYN报文发出去3s（默认值）还没有回应，就会重发一个SYN。这就是为什么少数连接会3s后才能建链。</p>
<p>幸运的是，arp层缓存队列长度是可配置的，用 sysctl -a | grep unres_qlen 就能看到，默认值为3。</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/10/10/%E4%B8%80%E6%AC%A1%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/10/10/%E4%B8%80%E6%AC%A1%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90%E8%BF%87%E7%A8%8B/" class="post-title-link" itemprop="url">一次抓包分析过程——Wireshark 新手上车</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-10-10 10:30:03" itemprop="dateCreated datePublished" datetime="2024-10-10T10:30:03+08:00">2024-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/tcpdump/" itemprop="url" rel="index"><span itemprop="name">tcpdump</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="一次抓包分析过程——Wireshark-新手上车"><a href="#一次抓包分析过程——Wireshark-新手上车" class="headerlink" title="一次抓包分析过程——Wireshark 新手上车"></a>一次抓包分析过程——Wireshark 新手上车</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>网友尝试做星球第一个必做实验的时候，什么内核参数都没改，发现请求经常会停滞 100ms，这种要怎么判断是局域网的网络问题还是应用问题呢？ 服务是 python3 -m http.server 启动的，看上去没有出现什么重传、窗口也没看到什么问题</p>
<p>因为不能提供环境给我，我尝试对这个抓包进行了分析，因为只有客户端抓包，所以分析结果是没有结论的，但分析过程比较适合入门 Wireshark，适合刚加入星球的、没分析过网络包的同学可以参考，熟手请忽略</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>整个抓包 28MB，跨度 600 毫秒，看得出带宽很大、RTT 极小(到Wireshark 里看看前几个包的交互 RT 就知道了)</p>
<p><img src="/images/951413iMgBlog/image-20240715093847359.png" alt="image-20240715093847359"></p>
<h3 id="详细分析"><a href="#详细分析" class="headerlink" title="详细分析"></a>详细分析</h3><p>看第一次卡 100ms 之前的抓包，在100ms 以前客户端ack 了所有Server 发出来的的tcp包(红框)，也就是说每一个发给客户端的包客户端都ack 完毕，证明客户端处理足够快，但是 8089端口不继续发包而是等了100ms再继续发，如下图：</p>
<p><img src="/images/951413iMgBlog/image-20240715094218182.png" alt="image-20240715094218182"></p>
<p>到这里的结论：</p>
<p>不是因为发送buffer、接收buffer太小导致的卡；也不是因为拥塞窗口导致的，就是Server 端没有发包。大概率是Server 进程卡了，或者Server 进程读取物理文件往OS buffer 写这些环节卡了（可以在服务端通过 strace -tt 看看进程在这 100 毫秒有没有往内核怼数据）</p>
<p>所以要继续在 Server 端来分析这个问题</p>
<p>怎么快速定位到红框、红线这里的包？</p>
<blockquote>
<p>到 Time Sequence 图上点平台两边的点都可以自动跳转到这里，每个点代表一个网络包，横坐标代表时间</p>
</blockquote>
<h2 id="其它分析"><a href="#其它分析" class="headerlink" title="其它分析"></a>其它分析</h2><p>将如下 Time Sequence 图使劲放大，从第一个包开始看，可以观察到教科书所说的慢启动</p>
<p><img src="/images/951413iMgBlog/image-20240715095134352.png" alt="image-20240715095134352"></p>
<p>整体看的话，慢启动几乎可以忽略，毕竟这个抓包是下载一个巨大的文件，如果是一个小文件这个慢启动还是影响很大的，如下图，红框部分看起来微不足道</p>
<p><img src="/images/951413iMgBlog/image-20240715095506381.png" alt="image-20240715095506381"></p>
<p>把时间范围放大，继续看，在卡之前红色箭头很长的，代表带宽、buffer有能力一次发送很多网络包，但是后面每次只发一点点网络包(绿色箭头长度)就卡了</p>
<p><img src="/images/951413iMgBlog/image-20240715095647702.png" alt="image-20240715095647702"></p>
<h2 id="重现"><a href="#重现" class="headerlink" title="重现"></a>重现</h2><p>我用 python3 当服务端未能重现这个卡100ms 的现象，拉取都很丝滑</p>
<p><img src="/images/951413iMgBlog/image-20240715101505977.png" alt="image-20240715101505977"></p>
<p>非常细节地去分析的话，也是能看到一些小问题的，比如1.9ms的卡顿、比如zero_window</p>
<p><img src="/images/951413iMgBlog/image-20240715103928266.png" alt="image-20240715103928266"></p>
<p>重现的时候，有1.9ms 这样的卡顿，但是不算有规律，因为这么小在整个传输过程中影响不大</p>
<p><img src="/images/951413iMgBlog/image-20240715103708750.png" alt="image-20240715103708750"></p>
<p>我重现的时候正好抓到了 seq 回绕，seq 是个 32位的无符号整数，到了最大值就从0又开始：</p>
<p><img src="/images/951413iMgBlog/image-20240715115500312.png" alt="image-20240715115500312"></p>
<p>此时的 Time Sequence: </p>
<p><img src="/images/951413iMgBlog/image-20240715115655516.png" alt="image-20240715115655516"></p>
<h2 id="建议"><a href="#建议" class="headerlink" title="建议"></a>建议</h2><p>可以用实验1里面的一些手段debug 一下Server 为什么卡了，除了 strace -tt 还可以用 ebpf 试试看看 Server 的调度上哪里顿了 100ms</p>
<p>新手如何通过Wireshark 来看抓包？</p>
<p>首先不要纯粹为了学习去看，而是要问你的问题是什么？如果网络传输速度慢，我们就看  Time Sequence(斜率越陡速度越快)，去看为什么发送端不发包了</p>
<ul>
<li>如正文里的卡顿平台，在250ms内差不多要卡240ms 不发包，速度自然不行</li>
<li>我重现抓包中的zero Windows</li>
<li>达到网络BDP 瓶颈了，去看拥塞窗口在最大值的时候会丢包，触发降速</li>
</ul>
<p>里面可以看、要看的东西太多，所以我也说不上要看什么，而是要问你的问题是什么</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/09/25/%E4%B8%80%E4%B8%AA%E5%8E%86%E6%97%B65%E5%B9%B4%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/09/25/%E4%B8%80%E4%B8%AA%E5%8E%86%E6%97%B65%E5%B9%B4%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/" class="post-title-link" itemprop="url">历时5年的net_write_timeout 报错分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-09-25 17:30:03" itemprop="dateCreated datePublished" datetime="2024-09-25T17:30:03+08:00">2024-09-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-29 15:19:06" itemprop="dateModified" datetime="2025-11-29T15:19:06+08:00">2025-11-29</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/MySQL/" itemprop="url" rel="index"><span itemprop="name">MySQL</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="历时5年的net-write-timeout-报错分析"><a href="#历时5年的net-write-timeout-报错分析" class="headerlink" title="历时5年的net_write_timeout 报错分析"></a>历时5年的net_write_timeout 报错分析</h1><p>全网关于 JDBC 报错：net_write_timeout 的最好&#x2F;最全总结</p>
<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>上一次为了讲如何分析几百万个抓包，所以把这个问题中的一部分简化写了这篇抓包篇：<a target="_blank" rel="noopener" href="https://articles.zsxq.com/id_lznw3w4zieuc.html">https://articles.zsxq.com/id_lznw3w4zieuc.html</a>  建议你先去看看把场景简化下，然后本篇中的分析涉及抓包部分就不再啰嗦讲解，请看抓包篇</p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>用户为了做数据分析需要把160个DB中的数据迁移到另外一个只读库中，有专门的迁移工具，但是这个迁移工具跑一阵后总是报错，报错堆栈显示是Tomcat 到DB之间的连接出了异常：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of &#x27;net_write_timeout&#x27; on the server.</span><br><span class="line">    at sun.reflect.GeneratedConstructorAccessor150.newInstance(Unknown Source)</span><br><span class="line">    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">    at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">    at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:989)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3749)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3649)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:4090)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:972)</span><br><span class="line">    at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:2123)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:374)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:354)</span><br><span class="line">    at com.mysql.jdbc.RowDataDynamic.close(RowDataDynamic.java:155)</span><br><span class="line">    at com.mysql.jdbc.ResultSetImpl.realClose(ResultSetImpl.java:6726)</span><br><span class="line">    at com.mysql.jdbc.ResultSetImpl.close(ResultSetImpl.java:865)</span><br><span class="line">    at com.alibaba.druid.pool.DruidPooledResultSet.close(DruidPooledResultSet.java:86)</span><br></pre></td></tr></table></figure>

<p>这个异常堆栈告诉我们Tomcat 到Database之间的连接异常了，似乎是 net_write_timeout 超时导致的</p>
<p>对应业务结构：</p>
<p><img src="/images/951413iMgBlog/image-20230706210452742.png" alt="image-20230706210452742"></p>
<h2 id="net-write-timeout-原理简介"><a href="#net-write-timeout-原理简介" class="headerlink" title="net_write_timeout 原理简介"></a>net_write_timeout 原理简介</h2><p>先看下 <a target="_blank" rel="noopener" href="https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_net_write_timeout"><code>net_write_timeout</code></a>的解释：</p>
<blockquote>
<p>The number of seconds to wait for a block to be written to a connection before aborting the write. 只针对执行查询中的等待超时，网络不好，tcp buffer满了（应用迟迟不读走数据）等容易导致mysql server端报net_write_timeout错误，指的是mysql server hang在那里长时间无法发送查询结果。</p>
</blockquote>
<p>报这个错就是DB 等了net_write_timeout这么久没写数据，可能是Tomcat 端卡死没有读走数据。</p>
<p>但是根据我多年来和这个报错打交道的经验告诉我：这个报错不只是因为net_write_timeout 超时导致的，任何Tomcat 到 DB间的连接断开了，都报这个错误，原因是JDBC 驱动搞不清楚断开的具体原因，统统当 net_write_timeout 了</p>
<p>一定要记住这个原理。如果这里不理解可以进一步阅读：<a target="_blank" rel="noopener" href="https://wx.zsxq.com/dweb2/index/topic_detail/412251415855228">https://wx.zsxq.com/dweb2/index/topic_detail/412251415855228</a> </p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>首先把Tomcat 集群从负载均衡上摘一个下来，这样没有业务流量干扰更利于测试和分析日志</p>
<p>然后让迁移数据工具直接连这个没有流量的节点，问题仍然稳定重现。</p>
<p>进一步提取迁移工具的SQL，然后走API手工提交给Tomcat 执行，问题仍然稳定重现，现在重现越来越简单了，效率高多了。</p>
<h3 id="Tomcat-上抓包"><a href="#Tomcat-上抓包" class="headerlink" title="Tomcat 上抓包"></a>Tomcat 上抓包</h3><p>因为没有业务流量干扰，抓包很干净，但是因为DB 节点太多，所以量还是很大的，分析如抓包篇：<a target="_blank" rel="noopener" href="https://articles.zsxq.com/id_lznw3w4zieuc.html">https://articles.zsxq.com/id_lznw3w4zieuc.html</a>  </p>
<p>如下图红框所示的地方可以看到MySQL Server 传着传着居然带了个 fin 包在里面，表示MySQL Server要断开连接了，无奈Client只能也发送quit 断开连接。红框告诉我们一个无比有力的证据MySQL Server 在不应该断开的地方断开了连接，问题在 MySQL Server 端</p>
<p><img src="/images/951413iMgBlog/image-20230620141017987.png" alt="image-20230620141017987"></p>
<p>看起来是Database 主动端开了连接，因为这个过程Tomcat 不需要发任何东西给 Database。这个现象5年前在其它用户场景下就抓到过了，最后问题也不了了之，这次希望搞清楚</p>
<h3 id="Database-分析"><a href="#Database-分析" class="headerlink" title="Database 分析"></a>Database 分析</h3><p>打开 DB 日志，捞取全量日志可以看到 DB 断开的原因是收到了kill Query！</p>
<p>有这个结果记住上面抓包图，以后类似这样莫名起来 DB 主动断开大概率就是 kill Query 导致的(经验攒得不容易！)</p>
<h3 id="Database-抓包"><a href="#Database-抓包" class="headerlink" title="Database 抓包"></a>Database 抓包</h3><p>确实能抓到kill，而且从用户账号来看就是从 Tomcat 发过去的！</p>
<h3 id="继续分析Tomcat-抓包"><a href="#继续分析Tomcat-抓包" class="headerlink" title="继续分析Tomcat 抓包"></a>继续分析Tomcat 抓包</h3><p>从 DB 分析来看还是有人主动 kill 导致的，所以继续分析Tomcat的抓包看是不是因为代码bug导致Tomcat 发了kill 给DB</p>
<p>大海捞针，搜 kill，找Tomcat 发给DB的tcp length 长度是16-20的(刚好容纳kill id) 总的来说就是找不到，很神奇</p>
<p>由于 DB上记录的 Tomcat IP、port 都被中间链路转换过几次了，根本没办法一一对应搞清楚是哪个Tomcat 节点发出来的</p>
<h3 id="继续尝试重现"><a href="#继续尝试重现" class="headerlink" title="继续尝试重现"></a>继续尝试重现</h3><p>分析完Tomcat 业务代码后感觉业务不会去kill，于是灵机一动在没有流量的Tomcat上跑了一个Sleep 600秒，不用任何数据，神奇的问题也稳定重现了，这下大概知道什么原因了，肯定是客户自己加了慢查询监控逻辑，一旦发现慢查询就 kill</p>
<p>于是问客户是不是有这种监控，果然有，停掉后反复重试不再有问题！</p>
<p>测试环境手工触发kill，然后能抓到下发的kill Query 给Database</p>
<p><img src="/images/951413iMgBlog/image-20230707150658392.png" alt="image-20230707150658392"></p>
<h2 id="未解谜题"><a href="#未解谜题" class="headerlink" title="未解谜题"></a>未解谜题</h2><p>为什么没在Tomcat 抓到发给Database的 kill ？</p>
<p>我反复去重现了，如果是我手工触发Tomcat kill是可以清晰地抓到Tomcat 会发160个kill 给Database，但是我任其自然等待用户监控来杀就一定抓不到kill 下发给DB</p>
<p>我猜和 Tomcat 集群有关，首先用户监控是走的LVS，通过其中一个Tomcat 可以查询到所有 Tomcat 上的请求，然后发起 kill</p>
<p>但因为节点太多无法证实！当然业务监控也可以监控DB 然后直接发kill，但是和抓包看到的发起kill的用户不对，发起 kill 的用户是Tomcat独一无二的。</p>
<h2 id="JDBC驱动报错-net-write-timeout-结论"><a href="#JDBC驱动报错-net-write-timeout-结论" class="headerlink" title="JDBC驱动报错 net_write_timeout 结论"></a>JDBC驱动报错 net_write_timeout 结论</h2><blockquote>
<p>Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server.</p>
</blockquote>
<p>这个报错不一定是 <code>net_write_timeout</code> 设置过小导致的，<strong>JDBC 在 streaming 流模式下只要连接异常就会报如上错误</strong>，比如：</p>
<ul>
<li>连接被 TCP reset</li>
<li>RDS 前端自带的Proxy 主动断开连接</li>
<li>连接因为某种原因(比如 QueryTimeOut) 触发 kill Query导致连接中断</li>
<li>RDS <a target="_blank" rel="noopener" href="https://aone.alibaba-inc.com/v2/project/687880/bug/50491193">端因为</a>kill 主动断开连接 &#x2F;&#x2F;比如用户监控RDS、DRDS脚本杀掉慢查询</li>
</ul>
<p>net_write_timeout：表示这么长时间RDS&#x2F;DN 无法写数据到网络层发给DRDS&#x2F;CN，原因是DRDS&#x2F;CN 长时间没将数据读走</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>首先一个错误现象对应多个完全不一样的错误原因是非常头疼的，这个问题反反复复在多个场景下出现，当然原因各异，但是这个传数据途中 DB 主动 fin连接还是第一次碰到并搞清楚，同样主动 fin 不一定是kill，但是我们要依照证据推进问题，既然是DB fin就有必要先从DB 来看原因。</p>
<p>从这个问题你可以先从什么是JDBC 流模式出发(mysql –quick 就是流模式，你可以快速查一个大数据试试；然后去掉–quick 对比一下)，结合网络buffer 来了解流模式：<a href="https://plantegg.github.io/2020/07/03/MySQL%20JDBC%20StreamResult%20%E5%92%8C%20net_write_timeout/">https://plantegg.github.io/2020/07/03/MySQL%20JDBC%20StreamResult%20%E5%92%8C%20net_write_timeout/</a></p>
<p>然后从流模式来学习MySQL 的 net_write_timeout，假如你的代码报了 net_write_timeout 你会分析吗？</p>
<p>最后从连接断开去总结，比如网络不好、比如内核bug、比如DB crash、比如 kill、比如……都会导致连接断开，但这一切对业务来说只有 net_write_timeout 一个现象</p>
<p>这个问题分享出来是因为非常综合，我惊抱怨 socketTimeout、Communication failure等异常，这些异常也挺常见导致的原因多种，但是和 net_write_timeout 比起来还是不如 net_write_timeout 更综合，所以分享给大家，建议这几篇一起阅读效果最好！</p>
<h2 id="实验模拟-Consider-raising-value-of-‘net-write-timeout’"><a href="#实验模拟-Consider-raising-value-of-‘net-write-timeout’" class="headerlink" title="实验模拟 Consider raising value of ‘net_write_timeout’"></a>实验模拟 Consider raising value of ‘net_write_timeout’</h2><p>使用 Java MySQL JDBC Driver 的同学经常碰到如下错误堆栈，到底这个错误是 net_write_timeout 设置太小还是别的原因也会导致这个问题？需要我们用实验验证一下： </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of &#x27;net_write_timeout&#x27; on the server.</span><br><span class="line">	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)</span><br><span class="line">	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)</span><br><span class="line">	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)</span><br><span class="line">	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425)</span><br><span class="line">	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3559)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3459)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3900)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:873)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.nextRow(MysqlIO.java:1996)</span><br><span class="line">	at com.mysql.jdbc.RowDataDynamic.nextRecord(RowDataDynamic.java:374)</span><br><span class="line">	at com.mysql.jdbc.RowDataDynamic.next(RowDataDynamic.java:354)</span><br><span class="line">	at com.mysql.jdbc.ResultSetImpl.next(ResultSetImpl.java:6312)</span><br><span class="line">	at Test.main(Test.java:38)</span><br><span class="line">Caused by: java.io.EOFException: Can not read response from server. Expected to read 8 bytes, read 3 bytes before connection was unexpectedly lost.</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:3011)</span><br><span class="line">	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3519)</span><br><span class="line">	... 8 more</span><br></pre></td></tr></table></figure>

<p>JDBC 驱动对这个错误有如下提示(坑人)：</p>
<blockquote>
<p>Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server. - com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Application was streaming results when the connection failed. Consider raising value of ‘net_write_timeout’ on the server.</p>
</blockquote>
<p>实验中的一些说明：</p>
<ol>
<li>netTimeoutForStreamingResults&#x3D;1 表示设置 net_write_timeout 为 1 秒，客户端会发送 set net_write_timeout&#x3D;1 给数据库</li>
<li>conn.setAutoCommit(false); &#x2F;&#x2F;流式读取必须要 关闭自动提交</li>
<li>stmt.setFetchSize(Integer.MIN_VALUE);</li>
</ol>
<p>以上 2&#x2F;3是触发流式读取的必要条件，第一条不设置默认是 600 秒，比较难等 :) </p>
<p>如果确实是 net_write_timeout 太小超时了, RDS 直接发 fin（但是 fin 前面还有一堆 response 包也在排队），然后 RDS 日志先报错：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2024-11-28T14:33:03.447397Z 12 [Note] Aborted connection 12 to db: &#x27;test&#x27; user: &#x27;root&#x27; host: &#x27;172.26.137.130&#x27; (Got timeout writing communication packets)</span><br></pre></td></tr></table></figure>

<p>此时客户端还慢悠悠地读，RDS 没有回任何错误信息给客户端，客户端读完所有 Response 然后直接读到连接断开就报 Consider raising value of ‘net_write_timeout’ on the server 了，如果客户端读的慢，比如要 10 分钟实际连接在 RDS 上 10 分钟前就进入 fin 了，但是 10 分钟后客户端才报错</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#netstat -anto | grep &quot;3307&quot;</span><br><span class="line">tcp6       0      0 :::3307                 :::*                    LISTEN      off (0.00/0/0)</span><br><span class="line">tcp6       0 140192 172.26.137.120:3307     172.26.137.130:51216    ESTABLISHED probe (0.04/0/0)</span><br><span class="line">2024年 11月 28日 星期四 15:01:43 CST</span><br><span class="line"></span><br><span class="line">//1秒中后此时 数据库感知到超时于是调用 close 断开连接，触发发送 fin给客户端，但是 fin 也需要排队，所以 140192增加了 1 变成140193</span><br><span class="line">tcp6       0      0 :::3307                 :::*                    LISTEN      off (0.00/0/0)</span><br><span class="line">tcp6       0 140193 172.26.137.120:3307     172.26.137.130:51216    FIN_WAIT1   probe (0.58/0/1)</span><br><span class="line">2024年 11月 28日 星期四 15:01:44 CST</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>重现代码，数据库上构造一个大表，比如 10万行就行，能堆满默认的 tcp buffer size：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">import java.sql.Connection;</span><br><span class="line">import java.sql.DriverManager;</span><br><span class="line">import java.sql.ResultSet;</span><br><span class="line">import java.sql.SQLException;</span><br><span class="line">import java.sql.Statement;</span><br><span class="line">import java.sql.PreparedStatement;</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * 编译：</span><br><span class="line"> *  javac -cp /root/java/*:. Test.java</span><br><span class="line"> *  运行：</span><br><span class="line"> *   java -cp .:./mysql-connector-java-5.1.45.jar Test &quot;jdbc:mysql://gf1:3307/test?useSSL=false&amp;useServerPrepStmts=true&amp;cachePrepStmts=true&amp;connectTimeout=500&amp;socketTimeout=1700&amp;netTimeoutForStreamingResults=1&quot; root 123 &quot;select *, id from streaming &quot; 3000</span><br><span class="line"> * netTimeoutForStreamingResults=1 表示RDS 等超过 1 秒都因为 tcp buffer 满无法继续发送数据就断开连接</span><br><span class="line"> *   */</span><br><span class="line">public class Test &#123;</span><br><span class="line">    private static String url;</span><br><span class="line">    private Str name;</span><br><span class="line"></span><br><span class="line">    public static void main(String args[]) throws NumberFormatException, InterruptedException, ClassNotFoundException &#123;</span><br><span class="line">        Class.forName(&quot;com.mysql.jdbc.Driver&quot;);</span><br><span class="line">        url = args[0];</span><br><span class="line">        String user = args[1];</span><br><span class="line">        String pass = args[2];</span><br><span class="line">        String sql = args[3];</span><br><span class="line">        String interval = args[4];</span><br><span class="line">        try &#123;</span><br><span class="line">            Connection conn = DriverManager.getConnection(url, user, pass);</span><br><span class="line">            while (true) &#123;</span><br><span class="line"></span><br><span class="line">								conn.setAutoCommit(false);</span><br><span class="line">								Statement stmt = conn.createStatement();</span><br><span class="line">								stmt.setFetchSize(Integer.MIN_VALUE);</span><br><span class="line"></span><br><span class="line">                long start = System.currentTimeMillis();</span><br><span class="line">                ResultSet rs = stmt.executeQuery(sql);</span><br><span class="line">                int count=0;</span><br><span class="line">                while (rs.next()) &#123;</span><br><span class="line">                  System.out.println(&quot;id:&quot;+rs.getInt(&quot;id&quot;)+&quot; count:&quot;+count);</span><br><span class="line">                  count++;</span><br><span class="line">                  if(count&lt;3) //1 秒后数据库端连接就已经关闭了，但是因为客户端读得慢，需要不 sleep 后才能读到 fin 然后报错，所以报错可以比实际晚很久</span><br><span class="line">                		Thread.sleep(1500);</span><br><span class="line">								&#125;</span><br><span class="line">                rs.close();</span><br><span class="line">                stmt.close();</span><br><span class="line">                Thread.sleep(Long.valueOf(interval));</span><br><span class="line">									break;</span><br><span class="line">            &#125;</span><br><span class="line">	    			conn.close();</span><br><span class="line">        &#125; catch (SQLException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Consider raising value of ‘net_write_timeout’ 这个报错数据库端不会返回任何错误码给客户端，只是发 fin 断开连接，对客户端来说这条连接是 net_write_timeout 超时了 还是 被kill(或者其他原因) 是没法区分的，所以不管什么原因，只要连接异常 MySQL JDBC Driver 就抛 net_write_timeout 错误</p>
<p>如图，3 秒钟后 fin 包混在数据库 response 就被发到了客户端，实际2 秒前数据库已经报错了，也就是客户端和数据库端报错时间会差 2 秒(具体差几秒取决于重现代码里 sleep 多久然后-1)</p>
<p><img src="/images/951413iMgBlog/image-20241128151324385.png" alt="image-20241128151324385"></p>
<h3 id="实验总结"><a href="#实验总结" class="headerlink" title="实验总结"></a>实验总结</h3><p>这个报错不一定是 <code>net_write_timeout</code> 设置过小导致的，<strong>JDBC 在 streaming 流模式下只要连接异常就会报如上错误</strong>，比如：</p>
<ul>
<li>连接被 TCP reset</li>
<li>RDS 前端自带的Proxy 主动断开连接</li>
<li>连接因为某种原因(比如 QueryTimeOut) 触发 kill Query导致连接中断</li>
<li>RDS 端因为kill 主动断开连接 &#x2F;&#x2F;比如用户监控RDS 脚本杀掉慢查询</li>
<li>开启流式读取后，只要客户端在读取查询结果没有结束就读到了 fin 包就会报这个错误</li>
</ul>
<p>可以将 netTimeoutForStreamingResults 设为 0 或者 100，然后在中途 kill 掉 MySQL 上的 SQL，你也会在客户端看到同样的错误, kill SQL 是在 MySQL 的报错日志中都是同样的：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2024-11-28T07:33:12.967012Z 23 [Note] Aborted connection 23 to db: &#x27;test&#x27; user: &#x27;root&#x27; host: &#x27;172.26.137.130&#x27; (Got an error writing communication packets)</span><br></pre></td></tr></table></figure>

<p>所以你看一旦客户端出现这个异常堆栈，除了抓包似乎没什么好办法，其实抓包也只能抓到数据库主动发了 fin 什么原因还是不知道，我恨这个没有错误码一统江湖的报错</p>
<p>net_write_timeout 后 RDS 直接发 fin（有时 fin 前面还有一堆 response 包也在排队），然后 rds 日志先报错：2024-11-28T06:33:03.447397Z 12 [Note] Aborted connection 12 to db: ‘test’ user: ‘root’ host: ‘172.26.137.130’ (Got timeout writing communication packets)</p>
<p>客户端慢悠悠地读，RDS 没有传任何错误信息给客户端，客户端读完所有 response 然后直接读到连接断开就报 Consider raising value of ‘net_write_timeout’ on the server 了，如果客户端读的慢，比如要 10 分钟实际连接在 RDS 上 10 分钟前就进入 fin 了，但是 10 分钟后客户端才报错</p>
<p>进阶阅读：<a href="https://plantegg.github.io/2024/09/25/%E4%B8%80%E4%B8%AA%E5%8E%86%E6%97%B65%E5%B9%B4%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/">https://plantegg.github.io/2024/09/25/%E4%B8%80%E4%B8%AA%E5%8E%86%E6%97%B65%E5%B9%B4%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90/</a>  和 <a target="_blank" rel="noopener" href="https://x.com/plantegg/status/1867535551337050153">https://x.com/plantegg/status/1867535551337050153</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/05/05/%E9%95%BF%E8%BF%9E%E6%8E%A5%E9%BB%91%E6%B4%9E%E9%87%8D%E7%8E%B0%E5%92%8C%E5%88%86%E6%9E%90-public/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/05/%E9%95%BF%E8%BF%9E%E6%8E%A5%E9%BB%91%E6%B4%9E%E9%87%8D%E7%8E%B0%E5%92%8C%E5%88%86%E6%9E%90-public/" class="post-title-link" itemprop="url">长连接黑洞重现和分析</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-05 08:30:03" itemprop="dateCreated datePublished" datetime="2024-05-05T08:30:03+08:00">2024-05-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/network/" itemprop="url" rel="index"><span itemprop="name">network</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="长连接黑洞重现和分析"><a href="#长连接黑洞重现和分析" class="headerlink" title="长连接黑洞重现和分析"></a>长连接黑洞重现和分析</h1><p>这是一个存在多年，遍及各个不同的业务又反反复复地在集团内部出现的一个问题，本文先通过重现展示这个问题，然后从业务、数据库、OS等不同的角度来分析如何解决它，这个问题值得每一位研发同学重视起来，避免再次踩到</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了高效率应对故障，本文尝试回答如下一些问题：</p>
<ul>
<li>为什么数据库crash 重启恢复后，业务还长时间不能恢复？</li>
<li>我依赖的业务做了高可用切换，但是我的业务长时间报错</li>
<li>我依赖的服务下掉了一个节点，为什么我的业务长时间报错 </li>
<li>客户做变配，升级云服务节点规格，为什么会导致客户业务长时间报错</li>
</ul>
<p>目的：希望通过这篇文章尽可能地减少故障时长、让业务快速从故障中恢复</p>
<h2 id="重现"><a href="#重现" class="headerlink" title="重现"></a>重现</h2><p>空说无凭，先也通过一次真实的重现来展示这个问题</p>
<h3 id="LVS-MySQL-高可用切换"><a href="#LVS-MySQL-高可用切换" class="headerlink" title="LVS+MySQL 高可用切换"></a>LVS+MySQL 高可用切换</h3><p>OS 默认配置参数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#sysctl -a |grep -E &quot;tcp_retries|keepalive&quot;</span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 30</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 5</span><br><span class="line">net.ipv4.tcp_keepalive_time = 10</span><br><span class="line">net.ipv4.tcp_retries1 = 3</span><br><span class="line">net.ipv4.tcp_retries2 = 15  //主要是这个参数，默认以及alios 几乎都是15</span><br></pre></td></tr></table></figure>



<p>LVS 对外服务端口是3001， 后面挂的是 3307，假设3307是当前的Master，Slave是 3306，当检测到3307异常后会从LVS 上摘掉 3307挂上 3306做高可用切换</p>
<p><img src="/images/951413iMgBlog/1713838496899-274cdfbd-aa6e-4f1f-9fcc-16725593c25e.png" alt="undefined"></p>
<p>切换前的 LVS 状态</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#ipvsadm -L --timeout</span><br><span class="line">Timeout (tcp tcpfin udp): 900 120 300</span><br><span class="line">#ipvsadm -L -n</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  127.0.0.1:3001 rr</span><br><span class="line">  -&gt; 127.0.0.1:3307               Masq    1      0          0</span><br></pre></td></tr></table></figure>

<p>Sysbench启动压力模拟用户访问，在 31秒的时候模拟管控检测到 3307的Master无法访问，所以管控执行切主把 3306的Slave 提升为新的 Master，同时到 LVS 摘掉 3307，挂上3306，此时管控端着冰可乐、翘着二郎腿，得意地说，你就看吧我们管控牛逼不、我们的高可用牛逼不，这一套行云流水3秒钟不到全搞定</p>
<p>切换命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#cat del3307.sh</span><br><span class="line">ipvsadm -d -t  127.0.0.1:3001 -r 127.0.0.1:3307 ; ipvsadm -a -t  127.0.0.1:3001 -r 127.0.0.1:3306 -m</span><br></pre></td></tr></table></figure>

<p>此时Sysbench运行状态，在第 32秒如期跌0：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#/usr/local/bin/sysbench --debug=on --mysql-user=&#x27;root&#x27; --mysql-password=&#x27;123&#x27; --mysql-db=&#x27;test&#x27; --mysql-host=&#x27;127.0.0.1&#x27; --mysql-port=&#x27;3001&#x27; --tables=&#x27;16&#x27;  --table-size=&#x27;10000&#x27; --range-size=&#x27;5&#x27; --db-ps-mode=&#x27;disable&#x27; --skip-trx=&#x27;on&#x27; --mysql-ignore-errors=&#x27;all&#x27; --time=&#x27;11080&#x27; --report-interval=&#x27;1&#x27; --histogram=&#x27;on&#x27; --threads=1 oltp_read_write run</span><br><span class="line">sysbench 1.1.0 (using bundled LuaJIT 2.1.0-beta3)</span><br><span class="line"></span><br><span class="line">Running the test with following options:</span><br><span class="line">Number of threads: 1</span><br><span class="line">Report intermediate results every 1 second(s)</span><br><span class="line">Debug mode enabled.</span><br><span class="line"></span><br><span class="line">Initializing random number generator from current time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Initializing worker threads...</span><br><span class="line"></span><br><span class="line">DEBUG: Worker thread (#0) started</span><br><span class="line">DEBUG: Reporting thread started</span><br><span class="line">DEBUG: Worker thread (#0) initialized</span><br><span class="line">Threads started!</span><br><span class="line"></span><br><span class="line">[ 1s ] thds: 1 tps: 51.89 qps: 947.00 (r/w/o: 739.44/207.56/0.00) lat (ms,95%): 35.59 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 2s ] thds: 1 tps: 60.03 qps: 1084.54 (r/w/o: 841.42/243.12/0.00) lat (ms,95%): 22.28 err/s 0.00 reconn/s: 0.00</span><br><span class="line">…………</span><br><span class="line">[ 29s ] thds: 1 tps: 68.00 qps: 1223.01 (r/w/o: 952.00/271.00/0.00) lat (ms,95%): 16.12 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 30s ] thds: 1 tps: 66.00 qps: 1188.00 (r/w/o: 924.00/264.00/0.00) lat (ms,95%): 16.71 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 31s ] thds: 1 tps: 67.00 qps: 1203.96 (r/w/o: 937.97/265.99/0.00) lat (ms,95%): 17.95 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 32s ] thds: 1 tps: 22.99 qps: 416.85 (r/w/o: 321.88/94.96/0.00) lat (ms,95%): 15.55 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 33s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 34s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 35s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br></pre></td></tr></table></figure>

<p>5分钟后故障报告大批量涌进来，客户：怎么回事，我们的业务挂掉10分钟了，报错都是访问MySQL 超时，赶紧给我看看，从监控确实看到10分钟后客户业务还没恢复：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ 601s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 602s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 603s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 604s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br></pre></td></tr></table></figure>

<p>这时 oncall 都被从被窝里拎了起来，不知谁说了一句赶紧恢复吧，先试试把应用重启，5秒钟后应用重启完毕，业务恢复，大家开心地笑了，又成功防御住一次故障升级，还是重启大法好！</p>
<p>在业务&#x2F;Sysbench QPS跌0 期间可以看到 3307被摘掉，3306 成功挂上去了，但是没有新连接建向 3306，业务&#x2F;Sysbench 使劲薅着 3307</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#ipvsadm -L -n --stats -t 127.0.0.1:3001</span><br><span class="line">Prot LocalAddress:Port               Conns   InPkts  OutPkts  InBytes OutBytes</span><br><span class="line">  -&gt; RemoteAddress:Port</span><br><span class="line">TCP  127.0.0.1:3001                      2   660294   661999 78202968  184940K</span><br><span class="line">  -&gt; 127.0.0.1:3306                      0        0        0        0        0</span><br><span class="line">  </span><br><span class="line">#ipvsadm -Lcn | head -10</span><br><span class="line">IPVS connection entries</span><br><span class="line">pro expire state       source             virtual            destination</span><br><span class="line">TCP 13:11  ESTABLISHED 127.0.0.1:33864    127.0.0.1:3001     127.0.0.1:3307</span><br><span class="line"></span><br><span class="line">#netstat -anto |grep -E &quot;Recv|33864|3001|33077&quot;</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       Timer</span><br><span class="line">tcp        0    248 127.0.0.1:33864         127.0.0.1:3001          ESTABLISHED probe (33.48/0/8)</span><br><span class="line">tcp6       0     11 127.0.0.1:3307          127.0.0.1:33864         ESTABLISHED on (49.03/13/0)</span><br></pre></td></tr></table></figure>

<p>直到 900多秒后 OS 重试了15次发现都失败，于是向业务&#x2F;Sysbench 返回连接异常，触发业务&#x2F;Sysbench 释放异常连接重建新连接，新连接指向了新的 Master 3306，业务恢复正常</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[ 957s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">DEBUG: Ignoring error 2013 Lost connection to MySQL server during query,</span><br><span class="line">DEBUG: Reconnecting </span><br><span class="line">DEBUG: Reconnected</span><br><span class="line">[ 958s ] thds: 1 tps: 53.00 qps: 950.97 (r/w/o: 741.98/208.99/0.00) lat (ms,95%): 30.26 err/s 0.00 reconn/s: 1.00</span><br><span class="line">[ 959s ] thds: 1 tps: 64.00 qps: 1154.03 (r/w/o: 896.02/258.01/0.00) lat (ms,95%): 22.69 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 960s ] thds: 1 tps: 66.00 qps: 1184.93 (r/w/o: 923.94/260.98/0.00) lat (ms,95%): 25.28 err/s 0.00 reconn/s: 0.00</span><br></pre></td></tr></table></figure>

<p>到这里重现了故障中经常碰到的业务需要900多秒才能慢慢恢复，这个问题也就是 <strong>TCP 长连接流量黑洞</strong></p>
<p>如果我们<strong>把 net.ipv4.tcp_retries2 改成5</strong> 再来做这个实验，就会发现业务&#x2F;Sysbench 只需要20秒就能恢复了，也就是这个流量黑洞从900多秒变成了20秒，这回 oncall 不用再被从被窝里拎出来了吧：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[ 62s ] thds: 1 tps: 66.00 qps: 1191.00 (r/w/o: 924.00/267.00/0.00) lat (ms,95%): 17.63 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 63s ] thds: 1 tps: 63.00 qps: 1123.01 (r/w/o: 874.00/249.00/0.00) lat (ms,95%): 17.63 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 64s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 65s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 66s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 67s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 68s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 69s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 70s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 71s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 72s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 73s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 74s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 75s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 76s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 77s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 78s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 79s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 80s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 81s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 82s ] thds: 1 tps: 0.00 qps: 0.00 (r/w/o: 0.00/0.00/0.00) lat (ms,95%): 0.00 err/s 0.00 reconn/s: 0.00</span><br><span class="line">DEBUG: Ignoring error 2013 Lost connection to MySQL server during query,</span><br><span class="line">DEBUG: Reconnecting </span><br><span class="line">DEBUG: Reconnected</span><br><span class="line">[ 83s ] thds: 1 tps: 26.00 qps: 457.01 (r/w/o: 357.01/100.00/0.00) lat (ms,95%): 16.41 err/s 0.00 reconn/s: 1.00</span><br><span class="line">[ 84s ] thds: 1 tps: 60.00 qps: 1086.94 (r/w/o: 846.96/239.99/0.00) lat (ms,95%): 26.68 err/s 0.00 reconn/s: 0.00</span><br><span class="line">[ 85s ] thds: 1 tps: 63.00 qps: 1134.02 (r/w/o: 882.01/252.00/0.00) lat (ms,95%): 23.10 err/s 0.00 reconn/s: 0.00</span><br></pre></td></tr></table></figure>

<h3 id="LVS-Nginx-上重现"><a href="#LVS-Nginx-上重现" class="headerlink" title="LVS + Nginx 上重现"></a>LVS + Nginx 上重现</h3><p>NGINX上重现这个问题：<a target="_blank" rel="noopener" href="https://asciinema.org/a/649890">https://asciinema.org/a/649890</a> 3分钟的录屏，这个视频构造了一个LVS 的HA切换过程，LVS后有两个Nginx，模拟一个Nginx(Master) 断网后，将第二个Nginx(Slave) 加入到LVS 并将第一个Nginx(Master) 从LVS 摘除，期望业务能立即恢复，但实际上可以看到之前的所有长连接都没有办法恢复，进入一个流量黑洞</p>
<h2 id="TCP-长连接流量黑洞原理总结"><a href="#TCP-长连接流量黑洞原理总结" class="headerlink" title="TCP 长连接流量黑洞原理总结"></a>TCP 长连接流量黑洞原理总结</h2><p>TCP 长连接在发送包的时候，如果没收到ack 默认会进行15次重传(net.ipv4.tcp_retries2&#x3D;15, 这个不要较真，会根据RTO 时间大致是15次)，累加起来大概是924秒，所以我们经常看到业务需要15分钟左右才恢复。这个问题存在所有TCP长连接中(几乎没有业务还在用短连接吧？)，问题的本质和 LVS&#x2F;k8s Service 都没关系</p>
<p>我这里重现带上 LVS 只是为了场景演示方便 </p>
<p>这个问题的本质就是如果Server突然消失(宕机、断网，来不及发 RST )客户端如果正在发东西给Server就会遵循TCP 重传逻辑不断地TCP retran , 如果一直收不到Server 的ack，大约重传15次，900秒左右。所以不是因为有 LVS 导致了这个问题，而是在某些场景下 LVS 有能力处理得更优雅，比如删除 RealServer的时候 LVS 完全可以感知这个动作并 reset 掉其上所有长连接</p>
<p>为什么在K8S 上这个问题更明显呢，K8S 讲究的就是服务不可靠，随时干掉POD(切断网络），如果干POD 之前能kill -9(触发reset)、或者close 业务触发断开连接那还好，但是大多时候啥都没干，有强摘POD、有直接隔离等等，这些操作都会导致对端只能TCP retran</p>
<h2 id="怎么解决"><a href="#怎么解决" class="headerlink" title="怎么解决"></a>怎么解决</h2><h3 id="业务方"><a href="#业务方" class="headerlink" title="业务方"></a>业务方</h3><p>业务方要对自己的请求超时时间有控制和兜底，不能任由一个请求长时间 Hang 在那里</p>
<p>比如JDBC URL 支持设置 SocketTimeout、ConnectTimeout，我相信其他产品也有类似的参数，业务方要设置这些值，不设置就是如上重现里演示的900多秒后才恢复</p>
<h4 id="SocketTimeout"><a href="#SocketTimeout" class="headerlink" title="SocketTimeout"></a>SocketTimeout</h4><p>只要是连接有机会设置 SocketTimeout 就一定要设置，具体值可以根据你们能接受的慢查询来设置；分析、AP类的请求可以设置大一点</p>
<p><strong>最重要的：任何业务只要你用到了TCP 长连接一定要配置一个恰当的SocketTimeout</strong>，比如 Jedis 是连接池模式，底层超时之后，会销毁当前连接，下一次重新建连，就会连接到新的切换节点上去并恢复</p>
<h4 id="RFC-5482-TCP-USER-TIMEOUT"><a href="#RFC-5482-TCP-USER-TIMEOUT" class="headerlink" title="RFC 5482 TCP_USER_TIMEOUT"></a><a target="_blank" rel="noopener" href="https://datatracker.ietf.org/doc/html/rfc5482">RFC 5482</a> <code>TCP_USER_TIMEOUT</code></h4><p><a target="_blank" rel="noopener" href="https://datatracker.ietf.org/doc/html/rfc5482">RFC 5482</a> 中增加了<code>TCP_USER_TIMEOUT</code>这个配置，通常用于定制当 TCP 网络连接中出现数据传输问题时，可以等待多长时间前释放网络资源，对应Linux 这个 <a target="_blank" rel="noopener" href="https://github.com/torvalds/linux/commit/dca43c75e7e545694a9dd6288553f55c53e2a3a3">commit </a></p>
<p><code>TCP_USER_TIMEOUT</code> 是一个整数值，它指定了当 TCP 连接的数据包在发送后多长时间内未被确认（即没有收到 ACK），TCP 连接会考虑释放这个连接。</p>
<p>打个比方，设置 <code>TCP_USER_TIMEOUT</code> 后，应用程序就可以指定说：“如果在 30 秒内我发送的数据没有得到确认，那我就认定网络连接出了问题，不再尝试继续发送，而是直接断开连接。”这对于确保连接质量和维护用户体验是非常有帮助的。</p>
<p>在 Linux 中，可以使用 <code>setsockopt</code> 函数来设置某个特定 socket 的 <code>TCP_USER_TIMEOUT</code> 值：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">int timeout = 30000; // 30 seconds</span><br><span class="line">setsockopt(sock, IPPROTO_TCP, TCP_USER_TIMEOUT, (char *)&amp;timeout, sizeof(timeout));</span><br></pre></td></tr></table></figure>

<p>在这行代码中，<code>sock</code> 是已经 established 的 TCP socket，我们将该 socket 的 <code>TCP_USER_TIMEOUT</code> 设置为 30000 毫秒，也就是 30 秒。如果设置成功，这个 TCP 连接在发送数据包后 30 秒内如果没有收到 ACK 确认，将开始进行 TCP 连接的释放流程。</p>
<p>TCP_USER_TIMEOUT 相较 SocketTimeout 可以做到更精确(不影响慢查询)，SocketTimeout 超时是不区分ACK 还是请求响应时间的，但是 TCP_USER_TIMEOUT 要求下层的API、OS 都支持。比如 JDK 不支持 TCP_USER_TIMEOUT，但是 <a target="_blank" rel="noopener" href="https://github.com/tomasol/netty/commit/3010366d957d7b8106e353f99e15ccdb7d391d8f#diff-a998f73b7303461ca171432d10832884c6e7b0955d9f5634b9a8302b42a4706c">Netty 框架自己搞了Native</a> 来实现对 TCP_USER_TIMEOUT 以及其它OS 参数的设置，在这些基础上<a target="_blank" rel="noopener" href="https://github.com/redis/lettuce/pull/2499">Redis 的Java 客户端 lettuce 依赖了 Netty ，所以也可以设置 TCP_USER_TIMEOUT</a></p>
<p>原本我是想在Druid 上提个feature 来支持 TCP_USER_TIMEOUT，这样集团绝大部分业务都可以无感知解决掉这个问题，但查下来发现 JDK 不支持设置这个值，想要在Druid 里面实现设置 TCP_USER_TIMEOUT 的话，得像 Netty 一样走Native 绕过JDK 来设置，这对 Druid 而言有点重了</p>
<h4 id="ConnectTimeout"><a href="#ConnectTimeout" class="headerlink" title="ConnectTimeout"></a>ConnectTimeout</h4><p>这个值是针对新连接创建超时时间设置，一般设置3-5秒就够长了</p>
<h4 id="连接池"><a href="#连接池" class="headerlink" title="连接池"></a>连接池</h4><p>建议参考这篇 <a target="_blank" rel="noopener" href="https://help.aliyun.com/document_detail/181399.html">《数据库连接池配置推荐》</a>  这篇里的很多建议也适合业务、应用等，你把数据库看成一个普通服务就好理解了</p>
<p>补充下如果用的是Druid 数据库连接池不要用它来设置你的  SocketTimeout 参数，因为他有bug 导致你觉得设置了但实际没设置上，<a target="_blank" rel="noopener" href="https://github.com/alibaba/druid/releases/tag/1.2.22">2024-03-16号的1.2.22</a>这个Release 才fix，所以强烈建议你讲 SocketTimeout 写死在JDBC URL 中简单明了</p>
<h3 id="OS-兜底"><a href="#OS-兜底" class="headerlink" title="OS 兜底"></a>OS 兜底</h3><p>假如业务是一个AP查询&#x2F;一次慢请求，一次查询&#x2F;请求就是需要半个小时，将 SocketTimeout 设置太小影响正常的查询，那么可以将如下 OS参数改小，从 OS 层面进行兜底</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.tcp_retries2 = 8</span><br><span class="line">net.ipv4.tcp_syn_retries = 4</span><br></pre></td></tr></table></figure>

<h4 id="keepalive"><a href="#keepalive" class="headerlink" title="keepalive"></a>keepalive</h4><p>keepalive 默认 7200秒太长了，建议改成20秒，可以在OS 镜像层面固化，然后各个业务可以 patch 自己的值；</p>
<p>如果一条连接限制超过 900 秒 LVS就会Reset 这条连接，但是我们将keepalive 设置小于900秒的话，即使业务上一直闲置，因为有 keepalive 触发心跳包，让 LVS 不至于 Reset，这也就避免了当业务取连接使用的时候才发现连接已经不可用被断开了，往往这个时候业务抛错误的时间很和真正 Reset 时间还差了很多，不好排查</p>
<p>在触发 TCP retransmission 后会停止 keepalive 探测</p>
<h3 id="LVS"><a href="#LVS" class="headerlink" title="LVS"></a>LVS</h3><p>如果你们试用了aliyun的SLB，当摘除节点的时候支持你设置一个时间，过了这个时间 aliyun的SLB 就会向这些连接的客户端发 Reset 干掉这些流量，让客户端触发新建连接，从故障中快速恢复，这是一个实例维度的参数，建议云上所有产品都支持起来，管控可以在购买 aliyun的SLB 的时候设置一个默认值：</p>
<p> <code>connection_drain_timeout</code> </p>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><h3 id="神奇的900秒"><a href="#神奇的900秒" class="headerlink" title="神奇的900秒"></a>神奇的900秒</h3><p>上面阐述的长连接流量黑洞一般是900+秒就恢复了，有时候我们经常在日志中看到 CommunicationsException: Communications link failure 900秒之类的错误，恰好 LVS 也是设置的 900秒闲置 Reset</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#ipvsadm -L --timeout</span><br><span class="line">Timeout (tcp tcpfin udp): 900 120 300</span><br></pre></td></tr></table></figure>

<h3 id="为什么这个问题这几年才明显暴露"><a href="#为什么这个问题这几年才明显暴露" class="headerlink" title="为什么这个问题这几年才明显暴露"></a>为什么这个问题这几年才明显暴露</h3><ul>
<li>工程师们混沌了几十年</li>
<li>之前因为出现频率低重启业务就糊弄过去了</li>
<li>对新连接不存在这个问题</li>
<li>有些连接池配置了Check 机制(Check机制一般几秒钟超时 fail)</li>
<li>微服务多了</li>
<li>云上 LVS 普及了</li>
<li>k8s service 大行其道</li>
</ul>
<h3 id="我用的-7层是不是就没有这个问题了？"><a href="#我用的-7层是不是就没有这个问题了？" class="headerlink" title="我用的 7层是不是就没有这个问题了？"></a>我用的 7层是不是就没有这个问题了？</h3><p>幼稚，你4层都挂了7层还能蹦跶，再说一遍只要是 TCP 长连接就有这个问题</p>
<h3 id="极端情况"><a href="#极端情况" class="headerlink" title="极端情况"></a>极端情况</h3><p>A 长连接 访问B 服务，B服务到A网络不通，假如B发生HA，一般会先Reset&#x2F;断开B上所有连接(比如 MySQL 会去kill 所有processlist；比如重启MySQL——假如这里的B是MySQL)，但是因为网络不通这里的reset、fin网络包都无法到达A，所以B是无法兜底这个异常场景， A无法感知B不可用了，会使用旧连接大约15分钟</p>
<p>最可怕的是 B 服务不响应，B所在的OS 还在响应，那么在A的视角 网络是正常的，这时只能A自己来通过超时兜底</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这种问题在 LVS 场景下暴露更明显了，但是又和LVS 没啥关系，任何业务长连接都会导致这个 900秒左右的流量黑洞，首先要在业务层面重视这个问题，要不以后数据库一挂掉还得重启业务才能从故障中将恢复，所以业务层面处理好了可以避免900秒黑洞和重启业务，达到快速从故障中恢复</p>
<p>再强调下这个问题如果去掉LVS&#x2F;k8s Service&#x2F;软负载等让两个服务直连，然后拔网线也会同样出现</p>
<p>最佳实践总结：</p>
<ul>
<li>如果你的业务支持设置 SocketTimeout 那么请一定要设置，但不一定适合分析类就是需要长时间返回的请求</li>
<li>最好的方式是设置 OS 层面的 TCP_USER_TIMEOUT 参数，只要长时间没有 ack 就报错返回，但 JDK 不支持直接设置</li>
<li>如果用了 ALB&#x2F;SLB 就一定要配置 connection_drain_timeout 这个参数</li>
<li>OS 镜像层面也可以将 tcp_retries2 设置为5-10次做一个兜底</li>
<li>对你的超时时间做到可控、可预期</li>
</ul>
<p>假如你的业务不是 Java，而是 Python 的话，请参考Java&#x2F;Python超时参数对照表：</p>
<table>
<thead>
<tr>
<th><strong>功能</strong></th>
<th><strong>JDBC (Java)</strong></th>
<th><strong>mysql-connector-python</strong></th>
<th><strong>PyMySQL</strong></th>
</tr>
</thead>
<tbody><tr>
<td>连接建立超时</td>
<td>connectTimeout</td>
<td>connect_timeout</td>
<td>connect_timeout</td>
</tr>
<tr>
<td>读写操作超时</td>
<td>socketTimeout</td>
<td>connection_timeout</td>
<td>read_timeout&#x2F;write_timeout</td>
</tr>
<tr>
<td>连接池等待超时</td>
<td>poolTimeout</td>
<td>pool_timeout</td>
<td>需手动实现</td>
</tr>
</tbody></table>
<h2 id="相关故障和资料"><a href="#相关故障和资料" class="headerlink" title="相关故障和资料"></a>相关故障和资料</h2><p>ALB 黑洞问题详述：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/BJWD2V_RM2rnU1y7LPB9aw">https://mp.weixin.qq.com/s/BJWD2V_RM2rnU1y7LPB9aw</a></p>
<p>数据库故障引发的“血案” ：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/nullllun/p/15073022.html">https://www.cnblogs.com/nullllun/p/15073022.html</a> 这篇描述较细致，推荐看看</p>
<p>tcp_retries2 的解释：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">tcp_retries1 - INTEGER</span><br><span class="line">    This value influences the time, after which TCP decides, that</span><br><span class="line">    something is wrong due to unacknowledged RTO retransmissions,</span><br><span class="line">    and reports this suspicion to the network layer.</span><br><span class="line">    See tcp_retries2 for more details.</span><br><span class="line"></span><br><span class="line">    RFC 1122 recommends at least 3 retransmissions, which is the</span><br><span class="line">    default.</span><br><span class="line"></span><br><span class="line">tcp_retries2 - INTEGER</span><br><span class="line">    This value influences the timeout of an alive TCP connection,</span><br><span class="line">    when RTO retransmissions remain unacknowledged.</span><br><span class="line">    Given a value of N, a hypothetical TCP connection following</span><br><span class="line">    exponential backoff with an initial RTO of TCP_RTO_MIN would</span><br><span class="line">    retransmit N times before killing the connection at the (N+1)th RTO.</span><br><span class="line"></span><br><span class="line">    The default value of 15 yields a hypothetical timeout of 924.6</span><br><span class="line">    seconds and is a lower bound for the effective timeout.</span><br><span class="line">    TCP will effectively time out at the first RTO which exceeds the</span><br><span class="line">    hypothetical timeout.</span><br><span class="line"></span><br><span class="line">    RFC 1122 recommends at least 100 seconds for the timeout,</span><br><span class="line">    which corresponds to a value of at least 8.</span><br></pre></td></tr></table></figure>

<p>tcp_retries2 默认值为15，根据RTO的值来决定，相当于13-30分钟(RFC1122规定，必须大于100秒)，但是这是很多年前的拍下来古董参数值，现在网络条件好多了，尤其是内网，个人认为改成 5-10 是比较恰当 azure 建议：<a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-best-practices-connection">https://learn.microsoft.com/en-us/azure/azure-cache-for-redis/cache-best-practices-connection</a> ，Oracle RAC的建议值是3：<a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/726753">https://access.redhat.com/solutions/726753</a></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/05/03/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA-%E7%BB%AD%E7%AF%87/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/05/03/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA-%E7%BB%AD%E7%AF%87/" class="post-title-link" itemprop="url">十年后数据库还是不敢拥抱NUMA-续篇</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-05-03 12:30:03" itemprop="dateCreated datePublished" datetime="2024-05-03T12:30:03+08:00">2024-05-03</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="十年后数据库还是不敢拥抱NUMA-续篇"><a href="#十年后数据库还是不敢拥抱NUMA-续篇" class="headerlink" title="十年后数据库还是不敢拥抱NUMA-续篇"></a>十年后数据库还是不敢拥抱NUMA-续篇</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p><a href="https://plantegg.github.io/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">十年后数据库还是不敢拥抱NUMA</a>， 这篇经典的纠正大家对NUMA 认知的文章一晃发布快3年了，这篇文章的核心结论是：</p>
<ul>
<li>之所以有不同的NUMA Node 是不同的CPU Core 到不同的内存距离远近不一样所决定的，这是个物理距离</li>
<li>程序跑在不同的核上要去读写内存可以让性能差异巨大，所以我们要尽量让一个程序稳定跑在一个Node 内</li>
<li>默认打开NUMA Node 其实挺好的</li>
</ul>
<p>写这个续篇是我收到很多解释，因为跨Node 导致性能抖动，所以集团在物理机OS 的启动参数里设置了 numa&#x3D;off ，也就是不管BIOS 中如何设置，我们只要在OS 层面设置一下 numa&#x3D;off 就能让程序稳定下来不再抖了！</p>
<p>我这几年也认为这是对的，只是让我有点不理解，既然不区分远近了，那物理上存在的远近距离(既抖动)如何能被消除掉的呢？</p>
<p>所以这个续篇打算通过测试来验证下这个问题</p>
<h2 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h2><p>BIOS 中有 numa node 设置的开关(注意这里是内存交错&#x2F;交织)，不同的主板这个BIOS设置可能不一样，但是大同小异，基本都有这个参数</p>
<p><img src="/images/951413iMgBlog/FrVuhXNHEf2LzigZPHHV6c7UNKrP-5057597.png" alt="img">﻿</p>
<p>Linux 启动引导参数里也可以设置numa&#x3D;on(默认值)&#x2F;off ，linux 引导参数设置案例：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#cat /proc/cmdline</span><br><span class="line">BOOT_IMAGE=/vmlinuz-3.10.0-327.x86_64  ro crashkernel=auto vconsole.font=latarcyrheb-sun16 vconsole.keymap=us BIOSdevname=0 console=tty0 console=ttyS0,115200 scsi_mod.scan=sync intel_idle.max_cstate=0 pci=pcie_bus_perf ipv6.disable=1 rd.driver.pre=ahci numa=on nosmt=force</span><br></pre></td></tr></table></figure>

<p>注意如上的 numa&#x3D;on 也可以改为 numa&#x3D;off</p>
<p>看完全置篇要记住一条铁律：CPU到内存的距离是物理远近决定的，你软件层面做些设置是没法优化这个距离，也就是没法优化这个时延 (这是个核心知识点，你要死死记住和理解，后面的一切实验数据都回过头来看这个核心知识点并揣摩)</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>测试机器CPU，如下是BIOS numa&#x3D;on、cmdline numa&#x3D;off所看到的，一个node</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#lscpu</span><br><span class="line">Architecture:          x86_64</span><br><span class="line">CPU op-mode(s):        32-bit, 64-bit</span><br><span class="line">Byte Order:            Little Endian</span><br><span class="line">CPU(s):                96</span><br><span class="line">On-line CPU(s) list:   0-95</span><br><span class="line">Thread(s) per core:    2</span><br><span class="line">Core(s) per socket:    24</span><br><span class="line">Socket(s):             2</span><br><span class="line">NUMA node(s):          1</span><br><span class="line">Vendor ID:             GenuineIntel</span><br><span class="line">CPU family:            6</span><br><span class="line">Model:                 85</span><br><span class="line">Model name:            Intel(R) Xeon(R) Platinum 8163 CPU @ 2.50GHz</span><br><span class="line">Stepping:              4</span><br><span class="line">CPU MHz:               2500.000</span><br><span class="line">CPU max MHz:           3100.0000</span><br><span class="line">CPU min MHz:           1000.0000</span><br><span class="line">BogoMIPS:              4998.89</span><br><span class="line">Virtualization:        VT-x</span><br><span class="line">L1d cache:             32K</span><br><span class="line">L1i cache:             32K</span><br><span class="line">L2 cache:              1024K</span><br><span class="line">L3 cache:              33792K</span><br><span class="line">NUMA node0 CPU(s):     0-95</span><br></pre></td></tr></table></figure>



<p>测试工具是<a target="_blank" rel="noopener" href="https://github.com/intel/lmbench">lmbench</a>，测试命令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for i in $(seq 0 6 95); do echo core:$i; numactl -C $i -m 0 ./bin/lat_mem_rd -W 5 -N 5 -t 64M; done &gt;lat.log 2&gt;&amp;1</span><br></pre></td></tr></table></figure>

<p>上述测试命令始终将内存绑定在 node0 上，然后用不同的物理core来读写这块内存，按照<a target="_blank" rel="noopener" href="https://ata.atatech.org/articles/11000205974">前一篇</a> 这个时延肯定有快慢之分</p>
<p>BIOS和引导参数各有两种设置方式，组合起来就是四种，我们分别设置并跑一下内存时延，测试结果：</p>
<table>
<thead>
<tr>
<th></th>
<th>BIOS ON</th>
<th>BIOS OFF</th>
</tr>
</thead>
<tbody><tr>
<td>cmdline numa&#x3D;on（默认值）</td>
<td>NUMA 开启，内存在Node内做交织，就近有快慢之分</td>
<td>bios 关闭后numa后，OS层面完全不知道下层的结构，默认全局内存做交织，时延是个平均值</td>
</tr>
<tr>
<td>cmdline numa&#x3D;off</td>
<td>交织关闭，效果同上</td>
<td>同上</td>
</tr>
</tbody></table>
<p>测试原始数据如下(测试结果文件名 lat.log.BIOSON.cmdlineOff 表示BIOS ON，cmdline OFF )：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">//从下面两组测试来看，BIOS层面 on后，不管OS 层面是否on，都不会跨node 做交织，抖动存在</span><br><span class="line">//BIOS on 即使在OS层面关闭numa也不跨node做内存交织，抖动存在</span><br><span class="line">//默认从内存高地址开始分配空间，所以0核要慢</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.BIOSON.cmdlineOff </span><br><span class="line">core:0 //第0号核</span><br><span class="line">64.00000 100.717 //64.0000为64MB， 100.717 是平均时延100.717ns 即0号核访问node0 下的内存64MB的平均延时是100纳秒</span><br><span class="line">core:24</span><br><span class="line">64.00000 68.484</span><br><span class="line">core:48</span><br><span class="line">64.00000 101.070</span><br><span class="line">core:72</span><br><span class="line">64.00000 68.483</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.BIOSON.cmdlineON</span><br><span class="line">core:0</span><br><span class="line">64.00000 67.094</span><br><span class="line">core:24</span><br><span class="line">64.00000 100.237</span><br><span class="line">core:48</span><br><span class="line">64.00000 67.614</span><br><span class="line">core:72</span><br><span class="line">64.00000 101.096</span><br><span class="line"></span><br><span class="line">//从下面两组测试来看只要BIOS off了内存就会跨 node 交织，大规模测试下内存 latency 是个平均值</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.BIOSOff.cmdlineOff //BIOS off 做内存交织，latency就是平均值</span><br><span class="line">core:0</span><br><span class="line">64.00000 85.657  //85 恰好是最大100，最小68的平均值</span><br><span class="line">core:24</span><br><span class="line">64.00000 85.741</span><br><span class="line">core:48</span><br><span class="line">64.00000 85.977</span><br><span class="line">core:72</span><br><span class="line">64.00000 86.671</span><br><span class="line"></span><br><span class="line">//BIOS 关闭后numa后，OS层面完全不知道下层的结构，默认一定是做交织</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.BIOSOff.cmdlineON</span><br><span class="line">core:0</span><br><span class="line">64.00000 89.123</span><br><span class="line">core:24</span><br><span class="line">64.00000 87.137</span><br><span class="line">core:48</span><br><span class="line">64.00000 87.239</span><br><span class="line">core:72</span><br><span class="line">64.00000 87.323</span><br></pre></td></tr></table></figure>

<p>从数据可以看到在BIOS 设置ON后，无论 OS cmdline 启动参数里是否设置了 ON 还是 OFF，内存延时都是抖动且一致的(这个有点诧异，说好的消除抖动的呢？)。如果BIOS 设置OFF后内存延时是个稳定的平均值(这个比较好理解)</p>
<h2 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h2><ul>
<li>内存交错时为什么 lmbench 测试得到的时延是平均值，而不是短板效应的最慢值？</li>
</ul>
<p>测试软件只能通过大规模数据的读写来测试获取一个平均值，所以当一大块内存读取时，虽然通过交织大块内存被切分到了快慢物理内存上，但是因为规模大慢的被平均掉了。(欢迎内核大佬指正)</p>
<ul>
<li>什么是内存交织？</li>
</ul>
<p>我的理解假如你有8块物理内存条，如果你有一个int 那么只能在其中一块上，如果你有1MB的数据那么会按cacheline 拆成多个块然后分别放到8块物理内存条上(有快有慢)这样带宽更大，最后测试得到一个平均值</p>
<p>如果你开启numa那么只会就近交织，比如0-3号内存条在0号core所在的node，OS 做内存交织的时候只会拆分到这0-3号内存条上，那么时延总是最小的那个，如上测试中的60多纳秒。</p>
<p>这个问题一直困扰了我几年，所以我最近再次测试验证了一下，主要是对 BIOS&#x3D;on 且 cmdline&#x3D;off 时有点困扰</p>
<h2 id="Intel-的-mlc-验证"><a href="#Intel-的-mlc-验证" class="headerlink" title="Intel 的 mlc 验证"></a>Intel 的 mlc 验证</h2><p>测试参数: BIOS&#x3D;on 同时 cmdline off</p>
<p>用<a target="_blank" rel="noopener" href="https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html">Intel 的 mlc 验证下</a>，这个结果有点意思，latency稳定在 145 而不是81 和 145两个值随机出现，应该是mlc默认选到了0核，对应lmbench的这组测试数据(为什么不是100.717， 因为测试方法不一样)：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">//如下是</span><br><span class="line">//从下面两种测试来看，BIOS层面 on后，不管OS 层面是否on，都不会跨node 做交织，抖动存在</span><br><span class="line">//BIOS on 即使在OS层面关闭numa也不跨node做内存交织，抖动存在</span><br><span class="line">#grep -E &quot;core|64.00000&quot; lat.log.BIOSON.cmdlineOff </span><br><span class="line">core:0</span><br><span class="line">64.00000 100.717</span><br><span class="line">core:24</span><br><span class="line">64.00000 68.484</span><br><span class="line">core:48</span><br><span class="line">64.00000 101.070</span><br><span class="line">core:72</span><br><span class="line">64.00000 68.483</span><br></pre></td></tr></table></figure>

<p>此时对应的mlc</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#./mlc</span><br><span class="line">Intel(R) Memory Latency Checker - v3.9</span><br><span class="line">Measuring idle latencies (in ns)...</span><br><span class="line">    Numa node</span><br><span class="line">Numa node    0</span><br><span class="line">    0   145.8  //多次测试稳定都是145纳秒</span><br><span class="line"></span><br><span class="line">Measuring Peak Injection Memory Bandwidths for the system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using traffic with the following read-write ratios</span><br><span class="line">ALL Reads     :  110598.7</span><br><span class="line">3:1 Reads-Writes :  93408.5</span><br><span class="line">2:1 Reads-Writes :  89249.5</span><br><span class="line">1:1 Reads-Writes :  64137.3</span><br><span class="line">Stream-triad like:  77310.4</span><br><span class="line"></span><br><span class="line">Measuring Memory Bandwidths between nodes within system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using Read-only traffic type</span><br><span class="line">    Numa node</span><br><span class="line">Numa node    0</span><br><span class="line">    0  110598.4</span><br><span class="line"></span><br><span class="line">Measuring Loaded Latencies for the system</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using Read-only traffic type</span><br><span class="line">Inject  Latency Bandwidth</span><br><span class="line">Delay (ns)  MB/sec</span><br><span class="line">==========================</span><br><span class="line"> 00000  506.00   111483.5</span><br><span class="line"> 00002  505.74   112576.9</span><br><span class="line"> 00008  505.87   112644.3</span><br><span class="line"> 00015  508.96   112643.6</span><br><span class="line"> 00050  574.36   112701.5</span><br></pre></td></tr></table></figure>



<p>当两个参数都为 on 时的mlc 测试结果：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#./mlc</span><br><span class="line">Intel(R) Memory Latency Checker - v3.9</span><br><span class="line">Measuring idle latencies (in ns)...</span><br><span class="line">    Numa node</span><br><span class="line">Numa node    0     1</span><br><span class="line">    0    81.6   145.9</span><br><span class="line">    1   144.9    81.2</span><br><span class="line"></span><br><span class="line">Measuring Peak Injection Memory Bandwidths for the system</span><br><span class="line">Bandwidths are in MB/sec (1 MB/sec = 1,000,000 Bytes/sec)</span><br><span class="line">Using all the threads from each core if Hyper-threading is enabled</span><br><span class="line">Using traffic with the following read-write ratios</span><br><span class="line">ALL Reads     :  227204.2</span><br><span class="line">3:1 Reads-Writes :  212432.5</span><br><span class="line">2:1 Reads-Writes :  210423.3</span><br><span class="line">1:1 Reads-Writes :  196677.2</span><br><span class="line">Stream-triad like:  189691.4</span><br></pre></td></tr></table></figure>

<p>说明：mlc和 lmbench 测试结果不一样，mlc 时81和145，lmbench测试是68和100，这是两种测试方法的差异而已，但是快慢差距基本是一致的</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在OS 启动引导参数里设置 numa&#x3D;off 完全没有必要、也不能解决抖动的问题，反而设置了 numa&#x3D;off 只能是掩耳盗铃，让用户看不到 NUMA 结构</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/04/26/%E6%B5%81%E9%87%8F%E4%B8%80%E6%A0%B7%E4%BD%86%E4%B8%BA%E4%BB%80%E4%B9%88CPU%E4%BD%BF%E7%94%A8%E7%8E%87%E5%B7%AE%E5%88%AB%E5%BE%88%E5%A4%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/04/26/%E6%B5%81%E9%87%8F%E4%B8%80%E6%A0%B7%E4%BD%86%E4%B8%BA%E4%BB%80%E4%B9%88CPU%E4%BD%BF%E7%94%A8%E7%8E%87%E5%B7%AE%E5%88%AB%E5%BE%88%E5%A4%A7/" class="post-title-link" itemprop="url">流量一样但为什么CPU使用率差别很大</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-04-26 12:30:03" itemprop="dateCreated datePublished" datetime="2024-04-26T12:30:03+08:00">2024-04-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/CPU/" itemprop="url" rel="index"><span itemprop="name">CPU</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="流量一样但为什么CPU使用率差别很大"><a href="#流量一样但为什么CPU使用率差别很大" class="headerlink" title="流量一样但为什么CPU使用率差别很大"></a>流量一样但为什么CPU使用率差别很大</h1><p>这是我翻到2013年的一篇文章，当时惊动所有公司高人，最后分析得知原因后所有人都跪拜，你要知道那是2013年，正好10年过去了，如果是现在用我们星球的理论去套的话，简直不要太容易</p>
<h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><blockquote>
<p>同样大小内存、同样的CPU、同样数量的请求、几乎可以忽略的io，两个机器的load却差异挺大。一个机器的load是12左右，另外一个机器却是30左右</p>
<p>你可以理解这是两台一摸一样的物理机挂在一个LVS 下，LVS 分发流量绝对均衡</p>
</blockquote>
<p>所以要找出为什么？</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>两台机器的资源使用率：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">//load低、CPU使用率低 的物理机，省略一部分核</span><br><span class="line">Cpu0  : 67.1%us,  1.6%sy,  0.0%ni, 30.6%id,  0.0%wa,  0.0%hi,  0.7%si,  0.0%st</span><br><span class="line">Cpu1  : 64.1%us,  1.6%sy,  0.0%ni, 34.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu2  : 63.0%us,  1.6%sy,  0.0%ni, 35.4%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu3  : 60.0%us,  1.3%sy,  0.0%ni, 38.4%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu4  : 59.8%us,  1.3%sy,  0.0%ni, 37.9%id,  1.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu5  : 56.7%us,  1.0%sy,  0.0%ni, 42.3%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu6  : 63.4%us,  1.3%sy,  0.0%ni, 34.6%id,  0.0%wa,  0.0%hi,  0.7%si,  0.0%st</span><br><span class="line">Cpu7  : 62.5%us,  2.0%sy,  0.0%ni, 35.5%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu8  : 58.5%us,  1.3%sy,  0.0%ni, 39.5%id,  0.0%wa,  0.0%hi,  0.7%si,  0.0%st</span><br><span class="line">Cpu9  : 55.8%us,  1.6%sy,  0.0%ni, 42.2%id,  0.3%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line"></span><br><span class="line">//load高、CPU使用率高 的物理机，省略一部分核</span><br><span class="line">Cpu0  : 90.1%us,  1.9%sy,  0.0%ni,  7.1%id,  0.0%wa,  0.0%hi,  1.0%si,  0.0%st</span><br><span class="line">Cpu1  : 88.5%us,  2.9%sy,  0.0%ni,  8.0%id,  0.0%wa,  0.0%hi,  0.6%si,  0.0%st</span><br><span class="line">Cpu2  : 90.4%us,  1.9%sy,  0.0%ni,  7.7%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br><span class="line">Cpu3  : 86.9%us,  2.6%sy,  0.0%ni, 10.2%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu4  : 87.5%us,  1.9%sy,  0.0%ni, 10.2%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu5  : 87.3%us,  1.9%sy,  0.0%ni, 10.5%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu6  : 90.4%us,  2.9%sy,  0.0%ni,  6.4%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu7  : 90.1%us,  1.9%sy,  0.0%ni,  7.6%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st</span><br><span class="line">Cpu8  : 89.5%us,  2.6%sy,  0.0%ni,  6.7%id,  0.0%wa,  0.0%hi,  1.3%si,  0.0%st</span><br><span class="line">Cpu9  : 90.7%us,  1.9%sy,  0.0%ni,  7.4%id,  0.0%wa,  0.0%hi,  0.0%si,  0.0%st</span><br></pre></td></tr></table></figure>

<p>可以分析产出为什么低，检查CPU是否降频、内存频率是否有差异——检查结果一致</p>
<p>10年前经过一阵 perf top 看热点后终于醒悟过来知道得去看 IPC，也就是相同CPU使用率下，其中慢的机器产出低了一半，那么继续通过perf看IPC：</p>
<p><img src="/images/951413iMgBlog/FrsOfjsmHa6Zwv67IBgTd-GTI2fT.png" alt="img"></p>
<p>可以看到两台机器的IPC是 0.3 VS 0.55，和CPU使用率差异基本一致，instructions几乎一样(意味着流量一样，LVS 不背锅)，但是处理同样的instructions 用掉的cpu-clock 几乎差了一倍，这应该是典型的内存时延大了一倍导致的。IPC 大致等于 instrunctions&#x2F;cpu-clock （IPC：instrunctions per cycles）</p>
<p>经检查这两台物理机都是两路，虽然CPU型号&#x2F;内存频率一致，但是主板间跨Socket的 QPI带宽差了一倍(主板是两个不同的服务商提供)。可以通过绑核测试不同Socket&#x2F;Node 下内存时延来确认这个问题</p>
<p>这是同一台机器下两个Socket 的内存带宽，所以如果跨Socket 内存访问多了就会导致时延更高、CPU使用率更高</p>
<p><img src="/images/951413iMgBlog/FmaZP2Wf1xiSoHyi2xHslbAVr71_.png" alt="img"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在今天我们看到这种问题就很容易了，但我还是要感叹一下在入门前简直太神奇，入门后也不过尔尔，希望你也早点入门。</p>
<p>第一：向CPU要产出，同样的使用率产出得一样，不一样的话肯定是偷懒了，偷懒的直接证据就是 IPC 低了，导致IPC 低最常见的是内存时延高(内存频率、跨Node&#x2F;Socket 等，或者内存碎片)；延伸阅读：<a target="_blank" rel="noopener" href="https://t.zsxq.com/10fYf762S">性能的本质 IPC</a> ，也是本星球唯二的必读实验</p>
<p>第二：测试工具很完善了，<a target="_blank" rel="noopener" href="https://github.com/intel/lmbench">lmbench</a> , 怎么用lmbench <a href="https://plantegg.github.io/2022/01/13/%E4%B8%8D%E5%90%8CCPU%E6%80%A7%E8%83%BD%E5%A4%A7PK/">可以看这篇</a> ; 怎么使用perf <a href="https://plantegg.github.io/2021/05/16/Perf_IPC%E4%BB%A5%E5%8F%8ACPU%E5%88%A9%E7%94%A8%E7%8E%87/">Perf IPC以及CPU性能</a></p>
<p>，学成后装逼可以看 <a href="https://plantegg.github.io/2022/03/15/%E8%AE%B0%E4%B8%80%E6%AC%A1%E5%90%AC%E9%A3%8E%E6%89%87%E5%A3%B0%E9%9F%B3%E6%9D%A5%E5%AE%9A%E4%BD%8D%E6%80%A7%E8%83%BD/">听风扇声音来定位性能瓶颈</a> </p>
<p>我以前说过每个领域都有一些核心知识点，IPC 就是CPU领域的核心知识点，和tcp的rmem&#x2F;wmem 一样很容易引导你入门</p>
<p>计算机专业里非要挑几个必学的知识点肯定得有计算机组成原理，但计算机组成原理内容太多，都去看也不现实，况且很多过时的东西，那么我只希望你能记住计算机组成原理里有个最核心的麻烦：内存墙——CPU 访问内存太慢导致了内存墙是我们碰到众多性能问题的最主要、最核心的一个，结合今天这个案例掌握IPC后再来学内存墙，再到理解计算机组成原理就对了，从一个实用的小点入手。</p>
<p>计算机专业里除掉组成原理(有点高大上，没那么接地气)，另外一个我觉得最有用的是网络——看着low但是接地气，问题多，很实用</p>
<p>2011年的文章：</p>
<h4 id="详解服务器内存带宽计算和使用情况测量"><a href="#详解服务器内存带宽计算和使用情况测量" class="headerlink" title="详解服务器内存带宽计算和使用情况测量"></a><strong><a target="_blank" rel="noopener" href="http://blog.yufeng.info/archives/1511">详解服务器内存带宽计算和使用情况测量</a></strong></h4><p>更好的工具来发现类似问题：<a target="_blank" rel="noopener" href="https://github.com/intel/numatop">https://github.com/intel/numatop</a></p>
<p><img src="/images/951413iMgBlog/FlOhgPPnxN3DcMRPUvNvbZOuQy0q.png" alt="img"></p>
<h2 id="如果你觉得看完对你很有帮助可以通过如下方式找到我"><a href="#如果你觉得看完对你很有帮助可以通过如下方式找到我" class="headerlink" title="如果你觉得看完对你很有帮助可以通过如下方式找到我"></a>如果你觉得看完对你很有帮助可以通过如下方式找到我</h2><p>find me on twitter: <a target="_blank" rel="noopener" href="https://twitter.com/plantegg">@plantegg</a></p>
<p>知识星球：<a target="_blank" rel="noopener" href="https://t.zsxq.com/0cSFEUh2J">https://t.zsxq.com/0cSFEUh2J</a></p>
<p>开了一个星球，在里面讲解一些案例、知识、学习方法，肯定没法让大家称为顶尖程序员(我自己都不是)，只是希望用我的方法、知识、经验、案例作为你的垫脚石，帮助你快速、早日成为一个基本合格的程序员。</p>
<p>争取在星球内：</p>
<ul>
<li>养成基本动手能力</li>
<li>拥有起码的分析推理能力–按我接触的程序员，大多都是没有逻辑的</li>
<li>知识上教会你几个关键的知识点</li>
</ul>
<img src="/images/951413iMgBlog/image-20240324161113874.png" alt="image-20240324161113874" style="zoom:50%;" />
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2024/03/25/%E6%97%A0%E6%8B%9B%E8%83%9C%E6%9C%89%E6%8B%9B/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2024/03/25/%E6%97%A0%E6%8B%9B%E8%83%9C%E6%9C%89%E6%8B%9B/" class="post-title-link" itemprop="url">无招胜有招--一周年总结</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-03-25 17:30:03" itemprop="dateCreated datePublished" datetime="2024-03-25T17:30:03+08:00">2024-03-25</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/others/" itemprop="url" rel="index"><span itemprop="name">others</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="无招胜有招–一周年总结"><a href="#无招胜有招–一周年总结" class="headerlink" title="无招胜有招–一周年总结"></a>无招胜有招–一周年总结</h1><p>大家抱着美好和雄赳赳的目标来到<a target="_blank" rel="noopener" href="https://wx.zsxq.com/dweb2/index/group/15552551584552">这个知识星球</a>，开始的时候兴奋地以为找到了银弹(其实银弹是有的，在文章最后)，经过一段时间后大概率发现没什么变化，然后就回到了以前的老路子上，我觉得关键问题是你没获取到星球的精华，所以这篇我打算反复再唠叨一下</p>
<h2 id="知识效率-工程效率"><a href="#知识效率-工程效率" class="headerlink" title="知识效率 工程效率"></a><strong><a target="_blank" rel="noopener" href="https://t.zsxq.com/14IBWajEq">知识效率 工程效率</a></strong></h2><p>虽然我们现在通过这篇《<a target="_blank" rel="noopener" href="https://t.zsxq.com/14IBWajEq">知识效率 工程效率</a>》知道了两者的差别， 但是还是需要记住通过积累可以将我们的学习能力从工程效率升级到知识效率(厚积薄发)，大部分时候没有做到薄发，是因为你以为理解了、积累了实际没理解</p>
<h2 id="核心知识点"><a href="#核心知识点" class="headerlink" title="核心知识点"></a><strong>核心知识点</strong></h2><p>尽力寻找每个领域的核心知识点，核心知识点的定义就是通过一两个这样的知识点能撬动对整个领域的理解，也就是常说的<a target="_blank" rel="noopener" href="http://www.baidu.com/link?url=9Hv8LOY09wOqjLFX-UuX35AxJjTDjmkHcSPm3ReeTWO-4rH-46hmz6aR4b-WP7PwZHUGkxEBhWt1iqHkM8uM56Au6Ada4lg6angCByW3J-BLDkxE45Aq-QqOTWzRspa4">纲挈目张</a></p>
<p>比如网络领域里：一个网络包是怎么流转的+抓包。假如你理解<a href="https://plantegg.github.io/2019/05/15/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E7%BD%91%E7%BB%9C--%E4%B8%80%E4%B8%AA%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%97%85%E7%A8%8B/">网络包的流转后</a>再去看<a href="https://plantegg.github.io/2019/06/20/%E5%B0%B1%E6%98%AF%E8%A6%81%E4%BD%A0%E6%87%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1--lvs%E5%92%8C%E8%BD%AC%E5%8F%91%E6%A8%A1%E5%BC%8F/">LVS 负载均衡的原理</a>你就发现只需要看一次你就能很好掌握LVS各个负载均衡的本质，而在这之前你反复看反复忘。掌握了这个知识点基本就可以通关整个领域，剩下的只是无招胜有招碰到一个挨个积累的问题了。</p>
<p>比如CPU领域理解超线程+IPC+会用perf和内存延时，理解超线程的本质是为什么一个核能干两个核的工作(这和操作系统的分时多任务背后原理是想通的)，那是因为我们的程序没法吃满流水线(也就是没法用完一个核的计算能力，用IPC去衡量)，没吃满闲置的时候就可以虚拟给另外一个进程用，比如CPU 跑起来最高IPC都能到4，但是无论你找一个Java还是MySQL 去看他们的IPC基本都在1以内，纯计算场景的IPC会高一点，IPC 可以到4但只跑到1的话也就是只用满了25%的能力，那当然可以再虚出来一个超线程提高效率。IPC 之所以低就是因为内存延时大，这么多年CPU的处理能力一直按摩尔定律在提升但是内存延时没有怎么提升，导致基本上我们常见的业务场景(Nginx&#x2F;MySQL&#x2F;Redis 等)都是CPU在等从内存取数据(所以搞了L1、L2、L3一堆cache)。</p>
<p>发散一下或者说<strong>留个作业</strong>你去看看<a href="https://plantegg.github.io/2021/05/14/%E5%8D%81%E5%B9%B4%E5%90%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%98%E6%98%AF%E4%B8%8D%E6%95%A2%E6%8B%A5%E6%8A%B1NUMA/">NUMA 的原理或者说本质就是为了让CPU知道就近分配读取内存以提升效率</a>。</p>
<p>你看<strong>整本计算机组成原理+性能的本质都在这一个知识点的范围内进行延伸和突破</strong>。</p>
<p>如果你发现一个核心知识点也欢迎写成博客文章分享出来</p>
<h2 id="读日志、错误信息"><a href="#读日志、错误信息" class="headerlink" title="读日志、错误信息"></a><strong>读日志、错误信息</strong></h2><p>我的经验只是大概20%左右的程序员会去耐心读别人的日志、报错信息，大部分摊摊手求助、放弃了</p>
<p>日志是最好的学习机会，我知道别人的日志写得很烂，但是你要能耐心多琢磨一点就会比别人更专业一点</p>
<h2 id="对知识的可观测性"><a href="#对知识的可观测性" class="headerlink" title="对知识的可观测性"></a><strong>对知识的可观测性</strong></h2><p>抓包、perf的使用这些平时要多积累，这点没有捷径，一个好的工程师肯定有<a href="https://plantegg.github.io/2016/10/12/ss%E7%94%A8%E6%B3%95%E5%A4%A7%E5%85%A8/">一堆好的锤子、瑞士军刀、工具包</a>的。在你掌握了知识点后要转化为工作效率，就得多积累这些工具，很多次我们碰到一个好的问题没分析出来是因为我们这种没有门槛的积累不够导致放弃了</p>
<p>比如需要抓包确认下，不会，一看tcpdump 一堆参数头疼放弃；比如想要知道长连接还是短连接，或者自己设置的长连接有没有生效，不会用netstat -o 这个参数去确认等；比如要下载个源码自己make&#x2F;install 中间报了几个错误不仔细看放弃；</p>
<p>反过来回到我们所说的工程效率，就是靠这些工具帮你实现可视、可以触摸，网络之所以大多数同学在大学都学过但是最后基本学懂，就是因为这些网络的东东你只看理论很难立即，但是让你抓过一次包分析下就会恍然大悟——这就是关键门槛你能跨过去</p>
<h2 id="好习惯"><a href="#好习惯" class="headerlink" title="好习惯"></a><strong>好习惯</strong></h2><p>在星球里我更希望你带走一个好的习惯而不是一个具体知识点，虽然星球里的具体知识点、案例胜过很多教材，但他们总有过时、用不上的时候，唯有好的习惯可以跟随你，帮你实现无招胜有招</p>
<h3 id="记笔记"><a href="#记笔记" class="headerlink" title="记笔记"></a><strong>记笔记</strong></h3><p>放低身段，不要高估自己的能力(认为自己是知识效率)，放低后你要怎么做呢：记笔记、记笔记、记笔记</p>
<p>只要是你在学习就要或者看书、看资料的时候觉得自己有点通透了，赶紧记录下来，因为大概率一个星期你就忘了，半年你就完全不记得自己以前看过一次了，我好多次看到一篇好文章就感叹自己学到了，兴奋地拉到文章最后想去评论下，结果发现居然有了自己的评论在下面 :)</p>
<h3 id="动手"><a href="#动手" class="headerlink" title="动手"></a><strong>动手</strong></h3><p>动手，看到后理解了，也记了笔记，其实最好还是要自己去重现，记下自己看到的现象和理解，动手又会有一堆门槛，搭环境、客观则、怎么验证等等，这个时候我前面说的可观测性里面积累的一大堆工具可以让你如有神助、重现起来效率就是比别人高</p>
<h3 id="汇总输出"><a href="#汇总输出" class="headerlink" title="汇总输出"></a><strong>汇总输出</strong></h3><p>最后笔记记完还没完，笔记基本是零散的，你反复积累后到了一定的时机就是要把他们总结汇总成一篇完整度较高的博客文章，这里当然有自己的虚荣心在这里，但更多的是为了自己查询方便，有了新的理解或者使用姿势我经常更新补充10年前的博客文章，不会写一篇新的，这个补充知识让我的知识结构更完善，不是为了多发一篇博文，我现在解决问题、使用工具基本要靠翻自己的博客文章照着操作</p>
<h3 id="慢就是快、少就是多"><a href="#慢就是快、少就是多" class="headerlink" title="慢就是快、少就是多"></a><strong>慢就是快、少就是多</strong></h3><p>往往我们喜欢求快，以为自己一看就懂；求多以为自己越看的多越厉害</p>
<h3 id="不要等着时间流投喂"><a href="#不要等着时间流投喂" class="headerlink" title="不要等着时间流投喂"></a><strong>不要等着时间流投喂</strong></h3><p>看这篇置顶：<a target="_blank" rel="noopener" href="https://t.zsxq.com/14Yel6KBg">https://t.zsxq.com/14Yel6KBg</a></p>
<h2 id="纲举目张"><a href="#纲举目张" class="headerlink" title="纲举目张"></a><strong>纲举目张</strong></h2><p>对公司的业务、一个软件的运转流程都要尽量做到理解</p>
<p>比如学MySQL 要尽量知道从一条SQL 怎么进来，进行哪些处理后得到了查询结果；比如前面讲过的一个网络包是怎么到达对端的；比如你们公司的请求是怎么从客户端到达服务端(中间经过了LVS、Nginx吗)，服务端又是那些服务得依赖和调用，有没有Redis、MQ、Database，最后数据又是怎么返回的，我知道这在一个公司很难(屎山很复杂)，但目前没有更好的方法让你快速掌握并立足</p>
<p>为什么出现问题后总有一两个人很快能猜出来问题可能在哪个环节，这一部分是经验但更多的是对系统的了解，你都不知道有Redis存在一旦出错了你肯定猜不到Redis这里来</p>
<p>可以看看我之前说的实习生的故事，完全真实哈：</p>
<blockquote>
<p>讲一个我碰到的实习生的事情</p>
<p>北邮毕业直接后直接到我司实习</p>
<p>特点：英语好、动手能力强、爱琢磨，除了程序、电脑没有其它爱好 :)</p>
<p>实习期间因为英语好把我司文档很快就翻烂了，对产品、业务逻辑的理解基本是顶尖的</p>
<p>实习期间很快成为所有老员工的红人，都离不开他，搭环境、了解业务流程</p>
<p>因为别人的习惯都是盯着自己眼前的这一趴，只有他对业务非常熟悉</p>
<p>实习后很快就转正了，又3年后transfer 去了美国总部</p>
<p>连女朋友都是老员工给牵线的，最后领证一起去了美国。为啥老员工这么热情，是大家真心喜欢他 </p>
</blockquote>
<p>再看看张一鸣自述的第一年的工作：</p>
<p><img src="/images/951413iMgBlog/FuQNw04aH2PQwnApyAKY1dXRh-nt.png" alt="img"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h2><p>我前面所说的我也没做太好，希望大家能做得更好，我第一次感受无招胜有招就是<a href="https://plantegg.github.io/2022/01/01/%E4%B8%89%E4%B8%AA%E6%95%85%E4%BA%8B/">故事一里面</a>，到故事二过去差不多10年，这10年里我一直在琢磨怎么才能无招胜有招，也有在积累，但是花了10年肯定效率不算高，所以在星球里我希望通过我的经验帮你们缩短一些时间</p>
<p>上面讲再多如果你只是看看那根本还是没用，买再多的课也没用，关键是看触动后能否有点改变。你可以从里面试着挑几个你认为容易操作，比如记笔记、比如不要等着时间流投喂，或者有感触的试试先改变或者遵循下看看能不能获得一些变化进而形成正向循环</p>
<p>或者从评论里开始说说你星球这一年真正有哪些改变、学到了啥、你的感悟，不方便的也可以微信我私聊一下</p>
<p>这篇就当成整个星球学习的一个总结吧</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/19/">19</a><a class="extend next" rel="next" title="下一页" aria-label="下一页" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>

</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
