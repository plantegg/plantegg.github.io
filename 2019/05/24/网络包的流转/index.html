<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 8.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"plantegg.github.io","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta name="description" content="Linux Network Stack文章目标  从一个网络包进到网卡后续如何流转，进而了解中间有哪些关键参数可以控制他们，有什么工具能帮忙可以看到各个环节的一些指征，以及怎么调整他们。  接收流程接收流程大纲在开始收包之前，也就是OS启动的时候，Linux要做许多的准备工作：  创建ksoftirqd线程，为它设置好它自己的线程函数，用来处理软中断 协议栈注册，linux要实现许多协议，比如ar">
<meta property="og:type" content="article">
<meta property="og:title" content="Linux Network Stack">
<meta property="og:url" content="https://plantegg.github.io/2019/05/24/%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%B5%81%E8%BD%AC/index.html">
<meta property="og:site_name" content="plantegg">
<meta property="og:description" content="Linux Network Stack文章目标  从一个网络包进到网卡后续如何流转，进而了解中间有哪些关键参数可以控制他们，有什么工具能帮忙可以看到各个环节的一些指征，以及怎么调整他们。  接收流程接收流程大纲在开始收包之前，也就是OS启动的时候，Linux要做许多的准备工作：  创建ksoftirqd线程，为它设置好它自己的线程函数，用来处理软中断 协议栈注册，linux要实现许多协议，比如ar">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220725164331535.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211027133622981">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211027133758754">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/f2d5399f-4fba-4159-9ce4-aefa78132a43.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/321779bd-79f8-42ab-b17f-857243d3eb3f.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/1557292725626-2e4b452b-8a9e-4d9f-91a6-64357fbd4e0e.png">
<meta property="og:image" content="https://plantegg.github.io/images/oss/ddd50d2c70215d477d72734b0834410a.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/8edd2edf-5ae9-4f96-83fb-cef367697661.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/308138af-b3a6-4404-93eb-82dce612ba5b.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/ba2f1764fab3a7b3f485836e8e566ffb.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220406163641215.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/79e46270-de0d-48d5-99d8-90ced2964154.png">
<meta property="og:image" content="https://plantegg.github.io/images/oss/daf7318302c0e7f42fb506d6b47fdbd5.png">
<meta property="og:image" content="https://plantegg.github.io/images/oss/68dc89e050901cd2478a0636a5f0dcbe.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20210310144555255.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20211210160634705.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/0126bbb59ac317337ca963ef83817159.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-5685512.">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20210714204347862.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/9492686528d67d6f63bcf46fde1d3f58.png">
<meta property="og:image" content="https://plantegg.github.io/images/oss/e26ce9ed-4075-4837-8064-ea4d4aef09b8.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/51f13ecb5002f628fbe1900ab8b820aa.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/d0fb11d926f5f67357d98b69c23d86ae.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/61fd62cdf0dc0270ce108a4d43a14c85.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20210511114834433.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/aaf4ff8bbcc26e9e5efe48c984abe508.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/1557291324544-ca69d448-08e4-46c4-9c49-8cf516fc3eaa.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/955fc732d8620561a9ebce992b0129b1.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211027113522111">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211027113545882">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211027123524056">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-0054201.">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20211221105837677">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-8447312.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/640-20220328140221555.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20211116101345648.png">
<meta property="og:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20230726101807001.png">
<meta property="og:image" content="https://plantegg.github.io/images/oss/a067b484c593aa3a4b6a525d1f93506e.png">
<meta property="article:published_time" content="2019-05-24T09:30:03.000Z">
<meta property="article:modified_time" content="2025-11-16T11:58:49.560Z">
<meta property="article:author" content="twitter @plantegg">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="TCP">
<meta property="article:tag" content="network">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://plantegg.github.io/images/951413iMgBlog/image-20220725164331535.png">


<link rel="canonical" href="https://plantegg.github.io/2019/05/24/%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%B5%81%E8%BD%AC/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://plantegg.github.io/2019/05/24/%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%B5%81%E8%BD%AC/","path":"2019/05/24/网络包的流转/","title":"Linux Network Stack"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Linux Network Stack | plantegg</title>
  








  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">plantegg</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">java tcp mysql performance network docker Linux</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Linux-Network-Stack"><span class="nav-number">1.</span> <span class="nav-text">Linux Network Stack</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%96%87%E7%AB%A0%E7%9B%AE%E6%A0%87"><span class="nav-number">1.1.</span> <span class="nav-text">文章目标</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%8E%A5%E6%94%B6%E6%B5%81%E7%A8%8B"><span class="nav-number">1.2.</span> <span class="nav-text">接收流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8E%A5%E6%94%B6%E6%B5%81%E7%A8%8B%E5%A4%A7%E7%BA%B2"><span class="nav-number">1.2.1.</span> <span class="nav-text">接收流程大纲</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E6%8E%A5%E6%94%B6%E6%B5%81%E7%A8%8B"><span class="nav-number">1.2.2.</span> <span class="nav-text">详细接收流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B8%E5%9E%8B%E7%9A%84%E6%8E%A5%E6%94%B6%E5%A0%86%E6%A0%88"><span class="nav-number">1.2.3.</span> <span class="nav-text">典型的接收堆栈</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E5%9B%9B%E5%B1%82%E5%8D%8F%E8%AE%AE%E6%A0%88%E6%9D%A5%E7%9C%8B%E6%94%B6%E5%8C%85%E6%B5%81%E7%A8%8B"><span class="nav-number">1.2.4.</span> <span class="nav-text">从四层协议栈来看收包流程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DMA%E9%A9%B1%E5%8A%A8%E9%83%A8%E5%88%86%E6%B5%81%E7%A8%8B%E5%9B%BE"><span class="nav-number">1.2.5.</span> <span class="nav-text">DMA驱动部分流程图</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linux-network-queues-overview"><span class="nav-number">1.2.6.</span> <span class="nav-text">Linux network queues overview</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#buffer%E5%92%8C%E6%B5%81%E6%8E%A7"><span class="nav-number">1.2.7.</span> <span class="nav-text">buffer和流控</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E4%BC%A0%E9%80%92%E6%95%B0%E6%8D%AE%E5%8C%85%E5%88%B0%E5%86%85%E6%A0%B8%E7%9A%84%E6%B5%81%E7%A8%8B%E5%9B%BE%E5%8F%8A%E5%8F%82%E6%95%B0"><span class="nav-number">1.2.8.</span> <span class="nav-text">网卡传递数据包到内核的流程图及参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%91%E9%80%81%E6%B5%81%E7%A8%8B"><span class="nav-number">1.3.</span> <span class="nav-text">发送流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E5%8C%85%E5%8D%A1%E9%A1%BF"><span class="nav-number">1.3.1.</span> <span class="nav-text">发包卡顿</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%91%E5%8C%85%E5%8D%A1%E6%AD%BB"><span class="nav-number">1.3.2.</span> <span class="nav-text">发包卡死</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%8E%E5%9B%9B%E5%B1%82%E5%8D%8F%E8%AE%AE%E6%A0%88%E6%9D%A5%E7%9C%8B%E5%8F%91%E5%8C%85%E6%B5%81%E7%A8%8B"><span class="nav-number">1.3.3.</span> <span class="nav-text">从四层协议栈来看发包流程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%BD%AF%E4%B8%AD%E6%96%AD"><span class="nav-number">1.4.</span> <span class="nav-text">软中断</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%86%85%E6%A0%B8%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0"><span class="nav-number">1.5.</span> <span class="nav-text">内核相关参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Ring-Buffer"><span class="nav-number">1.5.1.</span> <span class="nav-text">Ring Buffer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B0%83%E6%95%B4-Ring-Buffer-%E9%98%9F%E5%88%97%E6%95%B0%E9%87%8F"><span class="nav-number">1.5.1.1.</span> <span class="nav-text">调整 Ring Buffer 队列数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E5%90%84%E7%A7%8D%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E6%9F%A5%E7%9C%8B"><span class="nav-number">1.5.1.2.</span> <span class="nav-text">网卡各种统计数据查看</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BD%91%E5%8D%A1%E8%BF%9B%E5%87%BA%E9%98%9F%E5%88%97%E5%A4%A7%E5%B0%8F%E8%B0%83%E6%95%B4"><span class="nav-number">1.5.1.3.</span> <span class="nav-text">网卡进出队列大小调整</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#txqueuelen"><span class="nav-number">1.5.2.</span> <span class="nav-text">txqueuelen</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Interrupt-Coalescence-IC-rx-usecs-tx-usecs-rx-frames-tx-frames-hardware-IRQ"><span class="nav-number">1.5.3.</span> <span class="nav-text">Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Ethtool-%E7%BB%91%E5%AE%9A%E7%AB%AF%E5%8F%A3"><span class="nav-number">1.5.4.</span> <span class="nav-text">Ethtool 绑定端口</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ntuple-filtering-for-steering-network-flows"><span class="nav-number">1.5.4.1.</span> <span class="nav-text">ntuple filtering for steering network flows</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BD%AF%E4%B8%AD%E6%96%AD%E5%90%88%E5%B9%B6-GRO"><span class="nav-number">1.5.5.</span> <span class="nav-text">软中断合并 GRO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ifconfig-%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87"><span class="nav-number">1.5.6.</span> <span class="nav-text">ifconfig 监控指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#overruns"><span class="nav-number">1.5.6.1.</span> <span class="nav-text">overruns</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7%E6%8C%87%E6%A0%87-proc-net-softnet-stat"><span class="nav-number">1.5.7.</span> <span class="nav-text">监控指标 &#x2F;proc&#x2F;net&#x2F;softnet_stat</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#net-core-netdev-budget"><span class="nav-number">1.5.7.1.</span> <span class="nav-text">net.core.netdev_budget</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#netdev-max-backlog"><span class="nav-number">1.5.7.2.</span> <span class="nav-text">netdev_max_backlog</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#softnet-stat"><span class="nav-number">1.5.7.3.</span> <span class="nav-text">softnet_stat</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#TCP%E5%8D%8F%E8%AE%AE%E6%A0%88Buffer"><span class="nav-number">1.5.8.</span> <span class="nav-text">TCP协议栈Buffer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8E%A5%E6%94%B6Buffer"><span class="nav-number">1.5.8.1.</span> <span class="nav-text">接收Buffer</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E4%BD%93%E7%AE%80%E7%95%A5%E6%8E%A5%E6%94%B6%E5%8C%85%E6%B5%81%E7%A8%8B"><span class="nav-number">1.6.</span> <span class="nav-text">总体简略接收包流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%BB%E4%BD%93%E7%AE%80%E7%95%A5%E5%8F%91%E9%80%81%E5%8C%85%E6%B5%81%E7%A8%8B"><span class="nav-number">1.7.</span> <span class="nav-text">总体简略发送包流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E5%8C%85%E6%B5%81%E8%BD%AC%E7%BB%93%E6%9E%84%E5%9B%BE"><span class="nav-number">1.8.</span> <span class="nav-text">网络包流转结构图</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B7%A8%E6%9C%BA%E5%99%A8%E7%BD%91%E7%BB%9CIO"><span class="nav-number">1.8.1.</span> <span class="nav-text">跨机器网络IO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lo-%E7%BD%91%E5%8D%A1"><span class="nav-number">1.8.2.</span> <span class="nav-text">lo 网卡</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unix-Domain-Socket%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="nav-number">1.8.3.</span> <span class="nav-text">Unix Domain Socket工作原理</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B"><span class="nav-number">1.9.</span> <span class="nav-text">案例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#snat-dnat-%E5%AE%BF%E4%B8%BB%E6%9C%BAport%E5%86%B2%E7%AA%81%EF%BC%8C%E4%B8%A2%E5%8C%85"><span class="nav-number">1.9.1.</span> <span class="nav-text">snat&#x2F;dnat 宿主机port冲突，丢包</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E5%99%A8-bridge-%E9%80%9A%E8%BF%87udp%E8%AE%BF%E9%97%AE%E5%AE%BF%E4%B8%BB%E6%9C%BA%E6%9C%8D%E5%8A%A1%E5%A4%B1%E8%B4%A5"><span class="nav-number">1.9.2.</span> <span class="nav-text">容器(bridge)通过udp访问宿主机服务失败</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.10.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">twitter @plantegg</p>
  <div class="site-description" itemprop="description">java mysql tcp performance network docker Linux</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">190</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">282</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://plantegg.github.io/2019/05/24/%E7%BD%91%E7%BB%9C%E5%8C%85%E7%9A%84%E6%B5%81%E8%BD%AC/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="twitter @plantegg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="plantegg">
      <meta itemprop="description" content="java mysql tcp performance network docker Linux">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="Linux Network Stack | plantegg">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Linux Network Stack
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2019-05-24 17:30:03" itemprop="dateCreated datePublished" datetime="2019-05-24T17:30:03+08:00">2019-05-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-11-16 19:58:49" itemprop="dateModified" datetime="2025-11-16T19:58:49+08:00">2025-11-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/network/" itemprop="url" rel="index"><span itemprop="name">network</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="Linux-Network-Stack"><a href="#Linux-Network-Stack" class="headerlink" title="Linux Network Stack"></a>Linux Network Stack</h1><h2 id="文章目标"><a href="#文章目标" class="headerlink" title="文章目标"></a>文章目标</h2><blockquote>
<p> 从一个网络包进到网卡后续如何流转，进而了解中间有哪些关键参数可以控制他们，有什么工具能帮忙可以看到各个环节的一些指征，以及怎么调整他们。</p>
</blockquote>
<h2 id="接收流程"><a href="#接收流程" class="headerlink" title="接收流程"></a>接收流程</h2><h3 id="接收流程大纲"><a href="#接收流程大纲" class="headerlink" title="接收流程大纲"></a>接收流程大纲</h3><p>在开始收包之前，也就是OS启动的时候，Linux要做许多的准备工作：</p>
<ol>
<li>创建ksoftirqd线程，为它设置好它自己的线程函数，用来处理软中断</li>
<li>协议栈注册，linux要实现许多协议，比如arp，icmp，ip，udp，tcp，每一个协议都会将自己的处理函数注册一下，方便包来了迅速找到对应的处理函数</li>
<li>网卡驱动初始化，每个驱动都有一个初始化函数，内核会让驱动也初始化一下。在这个初始化过程中，把自己的DMA准备好，把NAPI的poll函数地址告诉内核</li>
<li>启动网卡，分配RX，TX队列，注册中断对应的处理函数</li>
</ol>
<p>以上是内核准备收包之前的重要工作，当上面都ready之后，就可以打开硬中断，等待数据包的到来了。</p>
<p>当数据到来了以后，第一个迎接它的是网卡：</p>
<ol>
<li>网卡将数据帧DMA到内存的RingBuffer中，然后向CPU发起中断通知</li>
<li>CPU响应中断请求，调用网卡启动时注册的中断处理函数</li>
<li>中断处理函数几乎没干啥，就发起了软中断请求</li>
<li>内核线程ksoftirqd线程发现有软中断请求到来，先关闭硬中断</li>
<li>ksoftirqd线程开始调用驱动的poll函数收包</li>
<li>poll函数将收到的包送到协议栈注册的ip_rcv函数中</li>
<li>ip_rcv函数再讲包送到udp_rcv函数中（对于tcp包就送到tcp_rcv）</li>
</ol>
<h3 id="详细接收流程"><a href="#详细接收流程" class="headerlink" title="详细接收流程"></a>详细接收流程</h3><ol>
<li>网络包进到网卡，网卡驱动校验MAC，看是否扔掉，取决是否是混杂 promiscuous mode</li>
<li>网卡在启动时会申请一个接收ring buffer，其条目都会指向一个skb的内存。</li>
<li>DMA完成数据报文从网卡硬件到内存到拷贝</li>
<li>网卡发送一个中断通知CPU。</li>
<li>CPU执行网卡驱动注册的中断处理函数，中断处理函数只做一些必要的工作，如读取硬件状态等，并把当前该网卡挂在NAPI的链表中;</li>
<li><strong>Driver “触发” soft IRQ(NET_RX_SOFTIRQ (其实就是设置对应软中断的标志位)</strong> </li>
<li>CPU中断处理函数返回后，会检查是否有软中断需要执行。因第6步设置了NET_RX_SOFTIRQ，则执行报文接收软中断。</li>
<li>在NET_RX_SOFTIRQ软中断中，执行NAPI操作，回调第5步挂载的驱动poll函数。</li>
<li>驱动会对interface进行poll操作，检查网卡是否有接收完毕的数据报文。</li>
<li>将网卡中已经接收完毕的数据报文取出，继续在软中断进行后续处理。注：驱动对interface执行poll操作时，会尝试循环检查网卡是否有接收完毕的报文，直到系统设置的<strong>net.core.netdev_budget</strong>上限(默认300)，或者已经就绪报文。</li>
<li><strong>net_rx_action</strong></li>
<li>内核分配 sk_buff 内存</li>
<li>内核填充 metadata: 协议等，移除 ethernet 包头信息</li>
<li><strong>将skb 传送给内核协议栈 netif_receive_skb</strong></li>
<li><code>__netif_receive_skb_core</code>：将数据送到抓包点（tap）或协议层(i.e. tcpdump)&#x2F;&#x2F; 出抓包点：dev_queue_xmit_nit</li>
<li>进入到由 netdev_max_backlog 控制的qdisc</li>
<li>开始 <strong>ip_rcv</strong> 处理流程，主要处理ip协议包头相关信息</li>
<li><strong>调用内核 netfilter 框架(iptables PREROUTING)</strong></li>
<li>进入L4 protocol <strong>tcp_v4_rcv</strong></li>
<li>找到对应的socket</li>
<li>根据 tcp_rmem 进入接收缓冲队列</li>
<li>内核将数据送给接收的应用</li>
</ol>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh%EF%BC%9A">http://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh：</a></p>
<p><img src="/images/951413iMgBlog/image-20220725164331535.png" alt="image-20220725164331535"></p>
<p>TAP 处理点就是 <strong>tcpdump 抓包</strong>、流量过滤。</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/linux-net-stack-implementation-rx-zh">注意：<strong>netfilter 或 iptables 规则都是在软中断上下文中执行的</strong>， 数量很多或规则很复杂时会导致<strong>网络延迟</strong>。</a></p>
<blockquote>
<p> 软中断：可以把软中断系统想象成一系列<strong>内核线程</strong>（每个 CPU 一个），这些线程执行针对不同 事件注册的处理函数（handler）。如果你用过 <code>top</code> 命令，可能会注意到 <code>ksoftirqd/0</code> 这个内核线程，其表示这个软中断线程跑在 CPU 0 上。</p>
<p> 硬中断发生在哪一个核上，它发出的软中断就由哪个核来处理。可以通过加大网卡队列数，这样硬中断工作、软中断工作都会有更多的核心参与进来。</p>
<p> __napi_schedule干两件事情，一件事情是把struct napi_struct 挂到struct softnet_data 上，注意softnet_data是一个per cpu变量，换句话说，软中断结构是挂在触发硬中断的同一个CPU上；另一件事情是调用__raise_softirq_irqoff 把irq_stat的__softirq_pending 字段置位，irq_stat 也是个per cpu 变量，表示当前这个cpu上有软中断待处理。</p>
</blockquote>
<p><img src="/images/951413iMgBlog/640-20211027133622981" alt="Image"></p>
<p>从上图可以看到tcpdump在协议栈之前，也就是netfilter过滤规则对tcpdump无效，发包则是反过来：</p>
<p><img src="/images/951413iMgBlog/640-20211027133758754" alt="Image"></p>
<p><img src="/images/951413iMgBlog/f2d5399f-4fba-4159-9ce4-aefa78132a43.png" alt="img"></p>
<h3 id="典型的接收堆栈"><a href="#典型的接收堆栈" class="headerlink" title="典型的接收堆栈"></a>典型的接收堆栈</h3><p><img src="/images/951413iMgBlog/321779bd-79f8-42ab-b17f-857243d3eb3f.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/1557292725626-2e4b452b-8a9e-4d9f-91a6-64357fbd4e0e.png" alt="undefined"> </p>
<h3 id="从四层协议栈来看收包流程"><a href="#从四层协议栈来看收包流程" class="headerlink" title="从四层协议栈来看收包流程"></a>从四层协议栈来看收包流程</h3><p><img src="/images/oss/ddd50d2c70215d477d72734b0834410a.png" alt="image.png"></p>
<h3 id="DMA驱动部分流程图"><a href="#DMA驱动部分流程图" class="headerlink" title="DMA驱动部分流程图"></a>DMA驱动部分流程图</h3><p><a target="_blank" rel="noopener" href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/">DMA是一个硬件逻辑</a>，数据传输到系统物理内存的过程中，全程不需要CPU的干预，除了占用总线之外(期间CPU不能使用总线)，没有任何额外开销。</p>
<p><img src="/images/951413iMgBlog/8edd2edf-5ae9-4f96-83fb-cef367697661.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/308138af-b3a6-4404-93eb-82dce612ba5b.png" alt="img"></p>
<p><img src="/images/951413iMgBlog/ba2f1764fab3a7b3f485836e8e566ffb.png" alt="image.png"></p>
<ol>
<li>驱动在内存中分配一片缓冲区用来接收数据包，叫做sk_buffer;</li>
<li>将上述缓冲区的地址和大小（即接收描述符），加入到rx ring buffer。描述符中的缓冲区地址是DMA使用的物理地址;</li>
<li>驱动通知网卡有一个新的描述符;</li>
<li>网卡从rx ring buffer中取出描述符，从而获知缓冲区的地址和大小;</li>
<li>网卡收到新的数据包;</li>
<li>网卡将新数据包通过DMA直接写到sk_buffer中。</li>
</ol>
<h3 id="Linux-network-queues-overview"><a href="#Linux-network-queues-overview" class="headerlink" title="Linux network queues overview"></a>Linux network queues overview</h3><p><img src="/images/951413iMgBlog/image-20220406163641215.png" alt="linux network queues"></p>
<p>可以通过perf来监控包的堆栈：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">perf trace --no-syscalls --event &#x27;net:*&#x27; ping baidu.com -c1</span><br></pre></td></tr></table></figure>



<h3 id="buffer和流控"><a href="#buffer和流控" class="headerlink" title="buffer和流控"></a>buffer和流控</h3><p>影响发送的速度的几个buffer和queue，接收基本一样</p>
<p><img src="/images/951413iMgBlog/79e46270-de0d-48d5-99d8-90ced2964154.png" alt="img"></p>
<h3 id="网卡传递数据包到内核的流程图及参数"><a href="#网卡传递数据包到内核的流程图及参数" class="headerlink" title="网卡传递数据包到内核的流程图及参数"></a>网卡传递数据包到内核的流程图及参数</h3><p>软中断NET_TX_SOFTIRQ的处理函数为net_tx_action，NET_RX_SOFTIRQ的为net_rx_action</p>
<p><img src="/images/oss/daf7318302c0e7f42fb506d6b47fdbd5.png" alt="image.png"></p>
<p>在网络子系统初始化中为NET_RX_SOFTIRQ注册了处理函数net_rx_action。所以<code>net_rx_action</code>函数就会被执行到了。</p>
<p><img src="/images/oss/68dc89e050901cd2478a0636a5f0dcbe.png" alt="image.png"></p>
<p>这里需要注意一个细节，<strong>硬中断中设置软中断标记，和ksoftirq的判断是否有软中断到达，都是基于smp_processor_id()的。这意味着只要硬中断在哪个CPU上被响应，那么软中断也是在这个CPU上处理的</strong>。所以说，如果你发现你的Linux软中断CPU消耗都集中在一个核上的话，做法是要把调整硬中断的CPU亲和性，来将硬中断打散到不同的CPU核上去。</p>
<p>软中断（也就是 Linux 里的 ksoftirqd 进程）里收到数据包以后，发现是 tcp 的包的话就会执行到 tcp_v4_rcv 函数。如果是 ESTABLISHED 状态下的数据包，则最终会把数据拆出来放到对应 socket 的接收队列中。然后调用 sk_data_ready 来唤醒用户进程。</p>
<p><img src="/images/951413iMgBlog/image-20210310144555255.png"></p>
<p>对应的堆栈（本堆栈有问题，si%打满）：</p>
<p><img src="/images/951413iMgBlog/image-20211210160634705.png" alt="image-20211210160634705"></p>
<p><code>igb_fetch_rx_buffer</code>和<code>igb_is_non_eop</code>的作用就是把数据帧从RingBuffer上取下来。为什么需要两个函数呢？因为有可能帧要占多个RingBuffer，所以是在一个循环中获取的，直到帧尾部。获取下来的一个数据帧用一个sk_buff来表示。<strong>收取完数据以后，对其进行一些校验，然后开始设置sbk变量的timestamp, VLAN id, protocol等字段</strong>。接下来进入到napi_gro_receive中，里面还会调用关键的 netif_receive_skb， 在<code>netif_receive_skb</code>中，数据包将被送到协议栈中，上图中的tcp_v4_rcv就是其中之一（tcp协议）</p>
<h2 id="发送流程"><a href="#发送流程" class="headerlink" title="发送流程"></a>发送流程</h2><ol>
<li>应用调 sendmsg</li>
<li>数据拷贝到sk_write_queue上的最后一个skb中，如果该skb指向的数据区已经满了，则调用sk_stream_alloc_skb创建一个新的skb，并挂到这个sk_write_queue上</li>
<li>TCP 分片 skb_buff</li>
<li>根据 tcp_wmem 缓存需要发送的包</li>
<li>构造TCP包头(src&#x2F;dst port)</li>
<li>ipv4 调用 tcp_write_xmit 和 tcp_transmit_skb</li>
<li>ip_queue_xmit, 构建 ip 包头(获取目标ip和port，找路由)</li>
<li>进入 netfilter 流程 nf_hook()，iptables规则在这里生效</li>
<li>路由流程 POST_ROUTING，iptables 的nat和mangle表会在这里设置规则，对数据包进行一些修改</li>
<li>ip_output 分片</li>
<li>进入L2 dev_queue_xmit，tc 网络流控在这里</li>
<li>填入 txqueuelen 队列</li>
<li>进入发送 Ring Buffer tx</li>
<li>驱动触发软中断 soft IRQ (NET_TX_SOFTIRQ)</li>
</ol>
<p>在传输层的出口函数tcp_transmit_skb中，会对这个skb进行克隆（skb_clone），克隆得到的子skb和原先的父skb 指向共同的数据区。并且会把struct skb_shared_info的dataref 的计数加一。</p>
<p>传输层以下各层处理的skb 实际就是这个克隆出来的skb，而原先的skb保留在TCP连接的发送队列上。</p>
<p>克隆skb再经过协议栈层层处理后进入到驱动程序的RingBuffer 中。随后网卡驱动真正将数据发送出去，当发送完成时，由硬中断通知 CPU，然后由中断处理程序来清理 RingBuffer中指向的skb。注意，这里只释放了这个skb结构本身，而skb指向的数据区，由于dataref而不会被释放。要等到<strong>TCP层接收到ACK</strong>后，再释放父skb的同时，释放数据区。</p>
<p>比如ip_queue_xmit发现无法路由到目标地址，就会丢弃发送包，这里丢弃的是克隆包，原始包还在发送队列里，所以TCP层就会在<strong>定时器到期后进行重传</strong>。</p>
<h3 id="发包卡顿"><a href="#发包卡顿" class="headerlink" title="发包卡顿"></a>发包卡顿</h3><blockquote>
<p><a target="_blank" rel="noopener" href="https://github.com/torvalds/linux/commit/1f3279ae0c13">内核从3.16开始有这样一个机制</a>，在生成的一个新的重传报文前，先判断之前的报文的是否还在qdisc里面，如果在，就避免生成一个新的报文。</p>
<p>也就是对内核而言这个包发送了但是没收到ack，但实际这个包还在本机qdisc queue或者driver queue里，所以没必要重传</p>
</blockquote>
<p>对应的监控计数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#netstat -s |grep -i spur</span><br><span class="line">    TCPSpuriousRtxHostQueues: 4163</span><br></pre></td></tr></table></figure>

<p>这个发包过程在发送端实际抓不到这个包，因为还没有真正发送，而是在发送端的queue里排队，但是对上层应用来说包发完了（回包ack也不需要应用来感知），所以抓包看起来正常，就是应用感觉卡了（卡的原因还是包在发送端内核 queue 排队，一般是 pfifo_fast <a target="_blank" rel="noopener" href="https://lore.kernel.org/netdev/d102074f-7489-e35a-98cf-e2cad7efd8a2@netrounds.com/">bug </a> 和 <a target="_blank" rel="noopener" href="https://lore.kernel.org/all/20220528101628.120193-1-gjfang@linux.alibaba.com/">bug2</a>）</p>
<p>关于 <a target="_blank" rel="noopener" href="https://patchwork.ozlabs.org/project/netdev/patch/1366303971.3205.62.camel@edumazet-glaptop/">TCPSpuriousRtxHostQueues</a> 指标的作用：</p>
<blockquote>
<p>Host queues (Qdisc + NIC) can hold packets so long that TCP can<br>eventually retransmit a packet before the first transmit even left<br>the host.</p>
<p>Its not clear right now if we could avoid this in the first place :</p>
<ul>
<li><p>We could arm RTO timer not at the time we enqueue packets, but<br>at the time we TX complete them (tcp_wfree())</p>
</li>
<li><p>Cancel the sending of the new copy of the packet if prior one<br>is still in queue.</p>
</li>
</ul>
<p>This patch adds instrumentation so that we can at least see how<br>often this problem happens.</p>
<p>TCPSpuriousRtxHostQueues SNMP counter is incremented every time<br>we detect the fast clone is not yet freed in tcp_transmit_skb()</p>
</blockquote>
<h3 id="发包卡死"><a href="#发包卡死" class="headerlink" title="发包卡死"></a>发包卡死</h3><p>《<a href="https://plantegg.github.io/2022/10/10/Linux%20BUG%E5%86%85%E6%A0%B8%E5%AF%BC%E8%87%B4%E7%9A%84%20TCP%E8%BF%9E%E6%8E%A5%E5%8D%A1%E6%AD%BB/">一个Linux 内核 bug 导致的 TCP连接卡死</a>》</p>
<h3 id="从四层协议栈来看发包流程"><a href="#从四层协议栈来看发包流程" class="headerlink" title="从四层协议栈来看发包流程"></a>从四层协议栈来看发包流程</h3><p><img src="/images/951413iMgBlog/0126bbb59ac317337ca963ef83817159.png" alt="image.png"></p>
<p>发包流程对应源代码：</p>
<p><img src="/images/951413iMgBlog/640-5685512." alt="Image"></p>
<p><code>net.core.dev_weight</code> 用来调整 <code>__qdisc_run</code> 的循环处理权重，调大后也就是 <code>__netif_schedule</code> 更多的被调用执</p>
<p>另外发包默认是系统调用完成的(占用 sy cpu)，只有在包太多，为了避免系统调用长时间占用 CPU 导致应用层卡顿，这个时候内核给了发包时间一个quota(net.core.dev_weight 参数来控制)，用完后即使包没发送完也退出发包的系统调用，队列中未发送完的包留待 tx-softirq 来发送(这是占用 si cpu)</p>
<p>tcp在做tcp_sendmsg 的时候会将应用层msg做copy到内核层的skb，在调用网络层执行tcp_transmit_skb 会将这个 skb再次copy交给网络层，内核态的skb继续保留直到收到ack。</p>
<p>tcp_transmit_skb 还会设置tcp头，在skb中 tcp头、ip头内存都预留好了，只需要填写内容。</p>
<p>然后就是ip层，主要是分包、路由控制，然后就是netfilter(比如iptables规则匹配)。再然后进入neighbour(arp) , 获取mac后进入网络层</p>
<p>用 <code>sudo ifconfig eth0 txqueuelen **</code> 来控制qdisc 发送队列长度</p>
<p><img src="/images/951413iMgBlog/image-20210714204347862.png" alt="image-20210714204347862"></p>
<p>粗略汇总一下进出堆栈：</p>
<p><img src="/images/951413iMgBlog/9492686528d67d6f63bcf46fde1d3f58.png" alt="image.png"></p>
<p><a target="_blank" rel="noopener" href="http://docshare02.docshare.tips/files/21804/218043783.pdf">http://docshare02.docshare.tips/files/21804/218043783.pdf</a> 中也有描述：</p>
<p><img src="/images/oss/e26ce9ed-4075-4837-8064-ea4d4aef09b8.png" alt="img"></p>
<h2 id="软中断"><a href="#软中断" class="headerlink" title="软中断"></a>软中断</h2><p>一般net_rx 远大于net_tx, 如下所示，这是因为每个包发送完成后还需要清理回收内存(释放 skb)，这是通过硬中断触发 rx-softirq 来完成的，无论是收包、还是发送包完毕都是触发这个rx-softirq。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#cut /proc/softirqs -c 1-70</span><br><span class="line">                    CPU0       CPU1       CPU2       CPU3       CPU4</span><br><span class="line">          HI:          3          0          0          0          0</span><br><span class="line">       TIMER: 1616454419 1001992045 1013647869 1366481348  884639123</span><br><span class="line">      NET_TX:     168326    1717390       7000       6083       5748</span><br><span class="line">      NET_RX:  771543422  132283770   96912580   77533029   85143572</span><br></pre></td></tr></table></figure>

<p>发送的时候如果 net.core.dev_weight 配额够的话直接通过系统调用就将包发送完毕，不需要触发软中断</p>
<h2 id="内核相关参数"><a href="#内核相关参数" class="headerlink" title="内核相关参数"></a>内核相关参数</h2><h3 id="Ring-Buffer"><a href="#Ring-Buffer" class="headerlink" title="Ring Buffer"></a>Ring Buffer</h3><p>Ring Buffer位于NIC和IP层之间，是一个典型的FIFO（先进先出）环形队列。Ring Buffer没有包含数据本身，而是包含了指向sk_buff（socket kernel buffers）的描述符。<br>可以使用ethtool -g eth0查看当前Ring Buffer的设置：</p>
<pre><code>$sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:		256
RX Mini:	0
RX Jumbo:	0
TX:		256
Current hardware settings:
RX:		256
RX Mini:	0
RX Jumbo:	0
TX:		256
</code></pre>
<p>上面的例子是一个小规格的ECS，接收队列、传输队列都为256。</p>
<pre><code>$sudo ethtool -g eth0
Ring parameters for eth0:
Pre-set maximums:
RX:		4096
RX Mini:	0
RX Jumbo:	0
TX:		4096
Current hardware settings:
RX:		4096
RX Mini:	0
RX Jumbo:	0
TX:		512
</code></pre>
<p>这是一台物理机，接收队列为4096，传输队列为512。接收队列已经调到了最大，传输队列还可以调大。<strong>队列越大丢包的可能越小，但数据延迟会增加</strong></p>
<h4 id="调整-Ring-Buffer-队列数量"><a href="#调整-Ring-Buffer-队列数量" class="headerlink" title="调整 Ring Buffer 队列数量"></a>调整 Ring Buffer 队列数量</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ethtool -l eth0</span><br><span class="line">Channel parameters for eth0:</span><br><span class="line">Pre-set maximums:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       8</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          1</span><br><span class="line">Combined:       8</span><br><span class="line"></span><br><span class="line">sudo ethtool -L eth0 combined 8</span><br><span class="line">sudo ethtool -L eth0 rx 8</span><br></pre></td></tr></table></figure>

<p>网卡多队列就是指的有多个RingBuffer，每个RingBufffer可以由一个core来处理</p>
<p><img src="/images/951413iMgBlog/51f13ecb5002f628fbe1900ab8b820aa.png" alt="image.png"></p>
<h4 id="网卡各种统计数据查看"><a href="#网卡各种统计数据查看" class="headerlink" title="网卡各种统计数据查看"></a>网卡各种统计数据查看</h4><pre><code>ethtool -S eth0 | grep errors

ethtool -S eth0 | grep rx_ | grep errors //查看网卡是否丢包，一般是ring buffer太小

//监控
ethtool -S eth0 | grep -e &quot;err&quot; -e &quot;drop&quot; -e &quot;over&quot; -e &quot;miss&quot; -e &quot;timeout&quot; -e &quot;reset&quot; -e &quot;restar&quot; -e &quot;collis&quot; -e &quot;over&quot; | grep -v &quot;\: 0&quot;
</code></pre>
<h4 id="网卡进出队列大小调整"><a href="#网卡进出队列大小调整" class="headerlink" title="网卡进出队列大小调整"></a>网卡进出队列大小调整</h4><pre><code>//查看目前的进出队列大小
ethtool -g eth0
//修改进出队列
ethtool -G eth0 rx 8192 tx 8192
</code></pre>
<p>要注意如果设置的值超过了允许的最大值，用默认的最大值，一些ECS之类的虚拟机、容器就不允许修改这个值。</p>
<h3 id="txqueuelen"><a href="#txqueuelen" class="headerlink" title="txqueuelen"></a>txqueuelen</h3><p>ifconfig 看到的 txqueuelen 跟Ring Buffer是两个东西，IP协议下面就是 txqueuelen，txqueuelen下面才到Ring Buffer. </p>
<p>常用的tc qdisc、netfilter就是在txqueuelen这一环节。 qdisc 的队列长度是我们用 ifconfig 来看到的 txqueuelen</p>
<p>发送队列就是指的这个txqueuelen，和网卡关联着。 而每个Core接收队列由内核参数： net.core.netdev_max_backlog来设置	</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">//当前值通过ifconfig可以查看到，修改：</span><br><span class="line">		ifconfig eth0 txqueuelen 2000</span><br><span class="line">		//监控</span><br><span class="line">		ip -s link</span><br><span class="line">		ip -s link show enp2s0f0</span><br></pre></td></tr></table></figure>

<p>如果txqueuelen 太小导致数据包被丢弃的情况，这类问题可以通过下面这个命令来观察：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ip -s -s link ls dev eth0</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000</span><br><span class="line">    link/ether 00:16:3e:12:9b:c0 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    RX: bytes  packets  errors  dropped overrun mcast</span><br><span class="line">    13189414480980 22529315912 0       0       0       0</span><br><span class="line">    RX errors: length   crc     frame   fifo    missed</span><br><span class="line">               0        0       0       0       0</span><br><span class="line">    TX: bytes  packets  errors  dropped carrier collsns</span><br><span class="line">    15487121408466 12925733540 0       0       0       0</span><br><span class="line">    TX errors: aborted  fifo   window heartbeat transns</span><br><span class="line">               0        0       0       0       2</span><br></pre></td></tr></table></figure>

<p>如果观察到 dropped 这一项不为 0，那就有可能是 txqueuelen 太小导致的。当遇到这种情况时，你就需要增大该值了，比如增加 eth0 这个网络接口的 txqueuelen：</p>
<blockquote>
<p> $ ifconfig eth0 txqueuelen 2000</p>
</blockquote>
<h3 id="Interrupt-Coalescence-IC-rx-usecs-tx-usecs-rx-frames-tx-frames-hardware-IRQ"><a href="#Interrupt-Coalescence-IC-rx-usecs-tx-usecs-rx-frames-tx-frames-hardware-IRQ" class="headerlink" title="Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)"></a>Interrupt Coalescence (IC) - rx-usecs, tx-usecs, rx-frames, tx-frames (hardware IRQ)</h3><p>可以通过降低终端的频率，也就是合并<strong>硬中断</strong>来提升处理网络包的能力，当然这是以增大网络包的延迟为代价。</p>
<pre><code>	//检查
	$ethtool -c eth0
	Coalesce parameters for eth0:
Adaptive RX: off  TX: off
stats-block-usecs: 0
sample-interval: 0
pkt-rate-low: 0
pkt-rate-high: 0

rx-usecs: 1
rx-frames: 0
rx-usecs-irq: 0
rx-frames-irq: 0

tx-usecs: 0
tx-frames: 0
tx-usecs-irq: 0
tx-frames-irq: 256

rx-usecs-low: 0
rx-frame-low: 0
tx-usecs-low: 0
tx-frame-low: 0

rx-usecs-high: 0
rx-frame-high: 0
tx-usecs-high: 0
tx-frame-high: 0
	//修改, 
	ethtool -C eth0 rx-usecs value tx-usecs value
	//监控
	cat /proc/interrupts
</code></pre>
<p>我们来说一下上述结果的大致含义</p>
<ul>
<li><p>Adaptive RX: 自适应中断合并，网卡驱动自己判断啥时候该合并啥时候不合并</p>
</li>
<li><p>rx-usecs：当过这么长时间过后，一个RX interrupt就会被产生。How many usecs to delay an RX interrupt after a packet arrives.</p>
</li>
<li><p>rx-frames：当累计接收到这么多个帧后，一个RX interrupt就会被产生。Maximum number of data frames to receive before an RX interrupt.</p>
</li>
<li><p><code>rx-usecs-irq</code>: How many usecs to delay an RX interrupt while an interrupt is being serviced by the host.</p>
</li>
<li><p><code>rx-frames-irq</code>: Maximum number of data frames to receive before an RX interrupt is generated while the system is servicing an interrupt.</p>
</li>
</ul>
<h3 id="Ethtool-绑定端口"><a href="#Ethtool-绑定端口" class="headerlink" title="Ethtool 绑定端口"></a><a target="_blank" rel="noopener" href="https://colobu.com/2019/12/09/monitoring-tuning-linux-networking-stack-receiving-data/#%E8%B0%83%E6%95%B4%E7%A1%AC%E4%B8%AD%E6%96%AD%E4%BA%B2%E5%92%8C%E6%80%A7%EF%BC%88IRQ_affinities%EF%BC%89">Ethtool 绑定端口</a></h3><h4 id="ntuple-filtering-for-steering-network-flows"><a href="#ntuple-filtering-for-steering-network-flows" class="headerlink" title="ntuple filtering for steering network flows"></a>ntuple filtering for steering network flows</h4><p>一些网卡支持 “ntuple filtering” 特性。该特性允许用户（通过 ethtool ）指定一些参数来在硬件上过滤收到的包，然后将其直接放到特定的 RX queue。例如，用户可以指定到特定目端口的 TCP 包放到 RX queue 1。</p>
<p>Intel 的网卡上这个特性叫 Intel Ethernet Flow Director，其他厂商可能也有他们的名字，这些都是出于市场宣传原因，底层原理是类似的。</p>
<p>我们后面会看到，ntuple filtering 是一个叫 Accelerated Receive Flow Steering(aRFS) 功能的核心部分之一，后者使得 ntuple filtering 的使用更加方便。</p>
<p>这个特性适用的场景：最大化数据本地性（data locality），以增加 CPU 处理网络数据时的缓存命中率。例如，考虑运行在 80 口的 web 服务器：</p>
<ol>
<li>webserver 进程运行在 80 口，并绑定到 CPU 2</li>
<li>和某个 RX queue 关联的硬中断绑定到 CPU 2</li>
<li>目的端口是 80 的 TCP 流量通过 ntuple filtering 绑定到 CPU 2</li>
<li>接下来所有到 80 口的流量，从数据包进来到数据到达用户程序的整个过程，都由 CPU 2 处理</li>
<li>仔细监控系统的缓存命中率、网络栈的延迟等信息，以验证以上配置是否生效</li>
</ol>
<p>检查 ntuple filtering 特性是否打开：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -k eth0</span><br><span class="line">Offload parameters for eth0:</span><br><span class="line">...</span><br><span class="line">ntuple-filters: off</span><br><span class="line">receive-hashing: on</span><br></pre></td></tr></table></figure>

<p>可以看到，上面的 ntuple 是关闭的。</p>
<p>打开：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -K eth0 ntuple on</span><br></pre></td></tr></table></figure>

<p>打开 ntuple filtering 功能，并确认打开之后，可以用 <code>ethtool -u</code> 查看当前的 ntuple<br>rules：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -u eth0</span><br><span class="line">40 RX rings available</span><br><span class="line">Total 0 rules</span><br></pre></td></tr></table></figure>

<p>可以看到当前没有 rules。</p>
<p><a target="_blank" rel="noopener" href="https://colobu.com/2019/12/09/monitoring-tuning-linux-networking-stack-receiving-data/#%E8%B0%83%E6%95%B4_RX_queue_%E7%9A%84%E6%9D%83%E9%87%8D%EF%BC%88weight%EF%BC%89">我们来加一条：目的端口是 80 的放到 RX queue 2</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ethtool -U eth0 flow-type tcp4 dst-port 80 action 2</span><br><span class="line"></span><br><span class="line">删除</span><br><span class="line">ethtool -U eth0 delete 1023</span><br></pre></td></tr></table></figure>

<p>你也可以用 ntuple filtering 在硬件层面直接 drop 某些 flow 的包。当特定 IP 过来的流量太大时，这种功能可能会派上用场。更多关于 ntuple 的信息，参 考 ethtool man page。</p>
<h3 id="软中断合并-GRO"><a href="#软中断合并-GRO" class="headerlink" title="软中断合并 GRO"></a>软中断合并 GRO</h3><p>GRO和硬中断合并的思想很类似，不过阶段不同。硬中断合并是在中断发起之前，而GRO已经到了软中断上下文中了。</p>
<p>如果应用中是大文件的传输，大部分包都是一段数据，不用GRO的话，会每次都将一个小包传送到协议栈（IP接收函数、TCP接收）函数中进行处理。开启GRO的话，Linux就会智能进行包的合并，之后将一个大包传给协议处理函数。这样CPU的效率也是就提高了。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ethtool -k eth0 | grep generic-receive-offload</span><br><span class="line">generic-receive-offload: on</span><br></pre></td></tr></table></figure>

<p>如果你的网卡驱动没有打开GRO的话，可以通过如下方式打开。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># ethtool -K eth0 gro on</span><br></pre></td></tr></table></figure>

<p>这是收包，发包对应参数是GSO</p>
<h3 id="ifconfig-监控指标"><a href="#ifconfig-监控指标" class="headerlink" title="ifconfig 监控指标"></a>ifconfig 监控指标</h3><ul>
<li>RX overruns: overruns意味着数据包没到Ring Buffer就被网卡物理层给丢弃了，而CPU无法及时的处理中断是造成Ring Buffer满的原因之一，例如中断分配的不均匀。或者Ring Buffer太小导致的（很少见），overruns数量持续增加，建议增大Ring Buffer ，使用ethtool ‐G 进行设置。</li>
<li>RX dropped: 表示数据包已经进入了Ring Buffer，但是由于内存不够等系统原因，导致在拷贝到内存的过程中被丢弃。如下四种情况导致dropped：Softnet backlog full（pfmemalloc &amp;&amp; !skb_pfmemalloc_protocol(skb)–分配内存失败）；Bad &#x2F; Unintended VLAN tags；Unknown &#x2F; Unregistered protocols；IPv6 frames</li>
<li>RX errors：表示总的收包的错误数量，这包括 too-long-frames 错误，Ring Buffer 溢出错误，crc 校验错误，帧同步错误，fifo overruns 以及 missed pkg 等等。</li>
</ul>
<h4 id="overruns"><a href="#overruns" class="headerlink" title="overruns"></a>overruns</h4><p>当驱动处理速度跟不上网卡收包速度时，驱动来不及分配缓冲区，NIC接收到的数据包无法及时写到sk_buffer，就会产生堆积，当NIC内部缓冲区写满后，就会丢弃部分数据，引起丢包。这部分丢包为rx_fifo_errors，在 &#x2F;proc&#x2F;net&#x2F;dev中体现为fifo字段增长，在ifconfig中体现为overruns指标增长。</p>
<h3 id="监控指标-proc-net-softnet-stat"><a href="#监控指标-proc-net-softnet-stat" class="headerlink" title="监控指标 &#x2F;proc&#x2F;net&#x2F;softnet_stat"></a>监控指标 &#x2F;proc&#x2F;net&#x2F;softnet_stat</h3><p>Important details about <code>/proc/net/softnet_stat</code>:</p>
<ul>
<li>Each line of <code>/proc/net/softnet_stat</code> corresponds to a <code>struct softnet_data</code> structure, of which there is 1 per CPU.</li>
<li>The values are separated by a single space and are displayed in hexadecimal</li>
<li>The first value, <code>sd-&gt;processed</code>, is the number of network frames processed. This can be more than the total number of network frames received if you are using ethernet bonding. There are cases where the ethernet bonding driver will trigger network data to be re-processed, which would increment the <code>sd-&gt;processed</code> count more than once for the same packet.</li>
<li>The second value, <code>sd-&gt;dropped</code>, is the number of network frames dropped because there was no room on the processing queue. More on this later.</li>
<li>The third value, <code>sd-&gt;time_squeeze</code>, is (as we saw) the number of times the <code>net_rx_action</code> loop terminated because the budget was consumed or the time limit was reached, but more work could have been. Increasing the <code>budget</code> as explained earlier can help reduce this. <strong>time_squeeze 计数在内核中只有一个地方会更新</strong>（比如内核 5.10），如果看到监控中有 time_squeeze 升高， 那一定就是执行到了以上 budget 用完的逻辑</li>
<li>The next 5 values are always 0.</li>
<li>The ninth value, <code>sd-&gt;cpu_collision</code>, is a count of the number of times a collision occurred when trying to obtain a device lock when transmitting packets. This article is about receive, so this statistic will not be seen below.</li>
<li>The tenth value, <code>sd-&gt;received_rps</code>, is a count of the number of times this CPU has been woken up to process packets via an <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Inter-processor_interrupt">Inter-processor Interrupt</a></li>
<li>The last value, <code>flow_limit_count</code>, is a count of the number of times the flow limit has been reached. Flow limiting is an optional <a target="_blank" rel="noopener" href="https://lwn.net/Articles/362339">Receive Packet Steering</a> feature that will be examined shortly.</li>
</ul>
<p>对应的代码实现：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// https://github.com/torvalds/linux/blob/v5.10/net/core/net-procfs.c#L172</span><br><span class="line">static int softnet_seq_show(struct seq_file *seq, void *v) &#123;</span><br><span class="line">    struct softnet_data *sd = v;</span><br><span class="line"></span><br><span class="line">    seq_printf(seq,</span><br><span class="line">           &quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;,</span><br><span class="line">           sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,</span><br><span class="line">           0, 0, 0, 0, /* was fastroute */</span><br><span class="line">           0,    /* was cpu_collision */</span><br><span class="line">           sd-&gt;received_rps, flow_limit_count,</span><br><span class="line">           softnet_backlog_len(sd), (int)seq-&gt;index);</span><br><span class="line">    return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h4 id="net-core-netdev-budget"><a href="#net-core-netdev-budget" class="headerlink" title="net.core.netdev_budget"></a>net.core.netdev_budget</h4><p>一次软中断(ksoftirqd进程)能处理包的上限，有就多处理，处理到300个了一定要停下来让CPU能继续其它工作。单次poll 收包是所有注册到这个 CPU 的 NAPI 变量收包数量之和不能大于这个阈值。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sysctl net.core.netdev_budget //3.10 kernel默认300， The default value of the budget is 300. This will cause the SoftIRQ process to drain 300 messages from the NIC before getting off the CPU</span><br></pre></td></tr></table></figure>

<p>如果 &#x2F;proc&#x2F;net&#x2F;softnet_stat <strong>第三列</strong>一直在增加的话需要，表示SoftIRQ 获取的CPU时间太短，来不及处理足够多的网络包，那么需要增大这个值，<strong>当这个值太大的话有可能导致包到了内核但是应用（userspace）抢不到时间片来读取这些packet。</strong></p>
<p>增大和查看 net.core.netdev_budget	</p>
<blockquote>
<p>sysctl -a | grep net.core.netdev_budget<br>sysctl -w net.core.netdev_budget&#x3D;400 &#x2F;&#x2F;临时性增大</p>
</blockquote>
<p>早期的时候网卡一般是10Mb的，现在基本都是10Gb的了，还是每一次软中断、上下文切换只处理一个包的话代价太大，需要改进性能。于是引入的NAPI，一次软中断会poll很多packet</p>
<p><img src="/images/951413iMgBlog/d0fb11d926f5f67357d98b69c23d86ae.png" alt="image.png"></p>
<p><a target="_blank" rel="noopener" href="https://github.blog/2019-11-21-debugging-network-stalls-on-kubernetes/">来源</a> This is much faster, but brings up another problem. What happens if we have so many packets to process that we spend all our time processing packets from the NIC, but we never have time to let the userspace processes actually drain those queues (read from TCP connections, etc.)? Eventually the queues would fill up, and we’d start dropping packets. To try and make this fair, the kernel limits the amount of packets processed in a given softirq context to a certain budget. Once this budget is exceeded, it wakes up a separate thread called <code>ksoftirqd</code> (you’ll see one of these in <code>ps</code> for each core) which processes these softirqs outside of the normal syscall&#x2F;interrupt path. This thread is scheduled using the standard process scheduler, which already tries to be fair.</p>
<p>于是在Poll很多packet的时候有可能网卡队列一直都有包，那么导致这个Poll动作无法结束，造成应用一直在卡住状态，于是可以通过netdev_max_backlog来设置Poll多少Packet后停止Poll以响应用户请求。</p>
<p><img src="/images/951413iMgBlog/61fd62cdf0dc0270ce108a4d43a14c85.png" alt="image.png"></p>
<p>一旦出现slow syscall（如上图黄色部分慢）就会导致packet处理被延迟。</p>
<p>发送包的时候系统调用循环发包，占用sy，只有当发包系统quota用完的时候，循环退出，剩下的包通过触发软中断的形式继续发送，此时占用si</p>
<h4 id="netdev-max-backlog"><a href="#netdev-max-backlog" class="headerlink" title="netdev_max_backlog"></a>netdev_max_backlog</h4><p>The netdev_max_backlog is a queue within the Linux kernel where traffic is stored after reception from the NIC, but before processing by the protocol stacks (IP, TCP, etc). There is one backlog queue per CPU core. </p>
<p>如果 &#x2F;proc&#x2F;net&#x2F;softnet_stat 第二列一直在增加的话表示netdev backlog queue overflows. 需要增大 netdev_max_backlog</p>
<p>增大和查看 netdev_max_backlog：<br>		sysctl -a |grep netdev_max_backlog<br>		sysctl -w net.core.netdev_max_backlog&#x3D;1024 &#x2F;&#x2F;临时性增大</p>
<p>netdev_max_backlog(接收)和txqueuelen(发送)相对应 </p>
<h4 id="softnet-stat"><a href="#softnet-stat" class="headerlink" title="softnet_stat"></a>softnet_stat</h4><p>关于<code>/proc/net/softnet_stat</code> 的重要细节:</p>
<ol>
<li>每一行代表一个 <code>struct softnet_data</code> 变量。因为每个 CPU core 都有一个该变量，所以每行 其实代表一个 CPU core</li>
<li>每列用空格隔开，数值用 16 进制表示</li>
<li>第一列 <code>sd-&gt;processed</code>，是处理的网络帧的数量。如果你使用了 ethernet bonding， 那这个值会大于总的网络帧的数量，因为 ethernet bonding 驱动有时会触发网络数据被 重新处理（re-processed）</li>
<li>第二列，<code>sd-&gt;dropped</code>，是因为处理不过来而 drop 的网络帧数量。后面会展开这一话题</li>
<li>第三列，<code>sd-&gt;time_squeeze</code>，前面介绍过了，由于 budget 或 time limit 用完而退出 <code>net_rx_action</code> 循环的次数</li>
<li>接下来的 5 列全是 0</li>
<li>第九列，<code>sd-&gt;cpu_collision</code>，是为了发送包而获取锁的时候有冲突的次数</li>
<li>第十列，<code>sd-&gt;received_rps</code>，是这个 CPU 被其他 CPU 唤醒去收包的次数</li>
<li>最后一列，<code>flow_limit_count</code>，是达到 flow limit 的次数。flow limit 是 RPS 的特性， 后面会稍微介绍一下</li>
</ol>
<h3 id="TCP协议栈Buffer"><a href="#TCP协议栈Buffer" class="headerlink" title="TCP协议栈Buffer"></a>TCP协议栈Buffer</h3><pre><code>	sysctl -a | grep net.ipv4.tcp_rmem   // receive
	sysctl -a | grep net.ipv4.tcp_wmem   // send
	//监控
	cat /proc/net/sockstat
</code></pre>
<h4 id="接收Buffer"><a href="#接收Buffer" class="headerlink" title="接收Buffer"></a>接收Buffer</h4><pre><code>$netstat -sn | egrep &quot;prune|collap&quot;; sleep 30; netstat -sn | egrep &quot;prune|collap&quot;
17671 packets pruned from receive queue because of socket buffer overrun
18671 packets pruned from receive queue because of socket buffer overrun
</code></pre>
<p>如果 “pruning” 一直在增加很有可能是程序中调用了 setsockopt(SO_RCVBUF) 导致内核关闭了动态调整功能，或者压力大，缓存不够了。具体Case：<a target="_blank" rel="noopener" href="https://blog.cloudflare.com/the-story-of-one-latency-spike/">https://blog.cloudflare.com/the-story-of-one-latency-spike/</a></p>
<p>nstat也可以看到比较多的数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$nstat -z |grep -i drop</span><br><span class="line">TcpExtLockDroppedIcmps          0                  0.0</span><br><span class="line">TcpExtListenDrops               0                  0.0</span><br><span class="line">TcpExtTCPBacklogDrop            0                  0.0</span><br><span class="line">TcpExtPFMemallocDrop            0                  0.0</span><br><span class="line">TcpExtTCPMinTTLDrop             0                  0.0</span><br><span class="line">TcpExtTCPDeferAcceptDrop        0                  0.0</span><br><span class="line">TcpExtTCPReqQFullDrop           0                  0.0</span><br><span class="line">TcpExtTCPOFODrop                0                  0.0</span><br><span class="line">TcpExtTCPZeroWindowDrop         0                  0.0</span><br><span class="line">TcpExtTCPRcvQDrop               0                  0.0</span><br></pre></td></tr></table></figure>



<h2 id="总体简略接收包流程"><a href="#总体简略接收包流程" class="headerlink" title="总体简略接收包流程"></a>总体简略接收包流程</h2><p><img src="/images/951413iMgBlog/image-20210511114834433.png" alt="image-20210511114834433"></p>
<p>带参数版收包流程：</p>
<p><img src="/images/951413iMgBlog/aaf4ff8bbcc26e9e5efe48c984abe508.png" alt="image.png"></p>
<h2 id="总体简略发送包流程"><a href="#总体简略发送包流程" class="headerlink" title="总体简略发送包流程"></a>总体简略发送包流程</h2><p><img src="/images/951413iMgBlog/1557291324544-ca69d448-08e4-46c4-9c49-8cf516fc3eaa.png"></p>
<p>带参数版发包流程：</p>
<p><img src="/images/951413iMgBlog/955fc732d8620561a9ebce992b0129b1.png" alt="image.png"></p>
<h2 id="网络包流转结构图"><a href="#网络包流转结构图" class="headerlink" title="网络包流转结构图"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247485270&idx=1&sn=503534e9f0560bfcfbd4539e028e0d57&scene=21#wechat_redirect">网络包流转结构图</a></h2><h3 id="跨机器网络IO"><a href="#跨机器网络IO" class="headerlink" title="跨机器网络IO"></a>跨机器网络IO</h3><p><img src="/images/951413iMgBlog/640-20211027113522111" alt="Image"></p>
<h3 id="lo-网卡"><a href="#lo-网卡" class="headerlink" title="lo 网卡"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247485270&idx=1&sn=503534e9f0560bfcfbd4539e028e0d57&chksm=a6e3066d91948f7b4b56dc85cf12e7656cb8ac8c0bfe737df1ee400fc45b812476e1cb9bff1a&scene=178&cur_album_id=1532487451997454337#rd">lo 网卡</a></h3><p>127.0.0.1(lo)本机网络 IO ，无需走到物理网卡，也不用进入RingBuffer驱动队列，但是还是要走内核协议栈，直接把 skb 传给接收协议栈（经过软中断）</p>
<p><img src="/images/951413iMgBlog/640-20211027113545882" alt="Image"></p>
<p>总的来说，本机网络 IO 和跨机 IO 比较起来，确实是节约了一些开销。发送数据不需要进 RingBuffer 的驱动队列，直接把 skb 传给接收协议栈（经过软中断）。但是在内核其它组件上，可是一点都没少，系统调用、协议栈（传输层、网络层等）、网络设备子系统、邻居子系统整个走了一个遍。连“驱动”程序都走了（虽然对于回环设备来说只是一个纯软件的虚拟出来的东东）。所以即使是本机网络 IO，也别误以为没啥开销，实际本机访问自己的eth0 ip也是走的lo网卡和访问127.0.0.1是一样的，测试用ab分别走127.0.0.1和eth0压nginx，在nginx进程跑满，ab还没满两者的nginx单核都是7万TPS左右，跨主机压nginx的单核也是7万左右（要调多ab的并发数）。</p>
<p>如果是同一台宿主机走虚拟bridge通信的话（同一宿主机下的不容docker容器通信）：</p>
<p><img src="/images/951413iMgBlog/640-20211027123524056" alt="Image"></p>
<table>
<thead>
<tr>
<th></th>
<th>ab  压 nginx单核（intel 8163 绑核）</th>
</tr>
</thead>
<tbody><tr>
<td>127.0.0.1</td>
<td>Requests per second:    69498.96 [#&#x2F;sec] (mean)<br/>Time per request:       0.086 [ms] (mean)</td>
</tr>
<tr>
<td>Eth0</td>
<td>Requests per second:    70261.93 [#&#x2F;sec] (mean)<br/>Time per request:       0.085 [ms] (mean)</td>
</tr>
<tr>
<td>跨主机压</td>
<td>Requests per second:    70119.05 [#&#x2F;sec] (mean)<br/>Time per request:       0.143 [ms] (mean)</td>
</tr>
</tbody></table>
<p>ab不支持unix domain socket，如果增加ab和nginx之间的时延，QPS急剧下降，但是增加ab的并发数完全可以把QPS拉回去。</p>
<h3 id="Unix-Domain-Socket工作原理"><a href="#Unix-Domain-Socket工作原理" class="headerlink" title="Unix Domain Socket工作原理"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/fHzKYlW0WMhP2jxh2H_59A">Unix Domain Socket工作原理</a></h3><p>接收connect 请求的时候，会申请一个新 socket 给 server 端将来使用，和自己的 socket 建立好连接关系以后，就放到服务器正在监听的 socket 的接收队列中。这个时候，服务器端通过 accept 就能获取到和客户端配好对的新 socket 了。</p>
<p><img src="/images/951413iMgBlog/640-0054201." alt="Image"></p>
<p>主要的连接操作都是在这个函数中完成的。和我们平常所见的 TCP 连接建立过程，这个连接过程简直是太简单了。没有三次握手，也没有全连接队列、半连接队列，更没有啥超时重传。</p>
<p>直接就是将两个 socket 结构体中的指针互相指向对方就行了。就是 unix_peer(newsk) &#x3D; sk 和 unix_peer(sk) &#x3D; newsk 这两句。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//file: net/unix/af_unix.c</span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">unix_stream_connect</span><span class="params">(<span class="keyword">struct</span> socket *sock, <span class="keyword">struct</span> sockaddr *uaddr,</span></span><br><span class="line"><span class="params">          <span class="type">int</span> addr_len, <span class="type">int</span> flags)</span></span><br><span class="line">&#123;</span><br><span class="line"> <span class="class"><span class="keyword">struct</span> <span class="title">sockaddr_un</span> *<span class="title">sunaddr</span> =</span> (<span class="keyword">struct</span> sockaddr_un *)uaddr;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 1. 为服务器侧申请一个新的 socket 对象</span></span><br><span class="line"> newsk = unix_create1(sock_net(sk), <span class="literal">NULL</span>);</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 2. 申请一个 skb，并关联上 newsk</span></span><br><span class="line"> skb = sock_wmalloc(newsk, <span class="number">1</span>, <span class="number">0</span>, GFP_KERNEL);</span><br><span class="line"> ...</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 3. 建立两个 sock 对象之间的连接</span></span><br><span class="line"> unix_peer(newsk) = sk;</span><br><span class="line"> newsk-&gt;sk_state  = TCP_ESTABLISHED;</span><br><span class="line"> newsk-&gt;sk_type  = sk-&gt;sk_type;</span><br><span class="line"> ...</span><br><span class="line"> sk-&gt;sk_state = TCP_ESTABLISHED;</span><br><span class="line"> unix_peer(sk) = newsk;</span><br><span class="line"></span><br><span class="line"> <span class="comment">// 4. 把连接中的一头（新 socket）放到服务器接收队列中</span></span><br><span class="line"> __skb_queue_tail(&amp;other-&gt;sk_receive_queue, skb);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//file: net/unix/af_unix.c</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> unix_peer(sk) (unix_sk(sk)-&gt;peer)</span></span><br></pre></td></tr></table></figure>

<p>收发包过程和复杂的 TCP 发送接收过程相比，这里的发送逻辑简单简单到令人发指。申请一块内存（skb），把数据拷贝进去。根据 socket 对象找到另一端，<strong>直接把 skb 给放到对端的接收队列里了</strong></p>
<p><img src="/images/951413iMgBlog/640-20211221105837677" alt="Image"></p>
<p>Unix Domain Socket和127.0.0.1通信相比，如果包的大小是1K以内，那么性能会有一倍以上的提升，包变大后性能的提升相对会小一些。</p>
<p><img src="/images/951413iMgBlog/640-8447312.png" alt="Image"></p>
<p><img src="/images/951413iMgBlog/640-20220328140221555.png" alt="Image"></p>
<p>再来一个<a target="_blank" rel="noopener" href="https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg">整体流转矢量图</a>:</p>
<p><img src="/images/951413iMgBlog/image-20211116101345648.png" alt="image-20211116101345648"></p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><h3 id="snat-dnat-宿主机port冲突，丢包"><a href="#snat-dnat-宿主机port冲突，丢包" class="headerlink" title="snat&#x2F;dnat 宿主机port冲突，丢包"></a>snat&#x2F;dnat 宿主机port冲突，丢包</h3><p><img src="/images/951413iMgBlog/image-20230726101807001.png" alt="image-20230726101807001"></p>
<ol>
<li><p>snat 就是要把 192.168.1.10和192.168.1.11的两个连接替换成宿主机的ip:port</p>
</li>
<li><p>主要是在宿主机找可用port分别给这两个连接用</p>
</li>
<li><p>找可用port分两步</p>
<ul>
<li><pre><code>找到可用port
</code></pre>
</li>
<li><pre><code>将可用port写到数据库，后面做连接追踪用(conntrack)
</code></pre>
</li>
</ul>
</li>
<li><p>上述两步不是事务，可能两个连接同时找到一个相同的可用port，但是只有第一个能写入成功，第二个fail，fail后这个包被扔掉</p>
</li>
<li><p>1秒钟后被扔掉的包重传，后续正常</p>
</li>
</ol>
<p>症状：</p>
<ul>
<li><p>问题发生概率不高，跟压力没有关系，跟容器也没有关系，只要有snat&#x2F;dnat和并发就会发生，只发生在创建连接的第一个syn包</p>
</li>
<li><p>可以通过conntrack工具来检查fail的数量</p>
</li>
<li><p>实际影响只是请求偶尔被拉长了1秒或者3秒</p>
</li>
<li><p>snat规则创建的时候增加参数：NF_NAT_RANGE_PROTO_RANDOM_FULLY 来将冲突降低几个数量级—-可以认为修复了这个问题</p>
<pre><code>  sudo conntrack -L -d ip-addr
</code></pre>
</li>
</ul>
<p>来自：<a target="_blank" rel="noopener" href="https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02">https://tech.xing.com/a-reason-for-unexplained-connection-timeouts-on-kubernetes-docker-abd041cf7e02</a></p>
<h3 id="容器-bridge-通过udp访问宿主机服务失败"><a href="#容器-bridge-通过udp访问宿主机服务失败" class="headerlink" title="容器(bridge)通过udp访问宿主机服务失败"></a>容器(bridge)通过udp访问宿主机服务失败</h3><p><img src="/images/oss/a067b484c593aa3a4b6a525d1f93506e.png" alt="image.png"></p>
<p>这个案例主要是讲述回包的逻辑，如果是tcp，那么用dest ip当自己的source ip，如果是UDP，无连接状态信息，那么会根据route来选择一块网卡(上面的IP) 来当source ip</p>
<p>来自：<a target="_blank" rel="noopener" href="http://cizixs.com/2017/08/21/docker-udp-issue">http://cizixs.com/2017/08/21/docker-udp-issue</a><br>	 <a target="_blank" rel="noopener" href="https://github.com/moby/moby/issues/15127">https://github.com/moby/moby/issues/15127</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p><a target="_blank" rel="noopener" href="http://blog.hyfather.com/blog/2013/03/04/ifconfig/">The Missing Man Page for ifconfig–关于ifconfig的种种解释</a></p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1400834?s=original-sharing">Linux数据报文的来龙去脉</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/leandromoreira/linux-network-performance-parameters">linux-network-performance-parameters</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/fczjuever/archive/2013/04/17/3026694.html">Linux之TCPIP内核参数优化</a></p>
<p><a target="_blank" rel="noopener" href="https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf">https://access.redhat.com/sites/default/files/attachments/20150325_network_performance_tuning.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/">Linux 网络协议栈收消息过程-Ring Buffer</a> : 支持 RSS 的网卡内部会有多个 Ring Buffer，NIC 收到 Frame 的时候能通过 Hash Function 来决定 Frame 该放在哪个 Ring Buffer 上，触发的 IRQ 也可以通过操作系统或者手动配置 IRQ affinity 将 IRQ 分配到多个 CPU 上。这样 IRQ 能被不同的 CPU 处理，从而做到 Ring Buffer 上的数据也能被不同的 CPU 处理，从而提高数据的并行处理能力。</p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/tuning-stack-tx-zh/">Linux 网络栈监控和调优：发送数据</a></p>
<p><a target="_blank" rel="noopener" href="http://arthurchiao.art/blog/tuning-stack-rx-zh/">Linux 网络栈监控和调优：接收数据（2016）</a> <a target="_blank" rel="noopener" href="https://blog.packagecloud.io/monitoring-tuning-linux-networking-stack-receiving-data/">英文版</a></p>
<p>收到包后内核层面的处理：<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5Njg5NDgwNA==&mid=2247484058&idx=1&sn=a2621bc27c74b313528eefbc81ee8c0f&scene=21#wechat_redirect">从网卡注册软中断处理函数到收包逻辑</a></p>
<p>收到包后应用和协议层面的处理：图解 | 深入理解高性能网络开发路上的绊脚石 - 同步阻塞网络 IO<a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/cIcw0S-Q8pBl1-WYN0UwnA">https://mp.weixin.qq.com/s/cIcw0S-Q8pBl1-WYN0UwnA</a> 当软中断上收到数据包时会通过调用 sk_data_ready 函数指针（实际被设置成了 sock_def_readable()） 来唤醒在 sock 上等待的进程</p>
<p><a target="_blank" rel="noopener" href="http://docshare02.docshare.tips/files/21804/218043783.pdf">http://docshare02.docshare.tips/files/21804/218043783.pdf</a></p>
<p><a target="_blank" rel="noopener" href="https://wiki.linuxfoundation.org/networking/kernel_flow">https://wiki.linuxfoundation.org/networking/kernel_flow</a></p>
<p><a target="_blank" rel="noopener" href="https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Netfilter-packet-flow.svg/2000px-Netfilter-packet-flow.svg.png">https://upload.wikimedia.org/wikipedia/commons/thumb/3/37/Netfilter-packet-flow.svg/2000px-Netfilter-packet-flow.svg.png</a></p>
<p><a target="_blank" rel="noopener" href="https://wiki.nix-pro.com/view/Packet_journey_through_Linux_kernel">https://wiki.nix-pro.com/view/Packet_journey_through_Linux_kernel</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/">https://blog.packagecloud.io/eng/2017/02/06/monitoring-tuning-linux-networking-stack-sending-data/</a></p>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&mid=2651747704&idx=3&sn=cd76ad912729a125fd56710cb42792ba&chksm=bd12ac358a6525235f51e3937d99ea113ed45542c51bc58bb9588fa1198f34d95b7d13ae1ae2&scene=21#wechat_redirect">美团redis丢包，调整多队列，绑核，软中断绑定node0</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Linux/" rel="tag"># Linux</a>
              <a href="/tags/TCP/" rel="tag"># TCP</a>
              <a href="/tags/network/" rel="tag"># network</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2019/05/16/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%B8%8D%E9%80%9A%E6%98%AF%E4%B8%AA%E5%A4%A7%E9%97%AE%E9%A2%98--%E5%8D%8A%E5%A4%9C%E9%B8%A1%E5%8F%AB/" rel="prev" title="网络通不通是个大问题–半夜鸡叫">
                  <i class="fa fa-angle-left"></i> 网络通不通是个大问题–半夜鸡叫
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/05/26/MySQL%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB%E7%9A%84%E4%B8%89%E9%A9%BE%E9%A9%AC%E8%BD%A6/" rel="next" title="MySQL知识体系的三驾马车">
                  MySQL知识体系的三驾马车 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">twitter @plantegg</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
